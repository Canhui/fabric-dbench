## Chapter 11

训练深层的神经网络

Page 239





## Chapter 19


## Vanishing/Exploding Gradients

#### Vanishing

#### Exploding

RNN Exploding

https://blog.csdn.net/u011734144/article/details/80165007





#### Solutions to Vanishing/Exploding Problem

Method1: Batch Normalization (what is a batch?)

Method2: Gradient Clipping (Mainly used in RNN): it will not change the original direction


Application: Training is slow, we need faster optimizers

Application: Keras Learning Rate Scheduler


Method3: Learning rate -- Dropout





#### Implementation

Java Callback()

Blog: https://www.bbsmax.com/A/l1dyxZox5e/

Java Callback() and Socket



#### Distributed Training

PS vs. MPI allreduce

batch or minibatch?




#### Machine Learning

Forward and Backward

How 'gradients' in machine learning is computed?






