<!DOCTYPE html>
<!-- saved from url=(0091)https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html -->
<html class=" js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/hands-on-machine-learning/9781492032632/part01.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="4626953" data-user-uuid="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79" data-username="17481074" data-account-type="B2B" data-activated-trial-date="" data-archive="9781492032632" data-publishers="O&#39;Reilly Media, Inc." data-htmlfile-name="part01.html" data-epub-title="Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781492032632"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><script type="text/javascript" async="" src="./Chapter16_files/cool-2.1.15.min.js.download"></script><script type="text/javascript" src="./Chapter16_files/510f1a6865"></script><script id="twitter-wjs" src="./Chapter16_files/widgets.js.download"></script><script src="./Chapter16_files/nr-1130.min.js.download"></script><script type="text/javascript" async="" src="./Chapter16_files/2508.js.download"></script><script async="" src="./Chapter16_files/fbevents.js.download"></script><script type="text/javascript" async="" src="./Chapter16_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter16_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter16_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter16_files/analytics.js.download"></script><script type="text/javascript" async="" src="./Chapter16_files/ec.js.download"></script><script type="text/javascript" async="" src="./Chapter16_files/bat.js.download"></script><script type="text/javascript" async="" src="./Chapter16_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter16_files/insight.min.js.download"></script><script type="text/javascript" async="" src="./Chapter16_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter16_files/linkid.js.download"></script><script async="" src="./Chapter16_files/gtm.js.download"></script><script async="" src="./Chapter16_files/analytics.js.download"></script><script type="text/javascript">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if("function"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e("handle"),a=e(3),u=e(4),f=e("ee").get("tracer"),c=e("loader"),s=NREUM;"undefined"==typeof window.newrelic&&(newrelic=s);var p=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],d="api-",l=d+"ixn-";a(p,function(e,n){s[n]=o(d+n,!0,"api")}),s.addPageAction=o(d+"addPageAction",!0),s.setCurrentRouteName=o(d+"routeName",!0),n.exports=newrelic,s.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(e,n){var t={},r=this,o="function"==typeof n;return i(l+"tracer",[c.now(),e,t],r),function(){if(f.emit((o?"":"no-")+"fn-start",[c.now(),r,o],t),o)try{return n.apply(this,arguments)}catch(e){throw f.emit("fn-err",[arguments,this,e],t),e}finally{f.emit("fn-end",[c.now()],t)}}}};a("actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(e,n){m[n]=o(l+n)}),newrelic.noticeError=function(e,n){"string"==typeof e&&(e=new Error(e)),i("err",[e,c.now(),!1,n])}},{}],2:[function(e,n,t){function r(e,n){if(!o)return!1;if(e!==o)return!1;if(!n)return!0;if(!i)return!1;for(var t=i.split("."),r=n.split("."),a=0;a<r.length;a++)if(r[a]!==t[a])return!1;return!0}var o=null,i=null,a=/Version\/(\S+)\s+Safari/;if(navigator.userAgent){var u=navigator.userAgent,f=u.match(a);f&&u.indexOf("Chrome")===-1&&u.indexOf("Chromium")===-1&&(o="Safari",i=f[1])}n.exports={agent:o,version:i,match:r}},{}],3:[function(e,n,t){function r(e,n){var t=[],r="",i=0;for(r in e)o.call(e,r)&&(t[i]=n(r,e[r]),i+=1);return t}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],4:[function(e,n,t){function r(e,n,t){n||(n=0),"undefined"==typeof t&&(t=e?e.length:0);for(var r=-1,o=t-n||0,i=Array(o<0?0:o);++r<o;)i[r]=e[n+r];return i}n.exports=r},{}],5:[function(e,n,t){n.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(e,n,t){function r(){}function o(e){function n(e){return e&&e instanceof r?e:e?f(e,u,i):i()}function t(t,r,o,i){if(!d.aborted||i){e&&e(t,r,o);for(var a=n(o),u=v(t),f=u.length,c=0;c<f;c++)u[c].apply(a,r);var p=s[y[t]];return p&&p.push([b,t,r,a]),a}}function l(e,n){h[e]=v(e).concat(n)}function m(e,n){var t=h[e];if(t)for(var r=0;r<t.length;r++)t[r]===n&&t.splice(r,1)}function v(e){return h[e]||[]}function g(e){return p[e]=p[e]||o(t)}function w(e,n){c(e,function(e,t){n=n||"feature",y[t]=n,n in s||(s[n]=[])})}var h={},y={},b={on:l,addEventListener:l,removeEventListener:m,emit:t,get:g,listeners:v,context:n,buffer:w,abort:a,aborted:!1};return b}function i(){return new r}function a(){(s.api||s.feature)&&(d.aborted=!0,s=d.backlog={})}var u="nr@context",f=e("gos"),c=e(3),s={},p={},d=n.exports=o();d.backlog=s},{}],gos:[function(e,n,t){function r(e,n,t){if(o.call(e,n))return e[n];var r=t();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,n,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return e[n]=r,r}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],handle:[function(e,n,t){function r(e,n,t,r){o.buffer([e],r),o.emit(e,n,t)}var o=e("ee").get("handle");n.exports=r,r.ee=o},{}],id:[function(e,n,t){function r(e){var n=typeof e;return!e||"object"!==n&&"function"!==n?-1:e===window?0:a(e,i,function(){return o++})}var o=1,i="nr@id",a=e("gos");n.exports=r},{}],loader:[function(e,n,t){function r(){if(!E++){var e=x.info=NREUM.info,n=l.getElementsByTagName("script")[0];if(setTimeout(s.abort,3e4),!(e&&e.licenseKey&&e.applicationID&&n))return s.abort();c(y,function(n,t){e[n]||(e[n]=t)}),f("mark",["onload",a()+x.offset],null,"api");var t=l.createElement("script");t.src="https://"+e.agent,n.parentNode.insertBefore(t,n)}}function o(){"complete"===l.readyState&&i()}function i(){f("mark",["domContent",a()+x.offset],null,"api")}function a(){return O.exists&&performance.now?Math.round(performance.now()):(u=Math.max((new Date).getTime(),u))-x.offset}var u=(new Date).getTime(),f=e("handle"),c=e(3),s=e("ee"),p=e(2),d=window,l=d.document,m="addEventListener",v="attachEvent",g=d.XMLHttpRequest,w=g&&g.prototype;NREUM.o={ST:setTimeout,SI:d.setImmediate,CT:clearTimeout,XHR:g,REQ:d.Request,EV:d.Event,PR:d.Promise,MO:d.MutationObserver};var h=""+location,y={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-1130.min.js"},b=g&&w&&w[m]&&!/CriOS/.test(navigator.userAgent),x=n.exports={offset:u,now:a,origin:h,features:{},xhrWrappable:b,userAgent:p};e(1),l[m]?(l[m]("DOMContentLoaded",i,!1),d[m]("load",r,!1)):(l[v]("onreadystatechange",o),d[v]("onload",r)),f("mark",["firstbyte",u],null,"api");var E=0,O=e(5)},{}]},{},["loader"]);</script><link rel="apple-touch-icon" href="https://learning.oreilly.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://learning.oreilly.com/favicon.ico" type="image/x-icon"><link href="./Chapter16_files/css" rel="stylesheet" type="text/css"><title>16. Natural Language Processing with RNNs and Attention - Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition</title><link rel="stylesheet" href="./Chapter16_files/output.68851547a55f.css" type="text/css"><link rel="stylesheet" type="text/css" href="./Chapter16_files/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="./Chapter16_files/font-awesome.min.css"><style type="text/css" title="ibis-book">@charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000 !important;padding-top:10px !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:2em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content p[data-type="footnote"]{font-size:90% !important;line-height:1.2em !important;margin-left:2.5em !important;text-indent:-2.3em !important}#sbo-rt-content p[data-type="footnote"] sup{display:inline-block !important;position:static !important;width:2em !important;text-align:right !important;font-size:100% !important;padding-right:.5em !important}#sbo-rt-content p[data-type="footnote"] a[href$="-marker"]{font-family:sans-serif !important;font-size:90% !important;color:#8e0012 !important}#sbo-rt-content p[data-type="footnote"] a[data-type="xref"]{margin:0 !important;padding:0 !important;text-indent:0 !important}#sbo-rt-content a[data-type="noteref"]{font-family:sans-serif !important;color:#8e0012;margin-left:0;padding-left:0}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dd{margin-left:1.5em !important;margin-bottom:.25em}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul>li,#sbo-rt-content ol ul,#sbo-rt-content ol ul>li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul>li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul>li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul>li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol>li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol>li,#sbo-rt-content ul ol,#sbo-rt-content ul ol>li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol>li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol>li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:middle;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content table.border tbody>tr:last-child>td{border-bottom:transparent}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:1em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content .width-10{width:10vw !important}#sbo-rt-content .width-20{width:20vw !important}#sbo-rt-content .width-30{width:30vw !important}#sbo-rt-content .width-40{width:40vw !important}#sbo-rt-content .width-50{width:50vw !important}#sbo-rt-content .width-60{width:60vw !important}#sbo-rt-content .width-70{width:70vw !important}#sbo-rt-content .width-80{width:80vw !important}#sbo-rt-content .width-90{width:90vw !important}#sbo-rt-content .width-full,#sbo-rt-content .width-100{width:100vw !important}#sbo-rt-content div[data-type="equation"].fifty-percent img{width:50%}</style><script> // <![CDATA[
    var g = {
      position_cache: {
        
          "chapter": "/api/v1/book/9781492032632/chapter/part01.html",
          "book_id": "9781492032632",
          "chapter_uri": "part01.html",
          "position": 100.0,
          "user_uuid": "d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79",
          "next_chapter_uri": "/library/view/hands-on-machine-learning/9781492032632/ch01.html"
        
      },
      title: "Hands\u002Don Machine Learning with Scikit\u002DLearn, Keras, and TensorFlow, 2nd Edition",
      author_list: "Aurélien Géron",
      format: "book",
      source: "application/epub+zip",
      is_system_book: true,
      is_public: false,
      loaded_from_server: true,
      allow_scripts: false,
      has_mathml: true
    };
    // ]]></script><script src="./Chapter16_files/modernizr.8e35451ddb64.js.download"></script><script>
    
      

      
        
          window.PUBLIC_ANNOTATIONS = true;
        
      

      window.MOBILE_PUBLIC_ANNOTATIONS = false;

    

    
      window.PRIVACY_CONTROL_OVERRIDE = false;
    

      window.PRIVACY_CONTROL_SWITCH = true;

      window.PUBLISHER_PAGES = true;

      window.SBO = {
        "constants": {
          "SITB_ENDPOINT": "/api/v2/sitb/",
          "SEARCH_SELECT_ENDPOINT": "https://learning.oreilly.com/api/v2/search/select/",
          "ENABLE_ONLINE_TRAINING": false
        }
      };
  </script><link rel="canonical" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta name="description" content=" Part I. The Fundamentals of Machine Learning "><meta property="og:title" content="I. The Fundamentals of Machine Learning"><meta itemprop="isPartOf" content="/library/view/hands-on-machine-learning/9781492032632/"><meta itemprop="name" content="I. The Fundamentals of Machine Learning"><meta property="og:url" itemprop="url" content="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://learning.oreilly.com/library/cover/9781492032632/"><meta property="og:description" itemprop="description" content=" Part I. The Fundamentals of Machine Learning "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O&#39;Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781492032649"><meta property="og:book:author" itemprop="author" content="Aurélien Géron"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: &lt;%= font_size %&gt; !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: &lt;%= font_family %&gt; !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: &lt;%= column_width %&gt;% !important; margin: 0 auto !important; }"></style><noscript><meta http-equiv="refresh" content="0; url=/library/no-js/" /></noscript><script type="text/javascript">
    (function(i,s,o,g,r,a,m) {
      i['GoogleAnalyticsObject']=r;
      i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();
      a=s.createElement(o),m=s.getElementsByTagName(o)[0];
      a.async=1;
      a.src=g;
      m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    var matches = document.cookie.match(/BrowserCookie\s*=\s*([a-f0-9\-]{36})/),
      user_uuid = null;

    if (matches && matches.length === 2) {
      user_uuid = matches[1];
    }

  
    ga('create', 'UA-39299553-7', {'userId': 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79' });
  

  
    
      ga('set', 'dimension1', 'B2B');
    
  

  ga('set', 'dimension6', user_uuid);

  
    ga('set', 'dimension2', 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79');
    
      ga('set', 'dimension7', '0012M0000229keZQAQ');
    
  

  

  

  //enable enhanced link tracking
  ga('require', 'linkid', 'linkid.js');

  // reading interface will track pageviews itself
  if (document.location.pathname.indexOf("/library/view") !== 0) {
    ga('send', 'pageview');
  }
  </script><script>
    var dataLayer = window.dataLayer || [];

    
      window.medalliaVsgUserIdentifier = 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79';
      dataLayer.push({userIdentifier: 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79'});
      dataLayer.push({loggedIn: 'yes'});

      
        window.medalliaVsgAccountIdentifier = '21bed0a7-6b7b-470c-8fa0-40a52db0b491';
        
        dataLayer.push({orgID: '21bed0a7-6b7b-470c-8fa0-40a52db0b491'});
        

        window.medalliaVsgIsIndividual = false;
        
          
          dataLayer.push({learningAccountType: 'enterprise'});
          
        

        
          dataLayer.push({learningPaidAccount: 'yes'});
        
      
    

    (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-5P4V6Z');
    (function () {
      var VERSION = 'V1.1';
      var AUTHOR = 'Awwad';
      if (!window.GtmHelper)
        window.GtmHelper = function () {
          var instance = this;
          var loc = document.location;
          this.version = VERSION;
          this.author = AUTHOR;
          this.readCookie = function (name) {
            var nameEQ = name + "=";
            var ca = document.cookie.split(';');
            for (var i = 0; i < ca.length; i++) {
              var c = ca[i];
              while (c.charAt(0) == ' ') c = c.substring(1, c.length);
              if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length, c.length);
            }
            return null;
          };
          this.createCookie = function (name, value, days, cookieDomain) {
            var domain = "";
            var expires = "";

            if (days) {
              var date = new Date();
              date.setTime(date.getTime() + Math.ceil(days * 24 * 60 * 60 * 1000));
              var expires = " expires=" + date.toGMTString() + ";";
            }

            if (typeof (cookieDomain) != 'undefined')
              domain = " domain=" + cookieDomain + "; ";

            document.cookie = name + "=" + value + ";" + expires + domain + "path=/";
          };

          this.isDuplicated = function (currentTransactionId) {
            // the previous transaction id:
            var previousTransIdValue = this.readCookie("previousTransId");

            if (currentTransactionId === previousTransIdValue) {
              return true; // Duplication
            } else {
              return false;
            }
          };
        }
    })()
  </script><script defer="" src="./Chapter16_files/vendor.a48a756c5182.js.download"></script><script defer="" src="./Chapter16_files/reader.f2a0c6bd2fee.js.download"></script><script src="./Chapter16_files/f(1).txt"></script><script src="./Chapter16_files/f(2).txt"></script><script src="./Chapter16_files/f(3).txt"></script><script src="./Chapter16_files/f(4).txt"></script><script async="" src="./Chapter16_files/MathJax.js.download"></script><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 2147483020;
}
.annotator-filter {
  z-index: 2147483010;
}</style><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
</style><script async="true" type="text/javascript" src="./Chapter16_files/roundtrip.js.download"></script><style type="text/css" id="kampyleStyle">.noOutline{outline: none !important;}.wcagOutline:focus{outline: 1px dashed #595959 !important;outline-offset: 2px !important;transition: none !important;}</style><script async="true" type="text/javascript" src="./Chapter16_files/roundtrip.js.download"></script><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Math-bold-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Script; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_AMS; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf?V=2.7.1') format('opentype')}
</style></head>


<body class="reading sidenav  scalefonts library nav-collapsed"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

    
  <noscript> 
    <iframe src="//www.googletagmanager.com/ns.html?id=GTM-5P4V6Z"
            height="0" width="0"
            style="display:none;visibility:hidden">
    </iframe>
  </noscript>



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#container" class="skip">Skip to content</a><header class="topbar t-topbar"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://learning.oreilly.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M11.738 14H9.254v-3.676a.617.617 0 0 0-.621-.613H7.39a.617.617 0 0 0-.62.613V14H4.284a.617.617 0 0 1-.622-.613V10.22c0-.327.132-.64.367-.87l3.547-3.493a.627.627 0 0 1 .875 0l3.54 3.499c.234.229.366.54.367.864v3.167a.617.617 0 0 1-.62.613zM7.57 2.181a.625.625 0 0 1 .882 0l5.77 5.692-.93.92-5.28-5.209-5.28 5.208-.932-.919 5.77-5.692z"></path></svg><span>Safari Home</span></a></li><li><a href="https://learning.oreilly.com/resource-centers/" class="t-resource-centers-nav l0 nav-icn"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="16px" height="16px" viewBox="0 0 16 16" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="Topic-Page-Design" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="Heron-Button" transform="translate(-20.000000, -78.000000)" fill="#4A3A30"><g id="Group-9" transform="translate(20.000000, 78.000000)"><rect id="Rectangle" x="9.6" y="0" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="9.6" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="0" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect></g></g></g></svg><span>Resource Centers</span></a></li><li><a href="https://learning.oreilly.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://learning.oreilly.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://learning.oreilly.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://learning.oreilly.com/learning-paths/" class="l1 nav-icn t-learningpaths-nav js-toggle-menu-item"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="32px" height="32px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 52.5 (67469) - http://www.bohemiancoding.com/sketch --><title>Mask</title><desc>Created with Sketch.</desc><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><path d="M0,16.0214227 C0,15.0387209 0.796453294,14.2411658 1.77779753,14.2411658 C2.75914177,14.2411658 3.55559506,15.0387209 3.55559506,16.0214227 C3.55559506,17.0041246 2.75914177,17.8016797 1.77779753,17.8016797 C0.796453294,17.8016797 0,17.0041246 0,16.0214227 Z M9.77788642,5.22914885 C8.9280992,5.72049977 7.84008711,5.42853763 7.34941499,4.57757479 C6.85874287,3.72661195 7.15030167,2.63709467 8.00008889,2.14574375 C8.84987611,1.65439282 9.9378882,1.94635496 10.4285603,2.7973178 C10.9192324,3.64828064 10.6276736,4.73779792 9.77788642,5.22914885 Z M4.57213969,7.35869225 C5.42192691,7.85004318 5.71348571,8.93956046 5.22281359,9.79052329 C4.73214147,10.6414861 3.64412938,10.9334483 2.79434216,10.4420974 C1.94455494,9.95074642 1.65299614,8.86122915 2.14366826,8.01026631 C2.63434038,7.15930347 3.72235247,6.86734132 4.57213969,7.35869225 Z M2.79434216,21.6007481 C3.64412938,21.1093972 4.73214147,21.4013594 5.22281359,22.2523222 C5.71348571,23.103285 5.42192691,24.1928023 4.57213969,24.6841532 C3.72235247,25.1755042 2.63434038,24.883542 2.14366826,24.0325792 C1.65299614,23.1816163 1.94455494,22.0920991 2.79434216,21.6007481 Z M7.34941499,27.4652707 C7.84008711,26.6143079 8.9280992,26.3223457 9.77788642,26.8136966 C10.6276736,27.3050476 10.9192324,28.3945649 10.4285603,29.2455277 C9.9378882,30.0964905 8.84987611,30.3884527 8.00008889,29.8971017 C7.15030167,29.4057508 6.85874287,28.3162335 7.34941499,27.4652707 Z M18.7118524,11.3165596 C21.3074367,12.8173162 22.1963355,16.1392758 20.6976522,18.738451 C19.1989689,21.3358459 15.8815987,22.2259744 13.2860143,20.726998 C10.6922077,19.2262414 9.80330893,15.9042818 11.3002144,13.3051066 C12.7988978,10.7059314 16.116268,9.81580294 18.7118524,11.3165596 Z M26.7821642,27.8093944 L30.1315348,31.1633985 C30.3982044,31.4304371 30.2097579,31.8844026 29.8346426,31.8844026 L21.5945511,31.8844026 C21.1287681,31.8844026 20.751875,31.5069881 20.751875,31.0405608 L20.751875,22.7890697 C20.751875,22.4134355 21.2052134,22.2247282 21.4701052,22.4899865 L24.2843587,25.3081333 C26.8337204,23.0240636 28.4444049,19.7092251 28.4444049,16.0223129 C28.4444049,9.15052091 22.8621207,3.56051397 15.9998222,3.56051397 L15.9998222,0 C24.8230314,0 32,7.18689745 32,16.0223129 C32,20.6919269 29.9750886,24.8790914 26.7821642,27.8093944 Z" id="Mask" fill="#8B889A"></path></g></svg><span>Learning Paths</span></a></li><li class="nav-highlights"><a href="https://learning.oreilly.com/u/d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" fill="#4A3C31"><path d="M13.325 18.071H8.036c0-6.736 4.324-10.925 14.464-12.477V0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35c5.142 0 9.175-3.515 9.175-8.816 0-4.628-2.367-7.293-6.253-8.113zm27.5 0h-5.26c0-6.736 4.295-10.925 14.435-12.477V0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35c5.113 0 9.146-3.515 9.146-8.816 0-4.628-2.338-7.293-6.253-8.113z" fill-rule="evenodd"></path></svg><span>Highlights</span></a></li><li><a href="https://learning.oreilly.com/u/preferences/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l1 no-icon">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://learning.oreilly.com/u/preferences/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l2">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9781492032632/chapter/part01.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"><div data-reactroot="" class="menu-dropdown-wrapper js-menu-dropdown-wrapper align-right"><img class="hidden" src="./Chapter16_files/ajax-transp.gif" alt="loading spinner"><div class="menu-control"><div class="control "><div class="js-playlists-menu"><button class="js-playlist-icon"><svg class="icon-add-to-playlist-sml" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill-rule="nonzero" fill="#000000"><g transform="translate(-1.000000, 0.000000)"><rect x="5" y="0" width="12" height="2"></rect><title>Playlists</title><path d="M4.5,14 C6.43299662,14 8,12.4329966 8,10.5 C8,8.56700338 6.43299662,7 4.5,7 C2.56700338,7 1,8.56700338 1,10.5 C1,12.4329966 2.56700338,14 4.5,14 Z M2.5,10 L4,10 L4,8.5 L5,8.5 L5,10 L6.5,10 L6.5,11 L5,11 L5,12.5 L4,12.5 L4,11 L2.5,11 L2.5,10 Z"></path><circle cx="2" cy="5" r="1"></circle><circle cx="1.94117647" cy="1" r="1"></circle><rect x="5" y="4" width="12" height="2"></rect><rect x="9" y="8" width="8" height="2"></rect><rect x="9" y="12" width="8" height="2"></rect></g></g></g></svg><div class="js-playlist-addto-label">Add&nbsp;To</div></button></div></div></div></div></div></div></li><li class="js-font-control-panel font-control-activator"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html&amp;text=Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%20I.%20The%20Fundamentals%20of%20Machine%20Learning&amp;body=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html%0D%0Afrom%20Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    
    <section role="document">
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">15. Processing Sequences Using RNNs and CNNs</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">17. Representation Learning and Generative Learning Using Autoencoders and GANs</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content"><div class="annotator-wrapper"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 16. Natural Language Processing with RNNs and Attention"><div class="chapter" id="nlp_chapter">
<h1><span class="label">Chapter 16. </span>Natural Language Processing with RNNs and Attention</h1>


<p>When Alan Turing imagined his famous <a href="https://homl.info/turingtest">Turing test</a><sup><a data-type="noteref" id="idm46263495030744-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263495030744" class="totri-footnote">1</a></sup> in 1950, his objective was to evaluate a machine’s ability to match human intelligence. He could have tested for many things, such as the ability to recognize cats in pictures, play chess, compose music, or escape a maze, but, interestingly, he chose a linguistic task. More specifically, he devised a <em>chatbot</em> capable of fooling its interlocutor into thinking it was human.<sup><a data-type="noteref" id="idm46263495028824-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263495028824" class="totri-footnote">2</a></sup> This test does have its weaknesses: a set of hard-coded rules can fool unsuspecting or naive humans (e.g., the machine could give vague predefined answers in response to some keywords; it could pretend that it is joking or drunk, to get a pass on its weirdest answers; or it could escape difficult questions by answering them with its own questions), and many aspects of human intelligence are utterly ignored (e.g., the ability to interpret nonverbal communication such as facial expressions, or to learn a manual task). But the test does highlight the fact that mastering language is arguably <em>Homo sapiens</em>’s greatest cognitive ability. Can we build a machine that can read and write natural language?</p>

<p>A common approach for natural language tasks is to use recurrent neural networks. We will therefore continue to explore RNNs (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#rnn_chapter">Chapter&nbsp;15</a>), starting with a Character-RNN, trained to predict the next character in a sentence. This will allow us to generate some original text, and in the process we will see how to build a TensorFlow Dataset on a very long sequence. We will first use a <em>stateless RNN</em> (which learns on random portions of text at each iteration, without any information on the rest of the text), then we will build a <em>stateful RNN</em> (which preserves the hidden state between training iterations and continues reading where it left off, allowing it to learn longer patterns). Next, we will build an RNN to perform <em>sentiment analysis</em> (e.g., reading movie reviews and extracting the rater’s feeling about the movie), this time treating sentences as sequences of words, rather than characters. Then we will show how RNNs can be used to build an Encoder–Decoder architecture capable of performing neural machine translation (NMT). For this, we will use the seq2seq API provided by the TensorFlow Addons project.</p>

<p>In the second part of this chapter, we will look at <em>attention mechanisms</em>. As their name suggests, these are neural network components that learn to select the part of the inputs that the rest of the model should focus on at each time step. First, we will see how to boost the performance of an RNN-based Encoder–Decoder architecture using attention, then we will drop RNNs altogether and look at a very successful attention-only architecture called the <em>Transformer</em>. Finally, we will take a look at some of the most important advances in NLP in 2018 and 2019, including incredibly powerful language models such as GPT-2 and BERT, both based on Transformers.</p>

<p>Let’s start with a simple and fun model that can write like Shakespeare (well, sort of).</p>






<section data-type="sect1" data-pdf-bookmark="Generating Shakespeare Text Using a Character RNN"><div class="sect1" id="idm46263495019528">
<h1>Generating Shakespeare Text Using a Character RNN</h1>

<p>In a famous <a href="https://homl.info/charrnn">2015 blog post</a> titled “The Unreasonable Effectiveness of Recurrent Neural Networks,” Andrej Karpathy showed how to train an RNN to predict the next character in a sentence. This <em>Char-RNN</em> can then be used to generate novel text, one character at a time. Here is a small sample of the text generated by a Char-RNN model after it was trained on all of Shakespeare’s work:</p>
<blockquote>
<p>PANDARUS:</p>

<p>Alas, I think he shall be come approached and the day</p>

<p>When little srain would be attain’d into being never fed,</p>

<p>And who is but a chain and subjects of his death,</p>

<p>I should not sleep.</p></blockquote>

<p>Not exactly a masterpiece, but it is still impressive that the model was able to learn words, grammar, proper punctuation, and more, just by learning to predict the next character in a sentence. Let’s look at how to build a Char-RNN, step by step, starting with the creation of the dataset.</p>








<section data-type="sect2" data-pdf-bookmark="Creating the Training Dataset"><div class="sect2" id="idm46263495012840">
<h2>Creating the Training Dataset</h2>

<p>First, let’s download all of Shakespeare’s work, using Keras’s handy <code>get_file()</code> function and downloading the data from Andrej Karpathy’s <a href="https://github.com/karpathy/char-rnn">Char-RNN project</a>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">shakespeare_url</code> <code class="o">=</code> <code class="s2">"https://homl.info/shakespeare"</code> <code class="c1"># shortcut URL</code>
<code class="n">filepath</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">utils</code><code class="o">.</code><code class="n">get_file</code><code class="p">(</code><code class="s2">"shakespeare.txt"</code><code class="p">,</code> <code class="n">shakespeare_url</code><code class="p">)</code>
<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">filepath</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
    <code class="n">shakespeare_text</code> <code class="o">=</code> <code class="n">f</code><code class="o">.</code><code class="n">read</code><code class="p">()</code></pre>

<p>Next, we must encode every character as an integer. One option is to create a custom preprocessing layer, as we did in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch13.html#data_chapter">Chapter&nbsp;13</a>. But in this case, it will be simpler to use Keras’s <code>Tokenizer</code> class. First we need to fit a Tokenizer to the text: it will find all the characters used in the text and map each of them to a different character ID, from 1 to the number of distinct characters (it does not start at 0, so we can use that value for masking, as we will see later in this chapter):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">tokenizer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">preprocessing</code><code class="o">.</code><code class="n">text</code><code class="o">.</code><code class="n">Tokenizer</code><code class="p">(</code><code class="n">char_level</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">tokenizer</code><code class="o">.</code><code class="n">fit_on_texts</code><code class="p">([</code><code class="n">shakespeare_text</code><code class="p">])</code></pre>

<p>We set <code>char_level=True</code> to get character-level encoding rather than the default word-level encoding. Note that this tokenizer converts the text to lowercase by default (but you can set <code>lower=False</code> if you do not want that). Now the tokenizer can encode a sentence (or a list of sentences) to a list of character IDs and back, and it tells us how many distinct characters there are and the total number of characters in the text:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">tokenizer</code><code class="o">.</code><code class="n">texts_to_sequences</code><code class="p">([</code><code class="s">"First"</code><code class="p">])</code>
<code class="go">[[20, 6, 9, 8, 3]]</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">tokenizer</code><code class="o">.</code><code class="n">sequences_to_texts</code><code class="p">([[</code><code class="mi">20</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">9</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">3</code><code class="p">]])</code>
<code class="go">['f i r s t']</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">max_id</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">tokenizer</code><code class="o">.</code><code class="n">word_index</code><code class="p">)</code> <code class="c"># number of distinct characters</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">dataset_size</code> <code class="o">=</code> <code class="n">tokenizer</code><code class="o">.</code><code class="n">document_count</code> <code class="c"># total number of characters</code></pre>

<p>Let’s encode the full text so each character is represented by its ID (we subtract 1 to get IDs from 0 to 38, rather than from 1 to 39):</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[</code><code class="n">encoded</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">tokenizer</code><code class="o">.</code><code class="n">texts_to_sequences</code><code class="p">([</code><code class="n">shakespeare_text</code><code class="p">]))</code> <code class="o">-</code> <code class="mi">1</code></pre>

<p>Before we continue, we need to split the dataset into a training set, a validation set, and a test set. We can’t just shuffle all the characters in the text, so how do you split a sequential dataset?</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="How to Split a Sequential Dataset"><div class="sect2" id="idm46263495012216">
<h2>How to Split a Sequential Dataset</h2>

<p>It is very important to avoid any overlap between the training set, the validation set, and the test set. For example, we can take the first 90% of the text for the training set, then the next 5% for the validation set, and the final 5% for the test set. It would also be a good idea to leave a gap between these sets to avoid the risk of a paragraph overlapping over two sets.</p>

<p>When dealing with time series, you would in general split across time, for example the years 2000 to 2012 for the training set, the years 2013 to 2015 for the validation set, and the years 2016 to 2019 for the test set. However, in some cases you may be able to split along other dimensions, which will give you a longer time period to train on. For example, if you have data about the financial health of 10,000 companies from 2000 to 2019, you might be able to split this data across the different companies. However, it is very likely that many of these companies are strongly correlated (e.g., whole economic sectors may go up or down jointly): if you have correlated companies across the training set and the test set, then your test set will not be as useful, as its measure of the generalization error will be optimistically biased.</p>

<p>So it is often safer to split across time. However, this implicitly assumes that the patterns the RNN can learn in the past (in the training set) will still exist in the future. In other words, we assume that the time series is <em>stationary</em> (at least in a wide sense)<sup><a data-type="noteref" id="idm46263494797064-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263494797064" class="totri-footnote">3</a></sup>. For many time series this assumption is reasonable (e.g., chemical reactions should be fine, since the laws of chemistry don’t change every day), but for many others it is not (e.g., financial markets are notoriously not stationary since patterns disappear as soon as traders spot them and start exploiting them). To make sure the time series is indeed sufficiently stationary, you can plot the model’s errors on the validation set across time: if the model performs much better on the first part of the validation set than on the last part, then the time series may not be stationary enough, and you might be better off training the model on a shorter time span.</p>

<p>In short, splitting a time series into a training set, a validation set, and a test set is not a trivial task, and it will depend strongly on the task at hand.</p>

<p>Now back to Shakespeare! Let’s take the first 90% of the text for the training set (keeping the rest for the validation set and the test set), and create a <code>tf.data.Dataset</code> that will return each character one by one from the training set:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">train_size</code> <code class="o">=</code> <code class="n">dataset_size</code> <code class="o">*</code> <code class="mi">90</code> <code class="o">//</code> <code class="mi">100</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="o">.</code><code class="n">from_tensor_slices</code><code class="p">(</code><code class="n">encoded</code><code class="p">[:</code><code class="n">train_size</code><code class="p">])</code></pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Chopping the Sequential Dataset into Multiple Windows"><div class="sect2" id="idm46263494711384">
<h2>Chopping the Sequential Dataset into Multiple Windows</h2>

<p>Now there is a single sequence of over a million characters in this training set, so we can’t just train the neural network directly on it: the RNN would be equivalent to a deep net with over a million layers, and we would have a single (very long) instance to train it. Instead, we will use the dataset’s <code>window()</code> method to convert this long sequence of characters into many smaller windows of text. Every instance in the dataset will be a fairly short substring of the whole text, and the RNN will be unrolled only over the length of these substrings. This is called <em>truncated backpropagation through time</em>. Let’s call the <code>window()</code> method to create a dataset of short text windows:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">n_steps</code> <code class="o">=</code> <code class="mi">100</code>
<code class="n">window_length</code> <code class="o">=</code> <code class="n">n_steps</code> <code class="o">+</code> <code class="mi">1</code> <code class="c1"># target = input shifted 1 character ahead</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">window</code><code class="p">(</code><code class="n">window_length</code><code class="p">,</code> <code class="n">shift</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">drop_remainder</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>You can try tuning <code>n_steps</code>: it is easier to train RNNs on shorter input sequences, but of course the RNN will not be able to learn any pattern longer than <code>n_steps</code>, so don’t make it too small.</p>
</div>

<p>By default, the <code>window()</code> method creates nonoverlapping windows, but to get the largest possible training set, we use <code>shift=1</code> so that the first window contains characters 0 to 100, the second contains characters 1 to 101, and so on. To ensure that all windows are exactly 101 characters long (which will allow us to create batches without having to do any padding), we set <code>drop_remainder=True</code> (or else the last 100 windows will contain 100 characters, 99 characters, and so on down to 1 character).</p>

<p>The <code>window()</code> method creates a <code>Dataset</code> that contains windows, each of which is also represented as a <code>Dataset</code>. It’s a <em>nested dataset</em> (analog to a list of lists). This is useful when you want to transform each window by calling its <code>Dataset</code> methods (e.g., to shuffle them or batch them). However, we cannot use a nested dataset directly for training, as our model will expect tensors as input, not datasets. So we must call the <code>flat_map()</code> method: it converts a nested dataset into a <em>flat dataset</em> (one that does not contain datasets). For example, suppose {1, 2, 3} represents a dataset containing the sequence of tensors 1, 2, and 3. If you flatten the nested dataset {{1, 2}, {3, 4, 5, 6}}, you get back the flat dataset {1, 2, 3, 4, 5, 6}. Moreover, the <code>flat_map()</code> method takes a function as argument which allows you to transform each dataset in the nested dataset before the flat dataset iterates over it. For example, it you pass the function <code>lambda ds: ds.batch(2)</code> to <code>flat_map()</code>, then it will transform the nested dataset {{1, 2}, {3, 4, 5, 6}} into the flat dataset {[1, 2], [3, 4], [5, 6]}: it’s a dataset of tensors of size 2. With that in mind, we are ready to flatten our dataset:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">flat_map</code><code class="p">(</code><code class="k">lambda</code> <code class="n">window</code><code class="p">:</code> <code class="n">window</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="n">window_length</code><code class="p">))</code></pre>

<p>Notice that we call <code>batch(window_length)</code> on each window: since all windows have exactly that length, we will get a single tensor for each of them. OK, now the dataset contains consecutive windows of 101 characters each. Since Gradient Descent works best when the instances in the training set are independent and identically distributed (IID, see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>), we need to shuffle these windows. Then we can batch these windows and separate the inputs (the first 100 characters) from the target (the last character):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">batch_size</code> <code class="o">=</code> <code class="mi">32</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">shuffle</code><code class="p">(</code><code class="mi">10000</code><code class="p">)</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="n">batch_size</code><code class="p">)</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="k">lambda</code> <code class="n">windows</code><code class="p">:</code> <code class="p">(</code><code class="n">windows</code><code class="p">[:,</code> <code class="p">:</code><code class="o">-</code><code class="mi">1</code><code class="p">],</code> <code class="n">windows</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">:]))</code></pre>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#window_dataset_diagram">Figure&nbsp;16-1</a> summarizes the dataset preparation steps discussed so far (showing windows of length 11 rather than 101, and a batch size of 3 instead of 32).</p>

<figure class="smallerseventy"><div id="window_dataset_diagram" class="figure">
<img src="./Chapter16_files/mls2_1601.png" alt="mls2 1601" width="1441" height="664" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1601.png">
<h6><span class="label">Figure 16-1. </span>Preparing a dataset of shuffled windows</h6>
</div></figure>

<p>As discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch13.html#data_chapter">Chapter&nbsp;13</a>, categorical input features should generally be encoded, usually as one-hot vectors or as embeddings. Here, we will encode each character using a one-hot vector because there are fairly few distinct characters (only 39):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">map</code><code class="p">(</code>
    <code class="k">lambda</code> <code class="n">X_batch</code><code class="p">,</code> <code class="n">Y_batch</code><code class="p">:</code> <code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">one_hot</code><code class="p">(</code><code class="n">X_batch</code><code class="p">,</code> <code class="n">depth</code><code class="o">=</code><code class="n">max_id</code><code class="p">),</code> <code class="n">Y_batch</code><code class="p">))</code></pre>

<p>Finally, we just need to add prefetching:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">prefetch</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code></pre>

<p>That’s it! Preparing the dataset was the hardest part. Now let’s create the model.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Building and Training the Char-RNN Model"><div class="sect2" id="idm46263494776712">
<h2>Building and Training the Char-RNN Model</h2>

<p>To predict the next character based on the previous 100 characters, we can just use an RNN with 2 <code>GRU</code> layers of 128 units each and 20% dropout on both the inputs (<code>dropout</code>) and the hidden states (<code>recurrent_dropout</code>). We can tweak these hyperparameters later, if needed. The output layer is a time-distributed dense layer like we saw in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#rnn_chapter">Chapter&nbsp;15</a>. This time the <code>Dense</code> layer must have 39 units (<code>max_id</code>) because there are 39 distinct characters in the text, and we want to output a probability for each possible character (at each time step). The output probabilities should sum up to 1 at each time step, so we apply the softmax activation function to the outputs of the <code>Dense</code> layer. We can then compile this model, using the <code>sparse_categorical_crossentropy</code> loss and an Adam optimizer. Finally, we are ready to train the model for several epochs (this may take many hours, depending on your hardware):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">,</code> <code class="n">max_id</code><code class="p">],</code>
                     <code class="n">dropout</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code> <code class="n">recurrent_dropout</code><code class="o">=</code><code class="mf">0.2</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
                     <code class="n">dropout</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code> <code class="n">recurrent_dropout</code><code class="o">=</code><code class="mf">0.2</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">TimeDistributed</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="n">max_id</code><code class="p">,</code>
                                                    <code class="n">activation</code><code class="o">=</code><code class="s2">"softmax"</code><code class="p">))</code>
<code class="p">])</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"sparse_categorical_crossentropy"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s2">"adam"</code><code class="p">)</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">dataset</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">20</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Using the Char-RNN Model"><div class="sect2" id="idm46263494464296">
<h2>Using the Char-RNN Model</h2>

<p>Now we have a model that can predict the next character in text written by Shakespeare. To feed it some text, we first need to preprocess it like we did earlier, so let’s create a little function for this:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">preprocess</code><code class="p">(</code><code class="n">texts</code><code class="p">):</code>
    <code class="n">X</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">tokenizer</code><code class="o">.</code><code class="n">texts_to_sequences</code><code class="p">(</code><code class="n">texts</code><code class="p">))</code> <code class="o">-</code> <code class="mi">1</code>
    <code class="k">return</code> <code class="n">tf</code><code class="o">.</code><code class="n">one_hot</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">max_id</code><code class="p">)</code></pre>

<p>Now let’s use the model to predict the next letter in some text:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code> <code class="o">=</code> <code class="n">preprocess</code><code class="p">([</code><code class="s">"How are yo"</code><code class="p">])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">Y_pred</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict_classes</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">tokenizer</code><code class="o">.</code><code class="n">sequences_to_texts</code><code class="p">(</code><code class="n">Y_pred</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)[</code><code class="mi">0</code><code class="p">][</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code> <code class="c"># 1st sentence, last char</code>
<code class="go">'u'</code></pre>

<p>Success! The model guessed right. Now let’s use this model to generate new text.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Generating Fake Shakespeare Text"><div class="sect2" id="idm46263494302392">
<h2>Generating Fake Shakespeare Text</h2>

<p>To generate new text using the Char-RNN model, we could just make the model predict the most likely next letter, add it at the end of the text, then give the extended text to the model to guess the next letter, and so on. But in practice this often leads to the same words being repeated over and over again. Instead, we can pick the next character randomly, with a probability equal to the estimated probability, using TensorFlow’s <code>tf.random.categorical()</code> function. This will generate more diverse and interesting text. The <code>categorical()</code> function samples random class indices, given the class log probabilities (logits). To have more control over the diversity of the generated text, we can divide the logits by a number called the <em>temperature</em>, which we can tweak as we wish: a temperature close to 0 will favor the high-probability characters, while a very high temperature will give all characters an equal probability. The following <code>next_char()</code> function uses this approach to pick the next character to add to the input text:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">next_char</code><code class="p">(</code><code class="n">text</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mi">1</code><code class="p">):</code>
    <code class="n">X_new</code> <code class="o">=</code> <code class="n">preprocess</code><code class="p">([</code><code class="n">text</code><code class="p">])</code>
    <code class="n">y_proba</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)[</code><code class="mi">0</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">:,</code> <code class="p">:]</code>
    <code class="n">rescaled_logits</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">math</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">y_proba</code><code class="p">)</code> <code class="o">/</code> <code class="n">temperature</code>
    <code class="n">char_id</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">categorical</code><code class="p">(</code><code class="n">rescaled_logits</code><code class="p">,</code> <code class="n">num_samples</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code> <code class="o">+</code> <code class="mi">1</code>
    <code class="k">return</code> <code class="n">tokenizer</code><code class="o">.</code><code class="n">sequences_to_texts</code><code class="p">(</code><code class="n">char_id</code><code class="o">.</code><code class="n">numpy</code><code class="p">())[</code><code class="mi">0</code><code class="p">]</code></pre>

<p>Next, we can write a small function that will repeatedly call <code>next_char()</code> to get the next character and append it to the given text:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">complete_text</code><code class="p">(</code><code class="n">text</code><code class="p">,</code> <code class="n">n_chars</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mi">1</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_chars</code><code class="p">):</code>
        <code class="n">text</code> <code class="o">+=</code> <code class="n">next_char</code><code class="p">(</code><code class="n">text</code><code class="p">,</code> <code class="n">temperature</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">text</code></pre>

<p>We are now ready to generate some text! Let’s try with different temperatures:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="k">print</code><code class="p">(</code><code class="n">complete_text</code><code class="p">(</code><code class="s">"t"</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mf">0.2</code><code class="p">))</code>
<code class="go">the belly the great and who shall be the belly the</code>
<code class="gp">&gt;&gt;&gt; </code><code class="k">print</code><code class="p">(</code><code class="n">complete_text</code><code class="p">(</code><code class="s">"w"</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mi">1</code><code class="p">))</code>
<code class="go">thing? or why you gremio.</code>
<code class="go">who make which the first</code>
<code class="gp">&gt;&gt;&gt; </code><code class="k">print</code><code class="p">(</code><code class="n">complete_text</code><code class="p">(</code><code class="s">"w"</code><code class="p">,</code> <code class="n">temperature</code><code class="o">=</code><code class="mi">2</code><code class="p">))</code>
<code class="go">th no cce:</code>
<code class="go">yeolg-hormer firi. a play asks.</code>
<code class="go">fol rusb</code></pre>

<p>Apparently our Shakespeare model works best at a temperature close to 1. To generate more convincing text, you could try using more GRU layers and more neurons per layer, train for longer, and add some regularization (e.g., set <code>recurrent_dropout=0.3</code> in the <code>GRU</code> layers). Moreover, the model is currently incapable of learning patterns longer than <code>n_steps</code>, which is just 100 characters. You could try making this window larger, but it will also make training harder, and even <code>LSTM</code> and <code>GRU</code> cells cannot handle very long sequences. Alternatively, you could use a stateful RNN.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Stateful RNN"><div class="sect2" id="idm46263493920760">
<h2>Stateful RNN</h2>

<p>Until now, we have used only <em>stateless RNNs</em>: at each training iteration, the model starts with a hidden state full of 0s, then it updates this state at each time step, and after the last time step, it throws it away, as it is not needed anymore. What if we told the RNN to preserve this final state after processing one training batch and use it as the initial state for the next training batch? This way the model can learn long-term patterns despite only backpropagating through short sequences. This is called a <em>stateful RNN</em>. Let’s see how to build one.</p>

<p>First, note that a stateful RNN only makes sense if each input sequence in a batch starts exactly where the corresponding sequence in the previous batch left off. So the first thing we need to do to build a stateful RNN is to use sequential and nonoverlapping input sequences (rather than the shuffled and overlapping sequences we used to train stateless RNNs). When creating the <code>Dataset</code>, we must therefore use <code>shift=n_steps</code> (instead of <code>shift=1</code>) when calling the <code>window()</code> method. Moreover, we must obviously <em>not</em> call the <code>shuffle()</code> method. Unfortunately, batching is much harder when preparing a dataset for a stateful RNN than it is for a stateless RNN. Indeed, if we call <code>batch(32)</code>, then 32 consecutive windows will be put in the same batch, and the following batch would not continue each of these window where it left off. The first batch would contain windows 1 to 32, and second batch would contain windows 33 to 64, so if you consider, say, the first window of each batch (i.e., windows 1 and 33), you can see that they are not consecutive. The simplest solution to this problem is to just use “batches” containing a single window:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">dataset</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="o">.</code><code class="n">from_tensor_slices</code><code class="p">(</code><code class="n">encoded</code><code class="p">[:</code><code class="n">train_size</code><code class="p">])</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">window</code><code class="p">(</code><code class="n">window_length</code><code class="p">,</code> <code class="n">shift</code><code class="o">=</code><code class="n">n_steps</code><code class="p">,</code> <code class="n">drop_remainder</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">flat_map</code><code class="p">(</code><code class="k">lambda</code> <code class="n">window</code><code class="p">:</code> <code class="n">window</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="n">window_length</code><code class="p">))</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="k">lambda</code> <code class="n">windows</code><code class="p">:</code> <code class="p">(</code><code class="n">windows</code><code class="p">[:,</code> <code class="p">:</code><code class="o">-</code><code class="mi">1</code><code class="p">],</code> <code class="n">windows</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">:]))</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">map</code><code class="p">(</code>
    <code class="k">lambda</code> <code class="n">X_batch</code><code class="p">,</code> <code class="n">Y_batch</code><code class="p">:</code> <code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">one_hot</code><code class="p">(</code><code class="n">X_batch</code><code class="p">,</code> <code class="n">depth</code><code class="o">=</code><code class="n">max_id</code><code class="p">),</code> <code class="n">Y_batch</code><code class="p">))</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">prefetch</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code></pre>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#stateful_rnn_dataset_diagram">Figure&nbsp;16-2</a> summarizes the first steps.</p>

<figure><div id="stateful_rnn_dataset_diagram" class="figure">
<img src="./Chapter16_files/mls2_1602.png" alt="mls2 1602" width="1441" height="584" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1602.png">
<h6><span class="label">Figure 16-2. </span>Preparing a dataset of consecutive sequence fragments for a stateful RNN</h6>
</div></figure>

<p>Batching is harder, but it is not impossible. For example, we could chop Shakespeare’s text into 32 texts of equal length, create one dataset of consecutive input sequences for each of them, and finally use <code>tf.train.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows))</code> to create proper consecutive batches, where the <em>n</em><sup>th</sup> input sequence in a batch starts off exactly where the <em>n</em><sup>th</sup> input sequence ended in the previous batch (see the notebook for the full code).</p>

<p>Now let’s create the stateful RNN. First we need to set <code>stateful=True</code> when creating every recurrent layer. Second, the stateful RNN needs to know the batch size (since it will preserve a state for each input sequence in the batch), so we must set the <code>batch_input_shape</code> argument in the first layer. Note that we can leave the second dimension unspecified, since the inputs could have any length:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">stateful</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
                     <code class="n">dropout</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code> <code class="n">recurrent_dropout</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code>
                     <code class="n">batch_input_shape</code><code class="o">=</code><code class="p">[</code><code class="n">batch_size</code><code class="p">,</code> <code class="bp">None</code><code class="p">,</code> <code class="n">max_id</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">stateful</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
                     <code class="n">dropout</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code> <code class="n">recurrent_dropout</code><code class="o">=</code><code class="mf">0.2</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">TimeDistributed</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="n">max_id</code><code class="p">,</code>
                                                    <code class="n">activation</code><code class="o">=</code><code class="s2">"softmax"</code><code class="p">))</code>
<code class="p">])</code></pre>

<p>At the end of each epoch, we need to reset the states before we go back to the beginning of the text. For this, we can use a small callback:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">ResetStatesCallback</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">callbacks</code><code class="o">.</code><code class="n">Callback</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf">on_epoch_begin</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">epoch</code><code class="p">,</code> <code class="n">logs</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">reset_states</code><code class="p">()</code></pre>

<p>And now we can compile and fit the model (for more epochs, because each epoch is much shorter than earlier, and there is only one instance per batch):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"sparse_categorical_crossentropy"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s2">"adam"</code><code class="p">)</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">dataset</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">callbacks</code><code class="o">=</code><code class="p">[</code><code class="n">ResetStatesCallback</code><code class="p">()])</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>After this model is trained, it will only be possible to use it to make predictions for batches of the same size as were used during training. To avoid this restriction, create an identical model, except stateless, and copy the stateful model’s weights to the stateless model.</p>
</div>

<p>Now that we have built a character-level model, it’s time to look at word-level models and tackle a common natural language processing task: <em>sentiment analysis</em>. In the process we will learn how to handle sequences of variable lengths using masking.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Sentiment Analysis"><div class="sect1" id="idm46263495019032">
<h1>Sentiment Analysis</h1>

<p>If MNIST is the “hello world” of computer vision, then the IMDb reviews dataset is the “hello world” of natural language processing: it consists of 50,000 movie reviews in English (25,000 from training, 25,000 for testing) extracted from the famous <a href="https://imdb.com/">Internet Movie Database</a>, along with a simple binary target for each review, indicating whether it is negative (0) or positive (1). Just like MNIST, the IMDb reviews dataset is popular for good reasons: it is simple enough to be tackled on a laptop in a reasonable amount of time, but challenging enough to be fun and rewarding. Keras provides a simple function to load it:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">),</code> <code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">datasets</code><code class="o">.</code><code class="n">imdb</code><code class="o">.</code><code class="n">load_data</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_train</code><code class="p">[</code><code class="mi">0</code><code class="p">][:</code><code class="mi">10</code><code class="p">]</code>
<code class="go">[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]</code></pre>

<p>Where are the movie reviews? Well, as you can see, the dataset is already preprocessed for you: <code>X_train</code> consists of a list of reviews, each of which is represented as a NumPy array of integers, where each integer represents a word. All punctuation was removed, and then words were converted to lowercase, split by spaces, and finally indexed by frequency (so low integers correspond to frequent words). The integers 0, 1, and 2 are special: they represent the padding token, the <em>start of sequence</em> (SoS) token, and unknown words, respectively. If you want to visualize a review, you can decode it like this:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">word_index</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">datasets</code><code class="o">.</code><code class="n">imdb</code><code class="o">.</code><code class="n">get_word_index</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">id_to_word</code> <code class="o">=</code> <code class="p">{</code><code class="n">id_</code> <code class="o">+</code> <code class="mi">3</code><code class="p">:</code> <code class="n">word</code> <code class="k">for</code> <code class="n">word</code><code class="p">,</code> <code class="n">id_</code> <code class="ow">in</code> <code class="n">word_index</code><code class="o">.</code><code class="n">items</code><code class="p">()}</code>
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">id_</code><code class="p">,</code> <code class="n">token</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">((</code><code class="s">"&lt;pad&gt;"</code><code class="p">,</code> <code class="s">"&lt;sos&gt;"</code><code class="p">,</code> <code class="s">"&lt;unk&gt;"</code><code class="p">)):</code>
<code class="gp">... </code>    <code class="n">id_to_word</code><code class="p">[</code><code class="n">id_</code><code class="p">]</code> <code class="o">=</code> <code class="n">token</code>
<code class="gp">...</code>
<code class="gp">&gt;&gt;&gt; </code><code class="s">" "</code><code class="o">.</code><code class="n">join</code><code class="p">([</code><code class="n">id_to_word</code><code class="p">[</code><code class="n">id_</code><code class="p">]</code> <code class="k">for</code> <code class="n">id_</code> <code class="ow">in</code> <code class="n">X_train</code><code class="p">[</code><code class="mi">0</code><code class="p">][:</code><code class="mi">10</code><code class="p">]])</code>
<code class="go">'&lt;sos&gt; this film was just brilliant casting location scenery story'</code></pre>

<p>In a real project, you will have to preprocess the text yourself. You can do that using the same <code>Tokenizer</code> class we used earlier, but this time setting <code>char_level=False</code> (which is the default). When encoding words, it filters out a lot of characters, including most punctation, line breaks, and tabs (but you can change this by setting the <code>filters</code> argument). Most importantly, it uses spaces to identify word boundaries. This is OK for English and many other scripts (written languages) that use spaces between words, but not all scripts use spaces this way. Chinese does not use spaces between words, Vietnamese uses spaces even within words, and languages like German often glue multiple words together, without spaces. Even in English, spaces are not always the best way to tokenize text: think of “San Francisco” or “#ILoveDeepLearning”.</p>

<p>Fortunately, there are better options! The <a href="https://homl.info/subword">Subword Regularization paper</a><sup><a data-type="noteref" id="idm46263493401656-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263493401656" class="totri-footnote">4</a></sup> by Taku Kudo introduced an unsupervised learning technique to tokenize and detokenize text at the subword level in a language-independent way, treating spaces like other characters. With this approach, even if your model encounters a word it has never seen before, it can still reasonably guess what it means. For example, it may never have seen the word “smartest” during training, but perhaps it learned the word “smart” and it also learned that the suffix “est” means “the most”, so it can infer the meaning of “smartest”. Google’s <a href="https://github.com/google/sentencepiece"><em>SentencePiece</em></a> project provides an open source implementation, described in the <a href="https://homl.info/sentencepiece">SentencePiece paper</a><sup><a data-type="noteref" id="idm46263493398568-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263493398568" class="totri-footnote">5</a></sup> by Taku Kudo and John Richardson.</p>

<p>Another option was proposed in an earlier <a href="https://homl.info/rarewords">2015 paper</a><sup><a data-type="noteref" id="idm46263493396088-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263493396088" class="totri-footnote">6</a></sup> by Rico Sennrich et al. which explored other ways of creating subword encodings (e.g., using <em>Byte-Pair Encoding</em>). Last but not least, the TensorFlow team released the <a href="https://homl.info/tftext">TF.Text</a> library in June 2019, which implements various tokenization strategies, including <a href="https://homl.info/wordpiece">WordPiece</a><sup><a data-type="noteref" id="idm46263493393128-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263493393128" class="totri-footnote">7</a></sup> (a variant of BPE).</p>

<p>If you want to deploy your model to a mobile device, or a web browser, and you don’t want to have to write a different preprocessing function every time, then you will want to handle preprocessing using only TensorFlow operations, so it can be included in the model itself. Let’s see how. First, let’s load the original IMDb reviews, as text (byte strings), using TensorFlow Datasets (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch13.html#data_chapter">Chapter&nbsp;13</a>):</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">tensorflow_datasets</code> <code class="kn">as</code> <code class="nn">tfds</code>

<code class="n">datasets</code><code class="p">,</code> <code class="n">info</code> <code class="o">=</code> <code class="n">tfds</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"imdb_reviews"</code><code class="p">,</code> <code class="n">as_supervised</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">with_info</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">train_size</code> <code class="o">=</code> <code class="n">info</code><code class="o">.</code><code class="n">splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">num_examples</code></pre>

<p>Next, let’s write the preprocessing function:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">preprocess</code><code class="p">(</code><code class="n">X_batch</code><code class="p">,</code> <code class="n">y_batch</code><code class="p">):</code>
    <code class="n">X_batch</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">strings</code><code class="o">.</code><code class="n">substr</code><code class="p">(</code><code class="n">X_batch</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">300</code><code class="p">)</code>
    <code class="n">X_batch</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">strings</code><code class="o">.</code><code class="n">regex_replace</code><code class="p">(</code><code class="n">X_batch</code><code class="p">,</code> <code class="s-Affix">b</code><code class="s2">"&lt;br</code><code class="se">\\</code><code class="s2">s*/?&gt;"</code><code class="p">,</code> <code class="s-Affix">b</code><code class="s2">" "</code><code class="p">)</code>
    <code class="n">X_batch</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">strings</code><code class="o">.</code><code class="n">regex_replace</code><code class="p">(</code><code class="n">X_batch</code><code class="p">,</code> <code class="s-Affix">b</code><code class="s2">"[^a-zA-Z']"</code><code class="p">,</code> <code class="s-Affix">b</code><code class="s2">" "</code><code class="p">)</code>
    <code class="n">X_batch</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">strings</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">X_batch</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">X_batch</code><code class="o">.</code><code class="n">to_tensor</code><code class="p">(</code><code class="n">default_value</code><code class="o">=</code><code class="s-Affix">b</code><code class="s2">"&lt;pad&gt;"</code><code class="p">),</code> <code class="n">y_batch</code></pre>

<p>It starts by truncating the reviews, keeping only the first 300 characters of each: this will speed up training, and it won’t impact performance too much because you can generally tell whether a review is positive or not in the first sentence or two. Then it uses <em>regular expressions</em> to replace <code>&lt;br /&gt;</code> tags with spaces, and also replace any character other than letters and quotes with spaces. For example, the text <code>"Well, I can’t&lt;br /&gt;"</code> will become <code>"Well  I can’t"</code>. Finally, the <code>preprocess()</code> function splits the reviews by the spaces, which returns a ragged tensor, and it converts this ragged tensor to a dense tensor, padding all reviews with the padding token <code>"&lt;pad&gt;"</code> so that they all have the same length.</p>

<p>Next, we need to construct the vocabulary. This requires going through the whole training set once, applying our <code>preprocess()</code> function, and using a <code>Counter</code> to count the number of occurrences of each word:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">collections</code> <code class="kn">import</code> <code class="n">Counter</code>
<code class="n">vocabulary</code> <code class="o">=</code> <code class="n">Counter</code><code class="p">()</code>
<code class="k">for</code> <code class="n">X_batch</code><code class="p">,</code> <code class="n">y_batch</code> <code class="ow">in</code> <code class="n">datasets</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="mi">32</code><code class="p">)</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">preprocess</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">review</code> <code class="ow">in</code> <code class="n">X_batch</code><code class="p">:</code>
        <code class="n">vocabulary</code><code class="o">.</code><code class="n">update</code><code class="p">(</code><code class="nb">list</code><code class="p">(</code><code class="n">review</code><code class="o">.</code><code class="n">numpy</code><code class="p">()))</code></pre>

<p>Let’s look at the three most common words:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">vocabulary</code><code class="o">.</code><code class="n">most_common</code><code class="p">()[:</code><code class="mi">3</code><code class="p">]</code>
<code class="go">[(b'&lt;pad&gt;', 215797), (b'the', 61137), (b'a', 38564)]</code></pre>

<p>Great! Now we probably don’t need our model to know all words in the dictionary to get good performance, so let’s truncate the vocabulary, keeping only the 10,000 most common words:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">vocab_size</code> <code class="o">=</code> <code class="mi">10000</code>
<code class="n">truncated_vocabulary</code> <code class="o">=</code> <code class="p">[</code>
    <code class="n">word</code> <code class="k">for</code> <code class="n">word</code><code class="p">,</code> <code class="n">count</code> <code class="ow">in</code> <code class="n">vocabulary</code><code class="o">.</code><code class="n">most_common</code><code class="p">()[:</code><code class="n">vocab_size</code><code class="p">]]</code></pre>

<p>Now we need to add a preprocessing step to replace each word with its ID (i.e., its index in the vocabulary). Just like we did in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch13.html#data_chapter">Chapter&nbsp;13</a>, we will create a lookup table for this, using 1,000 out-of-vocabulary (oov) buckets:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">words</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">constant</code><code class="p">(</code><code class="n">truncated_vocabulary</code><code class="p">)</code>
<code class="n">word_ids</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">truncated_vocabulary</code><code class="p">),</code> <code class="n">dtype</code><code class="o">=</code><code class="n">tf</code><code class="o">.</code><code class="n">int64</code><code class="p">)</code>
<code class="n">vocab_init</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">lookup</code><code class="o">.</code><code class="n">KeyValueTensorInitializer</code><code class="p">(</code><code class="n">words</code><code class="p">,</code> <code class="n">word_ids</code><code class="p">)</code>
<code class="n">num_oov_buckets</code> <code class="o">=</code> <code class="mi">1000</code>
<code class="n">table</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">lookup</code><code class="o">.</code><code class="n">StaticVocabularyTable</code><code class="p">(</code><code class="n">vocab_init</code><code class="p">,</code> <code class="n">num_oov_buckets</code><code class="p">)</code></pre>

<p>For example, let’s use this table to look up the IDs of a few words:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">table</code><code class="o">.</code><code class="n">lookup</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">constant</code><code class="p">([</code><code class="n">b</code><code class="s">"This movie was faaaaaantastic"</code><code class="o">.</code><code class="n">split</code><code class="p">()]))</code>
<code class="go">&lt;tf.Tensor: [...], dtype=int64, numpy=array([[   22,    12,    11, 10054]])&gt;</code></pre>

<p>Note that the words “this”, “movie”, and “was” were found in the table, so their IDs are lower than 10,000, while the word “faaaaaantastic” was not found, so it was mapped to one of the oov buckets, with an ID greater than or equal to 10,000.</p>
<div data-type="tip"><h6>Tip</h6>
<p>TF Transform (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch13.html#data_chapter">Chapter&nbsp;13</a>) provides some useful functions to handle such vocabularies. For example, check out the <code>tft.compute_and_apply_vocabulary()</code> function: it will go through the dataset to find all distinct words and build the vocabulary, and it will generate the TF operations required to encode each word using this vocabulary.</p>
</div>

<p>Now we are ready to create the final training set. We batch the reviews, then  convert them to short sequences of words using the <code>preprocess()</code> function, then  encode these words using a simple <code>encode_words()</code> function that uses the table we just built, and finally prefetch the next batch:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">encode_words</code><code class="p">(</code><code class="n">X_batch</code><code class="p">,</code> <code class="n">y_batch</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">table</code><code class="o">.</code><code class="n">lookup</code><code class="p">(</code><code class="n">X_batch</code><code class="p">),</code> <code class="n">y_batch</code>

<code class="n">train_set</code> <code class="o">=</code> <code class="n">datasets</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="mi">32</code><code class="p">)</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">preprocess</code><code class="p">)</code>
<code class="n">train_set</code> <code class="o">=</code> <code class="n">train_set</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">encode_words</code><code class="p">)</code><code class="o">.</code><code class="n">prefetch</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code></pre>

<p>At last we can create the model and train it:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">embed_size</code> <code class="o">=</code> <code class="mi">128</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">vocab_size</code> <code class="o">+</code> <code class="n">num_oov_buckets</code><code class="p">,</code> <code class="n">embed_size</code><code class="p">,</code>
                           <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="mi">128</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">)</code>
<code class="p">])</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"binary_crossentropy"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s2">"adam"</code><code class="p">,</code>
              <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s2">"accuracy"</code><code class="p">])</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">train_set</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code></pre>

<p>The first layer is an <code>Embedding</code> layer, which will convert word IDs into embeddings (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch13.html#data_chapter">Chapter&nbsp;13</a>). The embedding matrix needs to have one row per word ID (<code>vocab_size + num_oov_buckets</code>) and one column per embedding dimension (we chose 128 dimensions, but this is a hyperparameter you could tune). Whereas the inputs of the model will be a 2D tensor of shape [batch size, time steps], the output of the <code>Embedding</code> layer will be a 3D tensor of shape [batch size, time steps, embedding size].</p>

<p>The rest of the model is fairly straightforward: it is composed of two GRU layers, with the second one returning only the output of the last time step. The output layer is just a single neuron using the sigmoid activation function to output the estimated probability that the review expresses a positive sentiment regarding the movie. We then compile the model quite simply, and we fit it on the dataset we prepared earlier, for a few epochs.</p>








<section data-type="sect2" data-pdf-bookmark="Masking"><div class="sect2" id="idm46263492750344">
<h2>Masking</h2>

<p>As it stands, the model will need to learn that the padding tokens should be ignored. But we already know that! Why don’t we tell the model to ignore the padding tokens, so that it can focus on the data that actually matters? It’s actually trivial: simply add <code>mask_zero=True</code> when creating the <code>Embedding</code> layer. This means that padding tokens (whose ID is 0)<sup><a data-type="noteref" id="idm46263492747624-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263492747624" class="totri-footnote">8</a></sup> will be ignored by all downstream layers. That’s all!</p>

<p>The way this works is that the <code>Embedding</code> layer creates a <em>mask tensor</em> equal to <code>K.not_equal(inputs, 0)</code> (where <code>K = keras.backend</code>): it is a boolean tensor with the same shape as the inputs, and it is equal to <code>False</code> anywhere the word IDs are 0, or <code>True</code> otherwise. This mask tensor is then automatically propagated by the model to all subsequent layers, as long as the time dimension is preserved. So in this example, both <code>GRU</code> layers will receive this mask automatically, but since the second <code>GRU</code> layer does not return sequences (it only returns the output of the last time step), the mask will not be transmitted to the <code>Dense</code> layer. Each layer may handle the mask differently, but in general they simply ignore masked time steps (i.e., time steps for which the mask is <code>False</code>). For example, when a recurrent layer encounters a masked time step, it simply copies the output from the previous time step. If the mask propagates all the way to the output (in models that output sequences, which is not the case in this example), then it will be applied to the losses as well, so the masked time steps will not contribute to the loss (their loss will be 0).</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>The <code>LSTM</code> and <code>GRU</code> layers have an optimized implementation for GPUs, based on Nvidia’s cuDNN library. However, this implementation does not support masking. If your model uses a mask, then these layers will fall back to the (much slower) default implementation. Note that the optimized implementation also requires you to use the default values for several hyperparameters: <code>activation</code>, <code>recurrent_activation</code>, <code>recurrent_dropout</code>, <code>unroll</code>, <code>use_bias</code>, and <code>reset_after</code>.</p>
</div>

<p>All layers that receive the mask must support masking (or else an exception will be raised). This includes all recurrent layers, as well as the <code>TimeDistributed</code> layer and a few other layers. Any layer that supports masking must have a <code>supports_masking</code> attribute equal to <code>True</code>. If you want to implement your own custom layer with masking support, you should add a <code>mask</code> argument to the <code>call()</code> method (and obviously make the method use the mask somehow). Moreover, you should set <code>self.supports_masking = True</code> in the constructor. If your layer does not start with an <code>Embedding</code> layer, you may use the <code>keras.layers.Masking</code> layer instead: it sets the mask to <code>K.any(K.not_equal(inputs, 0), axis=-1)</code>, meaning that time steps where the last dimension is full of 0s will be masked out in subsequent layers (again, as long as the time dimension exists).</p>

<p>Using masking layers and automatic mask propagation works best for simple sequential models, but it will not always work for more complex models, such as when you need to mix <code>Conv1D</code> layers with recurrent layers. In such cases, you will need to explicitly compute the mask and pass it to the appropriate layers, using either the Functional API or the Subclassing API. For example, the following model is identical to the previous model, except it is built using the Functional API and handles masking manually:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">K</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">backend</code>
<code class="n">inputs</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Input</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">])</code>
<code class="n">mask</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Lambda</code><code class="p">(</code><code class="k">lambda</code> <code class="n">inputs</code><code class="p">:</code> <code class="n">K</code><code class="o">.</code><code class="n">not_equal</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="mi">0</code><code class="p">))(</code><code class="n">inputs</code><code class="p">)</code>
<code class="n">z</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">vocab_size</code> <code class="o">+</code> <code class="n">num_oov_buckets</code><code class="p">,</code> <code class="n">embed_size</code><code class="p">)(</code><code class="n">inputs</code><code class="p">)</code>
<code class="n">z</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">)(</code><code class="n">z</code><code class="p">,</code> <code class="n">mask</code><code class="o">=</code><code class="n">mask</code><code class="p">)</code>
<code class="n">z</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="mi">128</code><code class="p">)(</code><code class="n">z</code><code class="p">,</code> <code class="n">mask</code><code class="o">=</code><code class="n">mask</code><code class="p">)</code>
<code class="n">outputs</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">)(</code><code class="n">z</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">Model</code><code class="p">(</code><code class="n">inputs</code><code class="o">=</code><code class="p">[</code><code class="n">inputs</code><code class="p">],</code> <code class="n">outputs</code><code class="o">=</code><code class="p">[</code><code class="n">outputs</code><code class="p">])</code></pre>

<p>After training this model for a few epochs, it will become quite good at judging whether a review is positive or not. If you use the <code>TensorBoard</code> callback, you can visualize the embeddings in TensorBoard as they are being learned: it is fascinating to see words like “awesome” and “amazing” gradually cluster on one side of the embedding space, while words like “awful” and “terrible” cluster on the other side. Some words are not as positive as you might expect (at least with this model), such as the word “good”, presumably because many negative reviews contain the phrase “not good”. It’s impressive that the model is able to learn useful word embeddings based on just 25,000 movie reviews. Imagine how good the embeddings would be if we had billions of reviews to train on! Unfortunately we don’t, but perhaps we can still reuse word embeddings trained on some large text corpus (e.g., Wikipedia articles), even if it is not movie reviews? After all, the word “amazing” generally has the same meaning whether you use it to talk about movies or anything else. Moreover, perhaps embeddings would be useful for sentiment analysis even if they were trained on another task: since words like “awesome” and “amazing” have a similar meaning, they will likely cluster in the embedding space, even for other tasks (e.g., just predicting the next word in a sentence). If all positive words and all negative words form clusters, then this will be helpful for sentiment analysis. So instead of using so many parameters to learn word embeddings, let’s see if we couldn’t just reuse pretrained embeddings.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Reusing Pretrained Embeddings"><div class="sect2" id="idm46263492729368">
<h2>Reusing Pretrained Embeddings</h2>

<p>The TensorFlow Hub project makes it easy to reuse pretrained model components in your own models. These model components are called <em>modules</em>. Simply browse the TF Hub repository at <a href="https://tfhub.dev/"><em class="hyperlink">https://tfhub.dev</em></a>, find the one you need, copy the code example into your project, and the module will be automatically downloaded, along with its pretrained weights, and included in your model. Easy!</p>

<p>For example, let’s use the <code>nnlm-en-dim50</code> sentence embedding module, version 1, in our sentiment analysis model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">tensorflow_hub</code> <code class="kn">as</code> <code class="nn">hub</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">hub</code><code class="o">.</code><code class="n">KerasLayer</code><code class="p">(</code><code class="s2">"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1"</code><code class="p">,</code>
                   <code class="n">dtype</code><code class="o">=</code><code class="n">tf</code><code class="o">.</code><code class="n">string</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[],</code> <code class="n">output_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">50</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">)</code>
<code class="p">])</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"binary_crossentropy"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s2">"adam"</code><code class="p">,</code>
              <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s2">"accuracy"</code><code class="p">])</code></pre>

<p>The <code>hub.KerasLayer</code> layer downloads the module from the given URL. This particular module is a <em>sentence encoder</em>: it takes strings as input and encodes each one as a single vector (in this case, a 50-dimensional vector). Internally, it parses the string (splitting words on spaces) and embeds each word using an embedding matrix that was pretrained on a huge corpus: the Google News 7B corpus (seven billion words long!). Then it computes the mean of all the word embeddings, and the result is the sentence embedding.<sup><a data-type="noteref" id="idm46263492517640-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263492517640" class="totri-footnote">9</a></sup> We can then add two simple <code>Dense</code> layers to create a good sentiment analysis model. By default, a <code>hub.KerasLayer</code> is not trainable, but you can set <code>trainable=True</code> when creating it to change that so that you can fine-tune it for your task.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Not all TF Hub modules support TensorFlow&nbsp;2, so make sure you choose a module that does.</p>
</div>

<p>Next, we can just load the IMDb reviews dataset—no need to preprocess it (except for batching and prefetching)—and directly train the model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">datasets</code><code class="p">,</code> <code class="n">info</code> <code class="o">=</code> <code class="n">tfds</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"imdb_reviews"</code><code class="p">,</code> <code class="n">as_supervised</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">with_info</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">train_size</code> <code class="o">=</code> <code class="n">info</code><code class="o">.</code><code class="n">splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">num_examples</code>
<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">32</code>
<code class="n">train_set</code> <code class="o">=</code> <code class="n">datasets</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="n">batch_size</code><code class="p">)</code><code class="o">.</code><code class="n">prefetch</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">train_set</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code></pre>

<p>Note that the last part of the TF Hub module URL specified that we wanted version 1 of the model. This versioning ensures that if a new module version is released, it will not break our model. Conveniently, if you just enter this URL in a web browser, you will get the documentation for this module. By default, TF Hub will cache the downloaded files into the local system’s temporary directory. You may prefer to download them into a more permanent directory to avoid having to download them again after every system cleanup. To do that, set the <code>TFHUB_CACHE_DIR</code> environment variable to the directory of your choice (e.g., <code>os.environ["TFHUB_CACHE_DIR"] = "./my_tfhub_cache"</code>).</p>

<p>So far, we have looked at time series, text generation using Char-RNN, and sentiment analysis using word-level RNN models, training our own word embeddings or reusing pretrained embeddings. Let’s now look at another important NLP task: <em>neural machine translation</em> (NMT), first using a pure Encoder–Decoder model, then improving it with attention mechanisms, and finally looking the extraordinary Transformer architecture.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="An Encoder–Decoder Network for Neural Machine Translation"><div class="sect1" id="idm46263492418904">
<h1>An Encoder–Decoder Network for Neural Machine Translation</h1>

<p>Let’s take a look at a simple <a href="https://homl.info/103">neural machine translation model</a><sup><a data-type="noteref" id="idm46263492372776-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263492372776">10</a></sup> that will translate English sentences to French (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#machine_translation_diagram">Figure&nbsp;16-3</a>).</p>

<figure><div id="machine_translation_diagram" class="figure">
<img src="./Chapter16_files/mls2_1603.png" alt="mls2 1603" width="1441" height="974" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1603.png">
<h6><span class="label">Figure 16-3. </span>A simple machine translation model</h6>
</div></figure>

<p>In short, the English sentences are fed to the encoder, and the decoder outputs the French translations. Note that the French translations are also used as inputs to the decoder, but shifted back by one step. In other words, the decoder is given as input the word that it <em>should</em> have output at the previous step (regardless of what it actually output). For the very first word, it is given the start-of-sequence (SOS) token. The decoder is expected to end the sentence with an end-of-sequence (EOS) token.</p>

<p>Note that the English sentences are reversed before they are fed to the encoder. For example “I drink milk” is reversed to “milk drink I.” This ensures that the beginning of the English sentence will be fed last to the encoder, which is useful because that’s generally the first thing that the decoder needs to translate.</p>

<p>Each word is initially represented by its ID (e.g., 288 for the word “milk”). Next, an embedding layer returns the word embedding. These word embeddings are what is actually fed to the encoder and the decoder.</p>

<p>At each step, the decoder outputs a score for each word in the output vocabulary (i.e., French), and then the Softmax layer turns these scores into probabilities. For example, at the first step the word “Je” may have a probability of 20%, “Tu” may have a probability of 1%, and so on. The word with the highest probability is output. This is very much like a regular classification task, so you can train the model using the <code>"sparse_categorical_crossentropy"</code> loss, much like we did in the Char-RNN model.</p>

<p>Note that at inference time (after training), you will not have the target sentence to feed to the decoder. Instead, simply feed the decoder the word that it output at the previous step, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#inference_decoder_diagram">Figure&nbsp;16-4</a> (this will require an embedding lookup that is not shown on the diagram).</p>

<figure class="smallerninety"><div id="inference_decoder_diagram" class="figure">
<img src="./Chapter16_files/mls2_1604.png" alt="mls2 1604" width="1223" height="552" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1604.png">
<h6><span class="label">Figure 16-4. </span>Feeding the previous output word as input at inference time</h6>
</div></figure>

<p>OK, now you have the big picture. Still, there are a few more details to handle if you implement this model:</p>

<ul>
<li>
<p>First, so far we have assumed that all input sequences (to the encoder and to the decoder) have a constant length. But obviously sentence lengths vary. Since regular tensors have fixed shapes, they can only contain sentences of the same length. You can use masking to handle this, as discussed earlier. However, if the sentences have very different lengths, you can’t just crop them like we did for sentiment analysis (because we want full translations, not cropped translations), so instead, group sentences into buckets of similar lengths (e.g., a bucket for the 1- to 6-word sentences, another for the 7- to 12-word sentences, and so on), using padding for the shorter sequences to ensure all sentences have the same length (check out the <code>tf.data.experimental.bucket_by_sequence_length()</code> function for this). For example “I drink milk” becomes “&lt;pad&gt; &lt;pad&gt; &lt;pad&gt; milk drink I”.</p>
</li>
<li>
<p>Second, we want to ignore any output past the EOS token, so these tokens should not contribute to the loss (they must be masked out). For example, if the model outputs “Je bois du lait &lt;eos&gt; oui”, the loss for the last word should be ignored.</p>
</li>
<li>
<p>Third, when the output vocabulary is large (which is the case here), outputting a probability for each and every possible word would be terribly slow. If the target vocabulary contains, say, 50,000 French words, then the decoder would output 50,000-dimensional vectors, and then computing the softmax function over such a large vector would be very computationally intensive. To avoid this, one solution is to look only at the logits output by the model for the correct word and for a random sample of incorrect words, then compute an approximation of the loss based only on these logits. This <em>sampled softmax</em> technique was <a href="https://homl.info/104">introduced in 2015 by Sébastien Jean et al</a>.<sup><a data-type="noteref" id="idm46263492307624-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263492307624">11</a></sup> In TensorFlow you can use the <code>tf.nn.sampled_softmax_loss()</code> function for this during training and use the normal softmax function at inference time (sampled softmax cannot be used at inference time because it requires knowing the target).</p>
</li>
</ul>

<p>The <em>TensorFlow Addons</em> project includes many sequence-to-sequence tools to let you easily build production-ready Encoder–Decoders. For example, the following code creates a basic Encoder–Decoder model, similar to the one represented in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#machine_translation_diagram">Figure&nbsp;16-3</a>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">tensorflow_addons</code> <code class="kn">as</code> <code class="nn">tfa</code>

<code class="n">encoder_inputs</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Input</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">],</code> <code class="n">dtype</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">int32</code><code class="p">)</code>
<code class="n">decoder_inputs</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Input</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">],</code> <code class="n">dtype</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">int32</code><code class="p">)</code>
<code class="n">sequence_lengths</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Input</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[],</code> <code class="n">dtype</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">int32</code><code class="p">)</code>

<code class="n">embeddings</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">vocab_size</code><code class="p">,</code> <code class="n">embed_size</code><code class="p">)</code>
<code class="n">encoder_embeddings</code> <code class="o">=</code> <code class="n">embeddings</code><code class="p">(</code><code class="n">encoder_inputs</code><code class="p">)</code>
<code class="n">decoder_embeddings</code> <code class="o">=</code> <code class="n">embeddings</code><code class="p">(</code><code class="n">decoder_inputs</code><code class="p">)</code>

<code class="n">encoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">LSTM</code><code class="p">(</code><code class="mi">512</code><code class="p">,</code> <code class="n">return_state</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">encoder_outputs</code><code class="p">,</code> <code class="n">state_h</code><code class="p">,</code> <code class="n">state_c</code> <code class="o">=</code> <code class="n">encoder</code><code class="p">(</code><code class="n">encoder_embeddings</code><code class="p">)</code>
<code class="n">encoder_state</code> <code class="o">=</code> <code class="p">[</code><code class="n">state_h</code><code class="p">,</code> <code class="n">state_c</code><code class="p">]</code>

<code class="n">sampler</code> <code class="o">=</code> <code class="n">tfa</code><code class="o">.</code><code class="n">seq2seq</code><code class="o">.</code><code class="n">sampler</code><code class="o">.</code><code class="n">TrainingSampler</code><code class="p">()</code>

<code class="n">decoder_cell</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">LSTMCell</code><code class="p">(</code><code class="mi">512</code><code class="p">)</code>
<code class="n">output_layer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="n">vocab_size</code><code class="p">)</code>
<code class="n">decoder</code> <code class="o">=</code> <code class="n">tfa</code><code class="o">.</code><code class="n">seq2seq</code><code class="o">.</code><code class="n">basic_decoder</code><code class="o">.</code><code class="n">BasicDecoder</code><code class="p">(</code><code class="n">decoder_cell</code><code class="p">,</code> <code class="n">sampler</code><code class="p">,</code>
                                                 <code class="n">output_layer</code><code class="o">=</code><code class="n">output_layer</code><code class="p">)</code>
<code class="n">final_outputs</code><code class="p">,</code> <code class="n">final_state</code><code class="p">,</code> <code class="n">final_sequence_lengths</code> <code class="o">=</code> <code class="n">decoder</code><code class="p">(</code>
    <code class="n">decoder_embeddings</code><code class="p">,</code> <code class="n">initial_state</code><code class="o">=</code><code class="n">encoder_state</code><code class="p">,</code>
    <code class="n">sequence_length</code><code class="o">=</code><code class="n">sequence_lengths</code><code class="p">)</code>
<code class="n">Y_proba</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">final_outputs</code><code class="o">.</code><code class="n">rnn_output</code><code class="p">)</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">Model</code><code class="p">(</code><code class="n">inputs</code><code class="o">=</code><code class="p">[</code><code class="n">encoder_inputs</code><code class="p">,</code> <code class="n">decoder_inputs</code><code class="p">,</code> <code class="n">sequence_lengths</code><code class="p">],</code>
                    <code class="n">outputs</code><code class="o">=</code><code class="p">[</code><code class="n">Y_proba</code><code class="p">])</code></pre>

<p>The code is mostly self-explanatory, but there are a few points to note: first, we set <code>return_state=True</code> when creating the <code>LSTM</code> layer so that we can get its final hidden state and pass it to the decoder. Since we are using an <code>LSTM</code> cell, it actually returns two hidden states (short term and long term). The <code>TrainingSampler</code> is one of several samplers available in TensorFlow Addons: their role is to tell the decoder at each step what it should pretend the previous output was. During inference, this should be the embedding of the token that was actually output. During training, it should be the embedding of the previous target token: this is why we used the <code>TrainingSampler</code>. In practice, it is often a good idea to start training with the embedding of the target of the previous time step and gradually transition to using the embedding of the actual token that was output at the previous step. This idea was introduced in a <a href="https://homl.info/scheduledsampling">2015 paper</a><sup><a data-type="noteref" id="idm46263492068936-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263492068936">12</a></sup> by Samy Bengio et al. The <code>ScheduledEmbeddingTrainingSampler</code> will randomly choose between the target or the actual output, with a probability that you can gradually change during training.</p>








<section data-type="sect2" data-pdf-bookmark="Bidirectional RNNs"><div class="sect2" id="idm46263492067208">
<h2>Bidirectional RNNs</h2>

<p>A each time step, a regular recurrent layer only looks at past and present inputs before generating its output. In other words, it is “causal,” meaning it cannot look into the future. This type of RNN makes sense when forecasting time series, but for many NLP tasks, such as Neural Machine Translation, it is often preferable to look ahead at the next words before encoding a given word. For example, consider the phrases “the Queen of the United Kingdom”, “the queen of hearts”, and “the queen bee”: to properly encode the word “queen”, you need to look ahead. To implement this, run two recurrent layers on the same inputs, one reading the words from left to right the other reading them from right to left. Then we simply combine their outputs at each time step, typically by concatenating them. This is called a <em>bidirectional recurrent layer</em> (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#bidirectional_rnn_diagram">Figure&nbsp;16-5</a>).</p>

<figure><div id="bidirectional_rnn_diagram" class="figure">
<img src="./Chapter16_files/mls2_1605.png" alt="mls2 1605" width="964" height="602" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1605.png">
<h6><span class="label">Figure 16-5. </span>A bidirectional recurrent layer</h6>
</div></figure>

<p>To implement a bidirectional recurrent layer in Keras, wrap a recurrent layer in a <code>keras.layers.Bidirectional</code> layer. For example, the following code creates a bidirectional GRU layer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Bidirectional</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">))</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The <code>Bidirectional</code> layer will create a clone of the <code>GRU</code> layer (but in the reverse direction), and it will run both and concatenate their outputs. So although the GRU layer has 10 units, the <code>Bidirectional</code> layer will output 20 values per time step.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Beam Search"><div class="sect2" id="idm46263492052328">
<h2>Beam Search</h2>

<p>Suppose you train an Encoder–Decoder model, and use it to translate the French sentence “Comment vas-tu?” to English. You are hoping that it will output the proper translation: “How are you?” but unfortunately it outputs “How will you?” Looking at the training set, you notice many sentences such as “Comment vas-tu jouer?” which translates to “How will you play?”. So it wasn’t absurd for the model to output “How will” after seeing “Comment vas”. Unfortunately, in this case it was a mistake, and the model could not go back and fix it, so it tried to complete the sentence as best it could. By greedily outputting the most likely word at every step, it ended up with a suboptimal translation. How can we give the model a chance to go back and fix mistakes it made earlier? One of the most common solutions is <em>beam search</em>: it keeps track of a short list of the <em>k</em> most promising sentences (say, the top three), and at each decoder step, it tries to extend them by one word, then it keeps only the <em>k</em> most likely sentences. The parameter <em>k</em> is called the <em>beam width</em>.</p>

<p>For example, suppose you use the model to translate the sentence “Comment vas-tu?” using beam search with a beam width of 3. At the first decoder step, the model will output an estimated probability for each possible word. Suppose the top three words are “How” (75% estimated probability), “What” (3%), and “You” (1%). That’s our short-list so far. Next, we create three copies of our model and use them to find the next word for each sentence. Each model will output one estimated probability per word in the vocabulary. The first model will try to find the next word in the sentence “How”, and perhaps it will output a probability of 36% for the word “will”, 32% for the word “are”, 16% for the word “do”, and so on. Note that these are actually <em>conditional</em> probabilities, given that the sentence starts with “How”. The second model will try to complete the sentence “What” and perhaps it will output a conditional probability of 50% for the word “are”, given that the first word is “What”, and so on. Assuming the vocabulary has 10,000 words, each model will output 10,000 probabilities.</p>

<p>Next, we compute the probabilities of each of the 30,000 two-word sentences that these models considered (3 × 10,000). We do this by multiplying the estimated conditional probability of each word by the estimated probability of the sentence it completes. For example, the estimated probability of the sentence “How” was 75%, while the estimated conditional probability of the word “will” (given that the first word is “How”) was 36%, so the estimated probability of the sentence “How will” is 75% × 36% = 27%. After computing the probabilities of all 30,000 two-word sentences, we keep only the top three. Perhaps they all start with the word “How”: “How will” (27%), “How are” (24%), and “How do” (12%). Right now, the sentence “How will” is winning, but “How are” has not been eliminated.</p>

<p>Then we repeat the same process: we use three models to predict the next word in these three sentences, we compute the probabilities of all 30,000 three-word sentences we considered, and perhaps the top three are now “How are you” (10%), “How do you” (8%), and “How will you” (2%). At the next step we may get “How do you do” (7%), “How are you &lt;eos&gt;” (6%) and “How are you doing” (3%). Notice that “How will” was eliminated, and we now have three perfectly reasonable translations. We boosted our Encoder–Decoder model’s performance without any extra training, simply by using it more wisely.</p>

<p>You can implement beam search fairly easily using TensorFlow Addons:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">beam_width</code> <code class="o">=</code> <code class="mi">10</code>
<code class="n">decoder</code> <code class="o">=</code> <code class="n">tfa</code><code class="o">.</code><code class="n">seq2seq</code><code class="o">.</code><code class="n">beam_search_decoder</code><code class="o">.</code><code class="n">BeamSearchDecoder</code><code class="p">(</code>
    <code class="n">cell</code><code class="o">=</code><code class="n">decoder_cell</code><code class="p">,</code> <code class="n">beam_width</code><code class="o">=</code><code class="n">beam_width</code><code class="p">,</code> <code class="n">output_layer</code><code class="o">=</code><code class="n">output_layer</code><code class="p">)</code>
<code class="n">decoder_initial_state</code> <code class="o">=</code> <code class="n">tfa</code><code class="o">.</code><code class="n">seq2seq</code><code class="o">.</code><code class="n">beam_search_decoder</code><code class="o">.</code><code class="n">tile_batch</code><code class="p">(</code>
    <code class="n">encoder_state</code><code class="p">,</code> <code class="n">multiplier</code><code class="o">=</code><code class="n">beam_width</code><code class="p">)</code>
<code class="n">outputs</code><code class="p">,</code> <code class="n">_</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">decoder</code><code class="p">(</code>
    <code class="n">embedding_decoder</code><code class="p">,</code> <code class="n">start_tokens</code><code class="o">=</code><code class="n">start_tokens</code><code class="p">,</code> <code class="n">end_token</code><code class="o">=</code><code class="n">end_token</code><code class="p">,</code>
    <code class="n">initial_state</code><code class="o">=</code><code class="n">decoder_initial_state</code><code class="p">)</code></pre>

<p>We first create a <code>BeamSearchDecoder</code>, which wraps all the decoder clones (in this case 10 clones). Then we create one copy of the encoder’s final state for each decoder clone, and we pass these states to the decoder, along with the start and end tokens.</p>

<p>With all this, you can get good translations for fairly short sentences (especially if you use pretrained word embeddings). Unfortunately, this model will be really bad at translating long sentences. Once again, the problem comes from the limited short-term memory of RNNs. <em>Attention mechanisms</em> are the game-changing innovation that addressed this problem.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Attention Mechanisms"><div class="sect1" id="idm46263492373720">
<h1>Attention Mechanisms</h1>

<p>Consider the path from the word “milk” to its translation “lait” in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#machine_translation_diagram">Figure&nbsp;16-3</a>: it is quite long! This means that a representation of this word (along with all the other words) needs to be carried over many steps before it is actually used. Can’t we make this path shorter?</p>

<p>This was the core idea in a groundbreaking <a href="https://homl.info/attention">2014 paper</a><sup><a data-type="noteref" id="idm46263491789288-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491789288">13</a></sup> by Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. They introduced a technique that allowed the decoder to focus on the appropriate words (as encoded by the encoder) at each time step. For example, at the time step where the decoder needs to output the word “lait”, it will focus its attention on the word “milk”. This means that the path from an input word to its translation is now much shorter, so the short-term memory limitations of RNNs have much less impact. Attention mechanisms revolutionized Neural Machine Translation (and NLP in general), allowing it to improve the state of the art significantly, especially for long sentences (over 30 words).<sup><a data-type="noteref" id="idm46263491787752-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491787752">14</a></sup></p>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#attention_diagram">Figure&nbsp;16-6</a> shows this model’s architecture (slightly simplified, as we will see). On the left, you have the encoder and the decoder. Instead of just sending the encoder’s final hidden state to the decoder (which is still done, although it is not shown in the figure), we now send all of its outputs to the decoder. At each time step, the decoder’s memory cell computes a weighted sum of all these encoder outputs: this determines which words it will focus on at this step. The weight <em>α</em><sub>(<em>t</em>,<em>i</em>)</sub> is the weight of the <em>i</em><sup>th</sup> encoder output at the <em>t</em><sup>th</sup> decoder time step. For example, if the weight <em>α</em><sub>(3,2)</sub> is much larger than the weights <em>α</em><sub>(3,0)</sub> and <em>α</em><sub>(3,1)</sub>, then the decoder will pay much more attention to the word number 2 (milk) than to the other two words, at least at this time step. The rest of the decoder works just like earlier: at each time step, the memory cell receives the inputs we just discussed, plus the hidden state from the previous time step, and finally (although it is not represented in the diagram) it receives the target word from the previous time step (or at inference time, the output from the previous time step).</p>

<figure><div id="attention_diagram" class="figure">
<img src="./Chapter16_files/mls2_1606.png" alt="mls2 1606" width="1439" height="910" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1606.png">
<h6><span class="label">Figure 16-6. </span>Neural Machine Translation using an Encoder–Decoder Network with an attention model</h6>
</div></figure>

<p>But where do these <em>α</em><sub>(<em>t</em>,<em>i</em>)</sub> weights come from? It’s actually pretty simple: they are generated by a type of small neural network called an <em>alignment model</em> (or an <em>attention layer</em>), which is trained jointly with the rest of the Encoder–Decoder model. This alignment model is illustrated on the righthand side of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#attention_diagram">Figure&nbsp;16-6</a>. It starts with a time-distributed <code>Dense</code> layer<sup><a data-type="noteref" id="idm46263491957992-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491957992">15</a></sup> with a single neuron, which receives as input all the encoder outputs, concatenated with the decoder’s previous hidden state (e.g., <strong>h</strong><sub>(2)</sub>). This layer outputs a score (or energy) for each encoder output (e.g., <em>e</em><sub>(<em>3</em>, <em>2</em>)</sub>): this score measures how well each output is aligned with the decoder’s previous hidden state. Finally, all the scores go through a softmax layer to get a final weight for each encoder output (e.g., <em>α</em><sub>(3,2)</sub>). All the weights for a given decoder time step add up to one (since the softmax layer is not time-distributed). This particular attention mechanism is called <em>Bahdanau attention</em> (named after the paper’s first author). Since it concatenates the encoder output with the decoder’s previous hidden state, it is sometimes called <em>concatenative attention</em> (or <em>additive attention</em>).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>If the input sentence is <em>n</em> words long, and assuming the output sentence is about as long, then this model will need to compute about <em>n</em><sup>2</sup> weights. Fortunately, this quadratic computational complexity is still tractable because even long sentences don’t have thousands of words.</p>
</div>

<p>Another common attention mechanism was proposed shortly after in a <a href="https://homl.info/luongattention">2015 paper</a><sup><a data-type="noteref" id="idm46263491947800-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491947800">16</a></sup> by Minh-Thang Luong et al. Because the goal of the attention mechanism is to measure the similarity between one of the encoder’s outputs and the decoder’s previous hidden state, the authors proposed to simply compute the <em>dot product</em> (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>) of these two vectors, as this is often a fairly good similarity measure, and modern hardware can compute it much faster. For this to be possible, both vectors must have the same dimensionality. This is called <em>Luong attention</em> (again, after the paper’s first author), or sometimes <em>multiplicative attention</em>. The dot product gives a score, and all the scores (at a given decoder time step) go through a softmax layer to give the final weights, just like in Bahdanau attention. Another simplification they proposed was to use the decoder’s hidden state at the current time step rather than at the previous time step (i.e., <strong>h</strong><sub>(t)</sub> rather than <strong>h</strong><sub>(t–1)</sub>), then to use the output of the attention mechanism (noted <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-148-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;bold h overTilde Subscript left-parenthesis t right-parenthesis&quot;&gt;&lt;msub&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;&amp;#x1D421;&lt;/mi&gt;&lt;mo&gt;&amp;#x2DC;&lt;/mo&gt;&lt;/mover&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-6529" aria-label="bold h overTilde Subscript left-parenthesis t right-parenthesis" style="width: 1.596em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.545em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.031em, 1001.55em, 2.625em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-6530"><span class="msub" id="MathJax-Span-6531"><span style="display: inline-block; position: relative; width: 1.545em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mover" id="MathJax-Span-6532"><span style="display: inline-block; position: relative; width: 0.62em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6533" style="font-family: MathJax_Main-bold;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1000.58em, 3.55em, -1000.01em); top: -4.213em; left: 0.054em;"><span class="mo" id="MathJax-Span-6534" style=""><span style="font-family: MathJax_Size1;">˜</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6535"><span class="mo" id="MathJax-Span-6536" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6537" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6538" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.421em; border-left: 0px solid; width: 0px; height: 1.432em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="bold h overTilde Subscript left-parenthesis t right-parenthesis"><msub><mover accent="true"><mi>𝐡</mi><mo>˜</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></math></span></span><script type="math/mml" id="MathJax-Element-148"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="bold h overTilde Subscript left-parenthesis t right-parenthesis">
  <msub><mover accent="true"><mi>𝐡</mi> <mo>˜</mo></mover> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
</math></script>) directly to compute the decoder’s predictions (rather than using it to compute the decoder’s current hidden state). They also proposed a variant of the dot product mechanism where the encoder outputs first go through a linear transformation (i.e., a time-distributed <code>Dense</code> layer without a bias term) before the dot products are computed. This is called the “general” dot product approach. They compared both dot product approaches to the concatenative attention mechanism (adding a rescaling parameter vector <strong>v</strong>), and they observed that the dot product variants performed better than concatenative attention. For this reason, concatenative attention is much less used now. The equations for these three attention mechanisms are summarized in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#attention_mechanisms_equation">Equation 16-1</a>.</p>
<div id="attention_mechanisms_equation" data-type="equation">
<h5><span class="label">Equation 16-1. </span>Attention mechanisms</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-149-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;StartLayout 1st Row 1st Column bold h overTilde Subscript left-parenthesis t right-parenthesis 2nd Column equals sigma-summation Underscript i Endscripts alpha Subscript left-parenthesis t comma i right-parenthesis Baseline bold y Subscript left-parenthesis i right-parenthesis Baseline 2nd Row 1st Column with alpha Subscript left-parenthesis t comma i right-parenthesis Baseline 2nd Column equals StartFraction exp left-parenthesis e Subscript left-parenthesis t comma i right-parenthesis Baseline right-parenthesis Over sigma-summation Underscript i prime Endscripts exp left-parenthesis e Subscript left-parenthesis t comma i Sub Superscript prime Subscript right-parenthesis Baseline right-parenthesis EndFraction 3rd Row 1st Column and e Subscript left-parenthesis t comma i right-parenthesis Baseline 2nd Column equals StartLayout Enlarged left-brace 1st Row 1st Column bold h Subscript left-parenthesis t right-parenthesis Baseline Superscript upper T Baseline bold y Subscript left-parenthesis i right-parenthesis Baseline 2nd Column d o t 2nd Row 1st Column bold h Subscript left-parenthesis t right-parenthesis Baseline Superscript upper T Baseline bold upper W bold y Subscript left-parenthesis i right-parenthesis Baseline 2nd Column g e n e r a l 3rd Row 1st Column bold v Superscript upper T Baseline hyperbolic tangent left-parenthesis bold upper W left-bracket bold h Subscript left-parenthesis t right-parenthesis Baseline semicolon bold y Subscript left-parenthesis i right-parenthesis Baseline right-bracket right-parenthesis 2nd Column c o n c a t EndLayout EndLayout&quot; display=&quot;block&quot;&gt;&lt;mtable displaystyle=&quot;true&quot;&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;msub&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;&amp;#x1D421;&lt;/mi&gt;&lt;mo&gt;&amp;#x2DC;&lt;/mo&gt;&lt;/mover&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B1;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D432;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;with&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B1;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo form=&quot;prefix&quot;&gt;exp&lt;/mo&gt;&lt;mfenced separators=&quot;&quot; open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;&#39;&lt;/mo&gt;&lt;/msup&gt;&lt;/munder&gt;&lt;mo form=&quot;prefix&quot;&gt;exp&lt;/mo&gt;&lt;mfenced separators=&quot;&quot; open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;&#39;&lt;/mo&gt;&lt;/msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;and&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;msub&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced separators=&quot;&quot; open=&quot;{&quot; close=&quot;&quot;&gt;&lt;mtable&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D421;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D432;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D421;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mi&gt;&amp;#x1D416;&lt;/mi&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D432;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x1D42F;&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mo form=&quot;prefix&quot;&gt;tanh&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x1D416;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D421;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D432;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-6539" aria-label="StartLayout 1st Row 1st Column bold h overTilde Subscript left-parenthesis t right-parenthesis 2nd Column equals sigma-summation Underscript i Endscripts alpha Subscript left-parenthesis t comma i right-parenthesis Baseline bold y Subscript left-parenthesis i right-parenthesis Baseline 2nd Row 1st Column with alpha Subscript left-parenthesis t comma i right-parenthesis Baseline 2nd Column equals StartFraction exp left-parenthesis e Subscript left-parenthesis t comma i right-parenthesis Baseline right-parenthesis Over sigma-summation Underscript i prime Endscripts exp left-parenthesis e Subscript left-parenthesis t comma i Sub Superscript prime Subscript right-parenthesis Baseline right-parenthesis EndFraction 3rd Row 1st Column and e Subscript left-parenthesis t comma i right-parenthesis Baseline 2nd Column equals StartLayout Enlarged left-brace 1st Row 1st Column bold h Subscript left-parenthesis t right-parenthesis Baseline Superscript upper T Baseline bold y Subscript left-parenthesis i right-parenthesis Baseline 2nd Column d o t 2nd Row 1st Column bold h Subscript left-parenthesis t right-parenthesis Baseline Superscript upper T Baseline bold upper W bold y Subscript left-parenthesis i right-parenthesis Baseline 2nd Column g e n e r a l 3rd Row 1st Column bold v Superscript upper T Baseline hyperbolic tangent left-parenthesis bold upper W left-bracket bold h Subscript left-parenthesis t right-parenthesis Baseline semicolon bold y Subscript left-parenthesis i right-parenthesis Baseline right-bracket right-parenthesis 2nd Column c o n c a t EndLayout EndLayout" style="width: 23.139em; display: inline-block;"><span style="display: inline-block; position: relative; width: 22.471em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(-3.596em, 1022.12em, 7.355em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-6540"><span class="mtable" id="MathJax-Span-6541" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 22.162em; height: 0px;"><span style="position: absolute; clip: rect(4.167em, 1004.69em, 13.319em, -1000.01em); top: -9.869em; left: 0em;"><span style="display: inline-block; position: relative; width: 4.733em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1001.5em, 4.527em, -1000.01em); top: -8.635em; right: 0em;"><span class="mtd" id="MathJax-Span-6542"><span class="mrow" id="MathJax-Span-6543"><span class="msub" id="MathJax-Span-6544"><span style="display: inline-block; position: relative; width: 1.545em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mover" id="MathJax-Span-6545"><span style="display: inline-block; position: relative; width: 0.62em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6546" style="font-family: MathJax_Main-bold;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1000.58em, 3.55em, -1000.01em); top: -4.213em; left: 0.054em;"><span class="mo" id="MathJax-Span-6547" style=""><span style="font-family: MathJax_Size1;">˜</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6548"><span class="mo" id="MathJax-Span-6549" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6550" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6551" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1004.69em, 4.527em, -1000.01em); top: -5.345em; right: 0em;"><span class="mtd" id="MathJax-Span-6573"><span class="mrow" id="MathJax-Span-6574"><span class="mrow" id="MathJax-Span-6575"><span class="mspace" id="MathJax-Span-6576" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-6577" style="font-family: MathJax_Main;">with</span><span class="mspace" id="MathJax-Span-6578" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="msub" id="MathJax-Span-6579"><span style="display: inline-block; position: relative; width: 1.956em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6580" style="font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6581"><span class="mo" id="MathJax-Span-6582" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6583" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6584" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-6585" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-6586" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1004.23em, 4.527em, -1000.01em); top: -1.077em; right: 0em;"><span class="mtd" id="MathJax-Span-6627"><span class="mrow" id="MathJax-Span-6628"><span class="mrow" id="MathJax-Span-6629"><span class="mspace" id="MathJax-Span-6630" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-6631" style="font-family: MathJax_Main;">and</span><span class="mspace" id="MathJax-Span-6632" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="msub" id="MathJax-Span-6633"><span style="display: inline-block; position: relative; width: 1.802em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.42em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6634" style="font-family: MathJax_Math-italic;">e</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.465em;"><span class="mrow" id="MathJax-Span-6635"><span class="mo" id="MathJax-Span-6636" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6637" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6638" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-6639" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-6640" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 9.874em;"></span></span><span style="position: absolute; clip: rect(5.915em, 1016.46em, 16.866em, -1000.01em); top: -11.617em; left: 5.504em;"><span style="display: inline-block; position: relative; width: 16.661em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1006.08em, 5.35em, -1000.01em); top: -8.635em; left: 0em;"><span class="mtd" id="MathJax-Span-6552"><span class="mrow" id="MathJax-Span-6553"><span class="mrow" id="MathJax-Span-6554"><span class="mo" id="MathJax-Span-6555" style="font-family: MathJax_Main;">=</span><span class="munder" id="MathJax-Span-6556" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1001.4em, 4.63em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-6557" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1000.22em, 4.27em, -1000.01em); top: -2.928em; left: 0.62em;"><span class="mi" id="MathJax-Span-6558" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-6559" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.956em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6560" style="font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6561"><span class="mo" id="MathJax-Span-6562" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6563" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6564" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-6565" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-6566" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-6567"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6568" style="font-family: MathJax_Main-bold;">y</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.751em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6569"><span class="mo" id="MathJax-Span-6570" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6571" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-6572" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.162em, 1008.03em, 5.35em, -1000.01em); top: -5.345em; left: 0em;"><span class="mtd" id="MathJax-Span-6587"><span class="mrow" id="MathJax-Span-6588"><span class="mrow" id="MathJax-Span-6589"><span class="mo" id="MathJax-Span-6590" style="font-family: MathJax_Main;">=</span><span class="mstyle" id="MathJax-Span-6591" style="padding-left: 0.26em;"><span class="mrow" id="MathJax-Span-6592"><span class="mfrac" id="MathJax-Span-6593"><span style="display: inline-block; position: relative; width: 6.738em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(2.985em, 1004.69em, 4.527em, -1000.01em); top: -4.83em; left: 50%; margin-left: -2.414em;"><span class="mrow" id="MathJax-Span-6594"><span class="mo" id="MathJax-Span-6595" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">exp</span><span class="mfenced" id="MathJax-Span-6596"><span class="mo" id="MathJax-Span-6597" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="msub" id="MathJax-Span-6598"><span style="display: inline-block; position: relative; width: 1.802em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.42em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6599" style="font-family: MathJax_Math-italic;">e</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.465em;"><span class="mrow" id="MathJax-Span-6600"><span class="mo" id="MathJax-Span-6601" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6602" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6603" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-6604" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-6605" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6606" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1006.44em, 4.578em, -1000.01em); top: -3.185em; left: 50%; margin-left: -3.288em;"><span class="mrow" id="MathJax-Span-6607"><span class="munder" id="MathJax-Span-6608"><span style="display: inline-block; position: relative; width: 1.545em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1000.99em, 4.424em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-6609" style="font-family: MathJax_Size1; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.699em; left: 1.082em;"><span class="msup" id="MathJax-Span-6610"><span style="display: inline-block; position: relative; width: 0.414em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.22em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6611" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.265em; left: 0.26em;"><span class="mo" id="MathJax-Span-6612" style="font-size: 50%; font-family: MathJax_Main;">′</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6613" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">exp</span><span class="mfenced" id="MathJax-Span-6614"><span class="mo" id="MathJax-Span-6615" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="msub" id="MathJax-Span-6616"><span style="display: inline-block; position: relative; width: 1.956em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.42em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6617" style="font-family: MathJax_Math-italic;">e</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.465em;"><span class="mrow" id="MathJax-Span-6618"><span class="mo" id="MathJax-Span-6619" style=""><span><span style="font-size: 70.7%; font-family: MathJax_Main;">(</span></span></span><span class="mi" id="MathJax-Span-6620" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6621" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="msup" id="MathJax-Span-6622"><span style="display: inline-block; position: relative; width: 0.414em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.22em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6623" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.265em; left: 0.26em;"><span class="mo" id="MathJax-Span-6624" style="font-size: 50%; font-family: MathJax_Main;">′</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6625" style=""><span><span style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6626" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1006.75em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 6.738em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1016.46em, 8.023em, -1000.01em); top: -2.825em; left: 0em;"><span class="mtd" id="MathJax-Span-6641"><span class="mrow" id="MathJax-Span-6642"><span class="mrow" id="MathJax-Span-6643"><span class="mo" id="MathJax-Span-6644" style="font-family: MathJax_Main;">=</span><span class="mfenced" id="MathJax-Span-6645" style="padding-left: 0.26em;"><span class="mo" id="MathJax-Span-6646" style="vertical-align: 2.625em;"><span style="display: inline-block; position: relative; width: 0.877em; height: 0px;"><span style="position: absolute; font-family: MathJax_Size4; top: -3.134em; left: 0em;">⎧<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Size4; top: -0.152em; left: 0em;">⎩<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Size4; top: -1.386em; left: 0em;">⎨<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Size4; position: absolute; top: -2.877em; left: 0em;">⎪<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Size4; position: absolute; top: -2.671em; left: 0em;">⎪<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Size4; position: absolute; top: -2.465em; left: 0em;">⎪<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Size4; position: absolute; top: -0.512em; left: 0em;">⎪<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Size4; position: absolute; top: -0.306em; left: 0em;">⎪<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Size4; position: absolute; top: -0.1em; left: 0em;">⎪<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mtable" id="MathJax-Span-6647" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 14.399em; height: 0px;"><span style="position: absolute; clip: rect(2.985em, 1010.19em, 8.023em, -1000.01em); top: -5.756em; left: 0em;"><span style="display: inline-block; position: relative; width: 10.337em; height: 0px;"><span style="position: absolute; clip: rect(2.985em, 1003.71em, 4.578em, -1000.01em); top: -5.756em; left: 0em;"><span class="mtd" id="MathJax-Span-6648"><span class="mrow" id="MathJax-Span-6649"><span class="mrow" id="MathJax-Span-6650"><span class="msup" id="MathJax-Span-6651"><span style="display: inline-block; position: relative; width: 2.111em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.5em, 4.527em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6652"><span class="msub" id="MathJax-Span-6653"><span style="display: inline-block; position: relative; width: 1.545em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6654" style="font-family: MathJax_Main-bold;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6655"><span class="mo" id="MathJax-Span-6656" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6657" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6658" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 1.545em;"><span class="mi" id="MathJax-Span-6659" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mspace" id="MathJax-Span-6660" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="msub" id="MathJax-Span-6661"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6662" style="font-family: MathJax_Main-bold;">y</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.751em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6663"><span class="mo" id="MathJax-Span-6664" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6665" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-6666" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1005.1em, 4.578em, -1000.01em); top: -4.008em; left: 0em;"><span class="mtd" id="MathJax-Span-6673"><span class="mrow" id="MathJax-Span-6674"><span class="mrow" id="MathJax-Span-6675"><span class="msup" id="MathJax-Span-6676"><span style="display: inline-block; position: relative; width: 2.111em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.5em, 4.527em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6677"><span class="msub" id="MathJax-Span-6678"><span style="display: inline-block; position: relative; width: 1.545em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6679" style="font-family: MathJax_Main-bold;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6680"><span class="mo" id="MathJax-Span-6681" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6682" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6683" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 1.545em;"><span class="mi" id="MathJax-Span-6684" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mspace" id="MathJax-Span-6685" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-6686" style="font-family: MathJax_Main-bold;">W</span><span class="mspace" id="MathJax-Span-6687" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="msub" id="MathJax-Span-6688"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6689" style="font-family: MathJax_Main-bold;">y</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.751em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6690"><span class="mo" id="MathJax-Span-6691" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6692" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-6693" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1010.19em, 4.578em, -1000.01em); top: -2.311em; left: 0em;"><span class="mtd" id="MathJax-Span-6704"><span class="mrow" id="MathJax-Span-6705"><span class="mrow" id="MathJax-Span-6706"><span class="msup" id="MathJax-Span-6707"><span style="display: inline-block; position: relative; width: 1.185em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6708" style="font-family: MathJax_Main-bold;">v</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.62em;"><span class="mi" id="MathJax-Span-6709" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6710" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">tanh</span><span class="mrow" id="MathJax-Span-6711"><span class="mo" id="MathJax-Span-6712" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="mi" id="MathJax-Span-6713" style="font-family: MathJax_Main-bold;">W</span><span class="mrow" id="MathJax-Span-6714" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-6715" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">[</span></span><span class="msub" id="MathJax-Span-6716"><span style="display: inline-block; position: relative; width: 1.545em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6717" style="font-family: MathJax_Main-bold;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6718"><span class="mo" id="MathJax-Span-6719" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6720" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6721" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6722" style="font-family: MathJax_Main;">;</span><span class="msub" id="MathJax-Span-6723" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6724" style="font-family: MathJax_Main-bold;">y</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.751em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6725"><span class="mo" id="MathJax-Span-6726" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6727" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-6728" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6729" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">]</span></span></span><span class="mo" id="MathJax-Span-6730" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 5.761em;"></span></span><span style="position: absolute; clip: rect(2.573em, 1003.25em, 6.995em, -1000.01em); top: -5.139em; left: 11.108em;"><span style="display: inline-block; position: relative; width: 3.293em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.35em, 4.167em, -1000.01em); top: -5.756em; left: 0em;"><span class="mtd" id="MathJax-Span-6667"><span class="mrow" id="MathJax-Span-6668"><span class="mrow" id="MathJax-Span-6669"><span class="mi" id="MathJax-Span-6670" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-6671" style="font-family: MathJax_Math-italic;">o</span><span class="mi" id="MathJax-Span-6672" style="font-family: MathJax_Math-italic;">t</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1003.25em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mtd" id="MathJax-Span-6694"><span class="mrow" id="MathJax-Span-6695"><span class="mrow" id="MathJax-Span-6696"><span class="mi" id="MathJax-Span-6697" style="font-family: MathJax_Math-italic;">g<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-6698" style="font-family: MathJax_Math-italic;">e</span><span class="mi" id="MathJax-Span-6699" style="font-family: MathJax_Math-italic;">n</span><span class="mi" id="MathJax-Span-6700" style="font-family: MathJax_Math-italic;">e</span><span class="mi" id="MathJax-Span-6701" style="font-family: MathJax_Math-italic;">r</span><span class="mi" id="MathJax-Span-6702" style="font-family: MathJax_Math-italic;">a</span><span class="mi" id="MathJax-Span-6703" style="font-family: MathJax_Math-italic;">l</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.242em, 1002.79em, 4.167em, -1000.01em); top: -2.311em; left: 0em;"><span class="mtd" id="MathJax-Span-6731"><span class="mrow" id="MathJax-Span-6732"><span class="mrow" id="MathJax-Span-6733"><span class="mi" id="MathJax-Span-6734" style="font-family: MathJax_Math-italic;">c</span><span class="mi" id="MathJax-Span-6735" style="font-family: MathJax_Math-italic;">o</span><span class="mi" id="MathJax-Span-6736" style="font-family: MathJax_Math-italic;">n</span><span class="mi" id="MathJax-Span-6737" style="font-family: MathJax_Math-italic;">c</span><span class="mi" id="MathJax-Span-6738" style="font-family: MathJax_Math-italic;">a</span><span class="mi" id="MathJax-Span-6739" style="font-family: MathJax_Math-italic;">t</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 5.144em;"></span></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 5.761em;"></span></span></span><span style="display: inline-block; width: 0px; height: 11.622em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -5.293em; border-left: 0px solid; width: 0px; height: 11.071em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="StartLayout 1st Row 1st Column bold h overTilde Subscript left-parenthesis t right-parenthesis 2nd Column equals sigma-summation Underscript i Endscripts alpha Subscript left-parenthesis t comma i right-parenthesis Baseline bold y Subscript left-parenthesis i right-parenthesis Baseline 2nd Row 1st Column with alpha Subscript left-parenthesis t comma i right-parenthesis Baseline 2nd Column equals StartFraction exp left-parenthesis e Subscript left-parenthesis t comma i right-parenthesis Baseline right-parenthesis Over sigma-summation Underscript i prime Endscripts exp left-parenthesis e Subscript left-parenthesis t comma i Sub Superscript prime Subscript right-parenthesis Baseline right-parenthesis EndFraction 3rd Row 1st Column and e Subscript left-parenthesis t comma i right-parenthesis Baseline 2nd Column equals StartLayout Enlarged left-brace 1st Row 1st Column bold h Subscript left-parenthesis t right-parenthesis Baseline Superscript upper T Baseline bold y Subscript left-parenthesis i right-parenthesis Baseline 2nd Column d o t 2nd Row 1st Column bold h Subscript left-parenthesis t right-parenthesis Baseline Superscript upper T Baseline bold upper W bold y Subscript left-parenthesis i right-parenthesis Baseline 2nd Column g e n e r a l 3rd Row 1st Column bold v Superscript upper T Baseline hyperbolic tangent left-parenthesis bold upper W left-bracket bold h Subscript left-parenthesis t right-parenthesis Baseline semicolon bold y Subscript left-parenthesis i right-parenthesis Baseline right-bracket right-parenthesis 2nd Column c o n c a t EndLayout EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mover accent="true"><mi>𝐡</mi><mo>˜</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd><mtd columnalign="left"><mrow><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>α</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub><msub><mi>𝐲</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mrow></mtd></mtr><mtr><mtd columnalign="right"><mrow><mspace width="4.pt"></mspace><mtext>with</mtext><mspace width="4.pt"></mspace><msub><mi>α</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub></mrow></mtd><mtd columnalign="left"><mrow><mo>=</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mo form="prefix">exp</mo><mfenced separators="" open="(" close=")"><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub></mfenced></mrow><mrow><munder><mo>∑</mo><msup><mi>i</mi><mo>'</mo></msup></munder><mo form="prefix">exp</mo><mfenced separators="" open="(" close=")"><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><msup><mi>i</mi><mo>'</mo></msup><mo>)</mo></mrow></msub></mfenced></mrow></mfrac></mstyle></mrow></mtd></mtr><mtr><mtd columnalign="right"><mrow><mspace width="4.pt"></mspace><mtext>and</mtext><mspace width="4.pt"></mspace><msub><mi>e</mi><mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow></msub></mrow></mtd><mtd columnalign="left"><mrow><mo>=</mo><mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><msup><mrow><msub><mi>𝐡</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mrow><mi>T</mi></msup><mspace width="0.166667em"></mspace><msub><mi>𝐲</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mrow></mtd><mtd columnalign="left"><mrow><mi>d</mi><mi>o</mi><mi>t</mi></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><msup><mrow><msub><mi>𝐡</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mrow><mi>T</mi></msup><mspace width="0.166667em"></mspace><mi>𝐖</mi><mspace width="0.166667em"></mspace><msub><mi>𝐲</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mrow></mtd><mtd columnalign="left"><mrow><mi>g</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>l</mi></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><msup><mi>𝐯</mi><mi>T</mi></msup><mo form="prefix">tanh</mo><mrow><mo>(</mo><mi>𝐖</mi><mrow><mo>[</mo><msub><mi>𝐡</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>;</mo><msub><mi>𝐲</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>]</mo></mrow><mo>)</mo></mrow></mrow></mtd><mtd columnalign="left"><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>a</mi><mi>t</mi></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math></span></span></div><script type="math/mml" id="MathJax-Element-149"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="StartLayout 1st Row 1st Column bold h overTilde Subscript left-parenthesis t right-parenthesis 2nd Column equals sigma-summation Underscript i Endscripts alpha Subscript left-parenthesis t comma i right-parenthesis Baseline bold y Subscript left-parenthesis i right-parenthesis Baseline 2nd Row 1st Column with alpha Subscript left-parenthesis t comma i right-parenthesis Baseline 2nd Column equals StartFraction exp left-parenthesis e Subscript left-parenthesis t comma i right-parenthesis Baseline right-parenthesis Over sigma-summation Underscript i prime Endscripts exp left-parenthesis e Subscript left-parenthesis t comma i Sub Superscript prime Subscript right-parenthesis Baseline right-parenthesis EndFraction 3rd Row 1st Column and e Subscript left-parenthesis t comma i right-parenthesis Baseline 2nd Column equals StartLayout Enlarged left-brace 1st Row 1st Column bold h Subscript left-parenthesis t right-parenthesis Baseline Superscript upper T Baseline bold y Subscript left-parenthesis i right-parenthesis Baseline 2nd Column d o t 2nd Row 1st Column bold h Subscript left-parenthesis t right-parenthesis Baseline Superscript upper T Baseline bold upper W bold y Subscript left-parenthesis i right-parenthesis Baseline 2nd Column g e n e r a l 3rd Row 1st Column bold v Superscript upper T Baseline hyperbolic tangent left-parenthesis bold upper W left-bracket bold h Subscript left-parenthesis t right-parenthesis Baseline semicolon bold y Subscript left-parenthesis i right-parenthesis Baseline right-bracket right-parenthesis 2nd Column c o n c a t EndLayout EndLayout" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <msub><mover accent="true"><mi>𝐡</mi> <mo>˜</mo></mover> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <munder><mo>∑</mo> <mi>i</mi> </munder>
          <msub><mi>α</mi> <mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow> </msub>
          <msub><mi>𝐲</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msub>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mspace width="4.pt"></mspace>
          <mtext>with</mtext>
          <mspace width="4.pt"></mspace>
          <msub><mi>α</mi> <mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow> </msub>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mrow><mo form="prefix">exp</mo><mfenced separators="" open="(" close=")"><msub><mi>e</mi> <mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow> </msub></mfenced></mrow> <mrow><munder><mo>∑</mo> <msup><mi>i</mi> <mo>'</mo> </msup> </munder><mo form="prefix">exp</mo><mfenced separators="" open="(" close=")"><msub><mi>e</mi> <mrow><mo>(</mo><mi>t</mi><mo>,</mo><msup><mi>i</mi> <mo>'</mo> </msup><mo>)</mo></mrow> </msub></mfenced></mrow></mfrac>
          </mstyle>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mspace width="4.pt"></mspace>
          <mtext>and</mtext>
          <mspace width="4.pt"></mspace>
          <msub><mi>e</mi> <mrow><mo>(</mo><mi>t</mi><mo>,</mo><mi>i</mi><mo>)</mo></mrow> </msub>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mfenced separators="" open="{" close="">
            <mtable>
              <mtr>
                <mtd columnalign="left">
                  <mrow>
                    <msup><mrow><msub><mi>𝐡</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub></mrow> <mi>T</mi> </msup>
                    <mspace width="0.166667em"></mspace>
                    <msub><mi>𝐲</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msub>
                  </mrow>
                </mtd>
                <mtd columnalign="left">
                  <mrow>
                    <mi>d</mi>
                    <mi>o</mi>
                    <mi>t</mi>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd columnalign="left">
                  <mrow>
                    <msup><mrow><msub><mi>𝐡</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub></mrow> <mi>T</mi> </msup>
                    <mspace width="0.166667em"></mspace>
                    <mi>𝐖</mi>
                    <mspace width="0.166667em"></mspace>
                    <msub><mi>𝐲</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msub>
                  </mrow>
                </mtd>
                <mtd columnalign="left">
                  <mrow>
                    <mi>g</mi>
                    <mi>e</mi>
                    <mi>n</mi>
                    <mi>e</mi>
                    <mi>r</mi>
                    <mi>a</mi>
                    <mi>l</mi>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd columnalign="left">
                  <mrow>
                    <msup><mi>𝐯</mi> <mi>T</mi> </msup>
                    <mo form="prefix">tanh</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>𝐖</mi>
                      <mrow>
                        <mo>[</mo>
                        <msub><mi>𝐡</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
                        <mo>;</mo>
                        <msub><mi>𝐲</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msub>
                        <mo>]</mo>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mtd>
                <mtd columnalign="left">
                  <mrow>
                    <mi>c</mi>
                    <mi>o</mi>
                    <mi>n</mi>
                    <mi>c</mi>
                    <mi>a</mi>
                    <mi>t</mi>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></script>
</div>

<p>Here is how you can add Luong attention to an Encoder–Decoder model using TensorFlow Addons:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">attention_mechanism</code> <code class="o">=</code> <code class="n">tfa</code><code class="o">.</code><code class="n">seq2seq</code><code class="o">.</code><code class="n">attention_wrapper</code><code class="o">.</code><code class="n">LuongAttention</code><code class="p">(</code>
    <code class="n">units</code><code class="p">,</code> <code class="n">encoder_state</code><code class="p">,</code> <code class="n">memory_sequence_length</code><code class="o">=</code><code class="n">encoder_sequence_length</code><code class="p">)</code>
<code class="n">attention_decoder_cell</code> <code class="o">=</code> <code class="n">tfa</code><code class="o">.</code><code class="n">seq2seq</code><code class="o">.</code><code class="n">attention_wrapper</code><code class="o">.</code><code class="n">AttentionWrapper</code><code class="p">(</code>
    <code class="n">decoder_cell</code><code class="p">,</code> <code class="n">attention_mechanism</code><code class="p">,</code> <code class="n">attention_layer_size</code><code class="o">=</code><code class="n">n_units</code><code class="p">)</code></pre>

<p>We simply wrap the decoder cell in an <code>AttentionWrapper</code>, and we provide the desired attention mechanism (Luong attention in this example).</p>








<section data-type="sect2" data-pdf-bookmark="Visual Attention"><div class="sect2" id="idm46263491658840">
<h2>Visual Attention</h2>

<p>Attention mechanisms are now used far beyond Machine Translation. One of their first applications beyond NMT was in generating image captions using <a href="https://homl.info/visualattention">visual attention</a>:<sup><a data-type="noteref" id="idm46263491656584-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491656584">17</a></sup> a convolutional neural network first processes the image and outputs some feature maps, then a decoder RNN equipped with an attention mechanism generates the caption, one word at a time. At each decoder time step (each word), the decoder uses the attention model to focus on just the right part of the image. For example, in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#visual_attention_diagram">Figure&nbsp;16-7</a>,<sup><a data-type="noteref" id="idm46263491654600-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491654600">18</a></sup> the model generated the caption “A woman is throwing a frisbee in a park”, and you can see what part of the input image the decoder focused its attention on when is was about to output the word “frisbee”: clearly, most of its attention was focused on the frisbee.</p>

<figure><div id="visual_attention_diagram" class="figure">
<img src="./Chapter16_files/mls2_1607.png" alt="mls2 1607" width="1439" height="689" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1607.png">
<h6><span class="label">Figure 16-7. </span>Visual attention: an input image (left) and the model’s focus before producing the word “frisbee” (right)</h6>
</div></figure>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46263491651304">
<h5>Explainability</h5>
<p>One extra benefit of attention mechanisms is that they make it easier to understand what led the model to produce its output. This is called <em>explainability</em>. It can be especially useful when the model makes a mistake: for example, if an image of a dog walking in the snow is labelled as “a wolf walking in the snow”, then you can go back and check what the model focused on when it output the word “wolf”: perhaps you will find that it was paying attention not only to the dog, but also to the snow, hinting at a possible explanation: perhaps the way the model learned to distinguish dogs from wolves is by checking whether or not there’s a lot of snow around. You can then fix this by training the model with more images of wolves without snow, and dogs with snow. This example comes from a great <a href="https://homl.info/explainclass">2016 paper</a><sup><a data-type="noteref" id="idm46263491648152-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491648152">19</a></sup> by Marco Tulio Ribeiro et al. that uses a different approach to explainability: they learn an interpretable model locally around a classifier’s prediction. In some applications, explainability is not just a tool to debug a model; it can be a legal requirement (think of a system deciding whether or not it should grant you a loan).</p>
</div></aside>

<p>Attention mechanisms are so powerful that you can actually build state-of-the-art models using only attention mechanisms.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Attention Is All You Need: the Transformer Architecture"><div class="sect2" id="idm46263491646136">
<h2>Attention Is All You Need: the Transformer Architecture</h2>

<p>In a groundbreaking <a href="https://homl.info/transformer">2017 paper</a>,<sup><a data-type="noteref" id="idm46263491643960-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491643960">20</a></sup> a team of Google researchers suggested that “Attention Is All You Need.” They managed to create an architecture called the <em>Transformer</em>, which significantly improved the state of the art in NMT without using any recurrent layer or convolutional layers:<sup><a data-type="noteref" id="idm46263491642472-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491642472">21</a></sup> just attention mechanisms (plus embedding layers, dense layers, normalization layers and a few other bits and pieces). As an extra bonus, this architecture was also much faster to train and easier to parallelize, so they managed to train it at a fraction of the time and cost of the previous state-of-the-art models.</p>

<p>The Transformer architecture is represented in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#transformer_diagram">Figure&nbsp;16-8</a>.</p>

<figure class="smallerseventy"><div id="transformer_diagram" class="figure">
<img src="./Chapter16_files/mls2_1608.png" alt="mls2 1608" width="1440" height="2119" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1608.png">
<h6><span class="label">Figure 16-8. </span>The Transformer architecture<sup><a data-type="noteref" id="idm46263491637912-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491637912">22</a></sup></h6>
</div></figure>

<p>Let’s walk through this figure:</p>

<ul>
<li>
<p>The lefthand part is the encoder. Just like earlier, it takes as input a batch of sentences represented as sequences of word IDs (the input shape is <code>[batch_size, n_steps_in]</code>, where <code>n_steps_in</code> is the maximum input sentence length), and it encodes each word into a 512-dimensional representation (so the encoder’s output shape is <code>[batch_size, n_steps_in, 512]</code>). Note that the top part of the encoder is stacked N times (in the paper, N&nbsp;=&nbsp;6).</p>
</li>
<li>
<p>The righthand part is the decoder. During training, it takes the target sentence as input (also represented as a sequence of word IDs), shifted one time step to the right (i.e., a start-of-sequence token is inserted at the beginning). It also receives the outputs of the encoder (i.e., the arrows coming from the left side). Note that the top part of the decoder is also stacked N times, and the encoder stack’s final outputs are fed to the decoder at each of these N levels. Just like earlier, the decoder outputs a probability for each possible next word, at each time step (its output shape is <code>[batch_size, n_steps_out, vocab_size]</code>, where <code>n_steps_out</code> is the maximum output sentence length).</p>
</li>
<li>
<p>During inference, the decoder cannot be fed targets, so we feed it the previously output words (starting with a start-of-sequence token). So the model needs to be called repeatedly, predicting one more word at every round (which is fed to the decoder at the next round, until the end-of-sequence token is output).</p>
</li>
<li>
<p>Looking more closely, you can see that you are already familiar with most components: there are two embedding layers, 5 × N skip connections, each of them followed by a Layer normalization layer, 2 × N “Feed Forward” modules that are composed of two dense layers each (the first one using the ReLU activation function, the second with no activation function), and finally the output layer is a dense layer using the softmax activation function. All of these layers are time-distributed, so each word is treated independently of all the others. But how can we translate a sentence by only looking at one word at a time? Well, that’s where the new components come in:</p>

<ul>
<li>
<p>The encoder’s <em>Multi-Head Attention</em> layer encodes each word’s relationship with every other word in the same sentence, paying more attention to the most relevant ones. For example, the output of this layer for the word “Queen” in the sentence “They welcomed the Queen of the United Kingdom” will depend on all the words in the sentence, but it will probably pay more attention to the words “United” and “Kingdom” than to the words “They” or “welcomed”. This attention mechanism is called <em>self-attention</em> (the sentence is paying attention to itself). We will discuss exactly how it works shortly. The decoder’s <em>Masked Multi-Head Attention</em> layer does the same thing, but each word is only allowed to attend to words located before it. Finally, the decoder’s upper Multi-Head Attention layer is where the decoder pays attention to the words in the input sentence. For example, the decoder will probably pay close attention to the word “Queen” in the input sentence when it is about to output this word’s translation.</p>
</li>
<li>
<p>The <em>positional embeddings</em> are simply dense vectors (much like word embeddings) that represent the position of a word in the sentence. The <em>n</em><sup>th</sup> positional embedding is added to the word embedding of the <em>n</em><sup>th</sup> word in each sentence. This gives the model access to each word’s position, which is needed because the Multi-Head Attention layers do not consider the order or the position of the words; they only look at their relationships. Since all the other layers are time-distributed, they have no way of knowing the position of each word (either relative or absolute). Obviously, the relative and absolute word positions are important, so we need to give this information to the Transformer somehow, and positional embeddings are a good way to do this.</p>
</li>
</ul>
</li>
</ul>

<p>Let’s look a bit closer at both these novel components of the Transformer architecture, starting with the positional embeddings.</p>










<section data-type="sect3" data-pdf-bookmark="Positional embeddings"><div class="sect3" id="idm46263491620952">
<h3>Positional embeddings</h3>

<p>A positional embedding is a dense vector which encodes the position of a word within a sentence: the <em>i</em><sup>th</sup> positional embedding is simply added to the word embedding of the <em>i</em><sup>th</sup> word in the sentence. These positional embeddings can be learned by the model, but in the paper the authors preferred to use fixed positional embeddings, defined using the sine and cosine functions of different frequencies. The positional embedding matrix <strong>P</strong> is defined in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#positional_embeddings_equation">Equation 16-2</a> and represented at the bottom of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#positional_embeddings_diagram">Figure&nbsp;16-9</a> (transposed), where <em>P</em><sub><em>p</em>,<em>i</em></sub> is the <em>i</em><sup>th</sup> component of the embedding for the word located at the <em>p</em><sup>th</sup> position in the sentence.</p>
<div id="positional_embeddings_equation" data-type="equation">
<h5><span class="label">Equation 16-2. </span>Sine/cosine positional embeddings</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-150-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;StartLayout 1st Row 1st Column upper P Subscript p comma 2 i 2nd Column equals sine left-parenthesis p slash 10000 Superscript 2 i slash d Baseline right-parenthesis 2nd Row 1st Column upper P Subscript p comma 2 i plus 1 2nd Column equals cosine left-parenthesis p slash 10000 Superscript 2 i slash d Baseline right-parenthesis EndLayout&quot; display=&quot;block&quot;&gt;&lt;mtable displaystyle=&quot;true&quot;&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo form=&quot;prefix&quot;&gt;sin&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;10000&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo form=&quot;prefix&quot;&gt;cos&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;10000&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-6740" aria-label="StartLayout 1st Row 1st Column upper P Subscript p comma 2 i 2nd Column equals sine left-parenthesis p slash 10000 Superscript 2 i slash d Baseline right-parenthesis 2nd Row 1st Column upper P Subscript p comma 2 i plus 1 2nd Column equals cosine left-parenthesis p slash 10000 Superscript 2 i slash d Baseline right-parenthesis EndLayout" style="width: 13.113em; display: inline-block;"><span style="display: inline-block; position: relative; width: 12.702em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(-0.306em, 1012.35em, 4.013em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-6741"><span class="mtable" id="MathJax-Span-6742" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 12.393em; height: 0px;"><span style="position: absolute; clip: rect(2.265em, 1002.79em, 5.761em, -1000.01em); top: -4.213em; left: 0em;"><span style="display: inline-block; position: relative; width: 2.779em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.86em, 4.476em, -1000.01em); top: -5.139em; right: 0em;"><span class="mtd" id="MathJax-Span-6743"><span class="mrow" id="MathJax-Span-6744"><span class="msub" id="MathJax-Span-6745"><span style="display: inline-block; position: relative; width: 1.853em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.73em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6746" style="font-family: MathJax_Math-italic;">P<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6747"><span class="mi" id="MathJax-Span-6748" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-6749" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-6750" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span class="mi" id="MathJax-Span-6751" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1002.79em, 4.476em, -1000.01em); top: -2.877em; right: 0em;"><span class="mtd" id="MathJax-Span-6768"><span class="mrow" id="MathJax-Span-6769"><span class="msub" id="MathJax-Span-6770"><span style="display: inline-block; position: relative; width: 2.779em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.73em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6771" style="font-family: MathJax_Math-italic;">P<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6772"><span class="mi" id="MathJax-Span-6773" style="font-size: 70.7%; font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-6774" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-6775" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span class="mi" id="MathJax-Span-6776" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-6777" style="font-size: 70.7%; font-family: MathJax_Main;">+</span><span class="mn" id="MathJax-Span-6778" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.219em;"></span></span><span style="position: absolute; clip: rect(2.625em, 1008.65em, 6.944em, -1000.01em); top: -5.036em; left: 3.55em;"><span style="display: inline-block; position: relative; width: 8.846em; height: 0px;"><span style="position: absolute; clip: rect(2.728em, 1008.55em, 4.836em, -1000.01em); top: -5.139em; left: 0em;"><span class="mtd" id="MathJax-Span-6752"><span class="mrow" id="MathJax-Span-6753"><span class="mrow" id="MathJax-Span-6754"><span class="mo" id="MathJax-Span-6755" style="font-family: MathJax_Main;">=</span><span class="mo" id="MathJax-Span-6756" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">sin</span><span class="mo" id="MathJax-Span-6757" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">(</span></span><span class="mi" id="MathJax-Span-6758" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-6759" style="font-family: MathJax_Main;">/</span><span class="msup" id="MathJax-Span-6760"><span style="display: inline-block; position: relative; width: 3.91em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1002.48em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-6761" style="font-family: MathJax_Main;">10000</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 2.522em;"><span class="mrow" id="MathJax-Span-6762"><span class="mn" id="MathJax-Span-6763" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span class="mi" id="MathJax-Span-6764" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-6765" style="font-size: 70.7%; font-family: MathJax_Main;">/</span><span class="mi" id="MathJax-Span-6766" style="font-size: 70.7%; font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6767" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.728em, 1008.65em, 4.836em, -1000.01em); top: -2.877em; left: 0em;"><span class="mtd" id="MathJax-Span-6779"><span class="mrow" id="MathJax-Span-6780"><span class="mrow" id="MathJax-Span-6781"><span class="mo" id="MathJax-Span-6782" style="font-family: MathJax_Main;">=</span><span class="mo" id="MathJax-Span-6783" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">cos</span><span class="mo" id="MathJax-Span-6784" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">(</span></span><span class="mi" id="MathJax-Span-6785" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-6786" style="font-family: MathJax_Main;">/</span><span class="msup" id="MathJax-Span-6787"><span style="display: inline-block; position: relative; width: 3.91em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1002.48em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mn" id="MathJax-Span-6788" style="font-family: MathJax_Main;">10000</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 2.522em;"><span class="mrow" id="MathJax-Span-6789"><span class="mn" id="MathJax-Span-6790" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span class="mi" id="MathJax-Span-6791" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-6792" style="font-size: 70.7%; font-family: MathJax_Main;">/</span><span class="mi" id="MathJax-Span-6793" style="font-size: 70.7%; font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6794" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 5.041em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.851em; border-left: 0px solid; width: 0px; height: 4.239em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="StartLayout 1st Row 1st Column upper P Subscript p comma 2 i 2nd Column equals sine left-parenthesis p slash 10000 Superscript 2 i slash d Baseline right-parenthesis 2nd Row 1st Column upper P Subscript p comma 2 i plus 1 2nd Column equals cosine left-parenthesis p slash 10000 Superscript 2 i slash d Baseline right-parenthesis EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi>P</mi><mrow><mi>p</mi><mo>,</mo><mn>2</mn><mi>i</mi></mrow></msub></mtd><mtd columnalign="left"><mrow><mo>=</mo><mo form="prefix">sin</mo><mo>(</mo><mi>p</mi><mo>/</mo><msup><mn>10000</mn><mrow><mn>2</mn><mi>i</mi><mo>/</mo><mi>d</mi></mrow></msup><mo>)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right"><msub><mi>P</mi><mrow><mi>p</mi><mo>,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign="left"><mrow><mo>=</mo><mo form="prefix">cos</mo><mo>(</mo><mi>p</mi><mo>/</mo><msup><mn>10000</mn><mrow><mn>2</mn><mi>i</mi><mo>/</mo><mi>d</mi></mrow></msup><mo>)</mo></mrow></mtd></mtr></mtable></math></span></span></div><script type="math/mml" id="MathJax-Element-150"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="StartLayout 1st Row 1st Column upper P Subscript p comma 2 i 2nd Column equals sine left-parenthesis p slash 10000 Superscript 2 i slash d Baseline right-parenthesis 2nd Row 1st Column upper P Subscript p comma 2 i plus 1 2nd Column equals cosine left-parenthesis p slash 10000 Superscript 2 i slash d Baseline right-parenthesis EndLayout" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <msub><mi>P</mi> <mrow><mi>p</mi><mo>,</mo><mn>2</mn><mi>i</mi></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mo form="prefix">sin</mo>
          <mo>(</mo>
          <mi>p</mi>
          <mo>/</mo>
          <msup><mn>10000</mn> <mrow><mn>2</mn><mi>i</mi><mo>/</mo><mi>d</mi></mrow> </msup>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <msub><mi>P</mi> <mrow><mi>p</mi><mo>,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mo form="prefix">cos</mo>
          <mo>(</mo>
          <mi>p</mi>
          <mo>/</mo>
          <msup><mn>10000</mn> <mrow><mn>2</mn><mi>i</mi><mo>/</mo><mi>d</mi></mrow> </msup>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></script>
</div>

<figure><div id="positional_embeddings_diagram" class="figure">
<img src="./Chapter16_files/mls2_1609.png" alt="mls2 1609" width="1441" height="784" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1609.png">
<h6><span class="label">Figure 16-9. </span>Sine/cosine positional embedding matrix (transposed, bottom) with a focus on two values of <em>i</em> (top)</h6>
</div></figure>

<p>This solution gives the same performance as learned positional embeddings do, but it can extend to arbitrarily long sentences, which is why it’s favored. After the positional embeddings are added to the word embeddings, the rest of the model has access to the absolute position of each word in the sentence because there is a unique positional embedding for each position (e.g., the positional embedding for the word located at the 22nd position in a sentence is represented by the vertical dashed line at the bottom left of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#positional_embeddings_diagram">Figure&nbsp;16-9</a>, and you can see that it is unique to that position). Moreover, the choice of oscillating functions (sine and cosine) makes it possible for the model to learn relative positions as well. For example, words located 38 words apart (e.g., at positions <em>p</em>&nbsp;=&nbsp;22 and <em>p</em>&nbsp;=&nbsp;60) always have the same positional embedding values in the embedding dimensions <em>i</em>&nbsp;=&nbsp;100 and <em>i</em>&nbsp;=&nbsp;101, as you can see in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#positional_embeddings_diagram">Figure&nbsp;16-9</a>. This explains why we need both the sine and the cosine for each frequency: if we only used the sine (the blue wave at <em>i</em>&nbsp;=&nbsp;100), the model would not be able to distinguish positions <em>p</em>&nbsp;=&nbsp;25 and <em>p</em>&nbsp;=&nbsp;35 (marked by a cross).</p>

<p>There is no <code>PositionalEmbedding</code> layer in TensorFlow, but it is easy to create one. For efficiency reasons, we precompute the positional embedding matrix in the constructor (so we need to know the maximum sentence length, <code>max_steps</code>, and the number of dimensions for each word representation, <code>max_dims</code>). Then the <code>call()</code> method crops this embedding matrix to the size of the inputs, and it adds it to the inputs. Since we added an extra first dimension of size 1 when creating the positional embedding matrix, the rules of broadcasting will ensure that the matrix gets added to every sentence in the inputs:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">PositionalEncoding</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Layer</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">max_steps</code><code class="p">,</code> <code class="n">max_dims</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">tf</code><code class="o">.</code><code class="n">float32</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="nf-Magic">__init__</code><code class="p">(</code><code class="n">dtype</code><code class="o">=</code><code class="n">dtype</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code><code class="p">)</code>
        <code class="k">if</code> <code class="n">max_dims</code> <code class="o">%</code> <code class="mi">2</code> <code class="o">==</code> <code class="mi">1</code><code class="p">:</code> <code class="n">max_dims</code> <code class="o">+=</code> <code class="mi">1</code> <code class="c1"># max_dims must be even</code>
        <code class="n">p</code><code class="p">,</code> <code class="n">i</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">meshgrid</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="n">max_steps</code><code class="p">),</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="n">max_dims</code> <code class="o">//</code> <code class="mi">2</code><code class="p">))</code>
        <code class="n">pos_emb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">empty</code><code class="p">((</code><code class="mi">1</code><code class="p">,</code> <code class="n">max_steps</code><code class="p">,</code> <code class="n">max_dims</code><code class="p">))</code>
        <code class="n">pos_emb</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="p">:,</code> <code class="p">::</code><code class="mi">2</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sin</code><code class="p">(</code><code class="n">p</code> <code class="o">/</code> <code class="mi">10000</code><code class="o">**</code><code class="p">(</code><code class="mi">2</code> <code class="o">*</code> <code class="n">i</code> <code class="o">/</code> <code class="n">max_dims</code><code class="p">))</code><code class="o">.</code><code class="n">T</code>
        <code class="n">pos_emb</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="p">:,</code> <code class="mi">1</code><code class="p">::</code><code class="mi">2</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">cos</code><code class="p">(</code><code class="n">p</code> <code class="o">/</code> <code class="mi">10000</code><code class="o">**</code><code class="p">(</code><code class="mi">2</code> <code class="o">*</code> <code class="n">i</code> <code class="o">/</code> <code class="n">max_dims</code><code class="p">))</code><code class="o">.</code><code class="n">T</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">positional_embedding</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">constant</code><code class="p">(</code><code class="n">pos_emb</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dtype</code><code class="p">))</code>
    <code class="k">def</code> <code class="nf">call</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">):</code>
        <code class="n">shape</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">shape</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">inputs</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">positional_embedding</code><code class="p">[:,</code> <code class="p">:</code><code class="n">shape</code><code class="p">[</code><code class="o">-</code><code class="mi">2</code><code class="p">],</code> <code class="p">:</code><code class="n">shape</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]]</code></pre>

<p>Now we can create the first layers of the Transformer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">embed_size</code> <code class="o">=</code> <code class="mi">512</code><code class="p">;</code> <code class="n">max_steps</code> <code class="o">=</code> <code class="mi">500</code><code class="p">;</code> <code class="n">vocab_size</code> <code class="o">=</code> <code class="mi">10000</code>
<code class="n">encoder_inputs</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Input</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">],</code> <code class="n">dtype</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">int32</code><code class="p">)</code>
<code class="n">decoder_inputs</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Input</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">],</code> <code class="n">dtype</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">int32</code><code class="p">)</code>
<code class="n">embeddings</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">vocab_size</code><code class="p">,</code> <code class="n">embed_size</code><code class="p">)</code>
<code class="n">encoder_embeddings</code> <code class="o">=</code> <code class="n">embeddings</code><code class="p">(</code><code class="n">encoder_inputs</code><code class="p">)</code>
<code class="n">decoder_embeddings</code> <code class="o">=</code> <code class="n">embeddings</code><code class="p">(</code><code class="n">decoder_inputs</code><code class="p">)</code>
<code class="n">positional_encoding</code> <code class="o">=</code> <code class="n">PositionalEncoding</code><code class="p">(</code><code class="n">max_steps</code><code class="p">,</code> <code class="n">max_dims</code><code class="o">=</code><code class="n">embed_size</code><code class="p">)</code>
<code class="n">encoder_in</code> <code class="o">=</code> <code class="n">positional_encoding</code><code class="p">(</code><code class="n">encoder_embeddings</code><code class="p">)</code>
<code class="n">decoder_in</code> <code class="o">=</code> <code class="n">positional_encoding</code><code class="p">(</code><code class="n">decoder_embeddings</code><code class="p">)</code></pre>

<p>Now let’s look deeper into the heart of the Transformer model: the Multi-Head Attention layer.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Multi-Head Attention"><div class="sect3" id="idm46263491620328">
<h3>Multi-Head Attention</h3>

<p>To understand how a Multi-Head Attention layer works, we must first understand the <em>Scaled Dot-Product Attention</em> layer, which it is based on. Let’s suppose the encoder analyzed the input sentence “They played chess”, and it managed to understand that the word “They” is the subject and the word “played” is the verb, so it encoded this information in the representations of these words. Now suppose the decoder has already translated the subject, and it thinks that it should translate the verb next. For this, it needs to fetch the verb from the input sentence. This is analog to a dictionary lookup: it’s as if the encoder created a dictionary {“subject”: “They”, “verb”: “played”, …} and the decoder wanted to look up the value that corresponds to the key “verb”. However, the model does not have discrete tokens to represent the keys (like “subject” or “verb”); it has vectorized representations of these concepts (which it learned during training), so the key it will use for the lookup (called the <em>query</em>) will not perfectly match any key in the dictionary. The solution is to compute a similarity measure between the query and each key in the dictionary, and then use the softmax function to convert these similarity scores to weights that add up to 1. If the key that represents the verb is by far the most similar to the query, then that key’s weight will be close to 1. Then the model can compute a weighted sum of the corresponding values, so if the weight of the “verb” key is close to&nbsp;1, then the weighted sum will be very close to the representation of the word “played”. In short, you can think of this whole process as a differentiable dictionary lookup. The similarity measure used by the Transformer is just the dot product, like in Luong attention. In fact, the equation is the same as for Luong attention, except for a scaling factor. The equation is shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#scaled_dot_product_attention">Equation 16-3</a>, in a vectorized form.</p>
<div id="scaled_dot_product_attention" data-type="equation">
<h5><span class="label">Equation 16-3. </span>Scaled dot product attention</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-151-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;upper A t t e n t i o n left-parenthesis bold upper Q comma bold upper K comma bold upper V right-parenthesis equals s o f t m a x left-parenthesis StartFraction bold upper Q bold upper K Superscript upper T Baseline Over StartRoot d Subscript k e y s Baseline EndRoot EndFraction right-parenthesis bold upper V&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mo form=&quot;prefix&quot;&gt;Attention&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x1D410;&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;&amp;#x1D40A;&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;&amp;#x1D415;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo form=&quot;prefix&quot;&gt;softmax&lt;/mo&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;&amp;#x1D410;&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x1D40A;&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mfenced&gt;&lt;mi&gt;&amp;#x1D415;&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-6795" aria-label="upper A t t e n t i o n left-parenthesis bold upper Q comma bold upper K comma bold upper V right-parenthesis equals s o f t m a x left-parenthesis StartFraction bold upper Q bold upper K Superscript upper T Baseline Over StartRoot d Subscript k e y s Baseline EndRoot EndFraction right-parenthesis bold upper V" style="width: 20.671em; display: inline-block;"><span style="display: inline-block; position: relative; width: 20.054em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(2.059em, 1020.01em, 5.35em, -1000.01em); top: -3.956em; left: 0em;"><span class="mrow" id="MathJax-Span-6796"><span class="mrow" id="MathJax-Span-6797"><span class="mo" id="MathJax-Span-6798" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">Attention</span><span class="mrow" id="MathJax-Span-6799"><span class="mo" id="MathJax-Span-6800" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6801" style="font-family: MathJax_Main-bold;">Q</span><span class="mo" id="MathJax-Span-6802" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-6803" style="font-family: MathJax_Main-bold; padding-left: 0.157em;">K</span><span class="mo" id="MathJax-Span-6804" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-6805" style="font-family: MathJax_Main-bold; padding-left: 0.157em;">V</span><span class="mo" id="MathJax-Span-6806" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-6807" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mo" id="MathJax-Span-6808" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">softmax</span><span class="mfenced" id="MathJax-Span-6809"><span class="mo" id="MathJax-Span-6810" style="vertical-align: 0em;"><span style="font-family: MathJax_Size4;">(</span></span><span class="mstyle" id="MathJax-Span-6811"><span class="mrow" id="MathJax-Span-6812"><span class="mfrac" id="MathJax-Span-6813"><span style="display: inline-block; position: relative; width: 3.139em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(2.985em, 1002.33em, 4.373em, -1000.01em); top: -4.676em; left: 50%; margin-left: -1.18em;"><span class="mrow" id="MathJax-Span-6814"><span class="mi" id="MathJax-Span-6815" style="font-family: MathJax_Main-bold;">Q</span><span class="msup" id="MathJax-Span-6816"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.83em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6817" style="font-family: MathJax_Main-bold;">K</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 0.928em;"><span class="mi" id="MathJax-Span-6818" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.036em, 1002.99em, 4.578em, -1000.01em); top: -3.237em; left: 50%; margin-left: -1.488em;"><span class="msqrt" id="MathJax-Span-6819"><span style="display: inline-block; position: relative; width: 2.985em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.97em, 4.476em, -1000.01em); top: -4.008em; left: 0.979em;"><span class="mrow" id="MathJax-Span-6820"><span class="msub" id="MathJax-Span-6821"><span style="display: inline-block; position: relative; width: 1.956em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.52em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6822" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.517em;"><span class="mrow" id="MathJax-Span-6823"><span class="mi" id="MathJax-Span-6824" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span class="mi" id="MathJax-Span-6825" style="font-size: 70.7%; font-family: MathJax_Math-italic;">e</span><span class="mi" id="MathJax-Span-6826" style="font-size: 70.7%; font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-6827" style="font-size: 70.7%; font-family: MathJax_Math-italic;">s</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.602em, 1002.02em, 3.961em, -1000.01em); top: -4.522em; left: 0.979em;"><span style="display: inline-block; position: relative; width: 2.008em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.1em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 1.288em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.362em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.825em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1001.04em, 4.527em, -1000.01em); top: -3.956em; left: 0em;"><span style="font-family: MathJax_Size1;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1003.15em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.139em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-6828" style="vertical-align: 0em;"><span style="font-family: MathJax_Size4;">)</span></span></span><span class="mi" id="MathJax-Span-6829" style="font-family: MathJax_Main-bold;">V</span></span></span><span style="display: inline-block; width: 0px; height: 3.961em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.321em; border-left: 0px solid; width: 0px; height: 3.18em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="upper A t t e n t i o n left-parenthesis bold upper Q comma bold upper K comma bold upper V right-parenthesis equals s o f t m a x left-parenthesis StartFraction bold upper Q bold upper K Superscript upper T Baseline Over StartRoot d Subscript k e y s Baseline EndRoot EndFraction right-parenthesis bold upper V" display="block"><mrow><mo form="prefix">Attention</mo><mrow><mo>(</mo><mi>𝐐</mi><mo>,</mo><mi>𝐊</mi><mo>,</mo><mi>𝐕</mi><mo>)</mo></mrow><mo>=</mo><mo form="prefix">softmax</mo><mfenced open="(" close=")"><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>𝐐</mi><msup><mi>𝐊</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mrow><mi>k</mi><mi>e</mi><mi>y</mi><mi>s</mi></mrow></msub></msqrt></mfrac></mstyle></mfenced><mi>𝐕</mi></mrow></math></span></span></div><script type="math/mml" id="MathJax-Element-151"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="upper A t t e n t i o n left-parenthesis bold upper Q comma bold upper K comma bold upper V right-parenthesis equals s o f t m a x left-parenthesis StartFraction bold upper Q bold upper K Superscript upper T Baseline Over StartRoot d Subscript k e y s Baseline EndRoot EndFraction right-parenthesis bold upper V" display="block">
  <mrow>
    <mo form="prefix">Attention</mo>
    <mrow>
      <mo>(</mo>
      <mi>𝐐</mi>
      <mo>,</mo>
      <mi>𝐊</mi>
      <mo>,</mo>
      <mi>𝐕</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mo form="prefix">softmax</mo>
    <mfenced open="(" close=")">
      <mstyle scriptlevel="0" displaystyle="true">
        <mfrac><mrow><mi>𝐐</mi><msup><mi>𝐊</mi> <mi>T</mi> </msup></mrow> <msqrt><msub><mi>d</mi> <mrow><mi>k</mi><mi>e</mi><mi>y</mi><mi>s</mi></mrow> </msub></msqrt></mfrac>
      </mstyle>
    </mfenced>
    <mi>𝐕</mi>
  </mrow>
</math></script>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><strong>Q</strong> is a matrix containing one row per query. Its shape is [<em>n</em><sub>queries</sub>, <em>d</em><sub>keys</sub>], where n<sub>queries</sub> is the number of queries and <em>d</em><sub>keys</sub> is the number of dimensions of each query and each key.</p>
</li>
<li>
<p><strong>K</strong> is a matrix containing one row per key. Its shape is [<em>n</em><sub>keys</sub>, <em>d</em><sub>keys</sub>], where n<sub>keys</sub> is the number of keys and values.</p>
</li>
<li>
<p><strong>V</strong> is a matrix containing one row per value. Its shape is [<em>n</em><sub>keys</sub>, <em>d</em><sub>values</sub>], where <em>d</em><sub>values</sub> is the number of each value.</p>
</li>
<li>
<p>The shape of <strong>Q</strong> <strong>K</strong><sup><em>T</em></sup> is [<em>n</em><sub>queries</sub>, <em>n</em><sub>keys</sub>]: it contains one similarity score for each query/key pair. The output of the softmax function has the same shape, but all rows sum up to 1. The final output has a shape of [<em>n</em><sub>queries</sub>, d<sub>values</sub>]: there is one row per query, where each row represents the query result (a weighted sum of the values).</p>
</li>
<li>
<p>The scaling factor scales down the similarity scores to avoid saturating the softmax function, which would lead to tiny gradients.</p>
</li>
<li>
<p>It is possible to mask out some key/value pairs by adding a very large negative value to the corresponding similarity scores, just before computing the softmax. This is useful in the Masked Multi-Head Attention layer.</p>
</li>
</ul>

<p>In the encoder, this equation is applied to every input sentence in the batch, with <strong>Q</strong>, <strong>K</strong>, and <strong>V</strong> all equal to the list of words in the input sentence (so each word in the sentence will be compared to every word in the same sentence, including itself). Similarly, in the decoder’s masked attention layer, the equation will be applied to every target sentence in the batch, with <strong>Q</strong>, <strong>K</strong>, and <strong>V</strong> all equal to the list of words in the target sentence, but this time using a mask to prevent any word from comparing itself to words located after it (indeed, at inference time the decoder will only have access to the words it already output, not to future words, so during training we must mask out future output tokens). In the upper attention layer of the decoder, the keys <strong>K</strong> and values <strong>V</strong> are simply the list of word encodings produced by the encoder, and the queries <strong>Q</strong> are the list of word encodings produced by the decoder.</p>

<p>The <code>keras.layers.Attention</code> layer implements Scaled Dot-Product Attention, efficiently applying <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#scaled_dot_product_attention">Equation 16-3</a> to multiple sentences in a batch. Its inputs are just like <strong>Q</strong>, <strong>K</strong>, and <strong>V</strong>, except with an extra batch dimension (the first dimension).</p>
<div data-type="tip"><h6>Tip</h6>
<p>In TensorFlow, if <code>A</code> and <code>B</code> are tensors with more than two dimensions, say, of shape [2, 3, 4, 5] and [2, 3, 5, 6] respectively, then <code>tf.matmul(A, B)</code> will treat these tensors as 2&nbsp;×&nbsp;3 arrays where each cell contains a matrix, and it will multiply the corresponding matrices: the matrix at the <em>i</em><sup>th</sup> row and <em>j</em><sup>th</sup> column in <code>A</code> will be multiplied by the matrix at the <em>i</em><sup>th</sup> row and <em>j</em><sup>th</sup> column in <code>B</code>. Since the product of a 4&nbsp;×&nbsp;5 matrix with a 5&nbsp;×&nbsp;6 matrix is a 4&nbsp;×&nbsp;6 matrix, <code>tf.matmul(A, B)</code> will return an array of shape <code>[2, 3, 4, 6]</code>.</p>
</div>

<p>If we ignore the skip connections, the Layer Normalization layers, the Feed Forward blocks, and the fact that this is Scaled Dot-Product Attention, not exactly Multi-Head Attention, then the rest of the Transformer model can be implemented like this:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Z</code> <code class="o">=</code> <code class="n">encoder_in</code>
<code class="k">for</code> <code class="n">N</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">6</code><code class="p">):</code>
    <code class="n">Z</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Attention</code><code class="p">(</code><code class="n">use_scale</code><code class="o">=</code><code class="bp">True</code><code class="p">)([</code><code class="n">Z</code><code class="p">,</code> <code class="n">Z</code><code class="p">])</code>

<code class="n">encoder_outputs</code> <code class="o">=</code> <code class="n">Z</code>
<code class="n">Z</code> <code class="o">=</code> <code class="n">decoder_in</code>
<code class="k">for</code> <code class="n">N</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">6</code><code class="p">):</code>
    <code class="n">Z</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Attention</code><code class="p">(</code><code class="n">use_scale</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">causal</code><code class="o">=</code><code class="bp">True</code><code class="p">)([</code><code class="n">Z</code><code class="p">,</code> <code class="n">Z</code><code class="p">])</code>
    <code class="n">Z</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Attention</code><code class="p">(</code><code class="n">use_scale</code><code class="o">=</code><code class="bp">True</code><code class="p">)([</code><code class="n">Z</code><code class="p">,</code> <code class="n">encoder_outputs</code><code class="p">])</code>

<code class="n">outputs</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">TimeDistributed</code><code class="p">(</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="n">vocab_size</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"softmax"</code><code class="p">))(</code><code class="n">Z</code><code class="p">)</code></pre>

<p>The <code>use_scale=True</code> argument creates an additional parameter that lets the layer learn how to properly downscale the similarity scores. This is a bit different from the Transformer model, which always downscales the similarity scores by the same factor (<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-152-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;StartRoot d Subscript k e y s Baseline EndRoot&quot;&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-6830" aria-label="StartRoot d Subscript k e y s Baseline EndRoot" style="width: 3.087em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.985em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.134em, 1002.99em, 2.676em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-6831"><span class="msqrt" id="MathJax-Span-6832"><span style="display: inline-block; position: relative; width: 2.985em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.97em, 4.476em, -1000.01em); top: -4.008em; left: 0.979em;"><span class="mrow" id="MathJax-Span-6833"><span class="msub" id="MathJax-Span-6834"><span style="display: inline-block; position: relative; width: 1.956em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.52em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6835" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.517em;"><span class="mrow" id="MathJax-Span-6836"><span class="mi" id="MathJax-Span-6837" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span class="mi" id="MathJax-Span-6838" style="font-size: 70.7%; font-family: MathJax_Math-italic;">e</span><span class="mi" id="MathJax-Span-6839" style="font-size: 70.7%; font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-6840" style="font-size: 70.7%; font-family: MathJax_Math-italic;">s</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.602em, 1002.02em, 3.961em, -1000.01em); top: -4.522em; left: 0.979em;"><span style="display: inline-block; position: relative; width: 2.008em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.1em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 1.288em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.362em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.825em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1001.04em, 4.527em, -1000.01em); top: -3.956em; left: 0em;"><span style="font-family: MathJax_Size1;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.474em; border-left: 0px solid; width: 0px; height: 1.327em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="StartRoot d Subscript k e y s Baseline EndRoot"><msqrt><msub><mi>d</mi><mrow><mi>k</mi><mi>e</mi><mi>y</mi><mi>s</mi></mrow></msub></msqrt></math></span></span><script type="math/mml" id="MathJax-Element-152"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="StartRoot d Subscript k e y s Baseline EndRoot">
  <msqrt>
    <msub><mi>d</mi> <mrow><mi>k</mi><mi>e</mi><mi>y</mi><mi>s</mi></mrow> </msub>
  </msqrt>
</math></script>). The <code>causal=True</code> argument when creating the second attention layer ensures that each output token only attends to previous output tokens, not future ones.</p>

<p>Now it’s time to look at the final piece of the puzzle: what is a Multi-Head Attention layer? Its architecture is shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#multihead_attention_diagram">Figure&nbsp;16-10</a>.</p>

<figure><div id="multihead_attention_diagram" class="figure">
<img src="./Chapter16_files/mls2_1610.png" alt="mls2 1610" width="829" height="1110" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1610.png">
<h6><span class="label">Figure 16-10. </span>Multi-Head Attention layer architecture<sup><a data-type="noteref" id="idm46263490967592-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490967592">23</a></sup></h6>
</div></figure>

<p>As you can see, it is just a bunch of Scaled Dot-Product Attention layers, each preceded by a linear transformation of the values, keys, and queries (i.e., a time-distributed <code>Dense</code> layer with no activation function). All the outputs are simply concatenated, and they go through a final linear transformation (again, time-distributed). But why? What is the intuition behind this architecture? Well, consider the word “played” we discussed earlier (in the sentence “They played chess”): the encoder was smart enough to encode the fact that it is a verb. But the word representation also includes its position in the text, thanks to the positional encodings. And it probably includes many other features that are useful for its translation, such as the fact that it is in the past tense. In short, the word representation encodes many different characteristics of the word. If we just use a single Scaled Dot-Product Attention layer, we will only be able to query all of these characteristics in one shot. This is why the Multi-Head Attention layer applies multiple different linear transformations of the values, keys, and queries: this allows the model to apply many different projections of the word representation into different subspaces, focusing on a subset of the word’s characteristics. Perhaps one of the linear layers will project the word representation into a subspace where all that remains is the information that the word is a verb. Another linear layer will extract just the fact that it is past tense. And so on. Then the Scaled Dot-Product layers implement the lookup phase. And finally, we concatenate all the results and project them back to the original space.</p>

<p>At the time of this writing, there is no <code>Transformer</code> class or MultiHeadAttention classavailable for TensorFlow&nbsp;2 yet. However, you can check out TensorFlow’s great <a href="https://homl.info/transformertuto">tutorial for building a Transformer model for language understanding</a>. Moreover, the TF Hub team is currently porting several Transformer-based modules to TensorFlow&nbsp;2, and they should be available soon. In the meantime, I hope I have demonstrated that it is not that hard to implement a Transformer yourself, and it is certainly a great exercise!</p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Recent Innovations in Language Models"><div class="sect1" id="idm46263491361208">
<h1>Recent Innovations in Language Models</h1>

<p>The year 2018 has been called the “ImageNet moment for NLP”: progress has been astounding, with larger and larger architectures trained on immense datasets, using LSTM or Transformer-based architectures. I highly recommend you check out the following papers, all published in 2018:</p>

<ul>
<li>
<p>The <a href="https://homl.info/elmo">ELMo paper</a><sup><a data-type="noteref" id="idm46263490959496-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490959496">24</a></sup> by Matthew Peters introduced <em>Embeddings from Language Models</em> (ELMo): these are contextualized word embeddings learned from the internal states of a deep bidirectional language model. For example, the word “queen” will not have the same embedding in “Queen of the United Kingdom” and in “queen bee”.</p>
</li>
<li>
<p>The <a href="https://homl.info/ulmfit">ULMFiT paper</a><sup><a data-type="noteref" id="idm46263490956408-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490956408">25</a></sup> by Jeremy Howard and Sebastian Ruder demonstrated the effectiveness of unsupervised pretraining for NLP tasks: they trained an LSTM language model using self-supervised learning (i.e., generating the labels automatically from the data) on a huge text corpus, then they fine-tuned it on various tasks. They outperformed the state of the art on six text classification tasks by a large margin (reducing the error rate by over 18–24% in most cases). Moreover, they showed that by fine-tuning the pretrained model on just 100 labeled examples, they could achieve the same performance as a model trained from scratch on 10,000 examples.</p>
</li>
<li>
<p>The <a href="https://homl.info/gpt">GPT paper</a><sup><a data-type="noteref" id="idm46263490952856-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490952856">26</a></sup> by Alec Radford and other OpenAI researchers also demonstrated the effectiveness of unsupervised pretraining, but this time using a Transformer-like architecture. The authors pretrained a large but fairly simple architecture composed of a stack of 12 Transformer modules (using only Masked Multi-Head Attention layers) on a large dataset, once again trained using self-supervised learning. Then they fine-tuned it on various language tasks, using only minor adaptations for each task. The tasks were quite diverse: they included text classification, <em>entailment</em> (whether sentence A entails sentence B),<sup><a data-type="noteref" id="idm46263490951048-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490951048">27</a></sup> similarity (e.g., “Nice weather today” is very similar to “It is sunny”), and question answering (given a few paragraphs of text giving some context, the model must answer some multiple-choice questions). Just a few months later, in February 2019, Alec Radford, Jeffrey Wu and other OpenAI researchers published the <a href="https://homl.info/gpt2">GPT-2 paper</a><sup><a data-type="noteref" id="idm46263490949256-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490949256">28</a></sup> which proposed a very similar architecture, but larger still (with over 1.5 billion parameters!), and they showed that it could achieve good performance on many tasks without any fine-tuning. This is called <em>zero-shot learning</em> (ZSL). A smaller version of the GPT-2 model (with “just” 117 million parameters) is available at <a href="https://github.com/openai/gpt-2"><em class="hyperlink">https://github.com/openai/gpt-2</em></a>, along with its pretrained weights.</p>
</li>
<li>
<p>The <a href="https://homl.info/bert">BERT paper</a><sup><a data-type="noteref" id="idm46263490944920-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490944920">29</a></sup> by Jacob Devlin and other Google researchers also demonstrates the effectiveness of self-supervised pretraining on a large corpus, using a similar architecture as GPT, but using non-masked Multi-Head Attention layers (like in the Transformer’s encoder). This means that the model is naturally bidirectional, hence the B in BERT (Bidirectional Encoder Representations from Transformers). Most importantly, the authors proposed two pretraining tasks that explain most of the model’s strength:</p>

<ul>
<li>
<p><em>Masked language model</em> (MLM): each word in a sentence has a 15% probability of being masked, and the model is trained to predict the masked words. For example, if the original sentence is “She had fun at the birthday party”, then the model may be given the sentence “She &lt;mask&gt; fun at the &lt;mask&gt; party” and it must predict the words “had” and “birthday” (the other outputs will be ignored). To be more precise, each selected word has an 80% chance of being masked, a 10% chance of being replaced by a random word (to reduce the discrepancy between pretraining and fine-tuning, since the model will not see &lt;mask&gt; tokens during fine-tuning), and a 10% chance of being left alone (to bias the model toward the correct answer).</p>
</li>
<li>
<p><em>Next sentence prediction</em> (NSP): the model is trained to predict whether two sentences are consecutive or not. For example, it should predict that “The dog sleeps” and “It snores loudly” are consecutive sentences, while “The dog sleeps” and “The Earth orbits the Sun” are not consecutive. This is a challenging task, and it significantly improves the performance of the model when it is fine-tuned on tasks such as question answering or entailment.</p>
</li>
</ul>
</li>
</ul>

<p>As you can see, the main innovations in 2018 and 2019 have been better subword tokenization, shifting from LSTMs to Transformers, and pretraining universal language models using self-supervised learning, then fine-tuning them with very little architectural changes (or none at all). Things are moving fast; no one can say what architectures will prevail next year. Today, it’s clearly Transformers, but tomorrow it might be CNNs (e.g., check out this <a href="https://homl.info/pervasiveattention">2018 paper</a><sup><a data-type="noteref" id="idm46263490937528-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490937528">30</a></sup> that uses masked 2D convolutional layers for sequence-to-sequence tasks). Or it might even be RNNs, if they make a surprise comeback (e.g., check out this other <a href="https://homl.info/indrnn">2018 paper</a><sup><a data-type="noteref" id="idm46263490935976-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490935976">31</a></sup> that shows that by making neurons independent of each other in a given RNN layer, it is possible to train much deeper RNNs capable of learning much longer sequences).</p>

<p>In the next chapter we will discuss how to learn deep representations in an unsupervised way using autoencoders, and we will use Generative Adversarial Networks (GANs) to produce images and more!</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="idm46263490934248">
<h1>Exercises</h1>
<ol>
<li>
<p>What are the pros and cons of using a stateful RNN versus a stateless RNN?</p>
</li>
<li>
<p>Why do people use Encoder–Decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?</p>
</li>
<li>
<p>How can you deal with variable-length input sequences? What about variable-length output sequences?</p>
</li>
<li>
<p>What is beam search and why would you use it? What tool can you use to implement it?</p>
</li>
<li>
<p>What is an attention mechanism? How does it help?</p>
</li>
<li>
<p>What is the most important layer in Transformer architecture? What is its purpose?</p>
</li>
<li>
<p>When would you need to use sampled softmax?</p>
</li>
<li>
<p><em>Embedded Reber grammars</em> were used by Hochreiter and Schmidhuber in their paper about LSTMs. They are artificial grammars that produce strings such as “BPBTSXXVPSEPE.” Check out Jenny Orr’s <a href="https://homl.info/108">nice introduction</a> to this topic. Choose a particular embedded Reber grammar (such as the one represented on Jenny Orr’s page), then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don’t.</p>
</li>
<li>
<p>Train an Encoder–Decoder model that can convert a date string from one format to another (e.g., from “April 22, 2019” to “2019-04-22”).</p>
</li>
<li>
<p>Go through TensorFlow’s <a href="https://homl.info/nmttuto">Neural Machine Translation with Attention tutorial</a>.</p>
</li>
<li>
<p>Use one of the recent language models (e.g., BERT) to generate more convincing Shakespeare text.</p>
</li>

</ol>

<p>Solutions to these exercises are available in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/app01.html#solutions_appendix">Appendix&nbsp;A</a>.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46263495030744"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263495030744-marker" class="totri-footnote">1</a></sup> Alan Turing, “Computing Machinery and Intelligence,” <em>Mind</em> 49 (1950): 433–460.</p><p data-type="footnote" id="idm46263495028824"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263495028824-marker" class="totri-footnote">2</a></sup> Of course, the word <em>chatbot</em> came much later. Turing called his test the <em>imitation game</em>: machine A and human B chat with human interrogator C via text messages; the interrogator asks questions to figure out which one is the machine (A or B). The machine passes the test if it can fool the interrogator, while the human B must try to help the interrogator.</p><p data-type="footnote" id="idm46263494797064"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263494797064-marker" class="totri-footnote">3</a></sup> By definition, a stationary time series’ mean, varianc,e and <em>autocorrelations</em> (i.e., correlations between values in the time series separated by a given interval) do not change over time. This is quite restrictive; for example, it excludes time series with trends or cyclical patterns. RNNs are more tolerant in that they can learn trends and cyclical patterns.</p><p data-type="footnote" id="idm46263493401656"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263493401656-marker" class="totri-footnote">4</a></sup> Taku Kudo, <em>Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates</em> (Google, Inc., April 2018).</p><p data-type="footnote" id="idm46263493398568"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263493398568-marker" class="totri-footnote">5</a></sup> Taku Kudo and John Richardson, <em>SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing</em> (Google, Inc., August 2018).</p><p data-type="footnote" id="idm46263493396088"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263493396088-marker" class="totri-footnote">6</a></sup> Rico Sennrich et al., “Neural Machine Translation of Rare Words with Subword Units” in <em>Meeting of the Association for Computational Linguistics</em> 1 (2016).</p><p data-type="footnote" id="idm46263493393128"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263493393128-marker" class="totri-footnote">7</a></sup> Yonghui Wu et al., <em>Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation</em> (2016).</p><p data-type="footnote" id="idm46263492747624"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263492747624-marker" class="totri-footnote">8</a></sup> Their ID is 0 only because they are the most frequent “words” in the dataset. It would probably be a good idea to ensure that the padding tokens are always encoded as 0, even if they are not the most frequent.</p><p data-type="footnote" id="idm46263492517640"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263492517640-marker" class="totri-footnote">9</a></sup> To be precise, the sentence embedding is equal to the mean word embedding multiplied by the square root of the number of words in the sentence. This compensates for the fact that the mean of <em>n</em> vectors gets shorter as <em>n</em> grows.</p><p data-type="footnote" id="idm46263492372776"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263492372776-marker">10</a></sup> Ilya Sutskever et al., <em>Sequence to Sequence Learning with Neural Networks</em> (Google, Inc., December 2014).</p><p data-type="footnote" id="idm46263492307624"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263492307624-marker">11</a></sup> Sébastien Jean et al., “On Using Very Large Target Vocabulary for Neural Machine Translation” in <em>Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</em> 1 (Beijing, July 26-31, 2015): 1–10.</p><p data-type="footnote" id="idm46263492068936"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263492068936-marker">12</a></sup> Samy Bengio et al., <em>Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks</em> (Mountain View, CA: Google Research, September 2015).</p><p data-type="footnote" id="idm46263491789288"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491789288-marker">13</a></sup> Dzmitry Bahdanau et al., “Neural Machine Translation by Jointly Learning to Align and Translate” (Oral presentation of paper, ICLR 2015).</p><p data-type="footnote" id="idm46263491787752"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491787752-marker">14</a></sup> The most common metric used in NMT is the BiLingual Evaluation Understudy (BLEU) score, which compares each translation produced by the model with several good translations produced by humans: it counts the number of <em>n</em>-grams (sequences of <em>n</em> words) that appear in any of the target translations and adjusts the score to take into account the frequency of the produced <em>n</em>-grams in the target translations.</p><p data-type="footnote" id="idm46263491957992"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491957992-marker">15</a></sup> Recall that a time-distributed <code>Dense</code> layer is equivalent to a regular <code>Dense</code> layer that you apply independently at each time step (only much faster).</p><p data-type="footnote" id="idm46263491947800"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491947800-marker">16</a></sup> Minh-Thang Luong et al., “Effective Approaches to Attention-Based Neural Machine Translation” in proceedings of Empirical Methods in Natural Language Processing (2015): 1412–1421.</p><p data-type="footnote" id="idm46263491656584"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491656584-marker">17</a></sup> Kelvin Xu et al., “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention” in proceedings of ICML (2015): 2048–2057.</p><p data-type="footnote" id="idm46263491654600"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491654600-marker">18</a></sup> This is a part of figure 3 from the paper. It is reproduced with the kind authorization of the authors.</p><p data-type="footnote" id="idm46263491648152"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491648152-marker">19</a></sup> Marco Tulio Ribeiro et al., “‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier” in proceedings of Knowledge Discovery and Data Mining 2016: 1135-1144.</p><p data-type="footnote" id="idm46263491643960"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491643960-marker">20</a></sup> Ashish Vaswani et al., “Attention Is All You Need” in proceedings of NeurIPS 2017: 6000–6010.</p><p data-type="footnote" id="idm46263491642472"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491642472-marker">21</a></sup> Since the Transformer uses time-distributed dense layers, you could argue that it uses 1D convolutional layers with a kernel size of 1.</p><p data-type="footnote" id="idm46263491637912"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263491637912-marker">22</a></sup> This is figure 1 from the paper, reproduced with the kind authorization of the authors.</p><p data-type="footnote" id="idm46263490967592"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490967592-marker">23</a></sup> This is the right part of figure 2 from the paper, reproduced with the kind authorization of the authors.</p><p data-type="footnote" id="idm46263490959496"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490959496-marker">24</a></sup> Matthew Peters et al., “Deep Contextualized Word Representations” in North American Chapter of the Association for Computational Linguistics: Human Language Technologies 2018: 2227-2237.</p><p data-type="footnote" id="idm46263490956408"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490956408-marker">25</a></sup> Jeremy Howard and Sebastian Ruder, “Universal Language Model Fine-Tuning for Text Classification,” <em>ACL</em> 1 (2018): 328–339.</p><p data-type="footnote" id="idm46263490952856"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490952856-marker">26</a></sup> Alec Radford et al., “Improving Language Understanding by Generative Pre-Training” (2018).</p><p data-type="footnote" id="idm46263490951048"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490951048-marker">27</a></sup> For example, the sentence “Jane had a lot of fun at her friend’s birthday party” entails “Jane enjoyed the party”, but it is contradicted by “Everyone hated the party”, and it is unrelated to “The Earth is flat”.</p><p data-type="footnote" id="idm46263490949256"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490949256-marker">28</a></sup> Alec Radford et al., <em>Language Models Are Unsupervised Multitask Learners</em> (San Francisco: OpenAI, 2019).</p><p data-type="footnote" id="idm46263490944920"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490944920-marker">29</a></sup> Jacob Devlin et al., <em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em> (Google AI Language, May 2019).</p><p data-type="footnote" id="idm46263490937528"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490937528-marker">30</a></sup> Maha Elbayad et al., “Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Prediction” (2018).</p><p data-type="footnote" id="idm46263490935976"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#idm46263490935976-marker">31</a></sup> Shuai Li et al., “Independently Recurrent Neural Network (IndRNN): Building a Longer and Deeper RNN” in CVPR 2018: 5457–5466.</p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-17" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders">
		
		<li class="copy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#">Add Highlight</a></li>
		<li class="add-note"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#">
			Add Note
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">15. Processing Sequences Using RNNs and CNNs</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">17. Representation Learning and Generative Learning Using Autoencoders and GANs</div>
        </a>
    
  
  </div>

</section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    



        
      </div>
      



  <footer class="pagefoot">
    <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#" class="icon-up" style="display: none;"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      <li class="full-support"><a href="https://www.oreilly.com/online-learning/support/">Support</a></li>
      <li><a href="https://www.oreilly.com/online-learning/apps.html">Get the App</a></li>
      
        <li><a href="https://learning.oreilly.com/accounts/logout/">Sign Out</a></li>
      
    </ul>
    <span class="copyright">© 2019 <a href="https://learning.oreilly.com/" target="_blank">Safari</a>.</span>
    <a href="https://learning.oreilly.com/terms/">Terms of Service</a> /
    <a href="https://learning.oreilly.com/membership-agreement/">Membership Agreement</a> /
    <a href="https://www.oreilly.com/privacy.html">Privacy Policy</a>
  </footer>

<script type="text/javascript">window.NREUM||(NREUM={});NREUM.info={"applicationID":"172641827,79672898,93931619","errorBeacon":"bam.nr-data.net","agent":"","applicationTime":451,"licenseKey":"510f1a6865","transactionName":"YgdaZ0NSW0cEB0RdWltNfkZfUEFdCgofXFBHDVYdR1pQQxZeRl1QQj1aWkU=","beacon":"bam.nr-data.net","queueTime":4}</script>


    
    <script src="./Chapter16_files/saved_resource" charset="utf-8"></script><script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","1732687426968531");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=1732687426968531&amp;ev=PageView&amp;noscript=1"></noscript><script type="text/javascript" id="">(function(){window.medalliaUserIdentifier=document.documentElement.dataset.userUuid;window.medalliaUserName=document.documentElement.dataset.username})();</script>
<script type="text/javascript" id="" src="./Chapter16_files/embed.js.download"></script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script>
<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("set","agent","tmgoogletagmanager","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>

<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>
<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.09556792590964802"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.5229133033930915" width="0" height="0" alt="" src="./Chapter16_files/0"></div>
    <script src="./Chapter16_files/saved_resource(1)" charset="utf-8"></script>
  

<script src="./Chapter16_files/saved_resource(2)" type="text/javascript"></script><script type="text/javascript" id="">window._pp=window._pp||[];if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/register/")_pp.targetUrl="/confirm/trial";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/subscribe/")_pp.targetUrl="/confirm/paid";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/signup/")_pp.targetUrl="/confirm/paid";_pp.siteId="2508";
_pp.siteUId="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79";_pp.orderValue="undefined";_pp.orderId="undefined";(function(){var ppjs=document.createElement("script");ppjs.type="text/javascript";ppjs.async=true;ppjs.src=("https:"==document.location.protocol?"https:":"http:")+"//cdn.pbbl.co/r/"+_pp.siteId+".js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(ppjs,s)})();</script><div class="annotator-notice"></div><div class="font-flyout" style="top: 201px; left: 1194px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#">Reset</a>
</div>
</div><script type="text/javascript" async="" src="./Chapter16_files/generic1566415868241.js.download" charset="UTF-8"></script><div style="display: none; visibility: hidden;"><script>(function(){if(null!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')&&void 0!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')){var a=!1;window.addEventListener("blur",function(){a&&dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"facebook",eventVal:0,nonInteraction:0})});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseover",function(){window.focus();
a=!0});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseout",function(){a=!1})}try{window.twttr=function(b,a,d){var c,e=b.getElementsByTagName(a)[0];if(!b.getElementById(d))return b=b.createElement(a),b.id=d,b.src="//platform.twitter.com/widgets.js",e.parentNode.insertBefore(b,e),window.twttr||(c={_e:[],ready:function(a){c._e.push(a)}})}(document,"script","twitter-wjs"),twttr.ready(function(a){a.events.bind("tweet",trackTwitter)})}catch(b){}})();
null!==document.querySelector(".IN-widget")&&void 0!==document.querySelector(".IN-widget")&&document.querySelector(".IN-widget").addEventListener("click",function(){dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"linkedin",eventVal:0,nonInteraction:0})});
function trackTwitter(a){a&&(a.target&&"IFRAME"==a.target.nodeName&&(opt_target=extractParamFromUri(a.target.src,"url")),dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"twitter",eventVal:0,nonInteraction:0}))}function extractParamFromUri(a,b){if(a){var c=new RegExp("[\\?\x26#]"+b+"\x3d([^\x26#]*)");c=c.exec(a);if(null!=c)return unescape(c[1])}};</script></div><span><div id="KampyleAnimationContainer" style="z-index: 2147483000; border: 0px; position: fixed; display: block; width: 0px; height: 0px;"></div></span><iframe scrolling="no" frameborder="0" allowtransparency="true" src="./Chapter16_files/widget_iframe.097c1f5038f9e8a0d62a39a892838d66.html" title="Twitter settings iframe" style="display: none;"></iframe><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_AMS, sans-serif;"></div></div></body></html>