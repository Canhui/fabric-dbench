<!DOCTYPE html>
<!-- saved from url=(0091)https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html -->
<html class=" js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/hands-on-machine-learning/9781492032632/part01.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="4626953" data-user-uuid="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79" data-username="17481074" data-account-type="B2B" data-activated-trial-date="" data-archive="9781492032632" data-publishers="O&#39;Reilly Media, Inc." data-htmlfile-name="part01.html" data-epub-title="Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781492032632"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><script type="text/javascript" async="" src="./Chapter14_files/cool-2.1.15.min.js.download"></script><script type="text/javascript" src="./Chapter14_files/510f1a6865"></script><script id="twitter-wjs" src="./Chapter14_files/widgets.js.download"></script><script src="./Chapter14_files/nr-1130.min.js.download"></script><script type="text/javascript" async="" src="./Chapter14_files/2508.js.download"></script><script async="" src="./Chapter14_files/fbevents.js.download"></script><script type="text/javascript" async="" src="./Chapter14_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter14_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter14_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter14_files/analytics.js.download"></script><script type="text/javascript" async="" src="./Chapter14_files/ec.js.download"></script><script type="text/javascript" async="" src="./Chapter14_files/bat.js.download"></script><script type="text/javascript" async="" src="./Chapter14_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter14_files/insight.min.js.download"></script><script type="text/javascript" async="" src="./Chapter14_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter14_files/linkid.js.download"></script><script async="" src="./Chapter14_files/gtm.js.download"></script><script async="" src="./Chapter14_files/analytics.js.download"></script><script type="text/javascript">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if("function"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e("handle"),a=e(3),u=e(4),f=e("ee").get("tracer"),c=e("loader"),s=NREUM;"undefined"==typeof window.newrelic&&(newrelic=s);var p=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],d="api-",l=d+"ixn-";a(p,function(e,n){s[n]=o(d+n,!0,"api")}),s.addPageAction=o(d+"addPageAction",!0),s.setCurrentRouteName=o(d+"routeName",!0),n.exports=newrelic,s.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(e,n){var t={},r=this,o="function"==typeof n;return i(l+"tracer",[c.now(),e,t],r),function(){if(f.emit((o?"":"no-")+"fn-start",[c.now(),r,o],t),o)try{return n.apply(this,arguments)}catch(e){throw f.emit("fn-err",[arguments,this,e],t),e}finally{f.emit("fn-end",[c.now()],t)}}}};a("actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(e,n){m[n]=o(l+n)}),newrelic.noticeError=function(e,n){"string"==typeof e&&(e=new Error(e)),i("err",[e,c.now(),!1,n])}},{}],2:[function(e,n,t){function r(e,n){if(!o)return!1;if(e!==o)return!1;if(!n)return!0;if(!i)return!1;for(var t=i.split("."),r=n.split("."),a=0;a<r.length;a++)if(r[a]!==t[a])return!1;return!0}var o=null,i=null,a=/Version\/(\S+)\s+Safari/;if(navigator.userAgent){var u=navigator.userAgent,f=u.match(a);f&&u.indexOf("Chrome")===-1&&u.indexOf("Chromium")===-1&&(o="Safari",i=f[1])}n.exports={agent:o,version:i,match:r}},{}],3:[function(e,n,t){function r(e,n){var t=[],r="",i=0;for(r in e)o.call(e,r)&&(t[i]=n(r,e[r]),i+=1);return t}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],4:[function(e,n,t){function r(e,n,t){n||(n=0),"undefined"==typeof t&&(t=e?e.length:0);for(var r=-1,o=t-n||0,i=Array(o<0?0:o);++r<o;)i[r]=e[n+r];return i}n.exports=r},{}],5:[function(e,n,t){n.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(e,n,t){function r(){}function o(e){function n(e){return e&&e instanceof r?e:e?f(e,u,i):i()}function t(t,r,o,i){if(!d.aborted||i){e&&e(t,r,o);for(var a=n(o),u=v(t),f=u.length,c=0;c<f;c++)u[c].apply(a,r);var p=s[y[t]];return p&&p.push([b,t,r,a]),a}}function l(e,n){h[e]=v(e).concat(n)}function m(e,n){var t=h[e];if(t)for(var r=0;r<t.length;r++)t[r]===n&&t.splice(r,1)}function v(e){return h[e]||[]}function g(e){return p[e]=p[e]||o(t)}function w(e,n){c(e,function(e,t){n=n||"feature",y[t]=n,n in s||(s[n]=[])})}var h={},y={},b={on:l,addEventListener:l,removeEventListener:m,emit:t,get:g,listeners:v,context:n,buffer:w,abort:a,aborted:!1};return b}function i(){return new r}function a(){(s.api||s.feature)&&(d.aborted=!0,s=d.backlog={})}var u="nr@context",f=e("gos"),c=e(3),s={},p={},d=n.exports=o();d.backlog=s},{}],gos:[function(e,n,t){function r(e,n,t){if(o.call(e,n))return e[n];var r=t();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,n,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return e[n]=r,r}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],handle:[function(e,n,t){function r(e,n,t,r){o.buffer([e],r),o.emit(e,n,t)}var o=e("ee").get("handle");n.exports=r,r.ee=o},{}],id:[function(e,n,t){function r(e){var n=typeof e;return!e||"object"!==n&&"function"!==n?-1:e===window?0:a(e,i,function(){return o++})}var o=1,i="nr@id",a=e("gos");n.exports=r},{}],loader:[function(e,n,t){function r(){if(!E++){var e=x.info=NREUM.info,n=l.getElementsByTagName("script")[0];if(setTimeout(s.abort,3e4),!(e&&e.licenseKey&&e.applicationID&&n))return s.abort();c(y,function(n,t){e[n]||(e[n]=t)}),f("mark",["onload",a()+x.offset],null,"api");var t=l.createElement("script");t.src="https://"+e.agent,n.parentNode.insertBefore(t,n)}}function o(){"complete"===l.readyState&&i()}function i(){f("mark",["domContent",a()+x.offset],null,"api")}function a(){return O.exists&&performance.now?Math.round(performance.now()):(u=Math.max((new Date).getTime(),u))-x.offset}var u=(new Date).getTime(),f=e("handle"),c=e(3),s=e("ee"),p=e(2),d=window,l=d.document,m="addEventListener",v="attachEvent",g=d.XMLHttpRequest,w=g&&g.prototype;NREUM.o={ST:setTimeout,SI:d.setImmediate,CT:clearTimeout,XHR:g,REQ:d.Request,EV:d.Event,PR:d.Promise,MO:d.MutationObserver};var h=""+location,y={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-1130.min.js"},b=g&&w&&w[m]&&!/CriOS/.test(navigator.userAgent),x=n.exports={offset:u,now:a,origin:h,features:{},xhrWrappable:b,userAgent:p};e(1),l[m]?(l[m]("DOMContentLoaded",i,!1),d[m]("load",r,!1)):(l[v]("onreadystatechange",o),d[v]("onload",r)),f("mark",["firstbyte",u],null,"api");var E=0,O=e(5)},{}]},{},["loader"]);</script><link rel="apple-touch-icon" href="https://learning.oreilly.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://learning.oreilly.com/favicon.ico" type="image/x-icon"><link href="./Chapter14_files/css" rel="stylesheet" type="text/css"><title>14. Deep Computer Vision Using Convolutional Neural Networks - Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition</title><link rel="stylesheet" href="./Chapter14_files/output.68851547a55f.css" type="text/css"><link rel="stylesheet" type="text/css" href="./Chapter14_files/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="./Chapter14_files/font-awesome.min.css"><style type="text/css" title="ibis-book">@charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000 !important;padding-top:10px !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:2em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content p[data-type="footnote"]{font-size:90% !important;line-height:1.2em !important;margin-left:2.5em !important;text-indent:-2.3em !important}#sbo-rt-content p[data-type="footnote"] sup{display:inline-block !important;position:static !important;width:2em !important;text-align:right !important;font-size:100% !important;padding-right:.5em !important}#sbo-rt-content p[data-type="footnote"] a[href$="-marker"]{font-family:sans-serif !important;font-size:90% !important;color:#8e0012 !important}#sbo-rt-content p[data-type="footnote"] a[data-type="xref"]{margin:0 !important;padding:0 !important;text-indent:0 !important}#sbo-rt-content a[data-type="noteref"]{font-family:sans-serif !important;color:#8e0012;margin-left:0;padding-left:0}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dd{margin-left:1.5em !important;margin-bottom:.25em}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul>li,#sbo-rt-content ol ul,#sbo-rt-content ol ul>li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul>li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul>li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul>li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol>li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol>li,#sbo-rt-content ul ol,#sbo-rt-content ul ol>li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol>li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol>li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:middle;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content table.border tbody>tr:last-child>td{border-bottom:transparent}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:1em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content .width-10{width:10vw !important}#sbo-rt-content .width-20{width:20vw !important}#sbo-rt-content .width-30{width:30vw !important}#sbo-rt-content .width-40{width:40vw !important}#sbo-rt-content .width-50{width:50vw !important}#sbo-rt-content .width-60{width:60vw !important}#sbo-rt-content .width-70{width:70vw !important}#sbo-rt-content .width-80{width:80vw !important}#sbo-rt-content .width-90{width:90vw !important}#sbo-rt-content .width-full,#sbo-rt-content .width-100{width:100vw !important}#sbo-rt-content div[data-type="equation"].fifty-percent img{width:50%}</style><script> // <![CDATA[
    var g = {
      position_cache: {
        
          "chapter": "/api/v1/book/9781492032632/chapter/part01.html",
          "book_id": "9781492032632",
          "chapter_uri": "part01.html",
          "position": 100.0,
          "user_uuid": "d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79",
          "next_chapter_uri": "/library/view/hands-on-machine-learning/9781492032632/ch01.html"
        
      },
      title: "Hands\u002Don Machine Learning with Scikit\u002DLearn, Keras, and TensorFlow, 2nd Edition",
      author_list: "Aurélien Géron",
      format: "book",
      source: "application/epub+zip",
      is_system_book: true,
      is_public: false,
      loaded_from_server: true,
      allow_scripts: false,
      has_mathml: true
    };
    // ]]></script><script src="./Chapter14_files/modernizr.8e35451ddb64.js.download"></script><script>
    
      

      
        
          window.PUBLIC_ANNOTATIONS = true;
        
      

      window.MOBILE_PUBLIC_ANNOTATIONS = false;

    

    
      window.PRIVACY_CONTROL_OVERRIDE = false;
    

      window.PRIVACY_CONTROL_SWITCH = true;

      window.PUBLISHER_PAGES = true;

      window.SBO = {
        "constants": {
          "SITB_ENDPOINT": "/api/v2/sitb/",
          "SEARCH_SELECT_ENDPOINT": "https://learning.oreilly.com/api/v2/search/select/",
          "ENABLE_ONLINE_TRAINING": false
        }
      };
  </script><link rel="canonical" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta name="description" content=" Part I. The Fundamentals of Machine Learning "><meta property="og:title" content="I. The Fundamentals of Machine Learning"><meta itemprop="isPartOf" content="/library/view/hands-on-machine-learning/9781492032632/"><meta itemprop="name" content="I. The Fundamentals of Machine Learning"><meta property="og:url" itemprop="url" content="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://learning.oreilly.com/library/cover/9781492032632/"><meta property="og:description" itemprop="description" content=" Part I. The Fundamentals of Machine Learning "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O&#39;Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781492032649"><meta property="og:book:author" itemprop="author" content="Aurélien Géron"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: &lt;%= font_size %&gt; !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: &lt;%= font_family %&gt; !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: &lt;%= column_width %&gt;% !important; margin: 0 auto !important; }"></style><noscript><meta http-equiv="refresh" content="0; url=/library/no-js/" /></noscript><script type="text/javascript">
    (function(i,s,o,g,r,a,m) {
      i['GoogleAnalyticsObject']=r;
      i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();
      a=s.createElement(o),m=s.getElementsByTagName(o)[0];
      a.async=1;
      a.src=g;
      m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    var matches = document.cookie.match(/BrowserCookie\s*=\s*([a-f0-9\-]{36})/),
      user_uuid = null;

    if (matches && matches.length === 2) {
      user_uuid = matches[1];
    }

  
    ga('create', 'UA-39299553-7', {'userId': 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79' });
  

  
    
      ga('set', 'dimension1', 'B2B');
    
  

  ga('set', 'dimension6', user_uuid);

  
    ga('set', 'dimension2', 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79');
    
      ga('set', 'dimension7', '0012M0000229keZQAQ');
    
  

  

  

  //enable enhanced link tracking
  ga('require', 'linkid', 'linkid.js');

  // reading interface will track pageviews itself
  if (document.location.pathname.indexOf("/library/view") !== 0) {
    ga('send', 'pageview');
  }
  </script><script>
    var dataLayer = window.dataLayer || [];

    
      window.medalliaVsgUserIdentifier = 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79';
      dataLayer.push({userIdentifier: 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79'});
      dataLayer.push({loggedIn: 'yes'});

      
        window.medalliaVsgAccountIdentifier = '21bed0a7-6b7b-470c-8fa0-40a52db0b491';
        
        dataLayer.push({orgID: '21bed0a7-6b7b-470c-8fa0-40a52db0b491'});
        

        window.medalliaVsgIsIndividual = false;
        
          
          dataLayer.push({learningAccountType: 'enterprise'});
          
        

        
          dataLayer.push({learningPaidAccount: 'yes'});
        
      
    

    (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-5P4V6Z');
    (function () {
      var VERSION = 'V1.1';
      var AUTHOR = 'Awwad';
      if (!window.GtmHelper)
        window.GtmHelper = function () {
          var instance = this;
          var loc = document.location;
          this.version = VERSION;
          this.author = AUTHOR;
          this.readCookie = function (name) {
            var nameEQ = name + "=";
            var ca = document.cookie.split(';');
            for (var i = 0; i < ca.length; i++) {
              var c = ca[i];
              while (c.charAt(0) == ' ') c = c.substring(1, c.length);
              if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length, c.length);
            }
            return null;
          };
          this.createCookie = function (name, value, days, cookieDomain) {
            var domain = "";
            var expires = "";

            if (days) {
              var date = new Date();
              date.setTime(date.getTime() + Math.ceil(days * 24 * 60 * 60 * 1000));
              var expires = " expires=" + date.toGMTString() + ";";
            }

            if (typeof (cookieDomain) != 'undefined')
              domain = " domain=" + cookieDomain + "; ";

            document.cookie = name + "=" + value + ";" + expires + domain + "path=/";
          };

          this.isDuplicated = function (currentTransactionId) {
            // the previous transaction id:
            var previousTransIdValue = this.readCookie("previousTransId");

            if (currentTransactionId === previousTransIdValue) {
              return true; // Duplication
            } else {
              return false;
            }
          };
        }
    })()
  </script><script defer="" src="./Chapter14_files/vendor.a48a756c5182.js.download"></script><script defer="" src="./Chapter14_files/reader.f2a0c6bd2fee.js.download"></script><script src="./Chapter14_files/f(1).txt"></script><script src="./Chapter14_files/f(2).txt"></script><script src="./Chapter14_files/f(3).txt"></script><script src="./Chapter14_files/f(4).txt"></script><script async="" src="./Chapter14_files/MathJax.js.download"></script><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 2147483020;
}
.annotator-filter {
  z-index: 2147483010;
}</style><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
</style><script async="true" type="text/javascript" src="./Chapter14_files/roundtrip.js.download"></script><style type="text/css" id="kampyleStyle">.noOutline{outline: none !important;}.wcagOutline:focus{outline: 1px dashed #595959 !important;outline-offset: 2px !important;transition: none !important;}</style><script async="true" type="text/javascript" src="./Chapter14_files/roundtrip.js.download"></script><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Math-bold-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Script; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_AMS; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf?V=2.7.1') format('opentype')}
</style></head>


<body class="reading sidenav  scalefonts library nav-collapsed"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

    
  <noscript> 
    <iframe src="//www.googletagmanager.com/ns.html?id=GTM-5P4V6Z"
            height="0" width="0"
            style="display:none;visibility:hidden">
    </iframe>
  </noscript>



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#container" class="skip">Skip to content</a><header class="topbar t-topbar"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://learning.oreilly.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M11.738 14H9.254v-3.676a.617.617 0 0 0-.621-.613H7.39a.617.617 0 0 0-.62.613V14H4.284a.617.617 0 0 1-.622-.613V10.22c0-.327.132-.64.367-.87l3.547-3.493a.627.627 0 0 1 .875 0l3.54 3.499c.234.229.366.54.367.864v3.167a.617.617 0 0 1-.62.613zM7.57 2.181a.625.625 0 0 1 .882 0l5.77 5.692-.93.92-5.28-5.209-5.28 5.208-.932-.919 5.77-5.692z"></path></svg><span>Safari Home</span></a></li><li><a href="https://learning.oreilly.com/resource-centers/" class="t-resource-centers-nav l0 nav-icn"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="16px" height="16px" viewBox="0 0 16 16" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="Topic-Page-Design" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="Heron-Button" transform="translate(-20.000000, -78.000000)" fill="#4A3A30"><g id="Group-9" transform="translate(20.000000, 78.000000)"><rect id="Rectangle" x="9.6" y="0" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="9.6" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="0" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect></g></g></g></svg><span>Resource Centers</span></a></li><li><a href="https://learning.oreilly.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://learning.oreilly.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://learning.oreilly.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://learning.oreilly.com/learning-paths/" class="l1 nav-icn t-learningpaths-nav js-toggle-menu-item"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="32px" height="32px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 52.5 (67469) - http://www.bohemiancoding.com/sketch --><title>Mask</title><desc>Created with Sketch.</desc><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><path d="M0,16.0214227 C0,15.0387209 0.796453294,14.2411658 1.77779753,14.2411658 C2.75914177,14.2411658 3.55559506,15.0387209 3.55559506,16.0214227 C3.55559506,17.0041246 2.75914177,17.8016797 1.77779753,17.8016797 C0.796453294,17.8016797 0,17.0041246 0,16.0214227 Z M9.77788642,5.22914885 C8.9280992,5.72049977 7.84008711,5.42853763 7.34941499,4.57757479 C6.85874287,3.72661195 7.15030167,2.63709467 8.00008889,2.14574375 C8.84987611,1.65439282 9.9378882,1.94635496 10.4285603,2.7973178 C10.9192324,3.64828064 10.6276736,4.73779792 9.77788642,5.22914885 Z M4.57213969,7.35869225 C5.42192691,7.85004318 5.71348571,8.93956046 5.22281359,9.79052329 C4.73214147,10.6414861 3.64412938,10.9334483 2.79434216,10.4420974 C1.94455494,9.95074642 1.65299614,8.86122915 2.14366826,8.01026631 C2.63434038,7.15930347 3.72235247,6.86734132 4.57213969,7.35869225 Z M2.79434216,21.6007481 C3.64412938,21.1093972 4.73214147,21.4013594 5.22281359,22.2523222 C5.71348571,23.103285 5.42192691,24.1928023 4.57213969,24.6841532 C3.72235247,25.1755042 2.63434038,24.883542 2.14366826,24.0325792 C1.65299614,23.1816163 1.94455494,22.0920991 2.79434216,21.6007481 Z M7.34941499,27.4652707 C7.84008711,26.6143079 8.9280992,26.3223457 9.77788642,26.8136966 C10.6276736,27.3050476 10.9192324,28.3945649 10.4285603,29.2455277 C9.9378882,30.0964905 8.84987611,30.3884527 8.00008889,29.8971017 C7.15030167,29.4057508 6.85874287,28.3162335 7.34941499,27.4652707 Z M18.7118524,11.3165596 C21.3074367,12.8173162 22.1963355,16.1392758 20.6976522,18.738451 C19.1989689,21.3358459 15.8815987,22.2259744 13.2860143,20.726998 C10.6922077,19.2262414 9.80330893,15.9042818 11.3002144,13.3051066 C12.7988978,10.7059314 16.116268,9.81580294 18.7118524,11.3165596 Z M26.7821642,27.8093944 L30.1315348,31.1633985 C30.3982044,31.4304371 30.2097579,31.8844026 29.8346426,31.8844026 L21.5945511,31.8844026 C21.1287681,31.8844026 20.751875,31.5069881 20.751875,31.0405608 L20.751875,22.7890697 C20.751875,22.4134355 21.2052134,22.2247282 21.4701052,22.4899865 L24.2843587,25.3081333 C26.8337204,23.0240636 28.4444049,19.7092251 28.4444049,16.0223129 C28.4444049,9.15052091 22.8621207,3.56051397 15.9998222,3.56051397 L15.9998222,0 C24.8230314,0 32,7.18689745 32,16.0223129 C32,20.6919269 29.9750886,24.8790914 26.7821642,27.8093944 Z" id="Mask" fill="#8B889A"></path></g></svg><span>Learning Paths</span></a></li><li class="nav-highlights"><a href="https://learning.oreilly.com/u/d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" fill="#4A3C31"><path d="M13.325 18.071H8.036c0-6.736 4.324-10.925 14.464-12.477V0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35c5.142 0 9.175-3.515 9.175-8.816 0-4.628-2.367-7.293-6.253-8.113zm27.5 0h-5.26c0-6.736 4.295-10.925 14.435-12.477V0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35c5.113 0 9.146-3.515 9.146-8.816 0-4.628-2.338-7.293-6.253-8.113z" fill-rule="evenodd"></path></svg><span>Highlights</span></a></li><li><a href="https://learning.oreilly.com/u/preferences/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l1 no-icon">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://learning.oreilly.com/u/preferences/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l2">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9781492032632/chapter/part01.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"><div data-reactroot="" class="menu-dropdown-wrapper js-menu-dropdown-wrapper align-right"><img class="hidden" src="./Chapter14_files/ajax-transp.gif" alt="loading spinner"><div class="menu-control"><div class="control "><div class="js-playlists-menu"><button class="js-playlist-icon"><svg class="icon-add-to-playlist-sml" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill-rule="nonzero" fill="#000000"><g transform="translate(-1.000000, 0.000000)"><rect x="5" y="0" width="12" height="2"></rect><title>Playlists</title><path d="M4.5,14 C6.43299662,14 8,12.4329966 8,10.5 C8,8.56700338 6.43299662,7 4.5,7 C2.56700338,7 1,8.56700338 1,10.5 C1,12.4329966 2.56700338,14 4.5,14 Z M2.5,10 L4,10 L4,8.5 L5,8.5 L5,10 L6.5,10 L6.5,11 L5,11 L5,12.5 L4,12.5 L4,11 L2.5,11 L2.5,10 Z"></path><circle cx="2" cy="5" r="1"></circle><circle cx="1.94117647" cy="1" r="1"></circle><rect x="5" y="4" width="12" height="2"></rect><rect x="9" y="8" width="8" height="2"></rect><rect x="9" y="12" width="8" height="2"></rect></g></g></g></svg><div class="js-playlist-addto-label">Add&nbsp;To</div></button></div></div></div></div></div></div></li><li class="js-font-control-panel font-control-activator"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html&amp;text=Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%20I.%20The%20Fundamentals%20of%20Machine%20Learning&amp;body=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html%0D%0Afrom%20Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    
    <section role="document">
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch13.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">13. Loading and Preprocessing Data with TensorFlow</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">15. Processing Sequences Using RNNs and CNNs</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content"><div class="annotator-wrapper"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 14. Deep Computer Vision Using Convolutional Neural Networks"><div class="chapter" id="cnn_chapter">
<h1><span class="label">Chapter 14. </span>Deep Computer Vision Using Convolutional Neural Networks</h1>


<p>Although IBM’s Deep Blue supercomputer beat the chess world champion Garry Kasparov back in 1996, it wasn’t until fairly recently that computers were able to reliably perform seemingly trivial tasks such as detecting a puppy in a picture or recognizing spoken words. Why are these tasks so effortless to us humans? The answer lies in the fact that perception largely takes place outside the realm of our consciousness, within specialized visual, auditory, and other sensory modules in our brains. By the time sensory information reaches our consciousness, it is already adorned with high-level features; for example, when you look at a picture of a cute puppy, you cannot choose <em>not</em> to see the puppy, <em>not</em> to notice its cuteness. Nor can you explain <em>how</em> you recognize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective experience: perception is not trivial at all, and to understand it we must look at how the sensory modules work.</p>

<p>Convolutional neural networks (CNNs) emerged from the study of the brain’s visual cortex, and they have been used in image recognition since the 1980s. In the last few years, thanks to the increase in computational power, the amount of available training data, and the tricks presented in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a> for training deep nets, CNNs have managed to achieve superhuman performance on some complex visual tasks. They power image search services, self-driving cars, automatic video classification systems, and more. Moreover, CNNs are not restricted to visual perception: they are also successful at many other tasks, such as <em>voice recognition</em> and <em>natural language processing</em> (NLP); however, we will focus on visual applications for now.</p>

<p>In this chapter we will present where CNNs came from, what their building blocks look like, and how to implement them using TensorFlow and Keras. Then we will discuss some of the best CNN architectures, as well as other visual tasks, including <em>object detection</em> (classifying multiple objects in an image and placing bounding boxes around them) and <em>semantic segmentation</em> (classifying each pixel according to the class of the object it belongs to).</p>






<section data-type="sect1" data-pdf-bookmark="The Architecture of the Visual Cortex"><div class="sect1" id="idm46263500623480">
<h1>The Architecture of the Visual Cortex</h1>

<p>David H. Hubel and Torsten Wiesel performed a series of experiments on cats in <a href="https://homl.info/71">1958</a><sup><a data-type="noteref" id="idm46263500621160-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500621160" class="totri-footnote">1</a></sup> and <a href="https://homl.info/72">1959</a><sup><a data-type="noteref" id="idm46263500619400-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500619400" class="totri-footnote">2</a></sup> (and a <a href="https://homl.info/73">few years later on monkeys</a><sup><a data-type="noteref" id="idm46263500617560-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500617560" class="totri-footnote">3</a></sup>), giving crucial insights into the structure of the visual cortex (the authors received the Nobel Prize in Physiology or Medicine in 1981 for their work). In particular, they showed that many neurons in the visual cortex have a small <em>local receptive field</em>, meaning they react only to visual stimuli located in a limited region of the visual field (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cat_visual_cortex_diagram">Figure&nbsp;14-1</a>, in which the local receptive fields of five neurons are represented by dashed circles). The receptive fields of different neurons may overlap, and together they tile the whole visual field. Moreover, the authors showed that some neurons react only to images of horizontal lines, while others react only to lines with different orientations (two neurons may have the same receptive field but react to different line orientations). They also noticed that some neurons have larger receptive fields, and they react to more complex patterns that are combinations of the lower-level patterns. These observations led to the idea that the higher-level neurons are based on the outputs of neighboring lower-level neurons (in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cat_visual_cortex_diagram">Figure&nbsp;14-1</a>, notice that each neuron is connected only to a few neurons from the previous layer). This powerful architecture is able to detect all sorts of complex patterns in any area of the visual field.</p>

<figure><div id="cat_visual_cortex_diagram" class="figure">
<img src="./Chapter14_files/mls2_1401.png" alt="mls2 1401" width="1439" height="470" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1401.png">
<h6><span class="label">Figure 14-1. </span>Biological neurons in the visual cortex respond to specific patterns in small regions of the visual field called receptive fields; as the visual signal makes its way through consecutive brain modules, neurons respond to more complex patterns in larger receptive fields.</h6>
</div></figure>

<p>These studies of the visual cortex inspired the <a href="https://homl.info/74">neocognitron</a>,<sup><a data-type="noteref" id="idm46263500568024-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500568024" class="totri-footnote">4</a></sup> introduced in 1980, which gradually evolved into what we now call <em>convolutional neural networks</em>. An important milestone was a <a href="https://homl.info/75">1998 paper</a><sup><a data-type="noteref" id="idm46263500565784-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500565784" class="totri-footnote">5</a></sup> by Yann LeCun et al. that introduced the famous <em>LeNet-5</em> architecture, widely used by banks to recognize handwritten check numbers. This architecture has some building blocks that you already know, such as fully connected layers and sigmoid activation functions, but it also introduces two new building blocks: <em>convolutional layers</em> and <em>pooling layers</em>. Let’s look at them now.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Why not simply use a deep neural network with fully connected layers for image recognition tasks? Unfortunately, although this works fine for small images (e.g., MNIST), it breaks down for larger images because of the huge number of parameters it requires. For example, a 100 × 100 image has 10,000 pixels, and if the first layer has just 1,000 neurons (which already severely restricts the amount of information transmitted to the next layer), this means a total of 10 million connections. And that’s just the first layer. CNNs solve this problem using partially connected layers and weight sharing.</p>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Convolutional Layer"><div class="sect1" id="idm46263500561160">
<h1>Convolutional Layer</h1>

<p>The most important building block of a CNN is the <em>convolutional layer</em>:<sup><a data-type="noteref" id="idm46263500559016-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500559016" class="totri-footnote">6</a></sup> neurons in the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cnn_layers_diagram">Figure&nbsp;14-2</a>). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on small low-level features in the first hidden layer, then assemble them into larger higher-level features in the next hidden layer, and so on. This hierarchical structure is common in real-world images, which is one of the reasons why CNNs work so well for image recognition.</p>

<figure class="smallerseventy"><div id="cnn_layers_diagram" class="figure">
<img src="./Chapter14_files/mls2_1402.png" alt="mls2 1402" width="1151" height="687" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1402.png">
<h6><span class="label">Figure 14-2. </span>CNN layers with rectangular local receptive fields</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Until now, all multilayer neural networks we looked at had layers composed of a long line of neurons, and we had to flatten input images to 1D before feeding them to the neural network. Now each layer is represented in 2D, which makes it easier to match neurons with their corresponding inputs.</p>
</div>

<p>A neuron located in row <em>i</em>, column <em>j</em> of a given layer is connected to the outputs of the neurons in the previous layer located in rows <em>i</em> to <em>i</em> + <em>f</em><sub><em>h</em></sub> – 1, columns <em>j</em> to <em>j</em> + <em>f</em><sub><em>w</em></sub> – 1, where <em>f</em><sub><em>h</em></sub> and <em>f</em><sub><em>w</em></sub> are the height and width of the receptive field (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#slide_and_padding_diagram">Figure&nbsp;14-3</a>). In order for a layer to have the same height and width as the previous layer, it is common to add zeros around the inputs, as shown in the diagram. This is called <em>zero padding</em>.</p>

<figure class="smallerseventy"><div id="slide_and_padding_diagram" class="figure">
<img src="./Chapter14_files/mls2_1403.png" alt="mls2 1403" width="1102" height="776" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1403.png">
<h6><span class="label">Figure 14-3. </span>Connections between layers and zero padding</h6>
</div></figure>

<p>It is also possible to connect a large input layer to a much smaller layer by spacing out the receptive fields, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#stride_diagram">Figure&nbsp;14-4</a>. This dramatically reduces the model’s computational complexity. The shift from one receptive field to the next is called the <em>stride</em>. In the diagram, a 5 × 7 input layer (plus zero padding) is connected to a 3 × 4 layer, using 3 × 3 receptive fields and a stride of 2 (in this example the stride is the same in both directions, but it does not have to be so). A neuron located in row <em>i</em>, column <em>j</em>, in the upper layer is connected to the outputs of the neurons in the previous layer located in rows <em>i</em> × <em>s</em><sub><em>h</em></sub> to <em>i</em> × <em>s</em><sub><em>h</em></sub> + <em>f</em><sub><em>h</em></sub> – 1, columns <em>j</em> × <em>s</em><sub><em>w</em></sub> to <em>j</em> × <em>s</em><sub><em>w</em></sub> + <em>f</em><sub><em>w</em></sub> – 1, where <em>s</em><sub><em>h</em></sub> and <em>s</em><sub><em>w</em></sub> are the vertical and horizontal strides.</p>

<figure class="smallerseventy"><div id="stride_diagram" class="figure">
<img src="./Chapter14_files/mls2_1404.png" alt="mls2 1404" width="1049" height="683" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1404.png">
<h6><span class="label">Figure 14-4. </span>Reducing dimensionality using a stride of 2</h6>
</div></figure>








<section data-type="sect2" data-pdf-bookmark="Filters"><div class="sect2" id="idm46263500526584">
<h2>Filters</h2>

<p>A neuron’s weights can be represented as a small image the size of the receptive field. For example, <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#filters_diagram">Figure&nbsp;14-5</a> shows two possible sets of weights, called <em>filters</em> (or <em>convolution kernels</em>). The first one is represented as a black square with a vertical white line in the middle (it is a 7 × 7 matrix full of 0s except for the central column, which is full of 1s); neurons using these weights will ignore everything in their receptive field except for the central vertical line (since all inputs will get multiplied by 0, except for the ones located in the central vertical line). The second filter is a black square with a horizontal white line in the middle. Once again, neurons using these weights will ignore everything in their receptive field except for the central horizontal line.</p>

<p>Now if all neurons in a layer use the same vertical line filter (and the same bias term), and you feed the network the input image shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#filters_diagram">Figure&nbsp;14-5</a> (bottom image), the layer will output the top-left image. Notice that the vertical white lines get enhanced while the rest gets blurred. Similarly, the upper-right image is what you get if all neurons use the same horizontal line filter; notice that the horizontal white lines get enhanced while the rest is blurred out. Thus, a layer full of neurons using the same filter outputs a <em>feature map</em>, which highlights the areas in an image that activate the filter the most. Of course you do not have to define the filters manually: instead, during training the convolutional layer will automatically learn the most useful filters for its task, and the layers above will learn to combine them into more complex patterns.</p>

<figure><div id="filters_diagram" class="figure">
<img src="./Chapter14_files/mls2_1405.png" alt="mls2 1405" width="1440" height="904" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1405.png">
<h6><span class="label">Figure 14-5. </span>Applying two different filters to get two feature maps</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Stacking Multiple Feature Maps"><div class="sect2" id="idm46263500517976">
<h2>Stacking Multiple Feature Maps</h2>

<p>Up to now, for simplicity, I have represented the output of each convolutional layer as a 2D layer, but in reality a convolutional layer has multiple filters (you decide how many), and it outputs one feature map per filter, so it is more accurately represented in 3D (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cnn_layers_volume_diagram">Figure&nbsp;14-6</a>). It has one neuron per pixel in each feature map, and all neurons within a given feature map share the same parameters (i.e., the same weights and bias term). Neurons in different feature maps use different parameters. A neuron’s receptive field is the same as described earlier, but it extends across all the previous layers’ feature maps. In short, a convolutional layer simultaneously applies multiple trainable filters to its inputs, making it capable of detecting multiple features anywhere in its inputs.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The fact that all neurons in a feature map share the same parameters dramatically reduces the number of parameters in the model. Once the CNN has learned to recognize a pattern in one location, it can recognize it in any other location. In contrast, once a regular DNN has learned to recognize a pattern in one location, it can recognize it only in that particular location.</p>
</div>

<p>Input images are also composed of multiple sublayers: one per <em>color channel</em>. There are typically three: red, green, and blue (RGB). Grayscale images have just one channel, but some images may have much more—for example, satellite images that capture extra light frequencies (such as infrared).</p>

<figure><div id="cnn_layers_volume_diagram" class="figure">
<img src="./Chapter14_files/mls2_1406.png" alt="mls2 1406" width="1440" height="1286" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1406.png">
<h6><span class="label">Figure 14-6. </span>Convolution layers with multiple feature maps, and images with three color channels</h6>
</div></figure>

<p>Specifically, a neuron located in row <em>i</em>, column <em>j</em> of the feature map <em>k</em> in a given convolutional layer <em>l</em> is connected to the outputs of the neurons in the previous layer <em>l</em> – 1, located in rows <em>i</em> × <em>s</em><sub><em>h</em></sub> to <em>i</em> × <em>s</em><sub><em>h</em></sub> + <em>f</em><sub><em>h</em></sub> – 1 and columns <em>j</em> × <em>s</em><sub><em>w</em></sub> to <em>j</em> × <em>s</em><sub><em>w</em></sub> + <em>f</em><sub><em>w</em></sub> – 1, across all feature maps (in layer <em>l</em> – <em>1</em>). Note that all neurons located in the same row <em>i</em> and column <em>j</em> but in different feature maps are connected to the outputs of the exact same neurons in the previous layer.</p>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#convolutional_layer_equation">Equation 14-1</a> summarizes the preceding explanations in one big mathematical equation: it shows how to compute the output of a given neuron in a convolutional layer. It is a bit ugly due to all the different indices, but all it does is calculate the weighted sum of all the inputs, plus the bias term.</p>
<div data-type="equation" id="convolutional_layer_equation">
<h5><span class="label">Equation 14-1. </span>Computing the output of a neuron in a convolutional layer</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-142-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;munderover&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/munderover&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;munderover&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/munderover&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;munderover&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;&#39;&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;&#39;&lt;/mo&gt;&lt;/msup&gt;&lt;/msub&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/munderover&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;&#39;&lt;/mo&gt;&lt;/msup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;&#39;&lt;/mo&gt;&lt;/msup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;&#39;&lt;/mo&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;&#39;&lt;/mo&gt;&lt;/msup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;mtext&gt;with&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mfenced separators=&quot;&quot; open=&quot;{&quot; close=&quot;&quot;&gt;&lt;mtable&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;&#39;&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;&amp;#xD7;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;&#39;&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;&amp;#xD7;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;v&lt;/mi&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5693" style="width: 29.412em; display: inline-block;"><span style="display: inline-block; position: relative; width: 28.538em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.339em, 1028.34em, 4.733em, -1000.01em); top: -3.339em; left: 0em;"><span class="mrow" id="MathJax-Span-5694"><span class="mrow" id="MathJax-Span-5695"><span class="msub" id="MathJax-Span-5696"><span style="display: inline-block; position: relative; width: 1.853em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5697" style="font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.465em;"><span class="mrow" id="MathJax-Span-5698"><span class="mi" id="MathJax-Span-5699" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-5700" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-5701" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span class="mo" id="MathJax-Span-5702" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-5703" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5704" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="msub" id="MathJax-Span-5705" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 0.877em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.42em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5706" style="font-family: MathJax_Math-italic;">b</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.414em;"><span class="mi" id="MathJax-Span-5707" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5708" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="munderover" id="MathJax-Span-5709" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.596em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1001.4em, 4.63em, -1000.01em); top: -4.008em; left: 0.054em;"><span class="mo" id="MathJax-Span-5710" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.3em, 4.27em, -1000.01em); top: -2.928em; left: 0.157em;"><span class="mrow" id="MathJax-Span-5711"><span class="mi" id="MathJax-Span-5712" style="font-size: 70.7%; font-family: MathJax_Math-italic;">u</span><span class="mo" id="MathJax-Span-5713" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="mn" id="MathJax-Span-5714" style="font-size: 70.7%; font-family: MathJax_Main;">0</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.242em, 1001.55em, 4.321em, -1000.01em); top: -5.242em; left: 0em;"><span class="mrow" id="MathJax-Span-5715"><span class="msub" id="MathJax-Span-5716"><span style="display: inline-block; position: relative; width: 0.671em; height: 0px;"><span style="position: absolute; clip: rect(3.345em, 1000.37em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5717" style="font-size: 70.7%; font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.362em;"><span class="mi" id="MathJax-Span-5718" style="font-size: 50%; font-family: MathJax_Math-italic;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5719" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-5720" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mspace" id="MathJax-Span-5721" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mspace" id="MathJax-Span-5722" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="munderover" id="MathJax-Span-5723" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.648em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1001.4em, 4.63em, -1000.01em); top: -4.008em; left: 0.105em;"><span class="mo" id="MathJax-Span-5724" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.25em, 4.27em, -1000.01em); top: -2.928em; left: 0.208em;"><span class="mrow" id="MathJax-Span-5725"><span class="mi" id="MathJax-Span-5726" style="font-size: 70.7%; font-family: MathJax_Math-italic;">v</span><span class="mo" id="MathJax-Span-5727" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="mn" id="MathJax-Span-5728" style="font-size: 70.7%; font-family: MathJax_Main;">0</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.242em, 1001.61em, 4.321em, -1000.01em); top: -5.242em; left: 0em;"><span class="mrow" id="MathJax-Span-5729"><span class="msub" id="MathJax-Span-5730"><span style="display: inline-block; position: relative; width: 0.774em; height: 0px;"><span style="position: absolute; clip: rect(3.345em, 1000.37em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5731" style="font-size: 70.7%; font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.362em;"><span class="mi" id="MathJax-Span-5732" style="font-size: 50%; font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5733" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-5734" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mspace" id="MathJax-Span-5735" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mspace" id="MathJax-Span-5736" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="munderover" id="MathJax-Span-5737" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.802em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1001.4em, 4.63em, -1000.01em); top: -4.008em; left: 0.157em;"><span class="mo" id="MathJax-Span-5738" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.345em, 1001.45em, 4.27em, -1000.01em); top: -2.877em; left: 0.157em;"><span class="mrow" id="MathJax-Span-5739"><span class="mi" id="MathJax-Span-5740" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span class="mo" id="MathJax-Span-5741" style="font-size: 70.7%; font-family: MathJax_Main;">'</span><span class="mo" id="MathJax-Span-5742" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="mn" id="MathJax-Span-5743" style="font-size: 70.7%; font-family: MathJax_Main;">0</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.242em, 1001.71em, 4.373em, -1000.01em); top: -5.242em; left: 0em;"><span class="mrow" id="MathJax-Span-5744"><span class="msub" id="MathJax-Span-5745"><span style="display: inline-block; position: relative; width: 0.877em; height: 0px;"><span style="position: absolute; clip: rect(3.345em, 1000.37em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5746" style="font-size: 70.7%; font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.362em;"><span class="msup" id="MathJax-Span-5747"><span style="display: inline-block; position: relative; width: 0.465em; height: 0px;"><span style="position: absolute; clip: rect(3.653em, 1000.27em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5748" style="font-size: 50%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.162em; left: 0.311em;"><span class="mo" id="MathJax-Span-5749" style="font-size: 50%; font-family: MathJax_Main;">′</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5750" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-5751" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mspace" id="MathJax-Span-5752" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mspace" id="MathJax-Span-5753" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="msub" id="MathJax-Span-5754" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 2.522em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.52em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5755" style="font-family: MathJax_Math-italic;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.751em; left: 0.568em;"><span class="mrow" id="MathJax-Span-5756"><span class="msup" id="MathJax-Span-5757"><span style="display: inline-block; position: relative; width: 0.414em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.22em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5758" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.265em; left: 0.26em;"><span class="mo" id="MathJax-Span-5759" style="font-size: 50%; font-family: MathJax_Main;">′</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5760" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="msup" id="MathJax-Span-5761"><span style="display: inline-block; position: relative; width: 0.465em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.27em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5762" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.265em; left: 0.311em;"><span class="mo" id="MathJax-Span-5763" style="font-size: 50%; font-family: MathJax_Main;">′</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5764" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="msup" id="MathJax-Span-5765"><span style="display: inline-block; position: relative; width: 0.568em; height: 0px;"><span style="position: absolute; clip: rect(3.345em, 1000.37em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5766" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.316em; left: 0.362em;"><span class="mo" id="MathJax-Span-5767" style="font-size: 50%; font-family: MathJax_Main;">′</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5768" style="font-family: MathJax_Main;">.</span><span class="msub" id="MathJax-Span-5769" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 3.036em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.68em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5770" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.751em; left: 0.722em;"><span class="mrow" id="MathJax-Span-5771"><span class="mi" id="MathJax-Span-5772" style="font-size: 70.7%; font-family: MathJax_Math-italic;">u</span><span class="mo" id="MathJax-Span-5773" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-5774" style="font-size: 70.7%; font-family: MathJax_Math-italic;">v</span><span class="mo" id="MathJax-Span-5775" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="msup" id="MathJax-Span-5776"><span style="display: inline-block; position: relative; width: 0.568em; height: 0px;"><span style="position: absolute; clip: rect(3.345em, 1000.37em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5777" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.316em; left: 0.362em;"><span class="mo" id="MathJax-Span-5778" style="font-size: 50%; font-family: MathJax_Main;">′</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5779" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-5780" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mspace" id="MathJax-Span-5781" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-5782" style="font-family: MathJax_Main;">with</span><span class="mspace" id="MathJax-Span-5783" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mfenced" id="MathJax-Span-5784" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-5785" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">{</span></span><span class="mtable" id="MathJax-Span-5786" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 6.327em; height: 0px;"><span style="position: absolute; clip: rect(2.522em, 1006.28em, 5.144em, -1000.01em); top: -4.008em; left: 0em;"><span style="display: inline-block; position: relative; width: 6.327em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1006.18em, 4.321em, -1000.01em); top: -4.676em; left: 0em;"><span class="mtd" id="MathJax-Span-5787"><span class="mrow" id="MathJax-Span-5788"><span class="mrow" id="MathJax-Span-5789"><span class="mi" id="MathJax-Span-5790" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-5791" style="font-family: MathJax_Main;">'</span><span class="mo" id="MathJax-Span-5792" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mi" id="MathJax-Span-5793" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">i</span><span class="mo" id="MathJax-Span-5794" style="font-family: MathJax_Main; padding-left: 0.208em;">×</span><span class="msub" id="MathJax-Span-5795" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.42em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5796" style="font-family: MathJax_Math-italic;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.465em;"><span class="mi" id="MathJax-Span-5797" style="font-size: 70.7%; font-family: MathJax_Math-italic;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5798" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-5799" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">u</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1006.28em, 4.373em, -1000.01em); top: -3.237em; left: 0em;"><span class="mtd" id="MathJax-Span-5800"><span class="mrow" id="MathJax-Span-5801"><span class="mrow" id="MathJax-Span-5802"><span class="mi" id="MathJax-Span-5803" style="font-family: MathJax_Math-italic;">j</span><span class="mo" id="MathJax-Span-5804" style="font-family: MathJax_Main;">'</span><span class="mo" id="MathJax-Span-5805" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mi" id="MathJax-Span-5806" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">j</span><span class="mo" id="MathJax-Span-5807" style="font-family: MathJax_Main; padding-left: 0.208em;">×</span><span class="msub" id="MathJax-Span-5808" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.42em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5809" style="font-family: MathJax_Math-italic;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.465em;"><span class="mi" id="MathJax-Span-5810" style="font-size: 70.7%; font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5811" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-5812" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">v</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.345em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.321em; border-left: 0px solid; width: 0px; height: 3.286em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>z</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>=</mo><msub><mi>b</mi><mi>k</mi></msub><mo>+</mo><munderover><mo>∑</mo><mrow><mi>u</mi><mo>=</mo><mn>0</mn></mrow><mrow><msub><mi>f</mi><mi>h</mi></msub><mo>-</mo><mn>1</mn></mrow></munderover><mspace width="0.166667em"></mspace><mspace width="0.166667em"></mspace><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>0</mn></mrow><mrow><msub><mi>f</mi><mi>w</mi></msub><mo>-</mo><mn>1</mn></mrow></munderover><mspace width="0.166667em"></mspace><mspace width="0.166667em"></mspace><munderover><mo>∑</mo><mrow><mi>k</mi><mo>'</mo><mo>=</mo><mn>0</mn></mrow><mrow><msub><mi>f</mi><msup><mi>n</mi><mo>'</mo></msup></msub><mo>-</mo><mn>1</mn></mrow></munderover><mspace width="0.166667em"></mspace><mspace width="0.166667em"></mspace><msub><mi>x</mi><mrow><msup><mi>i</mi><mo>'</mo></msup><mo>,</mo><msup><mi>j</mi><mo>'</mo></msup><mo>,</mo><msup><mi>k</mi><mo>'</mo></msup></mrow></msub><mo>.</mo><msub><mi>w</mi><mrow><mi>u</mi><mo>,</mo><mi>v</mi><mo>,</mo><msup><mi>k</mi><mo>'</mo></msup><mo>,</mo><mi>k</mi></mrow></msub><mspace width="1.em"></mspace><mtext>with</mtext><mspace width="4.pt"></mspace><mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mi>i</mi><mo>'</mo><mo>=</mo><mi>i</mi><mo>×</mo><msub><mi>s</mi><mi>h</mi></msub><mo>+</mo><mi>u</mi></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mi>j</mi><mo>'</mo><mo>=</mo><mi>j</mi><mo>×</mo><msub><mi>s</mi><mi>w</mi></msub><mo>+</mo><mi>v</mi></mrow></mtd></mtr></mtable></mfenced></mrow></math></span></span></div><script type="math/mml" id="MathJax-Element-142"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <msub><mi>z</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow> </msub>
    <mo>=</mo>
    <msub><mi>b</mi> <mi>k</mi> </msub>
    <mo>+</mo>
    <munderover><mo>∑</mo> <mrow><mi>u</mi><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi> <mi>h</mi> </msub><mo>-</mo><mn>1</mn></mrow> </munderover>
    <mspace width="0.166667em"></mspace>
    <mspace width="0.166667em"></mspace>
    <munderover><mo>∑</mo> <mrow><mi>v</mi><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi> <mi>w</mi> </msub><mo>-</mo><mn>1</mn></mrow> </munderover>
    <mspace width="0.166667em"></mspace>
    <mspace width="0.166667em"></mspace>
    <munderover><mo>∑</mo> <mrow><mi>k</mi><mo>'</mo><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi> <msup><mi>n</mi> <mo>'</mo> </msup> </msub><mo>-</mo><mn>1</mn></mrow> </munderover>
    <mspace width="0.166667em"></mspace>
    <mspace width="0.166667em"></mspace>
    <msub><mi>x</mi> <mrow><msup><mi>i</mi> <mo>'</mo> </msup><mo>,</mo><msup><mi>j</mi> <mo>'</mo> </msup><mo>,</mo><msup><mi>k</mi> <mo>'</mo> </msup></mrow> </msub>
    <mo>.</mo>
    <msub><mi>w</mi> <mrow><mi>u</mi><mo>,</mo><mi>v</mi><mo>,</mo><msup><mi>k</mi> <mo>'</mo> </msup><mo>,</mo><mi>k</mi></mrow> </msub>
    <mspace width="1.em"></mspace>
    <mtext>with</mtext>
    <mspace width="4.pt"></mspace>
    <mfenced separators="" open="{" close="">
      <mtable>
        <mtr>
          <mtd columnalign="left">
            <mrow>
              <mi>i</mi>
              <mo>'</mo>
              <mo>=</mo>
              <mi>i</mi>
              <mo>×</mo>
              <msub><mi>s</mi> <mi>h</mi> </msub>
              <mo>+</mo>
              <mi>u</mi>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd columnalign="left">
            <mrow>
              <mi>j</mi>
              <mo>'</mo>
              <mo>=</mo>
              <mi>j</mi>
              <mo>×</mo>
              <msub><mi>s</mi> <mi>w</mi> </msub>
              <mo>+</mo>
              <mi>v</mi>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mfenced>
  </mrow>
</math></script>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><em>z</em><sub><em>i, j, k</em></sub> is the output of the neuron located in row <em>i</em>, column <em>j</em> in feature map <em>k</em> of the convolutional layer (layer <em>l</em>).</p>
</li>
<li>
<p>As explained earlier, <em>s</em><sub><em>h</em></sub> and <em>s</em><sub><em>w</em></sub> are the vertical and horizontal strides,  <em>f</em><sub><em>h</em></sub> and <em>f</em><sub><em>w</em></sub> are the height and width of the receptive field, and <em>f</em><sub><em>n</em>′</sub> is the number of feature maps in the previous layer (layer <em>l</em> – 1).</p>
</li>
<li>
<p><em>x</em><sub><em>i</em>′, <em>j</em>′, <em>k</em>′</sub> is the output of the neuron located in layer <em>l</em> – 1, row <em>i</em>′, column <em>j</em>′, feature map <em>k</em>′ (or channel <em>k</em>′ if the previous layer is the input layer).</p>
</li>
<li>
<p><em>b</em><sub><em>k</em></sub> is the bias term for feature map <em>k</em> (in layer <em>l</em>). You can think of it as a knob that tweaks the overall brightness of the feature map <em>k</em>.</p>
</li>
<li>
<p><em>w</em><sub><em>u</em>, <em>v</em>, <em>k</em>′ ,<em>k</em></sub> is the connection weight between any neuron in feature map <em>k</em> of the layer <em>l</em> and its input located at row <em>u</em>, column <em>v</em> (relative to the neuron’s receptive field), and feature map <em>k</em>′.</p>
</li>
</ul>
</div></section>













<section data-type="sect2" data-pdf-bookmark="TensorFlow Implementation"><div class="sect2" id="idm46263500517352">
<h2>TensorFlow Implementation</h2>

<p>In TensorFlow, each input image is typically represented as a 3D tensor of <code>shape [height, width, channels]</code>. A mini-batch is represented as a 4D tensor of <code>shape [mini-batch size, height, width, channels]</code>. The weights of a convolutional layer are represented as a 4D tensor of shape [<em>f</em><sub><em>h</em></sub>, <em>f</em><sub><em>w</em></sub>, <em>f</em><sub><em>n</em>′</sub>, <em>f</em><sub><em>n</em></sub>]. The bias terms of a convolutional layer are simply represented as a 1D tensor of shape [<em>f</em><sub><em>n</em></sub>].</p>

<p>Let’s look at a simple example. The following code loads two sample images, using Scikit-Learn’s <code>load_sample_images()</code> (which loads two color images, one of a Chinese temple, and the other of a flower), then it creates two filters and applies them to both images, and finally it displays one of the resulting feature maps:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_sample_image</code>

<code class="c1"># Load sample images</code>
<code class="n">china</code> <code class="o">=</code> <code class="n">load_sample_image</code><code class="p">(</code><code class="s2">"china.jpg"</code><code class="p">)</code> <code class="o">/</code> <code class="mi">255</code>
<code class="n">flower</code> <code class="o">=</code> <code class="n">load_sample_image</code><code class="p">(</code><code class="s2">"flower.jpg"</code><code class="p">)</code> <code class="o">/</code> <code class="mi">255</code>
<code class="n">images</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">china</code><code class="p">,</code> <code class="n">flower</code><code class="p">])</code>
<code class="n">batch_size</code><code class="p">,</code> <code class="n">height</code><code class="p">,</code> <code class="n">width</code><code class="p">,</code> <code class="n">channels</code> <code class="o">=</code> <code class="n">images</code><code class="o">.</code><code class="n">shape</code>

<code class="c1"># Create 2 filters</code>
<code class="n">filters</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">(</code><code class="mi">7</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="n">channels</code><code class="p">,</code> <code class="mi">2</code><code class="p">),</code> <code class="n">dtype</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code>
<code class="n">filters</code><code class="p">[:,</code> <code class="mi">3</code><code class="p">,</code> <code class="p">:,</code> <code class="mi">0</code><code class="p">]</code> <code class="o">=</code> <code class="mi">1</code>  <code class="c1"># vertical line</code>
<code class="n">filters</code><code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="p">:,</code> <code class="p">:,</code> <code class="mi">1</code><code class="p">]</code> <code class="o">=</code> <code class="mi">1</code>  <code class="c1"># horizontal line</code>

<code class="n">outputs</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">conv2d</code><code class="p">(</code><code class="n">images</code><code class="p">,</code> <code class="n">filters</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">)</code>

<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">outputs</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="p">:,</code> <code class="p">:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"gray"</code><code class="p">)</code> <code class="c1"># plot 1st image's 2nd feature map</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p>Let’s go through this code:</p>

<ul>
<li>
<p>The pixel intensity for each color channel is represented as a byte from 0 to 255, so we scale these features simply by dividing by 255, to get floats ranging from 0 to 1.</p>
</li>
<li>
<p>Then we create two 7 × 7 filters (one with a vertical white line in the middle, and the other with a horizontal white line in the middle).</p>
</li>
<li>
<p>We apply them to both images using the <code>tf.nn.conv2d()</code> function, which is part of TensorFlow’s low-level Deep Learning API. In this example, we use zero padding (<code>padding="same"</code>) and a stride of <code>2</code>.</p>
</li>
<li>
<p>Finally, we plot one of the resulting feature maps (similar to the top-right image in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#filters_diagram">Figure&nbsp;14-5</a>).</p>
</li>
</ul>

<p>The <code>tf.nn.conv2d()</code> line deserves a bit more explanation:</p>

<ul>
<li>
<p><code>images</code> is the input mini-batch (a 4D tensor, as explained earlier).</p>
</li>
<li>
<p><code>filters</code> is the set of filters to apply (also a 4D tensor, as explained earlier).</p>
</li>
<li>
<p><code>strides</code> is equal to 1, but it could also be a 1D array with four elements, where the two central elements are the vertical and horizontal strides (<em>s</em><sub><em>h</em></sub> and <em>s</em><sub><em>w</em></sub>). The first and last elements must currently be equal to 1. They may one day be used to specify a batch stride (to skip some instances) and a channel stride (to skip some of the previous layer’s feature maps or channels).</p>
</li>
<li>
<p><code>padding</code> must be either <code>"same"</code> or <code>"valid"</code>:</p>

<ul>
<li>
<p>If set to <code>"same"</code>, the convolutional layer uses zero padding if necessary. In this case, the number of output neurons is equal to the number of input neurons divided by the stride, rounded up (in this example, 13 / 5 = 2.6, rounded up to 3). Then zeros are added as evenly as possible around the inputs. When <code>strides=1</code>, the layer’s outputs will have the same spatial dimensions (width and height) as its inputs, hence the name <em>same</em>.</p>
</li>
<li>
<p>If set to <code>"valid"</code>, the convolutional layer does <em>not</em> use zero padding and may ignore some rows and columns at the bottom and right of the input image, depending on the stride, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#padding_options_diagram">Figure&nbsp;14-7</a> (for simplicity, only the horizontal dimension is shown here, but of course the same logic applies to the vertical dimension). This means that every neuron’s receptive field lies strictly within valid positions inside the input (it does not go out of bounds), hence the name <em>valid</em>.</p>
</li>
</ul>
</li>
</ul>

<figure><div id="padding_options_diagram" class="figure">
<img src="./Chapter14_files/mls2_1407.png" alt="mls2 1407" width="1438" height="883" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1407.png">
<h6><span class="label">Figure 14-7. </span>Padding option: input width, 13; filter width, 6; stride, 5</h6>
</div></figure>

<p>In this example, we manually defined the filters, but in a real CNN you would normally define filters as trainable variables, so the neural net can learn which filters work best, as explained earlier. Instead of manually creating the variables, use the <code>keras.layers.Conv2D</code> layer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">conv</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2D</code><code class="p">(</code><code class="n">filters</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
                           <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">)</code></pre>

<p>This code creates a <code>Conv2D</code> layer with 32 filters, each 3 × 3, using a stride of 1 (both horizontally and vertically), <code>"same"</code> padding, and applying the ReLU activation function to its outputs. As you can see, convolutional layers have quite a few hyperparameters: you must choose the number of filters, their height and width, the strides, and the padding type. As always, you can use cross-validation to find the right hyperparameter values, but this is very time-consuming. We will discuss common CNN architectures later, to give you some idea of which hyperparameter values work best in practice.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Memory Requirements"><div class="sect2" id="idm46263500424872">
<h2>Memory Requirements</h2>

<p>Another problem with CNNs is that the convolutional layers require a huge amount of RAM. This is especially true during training, because the reverse pass of backpropagation requires all the intermediate values computed during the forward pass.</p>

<p>For example, consider a convolutional layer with 5 × 5 filters, outputting 200 feature maps of size 150 × 100, with stride 1 and <code>"same"</code> padding. If the input is a 150 × 100 RGB image (three channels), then the number of parameters is (5 × 5 × 3 + 1) × 200 = 15,200 (the +1 corresponds to the bias terms), which is fairly small compared to a fully connected layer.<sup><a data-type="noteref" id="idm46263500182904-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500182904" class="totri-footnote">7</a></sup> However, each of the 200 feature maps contains 150 × 100 neurons, and each of these neurons needs to compute a weighted sum of its 5 × 5 × 3 = 75  inputs: that’s a total of 225 million float multiplications. Not as bad as a fully connected layer, but still quite computationally intensive. Moreover, if the feature maps are represented using 32-bit floats, then the convolutional layer’s output will occupy 200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM.<sup><a data-type="noteref" id="idm46263500180728-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500180728" class="totri-footnote">8</a></sup> And that’s just for one instance! If a training batch contains 100 instances, then this layer will use up 1.2 GB of RAM!</p>

<p>During inference (i.e., when making a prediction for a new instance) the RAM occupied by one layer can be released as soon as the next layer has been computed, so you only need as much RAM as required by two consecutive layers. But during training everything computed during the forward pass needs to be preserved for the reverse pass, so the amount of RAM needed is (at least) the total amount of RAM required by all layers.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If training crashes because of an out-of-memory error, you can try reducing the mini-batch size. Alternatively, you can try reducing dimensionality using a stride, or removing a few layers. Or you can try using 16-bit floats instead of 32-bit floats. Or you could distribute the CNN across multiple devices.</p>
</div>

<p>Now let’s look at the second common building block of CNNs: the <em>pooling layer</em>.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Pooling Layer"><div class="sect1" id="idm46263500560568">
<h1>Pooling Layer</h1>

<p>Once you understand how convolutional layers work, the pooling layers are quite easy to grasp. Their goal is to <em>subsample</em> (i.e., shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters (thereby limiting the risk of overfitting).</p>

<p>Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the inputs using an aggregation function such as the max or mean. <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#max_pooling_diagram">Figure&nbsp;14-8</a> shows a <em>max pooling layer</em>, which is the most common type of pooling layer. In this example, we use a 2 × 2 <em>pooling kernel</em>,<sup><a data-type="noteref" id="idm46263500171816-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500171816" class="totri-footnote">9</a></sup> with a stride of 2, and no padding. Only the max input value in each receptive field makes it to the next layer, while the other inputs are dropped. For example, in the lower-left receptive field in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#max_pooling_diagram">Figure&nbsp;14-8</a>, the input values are 1, 5, 3, 2, so only the max value, 5, is propagated to the next layer. Because of the stride of 2, the output image has half the height and half the width of the input image (rounded down since we use no padding).</p>

<figure class="smallereighty"><div id="max_pooling_diagram" class="figure">
<img src="./Chapter14_files/mls2_1408.png" alt="mls2 1408" width="1441" height="601" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1408.png">
<h6><span class="label">Figure 14-8. </span>Max pooling layer (2 × 2 pooling kernel, stride 2, no padding)</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>A pooling layer typically works on every input channel independently, so the output depth is the same as the input depth.</p>
</div>

<p>Other than reducing computations, memory usage and the number of parameters, a max pooling layer also introduces some level of <em>invariance</em> to small translations, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#pooling_invariance_diagram">Figure&nbsp;14-9</a>. Here we assume that the bright pixels have a lower value than dark pixels, and we consider three images (A, B, C) going through a max pooling layer with a 2 × 2 kernel and stride 2. Images B and C are the same as image A, but shifted by one and two pixels to the right. As you can see, the outputs of the max pooling layer for images A and B are identical. This is what translation invariance means. For image C, the output is different: it is shifted one pixel to the right (but there is still 75% invariance). By inserting a max pooling layer every few layers in a CNN, it is possible to get some level of translation invariance at a larger scale. Moreover, max pooling offers a small amount of rotational invariance and a slight scale invariance. Such invariance (even if it is limited) can be useful in cases where the prediction should not depend on these details, such as in classification tasks.</p>

<figure><div id="pooling_invariance_diagram" class="figure">
<img src="./Chapter14_files/mls2_1409.png" alt="mls2 1409" width="1403" height="942" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1409.png">
<h6><span class="label">Figure 14-9. </span>Invariance to small translations</h6>
</div></figure>

<p>But max pooling has some downsides: firstly, it is obviously very destructive: even with a tiny 2 × 2 kernel and a stride of 2, the output will be two times smaller in both directions (so its area will be four times smaller), simply dropping 75% of the input values. And in some applications, invariance is not desirable. Take <em>semantic segmentation</em>: this is the task of classifying each pixel in an image according to the object that pixel belongs to: obviously, if the input image is translated by one pixel to the right, the output should also be translated by one pixel to the right. The goal in this case is <em>equivariance</em>, not invariance: a small change to the inputs should lead to a corresponding small change in the output.</p>








<section data-type="sect2" data-pdf-bookmark="TensorFlow Implementation"><div class="sect2" id="idm46263500235224">
<h2>TensorFlow Implementation</h2>

<p>Implementing a max pooling layer in TensorFlow is quite easy. The following code creates a max pooling layer using a 2 × 2 kernel. The strides default to the kernel size, so this layer will use a stride of 2 (both horizontally and vertically). By default, it uses <code>"valid"</code> padding (i.e., no padding at all):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">max_pool</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">MaxPool2D</code><code class="p">(</code><code class="n">pool_size</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code></pre>

<p>To create an <em>average pooling layer</em>, just use <code>AvgPool2D</code> instead of <code>MaxPool2D</code>. As you might expect, it works exactly like a max pooling layer, except it computes the mean rather than the max. Average pooling layers used to be very popular, but people mostly use max pooling layers now, as they generally perform better. This may seem surprising, since computing the mean generally loses less information than computing the max. But on the other hand, max pooling preserves only the strongest feature, getting rid of all the meaningless ones, so the next layers get a cleaner signal to work with. Moreover, max pooling offers stronger translation invariance than average pooling, and it requires slightly less compute.</p>

<p>Note that max pooling and average pooling can be performed along the depth dimension rather than the spatial dimensions, although this is not as common. This can allow the CNN to learn to be invariant to various features. For example, it could learn multiple filters, each detecting a different rotation of the same pattern, such as hand-written digits (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#depth_wise_pooling_diagram">Figure&nbsp;14-10</a>), and the depth-wise max pooling layer would ensure that the output is the same regardless of the rotation. The CNN could similarly learn to be invariant to anything else: thickness, brightness, skew, color, and so on.</p>

<figure><div id="depth_wise_pooling_diagram" class="figure">
<img src="./Chapter14_files/mls2_1410.png" alt="mls2 1410" width="1440" height="1274" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1410.png">
<h6><span class="label">Figure 14-10. </span>Depth-wise max pooling can help the CNN learn any invariance</h6>
</div></figure>

<p>Keras does not include a depth-wise max pooling layer, but TensorFlow’s low-level Deep Learning API does: just use the <code>tf.nn.max_pool()</code> function, and specify the kernel size and strides as 4-tuples  (i.e., tuples of size 4). The first three values of each should be 1: this indicates that the kernel size and stride along the batch, height, and width dimensions should be 1. The last value should be whatever kernel size and stride you want along the depth dimension, for example, 3 (this must be a divisor of the input depth; it will not work if the previous layer outputs 20 feature maps, since 20 is not a multiple of 3):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">output</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">max_pool</code><code class="p">(</code><code class="n">images</code><code class="p">,</code>
                        <code class="n">ksize</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code>
                        <code class="n">strides</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code>
                        <code class="n">padding</code><code class="o">=</code><code class="s2">"valid"</code><code class="p">)</code></pre>

<p>If you want to include this as a layer in your Keras models, wrap it in a <code>Lambda</code> layer (or create a custom Keras layer):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">depth_pool</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Lambda</code><code class="p">(</code>
    <code class="k">lambda</code> <code class="n">X</code><code class="p">:</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">max_pool</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">ksize</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code> <code class="n">strides</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code>
                             <code class="n">padding</code><code class="o">=</code><code class="s2">"valid"</code><code class="p">))</code></pre>

<p>One last type of pooling layer that you will often see in modern architectures is the <em>global average pooling</em> layer. It works very differently: all it does is compute the mean of each entire feature map (it’s like an average pooling layer using a pooling kernel with the same spatial dimensions as the inputs). This means that it just outputs a single number per feature map and per instance. Although this is of course extremely destructive (most of the information in the feature map is lost), it can be useful as the output layer, as we will see later in this chapter. To create such a layer, simply use the <code>keras.layers.GlobalAvgPool2D</code> class:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">global_avg_pool</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">GlobalAvgPool2D</code><code class="p">()</code></pre>

<p>It’s equivalent to this simple <code>Lamba</code> layer, which computes the mean over the spatial dimensions (height and width):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">global_avg_pool</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Lambda</code><code class="p">(</code><code class="k">lambda</code> <code class="n">X</code><code class="p">:</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_mean</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">]))</code></pre>

<p>Now you know all the building blocks to create a convolutional neural network. Let’s see how to assemble them.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="CNN Architectures"><div class="sect1" id="idm46263500234600">
<h1>CNN Architectures</h1>

<p>Typical CNN architectures stack a few convolutional layers (each one generally followed by a ReLU layer), then a pooling layer, then another few convolutional layers (+ReLU), then another pooling layer, and so on. The image gets smaller and smaller as it progresses through the network, but it also typically gets deeper and deeper (i.e., with more feature maps), thanks to the convolutional layers (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cnn_architecture_diagram">Figure&nbsp;14-11</a>). At the top of the stack, a regular feedforward neural network is added, composed of a few fully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a softmax layer that outputs estimated class probabilities).</p>

<figure><div id="cnn_architecture_diagram" class="figure">
<img src="./Chapter14_files/mls2_1411.png" alt="mls2 1411" width="1440" height="345" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1411.png">
<h6><span class="label">Figure 14-11. </span>Typical CNN architecture</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>A common mistake is to use convolution kernels that are too large. For example, instead of using a convolutional layer with a 5 × 5 kernel, stack two layers with 3 × 3 kernels: it will use less parameters and require less computations, and it will usually perform better. One exception is for the first convolutional layer: it can typically have a large kernel (e.g., 5 × 5), usually with stride of 2 or more: this will reduce the spatial dimension of the image without losing too much information, and since the input image only has three channels in general, it will not be too costly.</p>
</div>

<p>Here is how you can implement a simple CNN to tackle the Fashion MNIST dataset (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#ann_chapter">Chapter&nbsp;10</a>):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2D</code><code class="p">(</code><code class="mi">64</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,</code>
                        <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">1</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">MaxPooling2D</code><code class="p">(</code><code class="mi">2</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2D</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2D</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">MaxPooling2D</code><code class="p">(</code><code class="mi">2</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2D</code><code class="p">(</code><code class="mi">256</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2D</code><code class="p">(</code><code class="mi">256</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">MaxPooling2D</code><code class="p">(</code><code class="mi">2</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.5</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">64</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.5</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"softmax"</code><code class="p">)</code>
<code class="p">])</code></pre>

<p>Let’s go through this model:</p>

<ul>
<li>
<p>The first layer uses 64 fairly large filters (7 × 7) but no stride because the input images are not very large. It also sets <code>input_shape=[28, 28, 1]</code>, because the images are 28 × 28 pixels, with a single color channel (i.e., grayscale).</p>
</li>
<li>
<p>Next, we have a max pooling layer, which uses a pool size of 2, so it divides each spatial dimension by a factor of 2.</p>
</li>
<li>
<p>Then we repeat the same structure twice: two convolutional layers followed by a max pooling layer. For larger images, we could repeat this structure several more times (the number of repetitions is a hyperparameter you can tune).</p>
</li>
<li>
<p>Note that the number of filters grows as we climb up the CNN toward the output layer (it is initially 64, then 128, then 256): it makes sense for it to grow, since the number of low-level features is often fairly low (e.g., small circles, horizontal lines), but there are many different ways to combine them into higher-level features. It is a common practice to double the number of filters after each pooling layer: since a pooling layer divides each spatial dimension by a factor of 2, we can afford doubling the number of feature maps in the next layer, without fear of exploding the number of parameters, memory usage, or computational load.</p>
</li>
<li>
<p>Next is the fully connected network, composed of two hidden dense layers and a dense output layer. Note that we must flatten its inputs, since a dense network expects a 1D array of features for each instance. We also add two dropout layers, with a dropout rate of 50% each, to reduce overfitting.</p>
</li>
</ul>

<p>This CNN reaches over 92% accuracy on the test set. It’s not state of the art, but it is pretty good, and clearly much better than what we achieved with dense networks in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#ann_chapter">Chapter&nbsp;10</a>.</p>

<p>Over the years, variants of this fundamental architecture have been developed, leading to amazing advances in the field. A good measure of this progress is the error rate in competitions such as the ILSVRC <a href="http://image-net.org/">ImageNet challenge</a>. In this competition the top-five error rate for image classification fell from over 26% to less than 2.3% in just six years. The top-five error rate is the number of test images for which the system’s top five predictions did not include the correct answer. The images are large (256 pixels high) and there are 1,000 classes, some of which are really subtle (try distinguishing 120 dog breeds). Looking at the evolution of the winning entries is a good way to understand how CNNs work.</p>

<p>We will first look at the classical LeNet-5 architecture (1998), then three of the winners of the ILSVRC challenge: AlexNet (2012), GoogLeNet (2014), and ResNet (2015).</p>








<section data-type="sect2" data-pdf-bookmark="LeNet-5"><div class="sect2" id="idm46263499663880">
<h2>LeNet-5</h2>

<p>The <a href="https://homl.info/lenet5">LeNet-5 architecture</a><sup><a data-type="noteref" id="lenet5-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#lenet5">10</a></sup> is perhaps the most widely known CNN architecture. As mentioned earlier, it was created by Yann LeCun in 1998 and has been widely used for handwritten digit recognition (MNIST). It is composed of the layers shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#lenet_5_architecture">Table&nbsp;14-1</a>.</p>
<table id="lenet_5_architecture">
<caption><span class="label">Table 14-1. </span>LeNet-5 architecture</caption>
<thead>
<tr>
<th>Layer</th>
<th>Type</th>
<th>Maps</th>
<th>Size</th>
<th>Kernel size</th>
<th>Stride</th>
<th>Activation</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Out</p></td>
<td><p>Fully Connected</p></td>
<td><p>–</p></td>
<td><p>10</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>RBF</p></td>
</tr>
<tr>
<td><p>F6</p></td>
<td><p>Fully Connected</p></td>
<td><p>–</p></td>
<td><p>84</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>tanh</p></td>
</tr>
<tr>
<td><p>C5</p></td>
<td><p>Convolution</p></td>
<td><p>120</p></td>
<td><p>1 × 1</p></td>
<td><p>5 × 5</p></td>
<td><p>1</p></td>
<td><p>tanh</p></td>
</tr>
<tr>
<td><p>S4</p></td>
<td><p>Avg Pooling</p></td>
<td><p>16</p></td>
<td><p>5 × 5</p></td>
<td><p>2 × 2</p></td>
<td><p>2</p></td>
<td><p>tanh</p></td>
</tr>
<tr>
<td><p>C3</p></td>
<td><p>Convolution</p></td>
<td><p>16</p></td>
<td><p>10 × 10</p></td>
<td><p>5 × 5</p></td>
<td><p>1</p></td>
<td><p>tanh</p></td>
</tr>
<tr>
<td><p>S2</p></td>
<td><p>Avg Pooling</p></td>
<td><p>6</p></td>
<td><p>14 × 14</p></td>
<td><p>2 × 2</p></td>
<td><p>2</p></td>
<td><p>tanh</p></td>
</tr>
<tr>
<td><p>C1</p></td>
<td><p>Convolution</p></td>
<td><p>6</p></td>
<td><p>28 × 28</p></td>
<td><p>5 × 5</p></td>
<td><p>1</p></td>
<td><p>tanh</p></td>
</tr>
<tr>
<td><p>In</p></td>
<td><p>Input</p></td>
<td><p>1</p></td>
<td><p>32 × 32</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
</tr>
</tbody>
</table>

<p>There are a few extra details to be noted:</p>

<ul>
<li>
<p>MNIST images are 28 × 28 pixels, but they are zero-padded to 32 × 32 pixels and normalized before being fed to the network. The rest of the network does not use any padding, which is why the size keeps shrinking as the image progresses through the network.</p>
</li>
<li>
<p>The average pooling layers are slightly more complex than usual: each neuron computes the mean of its inputs, then multiplies the result by a learnable coefficient (one per map) and adds a learnable bias term (again, one per map), then finally applies the activation function.</p>
</li>
<li>
<p>Most neurons in C3 maps are connected to neurons in only three or four S2 maps (instead of all six S2 maps). See table&nbsp;1 (page 8) in the original paper<sup><a data-type="noteref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#lenet5">10</a></sup> for details.</p>
</li>
<li>
<p>The output layer is a bit special: instead of computing the matrix multiplication of the inputs and the weight vector, each neuron outputs the square of the Euclidian distance between its input vector and its weight vector. Each output measures how much the image belongs to a particular digit class. The cross entropy cost function is now preferred, as it penalizes bad predictions much more, producing larger gradients and converging faster.</p>
</li>
</ul>

<p>Yann LeCun’s <a href="http://yann.lecun.com/">website</a> (“LENET” section) features great demos of LeNet-5 classifying digits.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="AlexNet"><div class="sect2" id="idm46263499609400">
<h2>AlexNet</h2>

<p>The <a href="https://homl.info/80">AlexNet CNN architecture</a><sup><a data-type="noteref" id="idm46263499607080-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499607080">11</a></sup> won the 2012 ImageNet ILSVRC challenge by a large margin: it achieved 17% top-five error rate, while the second best achieved only 26%! It was developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and Geoffrey Hinton. It is similar to LeNet-5, only much larger and deeper, and it was the first to stack convolutional layers directly on top of one another, instead of stacking a pooling layer on top of each convolutional layer. <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#alexnet_architecture">Table&nbsp;14-2</a> presents this architecture.</p>
<table id="alexnet_architecture">
<caption><span class="label">Table 14-2. </span>AlexNet architecture</caption>
<thead>
<tr>
<th>Layer</th>
<th>Type</th>
<th>Maps</th>
<th>Size</th>
<th>Kernel size</th>
<th>Stride</th>
<th>Padding</th>
<th>Activation</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Out</p></td>
<td><p>Fully Connected</p></td>
<td><p>–</p></td>
<td><p>1,000</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>Softmax</p></td>
</tr>
<tr>
<td><p>F9</p></td>
<td><p>Fully Connected</p></td>
<td><p>–</p></td>
<td><p>4,096</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>ReLU</p></td>
</tr>
<tr>
<td><p>F8</p></td>
<td><p>Fully Connected</p></td>
<td><p>–</p></td>
<td><p>4,096</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>ReLU</p></td>
</tr>
<tr>
<td><p>C7</p></td>
<td><p>Convolution</p></td>
<td><p>256</p></td>
<td><p>13 × 13</p></td>
<td><p>3 × 3</p></td>
<td><p>1</p></td>
<td><p>same</p></td>
<td><p>ReLU</p></td>
</tr>
<tr>
<td><p>C6</p></td>
<td><p>Convolution</p></td>
<td><p>384</p></td>
<td><p>13 × 13</p></td>
<td><p>3 × 3</p></td>
<td><p>1</p></td>
<td><p>same</p></td>
<td><p>ReLU</p></td>
</tr>
<tr>
<td><p>C5</p></td>
<td><p>Convolution</p></td>
<td><p>384</p></td>
<td><p>13 × 13</p></td>
<td><p>3 × 3</p></td>
<td><p>1</p></td>
<td><p>same</p></td>
<td><p>ReLU</p></td>
</tr>
<tr>
<td><p>S4</p></td>
<td><p>Max Pooling</p></td>
<td><p>256</p></td>
<td><p>13 × 13</p></td>
<td><p>3 × 3</p></td>
<td><p>2</p></td>
<td><p>valid</p></td>
<td><p>–</p></td>
</tr>
<tr>
<td><p>C3</p></td>
<td><p>Convolution</p></td>
<td><p>256</p></td>
<td><p>27 × 27</p></td>
<td><p>5 × 5</p></td>
<td><p>1</p></td>
<td><p>same</p></td>
<td><p>ReLU</p></td>
</tr>
<tr>
<td><p>S2</p></td>
<td><p>Max Pooling</p></td>
<td><p>96</p></td>
<td><p>27 × 27</p></td>
<td><p>3 × 3</p></td>
<td><p>2</p></td>
<td><p>valid</p></td>
<td><p>–</p></td>
</tr>
<tr>
<td><p>C1</p></td>
<td><p>Convolution</p></td>
<td><p>96</p></td>
<td><p>55 × 55</p></td>
<td><p>11 × 11</p></td>
<td><p>4</p></td>
<td><p>valid</p></td>
<td><p>ReLU</p></td>
</tr>
<tr>
<td><p>In</p></td>
<td><p>Input</p></td>
<td><p>3 (RGB)</p></td>
<td><p>227 × 227</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
<td><p>–</p></td>
</tr>
</tbody>
</table>

<p>To reduce overfitting, the authors used two regularization techniques: first they applied dropout (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>) with a 50% dropout rate during training to the outputs of layers F8 and F9. Second, they performed <em>data augmentation</em> by randomly shifting the training images by various offsets, flipping them horizontally, and changing the lighting conditions.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46263499539576">
<h5>Data Augmentation</h5>
<p>Data augmentation artificially increases the size of the training set by generating many realistic variants of each training instance. This reduces overfitting, making this a regularization technique. The generated instances should be as realistic as possible: ideally, given an image from the augmented training set, a human should not be able to tell whether it was augmented or not. Simply adding white noise will not help; the modifications should be learnable (white noise is not).</p>

<p>For example, you can slightly shift, rotate, and resize every picture in the training set by various amounts and add the resulting pictures to the training set (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#data_augmentation_diagram">Figure&nbsp;14-12</a>). This forces the model to be more tolerant to variations in the position, orientation, and size of the objects in the pictures. For a model that’s more tolerant of different lighting conditions, you can similarly generate many images with various contrasts. In general, you can also flip the pictures horizontally (except for text, and other asymmetrical objects). By combining these transformations, you can greatly increase the size of your training set.</p>

<figure><div id="data_augmentation_diagram" class="figure">
<img src="./Chapter14_files/mls2_1412.png" alt="mls2 1412" width="1438" height="886" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1412.png">
<h6><span class="label">Figure 14-12. </span>Generating new training instances from existing ones</h6>
</div></figure>
</div></aside>

<p>AlexNet also uses a competitive normalization step immediately after the ReLU step of layers C1 and C3, called <em>local response normalization</em>. The most strongly activated neurons inhibit other neurons located at the same position in neighboring feature maps (such competitive activation has been observed in biological neurons). This encourages different feature maps to specialize, pushing them apart and forcing them to explore a wider range of features, ultimately improving generalization. <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#lrn_equation">Equation 14-2</a> shows how to apply LRN.</p>
<div data-type="equation" id="lrn_equation">
<h5><span class="label">Equation 14-2. </span>Local response normalization</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-143-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;msup&gt;&lt;mfenced separators=&quot;&quot; open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B1;&lt;/mi&gt;&lt;munderover&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mtext&gt;low&lt;/mtext&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mtext&gt;high&lt;/mtext&gt;&lt;/msub&gt;&lt;/munderover&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mfenced&gt;&lt;mrow&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mi&gt;&amp;#x3B2;&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;mtext&gt;with&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mfenced separators=&quot;&quot; open=&quot;{&quot; close=&quot;&quot;&gt;&lt;mtable&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mtext&gt;high&lt;/mtext&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;min&lt;/mo&gt;&lt;mfenced separators=&quot;&quot; open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mtext&gt;low&lt;/mtext&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;max&lt;/mo&gt;&lt;mfenced separators=&quot;&quot; open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5813" style="width: 28.846em; display: inline-block;"><span style="display: inline-block; position: relative; width: 27.972em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.956em, 1027.62em, 6.378em, -1000.01em); top: -4.419em; left: 0em;"><span class="mrow" id="MathJax-Span-5814"><span class="mrow" id="MathJax-Span-5815"><span class="msub" id="MathJax-Span-5816"><span style="display: inline-block; position: relative; width: 0.774em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.42em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5817" style="font-family: MathJax_Math-italic;">b</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.414em;"><span class="mi" id="MathJax-Span-5818" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5819" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="msub" id="MathJax-Span-5820" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 0.825em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.52em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5821" style="font-family: MathJax_Math-italic;">a</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.517em;"><span class="mi" id="MathJax-Span-5822" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msup" id="MathJax-Span-5823"><span style="display: inline-block; position: relative; width: 8.537em; height: 0px;"><span style="position: absolute; clip: rect(2.213em, 1007.26em, 5.761em, -1000.01em); top: -4.265em; left: 0em;"><span class="mfenced" id="MathJax-Span-5824"><span class="mo" id="MathJax-Span-5825" style="vertical-align: 0em;"><span style="font-family: MathJax_Size4;">(</span></span><span class="mi" id="MathJax-Span-5826" style="font-family: MathJax_Math-italic;">k</span><span class="mo" id="MathJax-Span-5827" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-5828" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">α</span><span class="munderover" id="MathJax-Span-5829" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.956em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1001.4em, 4.63em, -1000.01em); top: -4.008em; left: 0.26em;"><span class="mo" id="MathJax-Span-5830" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.91em, 4.424em, -1000.01em); top: -2.928em; left: 0em;"><span class="mrow" id="MathJax-Span-5831"><span class="mi" id="MathJax-Span-5832" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span class="mo" id="MathJax-Span-5833" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="msub" id="MathJax-Span-5834"><span style="display: inline-block; position: relative; width: 1.082em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.27em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5835" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.311em;"><span class="mtext" id="MathJax-Span-5836" style="font-size: 50%; font-family: MathJax_Main;">low</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.293em, 1001.3em, 4.424em, -1000.01em); top: -5.345em; left: 0.311em;"><span class="msub" id="MathJax-Span-5837"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.27em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5838" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.311em;"><span class="mtext" id="MathJax-Span-5839" style="font-size: 50%; font-family: MathJax_Main;">high</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msup" id="MathJax-Span-5840" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.339em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.88em, 4.476em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-5841"><span class="msub" id="MathJax-Span-5842"><span style="display: inline-block; position: relative; width: 0.877em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.52em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5843" style="font-family: MathJax_Math-italic;">a</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.517em;"><span class="mi" id="MathJax-Span-5844" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 0.877em;"><span class="mn" id="MathJax-Span-5845" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5846" style="vertical-align: 0em;"><span style="font-family: MathJax_Size4;">)</span></span></span><span style="display: inline-block; width: 0px; height: 4.27em;"></span></span><span style="position: absolute; top: -5.653em; left: 7.509em;"><span class="mrow" id="MathJax-Span-5847"><span class="mo" id="MathJax-Span-5848" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mi" id="MathJax-Span-5849" style="font-size: 70.7%; font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mspace" id="MathJax-Span-5850" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-5851" style="font-family: MathJax_Main;">with</span><span class="mspace" id="MathJax-Span-5852" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mfenced" id="MathJax-Span-5853" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-5854" style="vertical-align: 2.316em;"><span style="display: inline-block; position: relative; width: 0.877em; height: 0px;"><span style="position: absolute; font-family: MathJax_Size4; top: -3.134em; left: 0em;">⎧<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Size4; top: -0.82em; left: 0em;">⎩<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Size4; top: -1.694em; left: 0em;">⎨<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Size4; position: absolute; top: -2.825em; left: 0em;">⎪<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Size4; position: absolute; top: -0.769em; left: 0em;">⎪<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mtable" id="MathJax-Span-5855" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 11.879em; height: 0px;"><span style="position: absolute; clip: rect(2.625em, 1011.68em, 7.046em, -1000.01em); top: -5.087em; left: 0em;"><span style="display: inline-block; position: relative; width: 11.879em; height: 0px;"><span style="position: absolute; clip: rect(2.728em, 1011.68em, 4.836em, -1000.01em); top: -5.139em; left: 0em;"><span class="mtd" id="MathJax-Span-5856"><span class="mrow" id="MathJax-Span-5857"><span class="mrow" id="MathJax-Span-5858"><span class="msub" id="MathJax-Span-5859"><span style="display: inline-block; position: relative; width: 1.802em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.42em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5860" style="font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.414em;"><span class="mtext" id="MathJax-Span-5861" style="font-size: 70.7%; font-family: MathJax_Main;">high</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5862" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mo" id="MathJax-Span-5863" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">min</span><span class="mfenced" id="MathJax-Span-5864"><span class="mo" id="MathJax-Span-5865" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">(</span></span><span class="mi" id="MathJax-Span-5866" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-5867" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mstyle" id="MathJax-Span-5868" style="padding-left: 0.208em;"><span class="mrow" id="MathJax-Span-5869"><span class="mfrac" id="MathJax-Span-5870"><span style="display: inline-block; position: relative; width: 0.62em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.396em, 1000.42em, 4.167em, -1000.01em); top: -4.676em; left: 50%; margin-left: -0.203em;"><span class="mi" id="MathJax-Span-5871" style="font-family: MathJax_Math-italic;">r</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1000.47em, 4.167em, -1000.01em); top: -3.339em; left: 50%; margin-left: -0.255em;"><span class="mn" id="MathJax-Span-5872" style="font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1000.63em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.62em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-5873" style="font-family: MathJax_Main;">,</span><span class="msub" id="MathJax-Span-5874" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5875" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.517em;"><span class="mi" id="MathJax-Span-5876" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5877" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mn" id="MathJax-Span-5878" style="font-family: MathJax_Main; padding-left: 0.208em;">1</span><span class="mo" id="MathJax-Span-5879" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">)</span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.728em, 1009.42em, 4.836em, -1000.01em); top: -2.877em; left: 0em;"><span class="mtd" id="MathJax-Span-5880"><span class="mrow" id="MathJax-Span-5881"><span class="mrow" id="MathJax-Span-5882"><span class="msub" id="MathJax-Span-5883"><span style="display: inline-block; position: relative; width: 1.545em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.42em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5884" style="font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.414em;"><span class="mtext" id="MathJax-Span-5885" style="font-size: 70.7%; font-family: MathJax_Main;">low</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5886" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mo" id="MathJax-Span-5887" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">max</span><span class="mfenced" id="MathJax-Span-5888"><span class="mo" id="MathJax-Span-5889" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">(</span></span><span class="mn" id="MathJax-Span-5890" style="font-family: MathJax_Main;">0</span><span class="mo" id="MathJax-Span-5891" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-5892" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">i</span><span class="mo" id="MathJax-Span-5893" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mstyle" id="MathJax-Span-5894" style="padding-left: 0.208em;"><span class="mrow" id="MathJax-Span-5895"><span class="mfrac" id="MathJax-Span-5896"><span style="display: inline-block; position: relative; width: 0.62em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.396em, 1000.42em, 4.167em, -1000.01em); top: -4.676em; left: 50%; margin-left: -0.203em;"><span class="mi" id="MathJax-Span-5897" style="font-family: MathJax_Math-italic;">r</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1000.47em, 4.167em, -1000.01em); top: -3.339em; left: 50%; margin-left: -0.255em;"><span class="mn" id="MathJax-Span-5898" style="font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1000.63em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.62em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-5899" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">)</span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 5.093em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.424em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.904em; border-left: 0px solid; width: 0px; height: 4.345em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>b</mi><mi>i</mi></msub><mo>=</mo><msub><mi>a</mi><mi>i</mi></msub><msup><mfenced separators="" open="(" close=")"><mi>k</mi><mo>+</mo><mi>α</mi><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><msub><mi>j</mi><mtext>low</mtext></msub></mrow><msub><mi>j</mi><mtext>high</mtext></msub></munderover><msup><mrow><msub><mi>a</mi><mi>j</mi></msub></mrow><mn>2</mn></msup></mfenced><mrow><mo>-</mo><mi>β</mi></mrow></msup><mspace width="1.em"></mspace><mtext>with</mtext><mspace width="4.pt"></mspace><mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><msub><mi>j</mi><mtext>high</mtext></msub><mo>=</mo><mo movablelimits="true" form="prefix">min</mo><mfenced separators="" open="(" close=")"><mi>i</mi><mo>+</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mi>r</mi><mn>2</mn></mfrac></mstyle><mo>,</mo><msub><mi>f</mi><mi>n</mi></msub><mo>-</mo><mn>1</mn></mfenced></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><msub><mi>j</mi><mtext>low</mtext></msub><mo>=</mo><mo movablelimits="true" form="prefix">max</mo><mfenced separators="" open="(" close=")"><mn>0</mn><mo>,</mo><mi>i</mi><mo>-</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mi>r</mi><mn>2</mn></mfrac></mstyle></mfenced></mrow></mtd></mtr></mtable></mfenced></mrow></math></span></span></div><script type="math/mml" id="MathJax-Element-143"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <msub><mi>b</mi> <mi>i</mi> </msub>
    <mo>=</mo>
    <msub><mi>a</mi> <mi>i</mi> </msub>
    <msup><mfenced separators="" open="(" close=")"><mi>k</mi><mo>+</mo><mi>α</mi><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><msub><mi>j</mi> <mtext>low</mtext> </msub></mrow> <msub><mi>j</mi> <mtext>high</mtext> </msub> </munderover><msup><mrow><msub><mi>a</mi> <mi>j</mi> </msub></mrow> <mn>2</mn> </msup></mfenced> <mrow><mo>-</mo><mi>β</mi></mrow> </msup>
    <mspace width="1.em"></mspace>
    <mtext>with</mtext>
    <mspace width="4.pt"></mspace>
    <mfenced separators="" open="{" close="">
      <mtable>
        <mtr>
          <mtd columnalign="left">
            <mrow>
              <msub><mi>j</mi> <mtext>high</mtext> </msub>
              <mo>=</mo>
              <mo movablelimits="true" form="prefix">min</mo>
              <mfenced separators="" open="(" close=")">
                <mi>i</mi>
                <mo>+</mo>
                <mstyle scriptlevel="0" displaystyle="true">
                  <mfrac><mi>r</mi> <mn>2</mn></mfrac>
                </mstyle>
                <mo>,</mo>
                <msub><mi>f</mi> <mi>n</mi> </msub>
                <mo>-</mo>
                <mn>1</mn>
              </mfenced>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd columnalign="left">
            <mrow>
              <msub><mi>j</mi> <mtext>low</mtext> </msub>
              <mo>=</mo>
              <mo movablelimits="true" form="prefix">max</mo>
              <mfenced separators="" open="(" close=")">
                <mn>0</mn>
                <mo>,</mo>
                <mi>i</mi>
                <mo>-</mo>
                <mstyle scriptlevel="0" displaystyle="true">
                  <mfrac><mi>r</mi> <mn>2</mn></mfrac>
                </mstyle>
              </mfenced>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mfenced>
  </mrow>
</math></script>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><em>b</em><sub><em>i</em></sub> is the normalized output of the neuron located in feature map <em>i</em>, at some row <em>u</em> and column <em>v</em> (note that in this equation we consider only neurons located at this row and column, so <em>u</em> and <em>v</em> are not shown).</p>
</li>
<li>
<p><em>a</em><sub><em>i</em></sub> is the activation of that neuron after the ReLU step, but before normalization.</p>
</li>
<li>
<p><em>k</em>, <em>α</em>, <em>β</em>, and <em>r</em> are hyperparameters. <em>k</em> is called the <em>bias</em>, and <em>r</em> is called the <em>depth radius</em>.</p>
</li>
<li>
<p><em>f</em><sub><em>n</em></sub> is the number of feature maps.</p>
</li>
</ul>

<p>For example, if <em>r</em> = 2 and a neuron has a strong activation, it will inhibit the activation of the neurons located in the feature maps immediately above and below its own.</p>

<p>In AlexNet, the hyperparameters are set as follows: <em>r</em> = 2, <em>α</em> = 0.00002, <em>β</em> = 0.75, and <em>k</em> = 1. This step can be implemented using the <code>tf.nn.local_response_normalization()</code> function (which you can wrap in a <code>Lambda</code> layer if you want to use it in a Keras model).</p>

<p>A variant of AlexNet called <em>ZF Net</em> was developed by Matthew Zeiler and Rob Fergus and won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked hyperparameters (number of feature maps, kernel size, stride, etc.).</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="GoogLeNet"><div class="sect2" id="idm46263499608776">
<h2>GoogLeNet</h2>

<p>The <a href="https://homl.info/81">GoogLeNet architecture</a> was developed by Christian Szegedy et al. from Google Research,<sup><a data-type="noteref" id="idm46263499403032-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499403032">12</a></sup> and it won the ILSVRC 2014 challenge by pushing the top-five error rate below 7%. This great performance came in large part from the fact that the network was much deeper than previous CNNs (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#googlenet_diagram">Figure&nbsp;14-14</a>). This was made possible by subnetworks called <em>inception modules</em>,<sup><a data-type="noteref" id="idm46263499400200-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499400200">13</a></sup> which allow GoogLeNet to use parameters much more efficiently than previous architectures: GoogLeNet actually has 10 times fewer parameters than AlexNet (roughly 6 million instead of 60 million).</p>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#inception_module_diagram">Figure&nbsp;14-13</a> shows the architecture of an inception module. The notation “3 × 3 + 1(S)” means that the layer uses a 3 × 3 kernel, stride 1, and <code>"same"</code> padding. The input signal is first copied and fed to four different layers. All convolutional layers use the ReLU activation function. Note that the second set of convolutional layers uses different kernel sizes (1 × 1, 3 × 3, and 5 × 5), allowing them to capture patterns at different scales. Also note that every single layer uses a stride of 1 and <code>"same"</code> padding (even the max pooling layer), so their outputs all have the same height and width as their inputs. This makes it possible to concatenate all the outputs along the depth dimension in the final <em>depth concat layer</em> (i.e., stack the feature maps from all four top convolutional layers). This concatenation layer can be implemented in TensorFlow using the <code>tf.concat()</code> operation, with <code>axis=3</code> (axis 3 is the depth).</p>

<figure class="smallerseventy"><div id="inception_module_diagram" class="figure">
<img src="./Chapter14_files/mls2_1413.png" alt="mls2 1413" width="1440" height="860" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1413.png">
<h6><span class="label">Figure 14-13. </span>Inception module</h6>
</div></figure>

<p>You may wonder why inception modules have convolutional layers with 1 × 1 kernels. Surely these layers cannot capture any features because they look at only one pixel at a time? In fact, these layers serve three purposes:</p>

<ul>
<li>
<p>First, although they cannot capture spatial patterns, they can capture patterns along the depth dimension.</p>
</li>
<li>
<p>Second, they are configured to output fewer feature maps than their inputs, so they serve as <em>bottleneck layers</em>, meaning they reduce dimensionality. This cuts the computational cost and the number of parameters, speeding up training and improving generalization.</p>
</li>
<li>
<p>Lastly, each pair of convolutional layers ([1 × 1, 3 × 3] and [1 × 1, 5 × 5]) acts like a single, powerful convolutional layer, capable of capturing more complex patterns. Indeed, instead of sweeping a simple linear classifier across the image (as a single convolutional layer does), this pair of convolutional layers sweeps a two-layer neural network across the image.</p>
</li>
</ul>

<p>In short, you can think of the whole inception module as a convolutional layer on steroids, able to output feature maps that capture complex patterns at various scales.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>The number of convolutional kernels for each convolutional layer is a hyperparameter. Unfortunately, this means that you have six more hyperparameters to tweak for every inception layer you add.</p>
</div>

<p>Now let’s look at the architecture of the GoogLeNet CNN (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#googlenet_diagram">Figure&nbsp;14-14</a>). The number of feature maps output by each convolutional layer and each pooling layer is shown before the kernel size. The architecture is so deep that it has to be represented in three columns, but GoogLeNet is actually one tall stack, including nine inception modules (the boxes with the spinning tops). The six numbers in the inception modules represent the number of feature maps output by each convolutional layer in the module (in the same order as in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#inception_module_diagram">Figure&nbsp;14-13</a>). Note that all the convolutional layers use the ReLU activation function.</p>

<figure><div id="googlenet_diagram" class="figure">
<img src="./Chapter14_files/mls2_1414.png" alt="mls2 1414" width="1440" height="1298" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1414.png">
<h6><span class="label">Figure 14-14. </span>GoogLeNet architecture</h6>
</div></figure>

<p>Let’s go through this network:</p>

<ul>
<li>
<p>The first two layers divide the image’s height and width by 4 (so its area is divided by 16), to reduce the computational load. The first layer uses a large kernel size so that much of the information is preserved.</p>
</li>
<li>
<p>Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier).</p>
</li>
<li>
<p>Two convolutional layers follow, where the first acts like a <em>bottleneck layer</em>. As explained earlier, you can think of this pair as a single, smarter convolutional layer.</p>
</li>
<li>
<p>Again, a local response normalization layer ensures that the previous layers capture a wide variety of patterns.</p>
</li>
<li>
<p>Next a max pooling layer reduces the image height and width by 2, again to speed up computations.</p>
</li>
<li>
<p>Then comes the tall stack of nine inception modules, interleaved with a couple max pooling layers to reduce dimensionality and speed up the net.</p>
</li>
<li>
<p>Next, the global average pooling layer outputs the mean of each feature map: this drops any remaining spatial information, which is fine because there was not much spatial information left at that point. Indeed, GoogLeNet input images are typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each dividing the height and width by 2, the feature maps are down to 7 × 7. Moreover, it is a classification task, not localization, so it does not matter where the object is. Thanks to the dimensionality reduction brought by this layer, there is no need to have several fully connected layers at the top of the CNN (like in AlexNet), and this considerably reduces the number of parameters in the network and limits the risk of overfitting.</p>
</li>
<li>
<p>The last layers are self-explanatory: dropout for regularization, then a fully connected layer with 1,000 units, since there are a 1,000 classes, and a softmax activation function to output estimated class probabilities.</p>
</li>
</ul>

<p>This diagram is slightly simplified: the original GoogLeNet architecture also included two auxiliary classifiers plugged on top of the third and sixth inception modules. They were both composed of one average pooling layer, one convolutional layer, two fully connected layers, and a softmax activation layer. During training, their loss (scaled down by 70%) was added to the overall loss. The goal was to fight the vanishing gradients problem and regularize the network. However, it was later shown that their effect was relatively minor.</p>

<p>Several variants of the GoogLeNet architecture were later proposed by Google researchers, including Inception-v3 and Inception-v4, using slightly different inception modules, and reaching even better performance.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="VGGNet"><div class="sect2" id="idm46263499404984">
<h2>VGGNet</h2>

<p>The runner-up in the ILSVRC 2014 challenge was <a href="https://homl.info/83">VGGNet</a>,<sup><a data-type="noteref" id="idm46263499367624-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499367624">14</a></sup> developed by Karen Simonyan and Andrew Zisserman from the Visual Geometry Group (VGG) research lab at Oxford University. It had a very simple and classical architecture, with 2 or 3 convolutional layers, a pooling layer, then again 2 or 3 convolutional layers, a pooling layer, and so on (with a total of just 16 or 19 convolutional layers, depending on the VGG variant), plus a final dense network with 2 hidden layers and the output layer. It used only 3 × 3 filters, but many filters.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="ResNet"><div class="sect2" id="idm46263499365592">
<h2>ResNet</h2>

<p>Kaiming He et al. won the ILSVRC 2015 challenge using a <a href="https://homl.info/82"><em>Residual Network</em></a> (or <em>ResNet</em>),<sup><a data-type="noteref" id="idm46263499362360-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499362360">15</a></sup> that delivered an astounding top-five error rate under 3.6%. The winning variant used an extremely deep CNN composed of 152 layers (other variants had 34, 50, and 101 layers). It confirmed the general trend: models are getting deeper and deeper, with fewer and fewer parameters. The key to being able to train such a deep network is to use <em>skip connections</em> (also called <em>shortcut connections</em>): the signal feeding into a layer is also added to the output of a layer located a bit higher up the stack. Let’s see why this is useful.</p>

<p>When training a neural network, the goal is to make it model a target function <em>h</em>(<strong>x</strong>). If you add the input <strong>x</strong> to the output of the network (i.e., you add a skip connection), then the network will be forced to model <em>f</em>(<strong>x</strong>) = <em>h</em>(<strong>x</strong>) – <strong>x</strong> rather than <em>h</em>(<strong>x</strong>). This is called <em>residual learning</em> (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#residual_learning_diagram">Figure&nbsp;14-15</a>).</p>

<figure class="smallerseventy"><div id="residual_learning_diagram" class="figure">
<img src="./Chapter14_files/mls2_1415.png" alt="mls2 1415" width="1440" height="786" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1415.png">
<h6><span class="label">Figure 14-15. </span>Residual learning</h6>
</div></figure>

<p>When you initialize a regular neural network, its weights are close to zero, so the network just outputs values close to zero. If you add a skip connection, the resulting network just outputs a copy of its inputs; in other words, it initially models the identity function. If the target function is fairly close to the identity function (which is often the case), this will speed up training considerably.</p>

<p>Moreover, if you add many skip connections, the network can start making progress even if several layers have not started learning yet (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#deep_residual_network_diagram">Figure&nbsp;14-16</a>). Thanks to skip connections, the signal can easily make its way across the whole network. The deep residual network can be seen as a stack of <em>residual units</em>, where each residual unit is a small neural network with a skip connection.</p>

<figure class="smallerseventy"><div id="deep_residual_network_diagram" class="figure">
<img src="./Chapter14_files/mls2_1416.png" alt="mls2 1416" width="1440" height="939" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1416.png">
<h6><span class="label">Figure 14-16. </span>Regular deep neural network (left) and deep residual network (right)</h6>
</div></figure>

<p>Now let’s look at ResNet’s architecture (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#resnet_diagram">Figure&nbsp;14-17</a>). It is surprisingly simple. It starts and ends exactly like GoogLeNet (except without a dropout layer), and in between is just a very deep stack of simple residual units. Each residual unit is composed of two convolutional layers (and no pooling layer!), with Batch Normalization (BN) and ReLU activation, using 3 × 3 kernels and preserving spatial dimensions (stride 1, <code>"same"</code> padding).</p>

<figure class="smallerseventyfive"><div id="resnet_diagram" class="figure">
<img src="./Chapter14_files/mls2_1417.png" alt="mls2 1417" width="1440" height="869" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1417.png">
<h6><span class="label">Figure 14-17. </span>ResNet architecture</h6>
</div></figure>

<p>Note that the number of feature maps is doubled every few residual units, at the same time as their height and width are halved (using a convolutional layer with stride 2). When this happens, the inputs cannot be added directly to the outputs of the residual unit because they don’t have the same shape (for example, this problem affects the skip connection represented by the dashed arrow in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#resnet_diagram">Figure&nbsp;14-17</a>). To solve this problem, the inputs are passed through a 1 × 1 convolutional layer with stride 2 and the right number of output feature maps (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#resize_skip_connection_diagram">Figure&nbsp;14-18</a>).</p>

<figure class="smallerfiftyfive"><div id="resize_skip_connection_diagram" class="figure">
<img src="./Chapter14_files/mls2_1418.png" alt="mls2 1418" width="1236" height="623" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1418.png">
<h6><span class="label">Figure 14-18. </span>Skip connection when changing feature map size and depth</h6>
</div></figure>

<p>ResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and the fully connected layer)<sup><a data-type="noteref" id="idm46263499336520-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499336520">16</a></sup> containing 3 residual units that output 64 feature maps, 4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will implement this architecture later in this chapter.</p>

<p>ResNets deeper than that, such as ResNet-152, use slightly different residual units. Instead of two 3 × 3 convolutional layers with, say, 256 feature maps, they use three convolutional layers: first a 1 × 1 convolutional layer with just 64 feature maps (4 times less), which acts as a bottleneck layer (as discussed already), then a 3 × 3 layer with 64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature maps (4 times 64) that restores the original depth. ResNet-152 contains 3 such RUs that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024 maps, and finally 3 RUs with 2,048 maps.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Google’s <a href="https://homl.info/84">Inception-v4</a><sup><a data-type="noteref" id="idm46263499332792-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499332792">17</a></sup> architecture merged the ideas of GoogLeNet and ResNet and achieved close to 3% top-five error rate on ImageNet classification.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Xception"><div class="sect2" id="idm46263499364968">
<h2>Xception</h2>

<p>Another variant of the GoogLeNet architecture is worth noting: <a href="https://homl.info/xception">Xception</a><sup><a data-type="noteref" id="idm46263499329112-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499329112">18</a></sup> (which stands for <em>Extreme Inception</em>) was proposed in 2016 by François Chollet (the author of Keras), and it significantly outperformed Inception-v3 on a huge vision task (350 million images and 17,000 classes). Just like Inception-v4, it merges the ideas of GoogLeNet and ResNet, but it replaces the inception modules with a special type of layer called a <em>depthwise separable convolution</em> (or <em>separable convolution</em> for short<sup><a data-type="noteref" id="idm46263499326312-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499326312">19</a></sup>). These layers had been used before in some CNN architectures, but they were not as central as in the Xception architecture. While a regular convolutional layer uses filters that try to simultaneously capture spatial patterns (e.g., an oval) and cross-channel patterns (e.g., mouth + nose + eyes = face), a separable convolutional layer makes the strong assumption that spatial patterns and cross-channel patterns can be modeled separately (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#separable_convolution_diagram">Figure&nbsp;14-19</a>). Thus, it is composed of two parts: the first part applies a single spatial filter for each input feature map, then the second part looks exclusively for cross-channel patterns—it is just a regular convolutional layer with 1 × 1 filters.</p>

<figure><div id="separable_convolution_diagram" class="figure">
<img src="./Chapter14_files/mls2_1419.png" alt="mls2 1419" width="1440" height="900" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1419.png">
<h6><span class="label">Figure 14-19. </span>Depthwise separable convolutional layer</h6>
</div></figure>

<p>Since separable convolutional layers only have one spatial filter per input channel, you should avoid using them after layers that have too few channels, such as the input layer (granted, that’s what <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#separable_convolution_diagram">Figure&nbsp;14-19</a> represents, but it is just for illustration purposes). For this reason, the Xception architecture starts with 2 regular convolutional layers, but then the rest of the architecture uses only separable convolutions (34 in all), plus a few max pooling layers and the usual final layers (a global average pooling layer, and a dense output layer).</p>

<p>You might wonder why Xception is considered a variant of GoogLeNet, since it contains no inception module at all. Well, as we discussed earlier, an inception module contains convolutional layers with 1 × 1 filters: these look exclusively for cross-channel patterns. However, the convolution layers that sit on top of them are regular convolutional layers that look both for spatial and cross-channel patterns. So you can think of an inception module as an intermediate between a regular convolutional layer (which considers spatial patterns and cross-channel patterns jointly) and a separable convolutional layer (which considers them separately). In practice, it seems that separable convolutions generally perform better.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Separable convolutions use fewer parameters, less memory, and fewer computations than regular convolutional layers, and in general they even perform better, so you should consider using them by default (except after layers with few channels).</p>
</div>

<p>The ILSVRC 2016 challenge was won by the CUImage team from the Chinese University of Hong Kong. They used an ensemble of many different techniques, including a sophisticated object-detection system called <a href="https://homl.info/gbdnet">GBD-Net</a>,<sup><a data-type="noteref" id="idm46263499316440-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499316440">20</a></sup> to achieve a top-five error rate below 3%. Although this result is unquestionably impressive, the complexity of the solution contrasted with the simplicity of ResNets. Moreover, one year later another fairly simple architecture performed even better, as we will see now.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="SENet"><div class="sect2" id="idm46263499314808">
<h2>SENet</h2>

<p>The winning architecture in the ILSVRC 2017 challenge was the <a href="https://homl.info/senet">Squeeze-and-Excitation Network.</a> (SENet)<sup><a data-type="noteref" id="idm46263499312296-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499312296">21</a></sup> This architecture extends existing architectures such as inception networks or ResNets, and boosts their performance. This allowed SENet to win the competition with an astonishing 2.25% top-five error rate! The extended versions of inception networks and ResNet are called <em>SE-Inception</em> and <em>SE-ResNet</em>, respectively. The boost comes from the fact that a SENet adds a small neural network, called a <em>SE block</em>, to every unit in the original architecture (i.e., every inception module or every residual unit), as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#senet_diagram">Figure&nbsp;14-20</a>.</p>

<figure><div id="senet_diagram" class="figure">
<img src="./Chapter14_files/mls2_1420.png" alt="mls2 1420" width="1440" height="914" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1420.png">
<h6><span class="label">Figure 14-20. </span>SE-Inception module (left) and SE-ResNet unit (right)</h6>
</div></figure>

<p>A SE block analyzes the output of the unit it is attached to, focusing exclusively on the depth dimension (it does not look for any spatial pattern), and it learns which features are usually most active together. It then uses this information to recalibrate the feature maps, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#recalibration_diagram">Figure&nbsp;14-21</a>. For example, a SE block may learn that mouths, noses, and eyes usually appear together in pictures: if you see a mouth and a nose, you should expect to see eyes as well. So if a SE block sees a strong activation in the mouth and nose feature maps, but only mild activation in the eye feature map, it will boost the eye feature map (more accurately, it will reduce irrelevant feature maps). If the eyes were somewhat confused with something else, this feature map recalibration will help resolve the ambiguity.</p>

<figure><div id="recalibration_diagram" class="figure">
<img src="./Chapter14_files/mls2_1421.png" alt="mls2 1421" width="1440" height="513" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1421.png">
<h6><span class="label">Figure 14-21. </span>An SE block performs feature map recalibration</h6>
</div></figure>

<p>A SE block is composed of just three layers: a global average pooling layer, a hidden dense layer using the ReLU activation function, and a dense output layer using the sigmoid activation function (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#seblock_diagram">Figure&nbsp;14-22</a>).</p>

<figure><div id="seblock_diagram" class="figure">
<img src="./Chapter14_files/mls2_1422.png" alt="mls2 1422" width="721" height="566" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1422.png">
<h6><span class="label">Figure 14-22. </span>SE block architecture</h6>
</div></figure>

<p>As earlier, the global average pooling layer computes the mean activation for each feature map: for example, if its input contains 256 feature maps, it will output 256 numbers representing the overall level of response for each filter. The next layer is where the “squeeze” happens: this layer has significantly fewer than 256 neurons, typically 16 times fewer than the number of feature maps (e.g., 16 neurons), so the 256 numbers get compressed into a small vector (e.g., 16 dimensions). This is a low-dimensional vector representation (i.e., an embedding) of the distribution of feature responses. This bottleneck step forces the SE block to learn a general representation of the feature combinations (we will see this principle in action again when we discuss autoencoders in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#autoencoders_chapter">Chapter&nbsp;17</a>). Finally, the output layer takes the embedding and outputs a recalibration vector containing one number per feature map (e.g., 256), each between 0 and 1. The feature maps are then multiplied by this recalibration vector, so irrelevant features (with a low recalibration score) get scaled down while relevant features (with a recalibration score close to 1) are left alone.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Implementing a ResNet-34 CNN Using Keras"><div class="sect1" id="idm46263499942904">
<h1>Implementing a ResNet-34 CNN Using Keras</h1>

<p>Most CNN architectures described so far are fairly straightforward to implement (although generally you would load a pretrained network instead, as we will see). To illustrate the process, let’s implement a ResNet-34 from scratch using Keras. First, let’s create a <code>ResidualUnit</code> layer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">ResidualUnit</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Layer</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">filters</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="nf-Magic">__init__</code><code class="p">(</code><code class="o">**</code><code class="n">kwargs</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">activation</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">activations</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">activation</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">main_layers</code> <code class="o">=</code> <code class="p">[</code>
            <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2D</code><code class="p">(</code><code class="n">filters</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="n">strides</code><code class="p">,</code>
                                <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,</code> <code class="n">use_bias</code><code class="o">=</code><code class="bp">False</code><code class="p">),</code>
            <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">BatchNormalization</code><code class="p">(),</code>
            <code class="bp">self</code><code class="o">.</code><code class="n">activation</code><code class="p">,</code>
            <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2D</code><code class="p">(</code><code class="n">filters</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
                                <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,</code> <code class="n">use_bias</code><code class="o">=</code><code class="bp">False</code><code class="p">),</code>
            <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">BatchNormalization</code><code class="p">()]</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">skip_layers</code> <code class="o">=</code> <code class="p">[]</code>
        <code class="k">if</code> <code class="n">strides</code> <code class="o">&gt;</code> <code class="mi">1</code><code class="p">:</code>
            <code class="bp">self</code><code class="o">.</code><code class="n">skip_layers</code> <code class="o">=</code> <code class="p">[</code>
                <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2D</code><code class="p">(</code><code class="n">filters</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="n">strides</code><code class="p">,</code>
                                    <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,</code> <code class="n">use_bias</code><code class="o">=</code><code class="bp">False</code><code class="p">),</code>
                <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">BatchNormalization</code><code class="p">()]</code>

    <code class="k">def</code> <code class="nf">call</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">):</code>
        <code class="n">Z</code> <code class="o">=</code> <code class="n">inputs</code>
        <code class="k">for</code> <code class="n">layer</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">main_layers</code><code class="p">:</code>
            <code class="n">Z</code> <code class="o">=</code> <code class="n">layer</code><code class="p">(</code><code class="n">Z</code><code class="p">)</code>
        <code class="n">skip_Z</code> <code class="o">=</code> <code class="n">inputs</code>
        <code class="k">for</code> <code class="n">layer</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">skip_layers</code><code class="p">:</code>
            <code class="n">skip_Z</code> <code class="o">=</code> <code class="n">layer</code><code class="p">(</code><code class="n">skip_Z</code><code class="p">)</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">activation</code><code class="p">(</code><code class="n">Z</code> <code class="o">+</code> <code class="n">skip_Z</code><code class="p">)</code></pre>

<p>As you can see, this code matches <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#resize_skip_connection_diagram">Figure&nbsp;14-18</a> pretty closely. In the constructor, we create all the layers we will need: the main layers are the ones on the right side of the diagram, and the skip layers are the ones on the left (only needed if the stride is greater than 1). Then in the <code>call()</code> method, we make the inputs go through the main layers and the skip layers (if any), then we add both outputs and apply the activation function.</p>

<p>Next, we can build the ResNet-34 using a <code>Sequential</code> model, since it’s really just a long sequence of layers (we can treat each residual unit as a single layer now that we have the <code>ResidualUnit</code> class):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2D</code><code class="p">(</code><code class="mi">64</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">224</code><code class="p">,</code> <code class="mi">224</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code>
                              <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,</code> <code class="n">use_bias</code><code class="o">=</code><code class="bp">False</code><code class="p">))</code>
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">BatchNormalization</code><code class="p">())</code>
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Activation</code><code class="p">(</code><code class="s2">"relu"</code><code class="p">))</code>
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">MaxPool2D</code><code class="p">(</code><code class="n">pool_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">))</code>
<code class="n">prev_filters</code> <code class="o">=</code> <code class="mi">64</code>
<code class="k">for</code> <code class="n">filters</code> <code class="ow">in</code> <code class="p">[</code><code class="mi">64</code><code class="p">]</code> <code class="o">*</code> <code class="mi">3</code> <code class="o">+</code> <code class="p">[</code><code class="mi">128</code><code class="p">]</code> <code class="o">*</code> <code class="mi">4</code> <code class="o">+</code> <code class="p">[</code><code class="mi">256</code><code class="p">]</code> <code class="o">*</code> <code class="mi">6</code> <code class="o">+</code> <code class="p">[</code><code class="mi">512</code><code class="p">]</code> <code class="o">*</code> <code class="mi">3</code><code class="p">:</code>
    <code class="n">strides</code> <code class="o">=</code> <code class="mi">1</code> <code class="k">if</code> <code class="n">filters</code> <code class="o">==</code> <code class="n">prev_filters</code> <code class="k">else</code> <code class="mi">2</code>
    <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">ResidualUnit</code><code class="p">(</code><code class="n">filters</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="n">strides</code><code class="p">))</code>
    <code class="n">prev_filters</code> <code class="o">=</code> <code class="n">filters</code>
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">GlobalAvgPool2D</code><code class="p">())</code>
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">())</code>
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"softmax"</code><code class="p">))</code></pre>

<p>The only slightly tricky part in this code is the loop that adds the <code>ResidualUnit</code> layers to the model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs have 128 filters, and so on. We then set the strides to 1 when the number of filters is the same as in the previous RU, or else we set it to 2. Then we add the <code>ResidualUnit</code>, and finally we update <code>prev_filters</code>.</p>

<p>It is amazing that in fewer than 40 lines of code, we can build the model that won the ILSVRC 2015 challenge! It demonstrates both the elegance of the ResNet model and the expressiveness of the Keras API. Implementing the other CNN architectures is not much harder. However, Keras comes with several of these architectures built in, so why not use them instead?</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Using Pretrained Models from Keras"><div class="sect1" id="idm46263498797320">
<h1>Using Pretrained Models from Keras</h1>

<p>In general, you won’t have to implement standard models like GoogLeNet or ResNet manually, since pretrained networks are readily available with a single line of code, in the <code>keras.applications</code> package. For example, you can load the ResNet-50 model, pretrained on ImageNet, with the following line of code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">applications</code><code class="o">.</code><code class="n">resnet50</code><code class="o">.</code><code class="n">ResNet50</code><code class="p">(</code><code class="n">weights</code><code class="o">=</code><code class="s2">"imagenet"</code><code class="p">)</code></pre>

<p>That’s all! This will create a ResNet-50 model and download weights pretrained on the ImageNet dataset. To use it, you first need to ensure that the images have the right size. A ResNet-50 model expects 224 × 224 images (other models may expect other sizes, such as 299 × 299), so let’s use TensorFlow’s <code>tf.image.resize()</code> function to resize the images we loaded earlier:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">images_resized</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">image</code><code class="o">.</code><code class="n">resize</code><code class="p">(</code><code class="n">images</code><code class="p">,</code> <code class="p">[</code><code class="mi">224</code><code class="p">,</code> <code class="mi">224</code><code class="p">])</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>The <code>tf.image.resize()</code> will not preserve the aspect ratio. If this is a problem, try cropping the images to the appropriate aspect ratio before resizing. Both operations can be done in one shot with <code>tf.image.crop_and_resize()</code>.</p>
</div>

<p>The pretrained models assume that the images are preprocessed in a specific way. In some cases they may expect the inputs to be scaled from 0 to 1, or -1 to 1, and so on. Each model provides a <code>preprocess_input()</code> function that you can use to preprocess your images. These functions assume that the pixel values range from 0 to 255, so we must multiply them by 255 (since earlier we scaled them to the 0–1 range):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">inputs</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">applications</code><code class="o">.</code><code class="n">resnet50</code><code class="o">.</code><code class="n">preprocess_input</code><code class="p">(</code><code class="n">images_resized</code> <code class="o">*</code> <code class="mi">255</code><code class="p">)</code></pre>

<p>Now we can use the pretrained model to make predictions:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Y_proba</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code></pre>

<p>As usual, the output <code>Y_proba</code> is a matrix with one row per image and one column per class (in this case, there are 1,000 classes). If you want to display the top K predictions, including the class name and the estimated probability of each predicted class, use the <code>decode_predictions()</code> function. For each image, it returns an array containing the top K predictions, where each prediction is represented as an array containing the class identifier,<sup><a data-type="noteref" id="idm46263498723480-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263498723480">22</a></sup> its name, and the corresponding confidence score:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">top_K</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">applications</code><code class="o">.</code><code class="n">resnet50</code><code class="o">.</code><code class="n">decode_predictions</code><code class="p">(</code><code class="n">Y_proba</code><code class="p">,</code> <code class="n">top</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
<code class="k">for</code> <code class="n">image_index</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">images</code><code class="p">)):</code>
    <code class="k">print</code><code class="p">(</code><code class="s2">"Image #{}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">image_index</code><code class="p">))</code>
    <code class="k">for</code> <code class="n">class_id</code><code class="p">,</code> <code class="n">name</code><code class="p">,</code> <code class="n">y_proba</code> <code class="ow">in</code> <code class="n">top_K</code><code class="p">[</code><code class="n">image_index</code><code class="p">]:</code>
        <code class="k">print</code><code class="p">(</code><code class="s2">"  {} - {:12s} {:.2f}%"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">class_id</code><code class="p">,</code> <code class="n">name</code><code class="p">,</code> <code class="n">y_proba</code> <code class="o">*</code> <code class="mi">100</code><code class="p">))</code>
    <code class="k">print</code><code class="p">()</code></pre>

<p>The output looks like this:</p>

<pre data-type="programlisting">Image #0
  n03877845 - palace       42.87%
  n02825657 - bell_cote    40.57%
  n03781244 - monastery    14.56%

Image #1
  n04522168 - vase         46.83%
  n07930864 - cup          7.78%
  n11939491 - daisy        4.87%</pre>

<p>The correct classes (monastery and daisy) appear in the top three results for both images. That’s pretty good, considering that the model had to choose among 1,000 classes.</p>

<p>As you can see, it is very easy to create a pretty good image classifier using a pretrained model. Other vision models are available in <code>keras.applications</code>, including several ResNet variants, GoogLeNet variants like InceptionV3 and Xception, VGGNet variants, and MobileNet and MobileNetV2 (lightweight models for use in mobile applications).</p>

<p>But what if you want to use an image classifier for classes of images that are not part of ImageNet? In that case, you may still benefit from the pretrained models to perform transfer learning.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Pretrained Models for Transfer Learning"><div class="sect1" id="idm46263498796696">
<h1>Pretrained Models for Transfer Learning</h1>

<p>If you want to build an image classifier, but you do not have enough training data, then it is often a good idea to reuse the lower layers of a pretrained model, as we discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>. For example, let’s train a model to classify pictures of flowers, reusing a pretrained Xception model. First, let’s load the dataset using TensorFlow Datasets (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch13.html#data_chapter">Chapter&nbsp;13</a>):</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">tensorflow_datasets</code> <code class="kn">as</code> <code class="nn">tfds</code>

<code class="n">dataset</code><code class="p">,</code> <code class="n">info</code> <code class="o">=</code> <code class="n">tfds</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"tf_flowers"</code><code class="p">,</code> <code class="n">as_supervised</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">with_info</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">dataset_size</code> <code class="o">=</code> <code class="n">info</code><code class="o">.</code><code class="n">splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">num_examples</code> <code class="c1"># 3670</code>
<code class="n">class_names</code> <code class="o">=</code> <code class="n">info</code><code class="o">.</code><code class="n">features</code><code class="p">[</code><code class="s2">"label"</code><code class="p">]</code><code class="o">.</code><code class="n">names</code> <code class="c1"># ["dandelion", "daisy", ...]</code>
<code class="n">n_classes</code> <code class="o">=</code> <code class="n">info</code><code class="o">.</code><code class="n">features</code><code class="p">[</code><code class="s2">"label"</code><code class="p">]</code><code class="o">.</code><code class="n">num_classes</code> <code class="c1"># 5</code></pre>

<p>Note that you can get information about the dataset by setting <code>with_info=True</code>. Here, we get the dataset size and the names of the classes. Unfortunately, there is only a <code>"train"</code> dataset, no test set or validation set, so we need to split the training set. The TF Datasets project provides an API for this. For example, let’s take the first 10% of the dataset for testing, the next 15% for validation, and the remaining 75% for training:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">test_split</code><code class="p">,</code> <code class="n">valid_split</code><code class="p">,</code> <code class="n">train_split</code> <code class="o">=</code> <code class="n">tfds</code><code class="o">.</code><code class="n">Split</code><code class="o">.</code><code class="n">TRAIN</code><code class="o">.</code><code class="n">subsplit</code><code class="p">([</code><code class="mi">10</code><code class="p">,</code> <code class="mi">15</code><code class="p">,</code> <code class="mi">75</code><code class="p">])</code>

<code class="n">test_set</code> <code class="o">=</code> <code class="n">tfds</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"tf_flowers"</code><code class="p">,</code> <code class="n">split</code><code class="o">=</code><code class="n">test_split</code><code class="p">,</code> <code class="n">as_supervised</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">valid_set</code> <code class="o">=</code> <code class="n">tfds</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"tf_flowers"</code><code class="p">,</code> <code class="n">split</code><code class="o">=</code><code class="n">valid_split</code><code class="p">,</code> <code class="n">as_supervised</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">train_set</code> <code class="o">=</code> <code class="n">tfds</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"tf_flowers"</code><code class="p">,</code> <code class="n">split</code><code class="o">=</code><code class="n">train_split</code><code class="p">,</code> <code class="n">as_supervised</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code></pre>

<p>Next we must preprocess the images. The CNN expects 224 × 224 images, so we need to resize them. We also need to run the image through Xception’s <code>preprocess_input()</code> function:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">preprocess</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="n">label</code><code class="p">):</code>
    <code class="n">resized_image</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">image</code><code class="o">.</code><code class="n">resize</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="p">[</code><code class="mi">224</code><code class="p">,</code> <code class="mi">224</code><code class="p">])</code>
    <code class="n">final_image</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">applications</code><code class="o">.</code><code class="n">xception</code><code class="o">.</code><code class="n">preprocess_input</code><code class="p">(</code><code class="n">resized_image</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">final_image</code><code class="p">,</code> <code class="n">label</code></pre>

<p>Let’s apply this preprocessing function to all three datasets, shuffle the training set, and add batching and prefetching to all datasets:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">batch_size</code> <code class="o">=</code> <code class="mi">32</code>
<code class="n">train_set</code> <code class="o">=</code> <code class="n">train_set</code><code class="o">.</code><code class="n">shuffle</code><code class="p">(</code><code class="mi">1000</code><code class="p">)</code>
<code class="n">train_set</code> <code class="o">=</code> <code class="n">train_set</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">preprocess</code><code class="p">)</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="n">batch_size</code><code class="p">)</code><code class="o">.</code><code class="n">prefetch</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="n">valid_set</code> <code class="o">=</code> <code class="n">valid_set</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">preprocess</code><code class="p">)</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="n">batch_size</code><code class="p">)</code><code class="o">.</code><code class="n">prefetch</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="n">test_set</code> <code class="o">=</code> <code class="n">test_set</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">preprocess</code><code class="p">)</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="n">batch_size</code><code class="p">)</code><code class="o">.</code><code class="n">prefetch</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code></pre>

<p>If you want to perform some data augmentation, change the preprocessing function for the training set, adding some random transformations to the training images. For example, use <code>tf.image.random_crop()</code> to randomly crop the images, use <code>tf.image.random_flip_left_right()</code> to randomly flip the images horizontally, and so on (see the notebook for an example).</p>
<div data-type="tip"><h6>Tip</h6>
<p>The <code>keras.preprocessing.image.ImageDataGenerator</code> class makes it easy to load images from disk and augment them in various ways: you can shift each image, rotate it, rescale it, flip it horizontally or vertically, shear it, or apply any transformation function you want to it. This is very convenient for simple projects. However, building a tf.data pipeline has many advantages: it can read the images efficiently (e.g., in parallel) from any source, not just the local disk, you can manipulate the <code>Dataset</code> as you wish, and if you write a preprocessing function based on <code>tf.image</code> operations, this function can be used both in the tf.data pipeline and in the model you will deploy to production (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#deployment_chapter">Chapter&nbsp;19</a>).</p>
</div>

<p>Next let’s load an Xception model, pretrained on ImageNet. We exclude the top of the network (by setting <code>include_top=False</code>): this excludes the global average pooling layer and the dense output layer. We then add our own global average pooling layer, based on the output of the base model, followed by a dense output layer with one unit per class, using the softmax activation function. Finally, we create the Keras <code>Model</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">base_model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">applications</code><code class="o">.</code><code class="n">xception</code><code class="o">.</code><code class="n">Xception</code><code class="p">(</code><code class="n">weights</code><code class="o">=</code><code class="s2">"imagenet"</code><code class="p">,</code>
                                                  <code class="n">include_top</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>
<code class="n">avg</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">GlobalAveragePooling2D</code><code class="p">()(</code><code class="n">base_model</code><code class="o">.</code><code class="n">output</code><code class="p">)</code>
<code class="n">output</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="n">n_classes</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"softmax"</code><code class="p">)(</code><code class="n">avg</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">Model</code><code class="p">(</code><code class="n">inputs</code><code class="o">=</code><code class="n">base_model</code><code class="o">.</code><code class="n">input</code><code class="p">,</code> <code class="n">outputs</code><code class="o">=</code><code class="n">output</code><code class="p">)</code></pre>

<p>As explained in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>, it’s usually a good idea to freeze the weights of the pretrained layers, at least at the beginning of training:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">layer</code> <code class="ow">in</code> <code class="n">base_model</code><code class="o">.</code><code class="n">layers</code><code class="p">:</code>
    <code class="n">layer</code><code class="o">.</code><code class="n">trainable</code> <code class="o">=</code> <code class="bp">False</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Since our model uses the base model’s layers directly, rather than the <code>base_model</code> object itself, setting <code>base_model.trainable=False</code> would have no effect.</p>
</div>

<p>Finally, we can compile the model and start training:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="mf">0.9</code><code class="p">,</code> <code class="n">decay</code><code class="o">=</code><code class="mf">0.01</code><code class="p">)</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"sparse_categorical_crossentropy"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="n">optimizer</code><code class="p">,</code>
              <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s2">"accuracy"</code><code class="p">])</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">train_set</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">validation_data</code><code class="o">=</code><code class="n">valid_set</code><code class="p">)</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>This will be very slow, unless you have a GPU. If you do not, then you should run this chapter’s notebook in Colab, using a GPU runtime (it’s free!). See the instructions at <a href="https://github.com/ageron/handson-ml2"><em class="hyperlink">https://github.com/ageron/handson-ml2</em></a>.</p>
</div>

<p>After training the model for a few epochs, its validation accuracy should reach about 75-80% and stop making much progress. This means that the top layers are now pretty well trained, so we are ready to unfreeze all layers (or you could try unfreezing just the top ones) and continue training (don’t forget to compile the model when you freeze or unfreeze layers). This time we use a much lower learning rate to avoid damaging the pretrained weights:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">layer</code> <code class="ow">in</code> <code class="n">base_model</code><code class="o">.</code><code class="n">layers</code><code class="p">:</code>
    <code class="n">layer</code><code class="o">.</code><code class="n">trainable</code> <code class="o">=</code> <code class="bp">True</code>

<code class="n">optimizer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">0.01</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="mf">0.9</code><code class="p">,</code> <code class="n">decay</code><code class="o">=</code><code class="mf">0.001</code><code class="p">)</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="o">...</code><code class="p">)</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="o">...</code><code class="p">)</code></pre>

<p>It will take a while, but this model should reach around 95% accuracy on the test set. With that, you can start training amazing image classifiers! But there’s more to computer vision than just classification. For example, what if you also want to know <em>where</em> the flower is in the picture? Let’s look at this now.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Classification and Localization"><div class="sect1" id="idm46263498566472">
<h1>Classification and Localization</h1>

<p>Localizing an object in a picture can be expressed as a regression task, as discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#ann_chapter">Chapter&nbsp;10</a>: to predict a bounding box around the object, a common approach is to predict the horizontal and vertical coordinates of the object’s center, as well as its height and width. This means we have four numbers to predict. It does not require much change to the model; we just need to add a second dense output layer with four units (typically on top of the global average pooling layer), and it can be trained using the MSE loss:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">base_model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">applications</code><code class="o">.</code><code class="n">xception</code><code class="o">.</code><code class="n">Xception</code><code class="p">(</code><code class="n">weights</code><code class="o">=</code><code class="s2">"imagenet"</code><code class="p">,</code>
                                                  <code class="n">include_top</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>
<code class="n">avg</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">GlobalAveragePooling2D</code><code class="p">()(</code><code class="n">base_model</code><code class="o">.</code><code class="n">output</code><code class="p">)</code>
<code class="n">class_output</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="n">n_classes</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"softmax"</code><code class="p">)(</code><code class="n">avg</code><code class="p">)</code>
<code class="n">loc_output</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">4</code><code class="p">)(</code><code class="n">avg</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">Model</code><code class="p">(</code><code class="n">inputs</code><code class="o">=</code><code class="n">base_model</code><code class="o">.</code><code class="n">input</code><code class="p">,</code>
                    <code class="n">outputs</code><code class="o">=</code><code class="p">[</code><code class="n">class_output</code><code class="p">,</code> <code class="n">loc_output</code><code class="p">])</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="p">[</code><code class="s2">"sparse_categorical_crossentropy"</code><code class="p">,</code> <code class="s2">"mse"</code><code class="p">],</code>
              <code class="n">loss_weights</code><code class="o">=</code><code class="p">[</code><code class="mf">0.8</code><code class="p">,</code> <code class="mf">0.2</code><code class="p">],</code> <code class="c1"># depends on what you care most about</code>
              <code class="n">optimizer</code><code class="o">=</code><code class="n">optimizer</code><code class="p">,</code> <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s2">"accuracy"</code><code class="p">])</code></pre>

<p>But now we have a problem: the flowers dataset does not have bounding boxes around the flowers. So we need to add them ourselves. This is often one of the hardest and most costly part of a Machine Learning project: getting the labels. It’s a good idea to spend time looking for the right tools. To annotate images with bounding boxes, you may want to use an open source image labeling tool like VGG Image Annotator, LabelImg, OpenLabeler or ImgLab, or perhaps a commercial tool like LabelBox or Supervisely. You may also want to consider crowdsourcing platforms such as Amazon Mechanical Turk if you have a very large number of images to annotate. However, it is quite a lot of work to set up a crowdsourcing platform, prepare the form to be sent to the workers, supervise them, and ensure that the quality of the bounding boxes they produce is good, so make sure it is worth the effort. If there are just a few thousand images to label, and you don’t plan to do this frequently, it may be preferable to do it yourself. Adriana Kovashka et al. wrote a very practical <a href="https://homl.info/crowd">paper</a><sup><a data-type="noteref" id="idm46263497874968-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497874968">23</a></sup> about crowdsourcing in computer vision. I recommend you check it out, even if you do not plan to use crowdsourcing.</p>

<p>So let’s suppose you obtained the bounding boxes for every image in the flowers dataset (for now we will assume there is a single bounding box per image). You then need to create a dataset whose items will be batches of preprocessed images along with their class labels and their bounding boxes. Each item should be a tuple of the form: <code>(images, (class_labels, bounding_boxes))</code>. Then you are ready to train your model!</p>
<div data-type="tip"><h6>Tip</h6>
<p>The bounding boxes should be normalized so that the horizontal and vertical coordinates, as well as the height and width, all range from 0 to 1. Also, it is common to predict the square root of the height and width rather than the height and width directly: this way, a 10-pixel error for a large bounding box will not be penalized as much as a 10-pixel error for a small bounding box.</p>
</div>

<p>The MSE often works fairly well as a cost function to train the model, but it is not a great metric to evaluate how well the model can predict bounding boxes. The most common metric for this is the <em>Intersection over Union</em> (IoU), the area of overlap between the predicted bounding box and the target bounding box, divided by the area of their union (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#iou_diagram">Figure&nbsp;14-23</a>). In tf.keras, it is implemented by the <code>tf.keras.metrics.MeanIoU</code> class.</p>

<figure><div id="iou_diagram" class="figure">
<img src="./Chapter14_files/mls2_1423.png" alt="mls2 1423" width="1439" height="758" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1423.png">
<h6><span class="label">Figure 14-23. </span>Intersection over Union (IoU) metric for bounding boxes</h6>
</div></figure>

<p>Classifying and localizing a single object is nice, but what if the images contain multiple objects (as is often the case in the flowers dataset)?</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Object Detection"><div class="sect1" id="idm46263497740888">
<h1>Object Detection</h1>

<p>The task of classifying and localizing multiple objects in an image is called <em>object detection</em>. Until a few years ago, a common approach was to take a CNN that was trained to classify and locate a single object, then slide it across the image, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#sliding_cnn_diagram">Figure&nbsp;14-24</a>. In this example, the image was chopped into a 6 × 8 grid, and we show a CNN (the thick black rectangle) sliding across all 3 × 3 regions. When the CNN was looking at the top left of the image, it detected part of the leftmost rose, and then it detected that same rose again when it was first shifted one step to the right. At the next step, it started detecting part of the topmost rose, and then it detected it again once it was shifted one more step to the right. You would then continue to slide the CNN through the whole image, looking at all 3 × 3 regions. Moreover, since objects can have varying sizes, you would also slide the CNN across regions of different sizes. For example, once you are done with the 3 × 3 regions, you might want to slide the CNN across all 4 × 4 regions as well.</p>

<figure><div id="sliding_cnn_diagram" class="figure">
<img src="./Chapter14_files/mls2_1424.png" alt="mls2 1424" width="1326" height="952" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1424.png">
<h6><span class="label">Figure 14-24. </span>Detecting multiple objects by sliding a CNN across the image</h6>
</div></figure>

<p>This technique is fairly straightforward, but as you can see it will detect the same object multiple times, at slightly different positions. Some post-processing will then be needed to get rid of all the unnecessary bounding boxes. A common approach for this is called <em>non-max suppression</em>. Here’s how you do it:</p>
<ol>
<li>
<p>First, you need to add an extra <em>objectness</em> output to your CNN, to estimate the probability that a flower is indeed present in the image (alternatively, you could add a “no-flower” class, but this usually does not work as well). It must use the sigmoid activation function, and you can train it using the <code>"binary_crossentropy"</code> loss. Then get rid of all the bounding boxes for which the objectness score is below some threshold: this will drop all the bounding boxes that don’t actually contain a flower.</p>
</li>
<li>
<p>Find the bounding box with the highest objectness score, and get rid of all the other bounding boxes that overlap a lot with it (e.g., with an IoU greater than 60%). For example, in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#sliding_cnn_diagram">Figure&nbsp;14-24</a>, the bounding box with the max objectness score is the thick bounding box over the topmost rose (the objectness score is represented by the thickness of the bounding boxes). The other bounding box over that same rose overlaps a lot with the max bounding box, so we will get rid of it.</p>
</li>
<li>
<p>Repeat step two until there are no more bounding boxes to get rid of.</p>
</li>

</ol>

<p>This simple approach to object detection works pretty well, but it requires running the CNN many times, so it is quite slow. Fortunately, there is a much faster way to slide a CNN across an image: using a <em>Fully Convolutional Network</em>.</p>








<section data-type="sect2" data-pdf-bookmark="Fully Convolutional Networks (FCNs)"><div class="sect2" id="idm46263497726952">
<h2>Fully Convolutional Networks (FCNs)</h2>

<p>The idea of FCNs was first introduced in a <a href="https://homl.info/fcn">2015 paper</a><sup><a data-type="noteref" id="idm46263497724808-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497724808">24</a></sup> by Jonathan Long et al., for semantic segmentation (the task of classifying every pixel in an image according to the class of the object it belongs to). They pointed out that you could replace the dense layers at the top of a CNN by convolutional layers. To understand this, let’s look at an example: suppose a dense layer with 200 neurons sits on top of a convolutional layer that outputs 100 feature maps, each of size 7 × 7 (this is the feature map size, not the kernel size). Each neuron will compute a weighted sum of all 100 × 7 × 7 activations from the convolutional layer (plus a bias term). Now let’s see what happens if we replace the dense layer with a convolution layer using 200 filters, each 7 × 7, and with <code>"valid"</code> padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel is exactly the size of the input feature maps and we are using <code>"valid"</code> padding). In other words, it will output 200 numbers, just like the dense layer did; and if you look closely at the computations performed by a convolutional layer, you will notice that these numbers will be precisely the same as the dense layer produced. The only difference is that the dense layer’s output was a tensor of shape [batch size, 200], while the convolutional layer will output a tensor of shape [batch size, 1, 1, 200].</p>
<div data-type="tip"><h6>Tip</h6>
<p>To convert a dense layer to a convolutional layer, the number of filters in the convolutional layer must be equal to the number of units in the dense layer, the filter size must be equal to the size of the input feature maps, and you must use <code>"valid"</code> padding. The stride may be set to 1 or more, as we will see shortly.</p>
</div>

<p>Why is this important? Well, while a dense layer expects a specific input size (since it has one weight per input feature), a convolutional layer will happily process images of any size<sup><a data-type="noteref" id="idm46263497719400-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497719400">25</a></sup> (however, it does expect its inputs to have a specific number of channels, since each kernel contains a different set of weights for each input channel). Since an FCN contains only convolutional layers (and pooling layers, which have the same property), it can be trained and executed on images of any size!</p>

<p>For example, suppose we already trained a CNN for flower classification and localization. It was trained on 224 × 224 images, and it outputs 10 numbers: outputs 0 to 4 are sent through the softmax activation function, and this gives the class probabilities (one per class); output 5 is sent through the logistic activation function, and this gives the objectness score; outputs 6 to 9 do not use any activation function, and they represent the bounding box’s center coordinates, as well as its height and width. We can now convert its dense layers to convolutional layers. In fact, we don’t even need to retrain it; we can just copy the weights from the dense layers to the convolutional layers! Alternatively, we could have converted the CNN into an FCN before training.</p>

<p>Now suppose the last convolutional layer before the output layer (also called the bottleneck layer) outputs 7 × 7 feature maps when the network is fed a 224 × 224 image (see the left side of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#fcn_diagram">Figure&nbsp;14-25</a>). If we feed the FCN a 448 × 448 image (see the right side of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#fcn_diagram">Figure&nbsp;14-25</a>), the bottleneck layer will now output 14 × 14 feature maps.<sup><a data-type="noteref" id="idm46263497714488-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497714488">26</a></sup> Since the dense output layer was replaced by a convolutional layer using 10 filters of size 7 × 7, <code>"valid"</code> padding and stride 1, the output will be composed of 10 features maps, each of size 8 × 8 (since 14 - 7 + 1 = 8). In other words, the FCN will process the whole image only once, and it will output an 8 × 8 grid where each cell contains 10 numbers (5 class probabilities, 1 objectness score, and 4 bounding box coordinates). It’s exactly like taking the original CNN and sliding it across the image using 8 steps per row and 8 steps per column: to visualize this, imagine chopping the original image into a 14 × 14 grid, then sliding a 7 × 7 window across this grid; there will be 8 × 8 = 64 possible locations for the window, hence 8 × 8 predictions. However, the FCN approach is <em>much</em> more efficient, since the network only looks at the image once. In fact, <em>You Only Look Once</em> (YOLO) is the name of a very popular object detection architecture!</p>

<figure><div id="fcn_diagram" class="figure">
<img src="./Chapter14_files/mls2_1425.png" alt="mls2 1425" width="1440" height="1155" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1425.png">
<h6><span class="label">Figure 14-25. </span>A Fully Convolutional Network processing a small image (left) and a large one (right)</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="You Only Look Once (YOLO)"><div class="sect2" id="idm46263497708392">
<h2>You Only Look Once (YOLO)</h2>

<p>YOLO is an extremely fast and accurate object detection architecture proposed by Joseph Redmon et al. in a <a href="https://homl.info/yolo">2015 paper</a>,<sup><a data-type="noteref" id="idm46263497705976-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497705976">27</a></sup> and subsequently improved <a href="https://homl.info/yolo2">in 2016</a><sup><a data-type="noteref" id="idm46263497704600-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497704600">28</a></sup> (YOLOv2) and <a href="https://homl.info/yolo3">in 2018</a><sup><a data-type="noteref" id="idm46263497703240-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497703240">29</a></sup> (YOLOv3). It is so fast that it can run in realtime on a video (check out this nice <a href="https://homl.info/yolodemo">demo</a>).</p>

<p>YOLOv3’s architecture is quite similar to the one we just discussed, but with a few important differences:</p>

<ul>
<li>
<p>First, it outputs five bounding boxes for each grid cell (instead of just one), and each bounding box comes with an objectness score. It also outputs 20 class probabilities per grid cell, as it was trained on the PASCAL VOC dataset, which contains 20 classes. That’s a total of 45 numbers per grid cell: 5 * 4 bounding boxes, each with 4 coordinates, plus 5 objectness scores, plus 20 class probabilities.</p>
</li>
<li>
<p>Second, instead of predicting the absolute coordinates of the bounding box centers, YOLOv3 predicts an offset relative to the coordinates of the grid cell, where (0, 0) means the top left of that cell and (1, 1) means the bottom right. For each grid cell, YOLOv3 is trained to predict only bounding boxes whose center lies in that cell (but the bounding box itself generally extends well beyond the grid cell). YOLOv3 applies the logistic activation function to the bounding box coordinates to ensure they remain in the 0 to 1 range.</p>
</li>
<li>
<p>Third, before training the neural net, YOLOv3 finds five representative bounding box dimensions, called <em>anchor boxes</em> (or <em>bounding box priors</em>). It does this by applying the K-Means algorithm (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#unsupervised_learning_chapter">Chapter&nbsp;9</a>) to the height and width of the training set bounding boxes. For example, if the training images contain many pedestrians, then one of the anchor boxes will likely have the dimensions of a typical pedestrian. Then when the neural net predicts five bounding boxes per grid cell, it actually predicts how much to rescale each of the anchor boxes. For example, suppose one anchor box is 100 pixels tall and 50 pixels wide, and the network predicts, say, a vertical rescaling factor of 1.5 and a horizontal rescaling of 0.9 (for one of the grid cells). This will result in a predicted bounding box of size 150 × 45 pixels. To be more precise, for each grid cell and each anchor box, the network predicts the log of the vertical and horizontal rescaling factors. Having these priors makes the network more likely to predict bounding boxes of the appropriate dimensions, and it also speeds up training because it will more quickly learn what reasonable bounding boxes look like.</p>
</li>
<li>
<p>Fourth, the network is trained using images of different scales: every few batches during training, the network randomly chooses a new image dimension (from 330 × 330 to 608 × 608 pixels). This allows the network to learn to detect objects at different scales. Moreover, it makes it possible to use YOLOv3 at different scales: the smaller scale will be less accurate but faster than the larger scale, so you can choose the right trade-off for your use case.</p>
</li>
</ul>

<p>There are a few more innovations you might be interested in, such as the use of skip connections to recover some of the spatial resolution that is lost in the CNN (we will discuss this shortly when we look at semantic segmentation). In the 2016 paper, the authors introduce the YOLO9000 model that uses hierarchical classification: the model predicts a probability for each node in a visual hierarchy called <em>WordTree</em>. This makes it possible for the network to predict with high confidence that an image represents, say, a dog, even though it is unsure what specific type of dog. So I encourage you to go ahead and read all three papers: they are quite pleasant to read, and they provide excellent examples of how Deep Learning systems can be incrementally improved.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46263497690984">
<h5>Mean Average Precision (mAP)</h5>
<p>A very common metric used in object detection tasks is the <em>mean Average Precision</em> (mAP). “Mean Average” sounds a bit redundant, doesn’t it? To understand this metric, let’s go back to two classification metrics we discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch03.html#classification_chapter">Chapter&nbsp;3</a>: precision and recall. Remember the trade-off: the higher the recall, the lower the precision. You can visualize this in a precision-recall curve (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch03.html#precision_vs_recall_plot">Figure&nbsp;3-5</a>). To summarize this curve into a single number, we could compute its area under the curve (AUC). But note that the precision-recall curve may contain a few sections where precision actually goes up when recall increases, especially at low recall values (you can see this at the top left of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch03.html#precision_vs_recall_plot">Figure&nbsp;3-5</a>). This is one of the motivations for the mAP metric.</p>

<p>Suppose the classifier has a 90% precision at 10% recall, but a 96% precision at 20% recall: there’s really no trade-off here: it simply makes more sense to use the classifier at 20% recall rather than at 10% recall, as you will get both higher recall and higher precision. So instead of looking at the precision <em>at</em> 10% recall, we should really be looking at the <em>maximum</em> precision that the classifier can offer with <em>at least</em> 10% recall. It would be 96%, not 90%. So one way to get a fair idea of the model’s performance is to compute the maximum precision you can get with at least 0% recall, then 10% recall, 20%, and so on up to 100%, and then calculate the mean of these maximum precisions. This is called the <em>Average Precision</em> (AP) metric. Now when there are more than two classes, we can compute the AP for each class, and then compute the mean AP (mAP). That’s it!</p>

<p>In an object detection system, there is an additional level of complexity: what if the system detected the correct class, but at the wrong location (i.e., the bounding box is completely off)? Surely we should not count this as a positive prediction. So one approach is to define an IOU threshold: for example, we may consider that a prediction is correct only if the IOU is greater than, say, 0.5, and the predicted class is correct. The corresponding mAP is generally noted mAP@0.5 (or mAP@50%, or sometimes just AP<sub>50</sub>). In some competitions (such as the Pascal VOC challenge), this is what is done. In others (such as the COCO competition), the mAP is computed for different IOU thresholds (0.50, 0.55, 0.60, …, 0.95), and the final metric is the mean of all these mAPs (noted AP@[.50:.95] or AP@[.50:0.05:.95]). Yes, that’s a mean mean average.</p>
</div></aside>

<p>Several YOLO implementations built using TensorFlow are available on GitHub. In particular, check out <a href="https://homl.info/yolotf2">Zihao Zang’s TensorFlow 2 implementation</a>. Other object detection models are available in the TensorFlow Models project, many with pretrained weights, and some have even been ported to TF Hub, such as <a href="https://homl.info/ssd">SSD</a><sup><a data-type="noteref" id="idm46263497678968-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497678968">30</a></sup> and <a href="https://homl.info/fasterrcnn">Faster-RCNN</a>,<sup><a data-type="noteref" id="idm46263497677032-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497677032">31</a></sup> which are both quite popular. SSD is also a “single shot” detection model, similar to YOLO. Faster R-CNN is more complex: the image first goes through a CNN, and the output is passed to a <em>Region Proposal Network</em> (RPN) that proposes bounding boxes that are most likely to contain an object, and a classifier is run for each bounding box, based on the cropped output of the CNN.</p>

<p>The choice of detection system depends on many factors: speed, accuracy, available pretrained models, training time, complexity, etc. The papers contain tables of metrics, but there is quite a lot of variability in the testing environments, and the technologies evolve so fast that it is difficulty to make a fair comparison that will be useful for most people and remain valid for more than a few months.</p>

<p>Great! So we can locate objects by drawing bounding boxes around them. But perhaps you might want to be a bit more precise. Let’s see how to go down to the pixel level.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Semantic Segmentation"><div class="sect1" id="idm46263497673496">
<h1>Semantic Segmentation</h1>

<p>In <em>semantic segmentation</em>, each pixel is classified according to the class of the object it belongs to (e.g., road, car, pedestrian, building, etc.), as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#semantic_segmentation_diagram">Figure&nbsp;14-26</a>. Note that different objects of the same class are <em>not</em> distinguished. For example, all the bicycles on the right side of the segmented image end up as one big lump of pixels. The main difficulty in this task is that when images go through a regular CNN, they gradually lose their spatial resolution (due to the layers with strides greater than 1): so a regular CNN may end up knowing that there’s a person in the image, somewhere in the bottom left of the image, but it will not be much more precise than that.</p>

<figure><div id="semantic_segmentation_diagram" class="figure">
<img src="./Chapter14_files/mls2_1426.png" alt="mls2 1426" width="1440" height="571" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1426.png">
<h6><span class="label">Figure 14-26. </span>Semantic segmentation</h6>
</div></figure>

<p>Just like for object detection, there are many different approaches to tackle this problem, some quite complex. However, a fairly simple solution was proposed in the 2015 paper by Jonathan Long et al. we discussed earlier. The authors start by taking a pretrained CNN and turning into an FCN. The CNN applies a stride of 32 to the input image overall (i.e., if you add up all the strides greater than 1), meaning the last layer outputs feature maps that are 32 times smaller than the input image. This is clearly too coarse, so they add a single <em>upsampling layer</em> that multiplies the resolution by 32. There are several solutions available for upsampling (increasing the size of an image), such as bilinear interpolation, but it only works reasonably well up to ×4 or ×8. Instead, they used a <em>transposed convolutional layer</em>:<sup><a data-type="noteref" id="idm46263497665240-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497665240">32</a></sup> it is equivalent to first stretching the image by inserting empty rows and columns (full of zeros), then performing a regular convolution (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#conv2d_transpose_diagram">Figure&nbsp;14-27</a>). Alternatively, some people prefer to think of it as a regular convolutional layer that uses fractional strides (e.g., 1/2 in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#conv2d_transpose_diagram">Figure&nbsp;14-27</a>). The <em>transposed convolutional layer</em> can be initialized to perform something close to linear interpolation, but since it is a trainable layer, it will learn to do better during training. In tf.keras, you can use the <code>Conv2DTranspose</code> layer.</p>

<figure><div id="conv2d_transpose_diagram" class="figure">
<img src="./Chapter14_files/mls2_1427.png" alt="mls2 1427" width="1084" height="709" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1427.png">
<h6><span class="label">Figure 14-27. </span>Upsampling using a transposed convolutional layer</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In a transposed convolution layer, the stride defines how much the input will be stretched, not the size of the filter steps, so the larger the stride, the larger the output (unlike for convolutional layers or pooling layers).</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46263497611752">
<h5>TensorFlow Convolution Operations</h5>
<p>TensorFlow also offers a few other kinds of convolutional layers:</p>
<dl>
<dt><code>keras.layers.Conv1D</code></dt>
<dd>
<p>Creates a convolutional layer for 1D inputs, such as time series or text (sequences of letters or words), as we will see in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#rnn_chapter">Chapter&nbsp;15</a>.</p>
</dd>
<dt><code>keras.layers.Conv3D</code></dt>
<dd>
<p>Creates a convolutional layer for 3D inputs, such as 3D PET scan.</p>
</dd>
<dt><code>dilation_rate</code></dt>
<dd>
<p>Setting the <code>dilation_rate</code> hyperparameter of any convolutional layer to a value of 2 or more creates an <em>à-trous convolutional layer</em> (“à trous” is French for “with holes”). This is equivalent to using a regular convolutional layer with a filter dilated by inserting rows and columns of zeros (i.e., holes). For example, a 1 × 3 filter equal to <code>[[1,2,3]]</code> may be dilated with a <em>dilation rate</em> of 4, resulting in a <em>dilated filter</em> <code>[[1, 0, 0, 0, 2, 0, 0, 0, 3]]</code>. This allows the convolutional layer to have a larger receptive field at no computational price and using no extra parameters.</p>
</dd>
<dt><code>tf.nn.depthwise_conv2d()</code></dt>
<dd>
<p>Can be used to create a <em>depthwise convolutional layer</em> (but you need to create the variables yourself). It applies every filter to every individual input channel independently. Thus, if there are <em>f</em><sub><em>n</em></sub> filters and <em>f</em><sub><em>n</em>′</sub>  input channels, then this will output <em>f</em><sub><em>n</em></sub> × <em>f</em><sub><em>n</em>′</sub> feature maps.</p>
</dd>
</dl>
</div></aside>

<p>This solution is OK, but still too imprecise. To do better, the authors added skip connections from lower layers: for example, they upsampled the output image by a factor of 2 (instead of 32), and they added the output of a lower layer that had this double resolution. Then they upsampled the result by a factor of 16, leading to a total upsampling factor of 32 (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#skip_plus_upsample_diagram">Figure&nbsp;14-28</a>). This recovered some of the spatial resolution that was lost in earlier pooling layers. In their best architecture, they used a second similar skip connection to recover even finer details from an even lower layer: in short, the output of the original CNN goes through the following extra steps: upscale ×2, add the output of a lower layer (of the appropriate scale), upscale ×2, add the output of an even lower layer, and finally upscale ×8. It is even possible to scale up beyond the size of the original image: this can be used to increase the resolution of an image, which is a technique called <em>super-resolution</em>.</p>

<figure><div id="skip_plus_upsample_diagram" class="figure">
<img src="./Chapter14_files/mls2_1428.png" alt="mls2 1428" width="1440" height="423" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1428.png">
<h6><span class="label">Figure 14-28. </span>Skip layers recover some spatial resolution from lower layers</h6>
</div></figure>

<p>Once again, many GitHub repositories provide TensorFlow implementations of semantic segmentation (TensorFlow&nbsp;1 for now), and you will even find a pretrained <em>instance segmentation</em> model in the TensorFlow Models project. Instance segmentation is similar to semantic segmentation, but instead of merging all objects of the same class into one big lump, each object is distinguished from the others (e.g., it identifies each individual bicycle). At the present, they provide multiple implementations of the <em>Mask R-CNN</em> architecture, which was proposed in a <a href="https://homl.info/maskrcnn">2017 paper</a>: it extends the Faster R-CNN model by additionally producing a pixel mask for each bounding box. So not only do you get a bounding box around each object, with a set of estimated class probabilities, you also get a pixel mask that locates pixels in the bounding box that belong to the object.</p>

<p>As you can see, the field of Deep Computer Vision is vast and moving fast, with all sorts of architectures popping out every year, all based on convolutional neural networks. The progress made in just a few years has been astounding, and researchers are now focusing on harder and harder problems, such as <em>adversarial learning</em> (which attempts to make the network more resistant to images designed to fool it), explainability (understanding why the network makes a specific classification), realistic <em>image generation</em> (which we will come back to in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#autoencoders_chapter">Chapter&nbsp;17</a>), and <em>single-shot learning</em> (a system that can recognize an object after it has seen it just once). Some even explore completely novel architectures, such as Geoffrey Hinton’s <a href="https://homl.info/capsnet"><em>capsule networks</em></a><sup><a data-type="noteref" id="idm46263497583720-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497583720">33</a></sup> (I presented them in a couple of <a href="https://homl.info/capsnetvideos">videos</a>, with the corresponding code in a notebook). Now on to the next chapter, where we will look at how to process sequential data such as time series using recurrent neural networks and convolutional neural networks.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="idm46263497672872">
<h1>Exercises</h1>
<ol>
<li>
<p>What are the advantages of a CNN over a fully connected DNN for image classification?</p>
</li>
<li>
<p>Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and <code>"same"</code> padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels. What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?</p>
</li>
<li>
<p>If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?</p>
</li>
<li>
<p>Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?</p>
</li>
<li>
<p>When would you want to add a <em>local response normalization</em> layer?</p>
</li>
<li>
<p>Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?</p>
</li>
<li>
<p>What is a Fully Convolutional Network? How can you convert a dense layer into a convolutional layer?</p>
</li>
<li>
<p>What is the main technical difficulty of semantic segmentation?</p>
</li>
<li>
<p>Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST.</p>
</li>
<li>
<p>Use transfer learning for large image classification, going through these steps:</p>
<ol>
<li>
<p>Create a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), or alternatively you can use an existing dataset (e.g., from TensorFlow Datasets).</p>
</li>
<li>
<p>Split it into a training set, a validation set, and a test set.</p>
</li>
<li>
<p>Build the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation.</p>
</li>
<li>
<p>Fine-tune a pretrained model on this dataset.</p>
</li>

</ol>
</li>
<li>
<p>Go through TensorFlow’s <a href="https://homl.info/styletuto">Style Transfer tutorial</a>. It is a fun way to generate art using Deep Learning.</p>
</li>

</ol>

<p>Solutions to these exercises are available in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/app01.html#solutions_appendix">Appendix&nbsp;A</a>.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46263500621160"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500621160-marker" class="totri-footnote">1</a></sup> David H. Hubel, “Single Unit Activity in Striate Cortex of Unrestrained Cats,” <em>The Journal of Physiology</em> 147 (1959): 226–238.</p><p data-type="footnote" id="idm46263500619400"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500619400-marker" class="totri-footnote">2</a></sup> David H. Hubel and Torsten N. Wiesel, “Receptive Fields of Single Neurons in the Cat’s Striate Cortex,” <em>The Journal of Physiology</em> 148 (1959): 574–591.</p><p data-type="footnote" id="idm46263500617560"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500617560-marker" class="totri-footnote">3</a></sup> David H. Hubel and Torsten N. Wiesel, “Receptive Fields and Functional Architecture of Monkey Striate Cortex,” <em>The Journal of Physiology</em> 195 (1968): 215–243.</p><p data-type="footnote" id="idm46263500568024"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500568024-marker" class="totri-footnote">4</a></sup> Kunihiko Fukushima, “Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position,” <em>Biological Cybernetics</em> 36 (1980): 193–202.</p><p data-type="footnote" id="idm46263500565784"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500565784-marker" class="totri-footnote">5</a></sup> Yann LeCun et al., “Gradient-Based Learning Applied to Document Recognition” in <em>Proceedings of the IEEE</em> (1998).</p><p data-type="footnote" id="idm46263500559016"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500559016-marker" class="totri-footnote">6</a></sup> A convolution is a mathematical operation that slides one function over another and measures the integral of their pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform and is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very similar to convolutions (see <a href="https://homl.info/76"><em class="hyperlink">https://homl.info/76</em></a> for more details).</p><p data-type="footnote" id="idm46263500182904"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500182904-marker" class="totri-footnote">7</a></sup> A fully connected layer with 150 × 100 neurons, each connected to all 150 × 100 × 3 inputs, would have 150<sup>2</sup> × 100<sup>2</sup> × 3 = 675 million parameters!</p><p data-type="footnote" id="idm46263500180728"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500180728-marker" class="totri-footnote">8</a></sup> In the international system of units (SI), 1 MB = 1,000 kB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits.</p><p data-type="footnote" id="idm46263500171816"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263500171816-marker" class="totri-footnote">9</a></sup> Other kernels we’ve discussed so far had weights, but pooling kernels do not: they are just stateless sliding windows.</p><p data-type="footnote" id="lenet5"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#lenet5-marker">10</a></sup> Yann LeCun et al., “Gradient-Based Learning Applied to Document Recognition,” .</p><p data-type="footnote" id="idm46263499607080"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499607080-marker">11</a></sup> Alex Krizhevsky et al., “ImageNet Classification with Deep Convolutional Neural Networks,” <em>Neural Information Processing Systems Conference</em> (2012).</p><p data-type="footnote" id="idm46263499403032"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499403032-marker">12</a></sup> Christian Szegedy et al., “Going Deeper with Convolutions,” <em>The IEEE Conference on Computer Vision and Pattern Recognition</em> (2015): 1–9.</p><p data-type="footnote" id="idm46263499400200"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499400200-marker">13</a></sup> In the 2010 movie <em>Inception</em>, the characters keep going deeper and deeper into multiple layers of dreams, hence the name of these modules.</p><p data-type="footnote" id="idm46263499367624"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499367624-marker">14</a></sup> Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recognition,” <em>ICLR</em> (2015).</p><p data-type="footnote" id="idm46263499362360"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499362360-marker">15</a></sup> Kaiming He et al., <em>Deep Residual Learning for Image Recognition</em> (Microsoft Research technical report, December 2015).</p><p data-type="footnote" id="idm46263499336520"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499336520-marker">16</a></sup> It is a common practice when describing a neural network to count only layers with parameters.</p><p data-type="footnote" id="idm46263499332792"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499332792-marker">17</a></sup> Christian Szegedy et al., <em>Inception–v4, Inception-ResNet and the Impact of Residual Connections on Learning</em> (Google Inc., August 2016).</p><p data-type="footnote" id="idm46263499329112"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499329112-marker">18</a></sup> François Chollet, <em>Xception: Deep Learning with Depthwise Separable Convolutions</em> (Google Inc., April 2017).</p><p data-type="footnote" id="idm46263499326312"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499326312-marker">19</a></sup> This name can sometimes be ambiguous, since spatially separable convolutions are often called “separable convolutions” as well.</p><p data-type="footnote" id="idm46263499316440"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499316440-marker">20</a></sup> Xingyu Zeng et al., “Crafting GBD-Net for Object Detection,” <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 40, no. 9 (2018): 2109–2123.</p><p data-type="footnote" id="idm46263499312296"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263499312296-marker">21</a></sup> Jie Hu et al., “Squeeze-and-Excitation Networks” in proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2018: 7132–7141.</p><p data-type="footnote" id="idm46263498723480"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263498723480-marker">22</a></sup> In the ImageNet dataset, each image is associated to a word in the <a href="https://wordnet.princeton.edu/">WordNet dataset</a>: the class ID is just a WordNet ID.</p><p data-type="footnote" id="idm46263497874968"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497874968-marker">23</a></sup> Adriana Kovashka et al., “Crowdsourcing in Computer Vision,” <em>Foundations and Trends in Computer Graphics and Vision</em> 10, no. 3 (2014): 177–243.</p><p data-type="footnote" id="idm46263497724808"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497724808-marker">24</a></sup> Jonathan Long et al., “Fully Convolutional Networks for Semantic Segmentation” CVPR 2015: 3431–3440.</p><p data-type="footnote" id="idm46263497719400"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497719400-marker">25</a></sup> There is one small exception: a convolutional layer using <code>"valid"</code> padding will complain if the input size is smaller than the kernel size.</p><p data-type="footnote" id="idm46263497714488"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497714488-marker">26</a></sup> This assumes we used only <code>"same"</code> padding in the network: indeed, <code>"valid"</code> padding would reduce the size of the feature maps. Moreover, 448 can be neatly divided by 2 several times until we reach 7, without any rounding error. If any layer uses a different stride than 1 or 2, then there may be some rounding error, so again the feature maps may end up being smaller.</p><p data-type="footnote" id="idm46263497705976"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497705976-marker">27</a></sup> Joseph Redmon et al., “You Only Look Once: Unified, Real-Time Object Detection” CVPR 2016: 779–788.</p><p data-type="footnote" id="idm46263497704600"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497704600-marker">28</a></sup> Joseph Redmon and Ali Farhadi, “YOLO9000: Better, Faster, Stronger” CVPR 2017: 6517–6525.</p><p data-type="footnote" id="idm46263497703240"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497703240-marker">29</a></sup> Joseph Redmon and Ali Farhadi, “YOLOv3: An Incremental Improvement” (2015).</p><p data-type="footnote" id="idm46263497678968"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497678968-marker">30</a></sup> Wei Liu et al., “SSD: Single Shot Multibox Detector,” <em>European Conference on Computer Vision</em> 1 (2016): 21–37.</p><p data-type="footnote" id="idm46263497677032"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497677032-marker">31</a></sup> Shaoqing Ren et al., “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,” in <em>Advances in Neural Information Processing Systems</em> (2015): 91–99.</p><p data-type="footnote" id="idm46263497665240"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497665240-marker">32</a></sup> This type of layer is sometimes referred to as a <em>deconvolution layer</em>, but it does <em>not</em> perform what mathematicians call a deconvolution, so this name should be avoided.</p><p data-type="footnote" id="idm46263497583720"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#idm46263497583720-marker">33</a></sup> Geoffrey Hinton et al., “Matrix Capsules with EM Routing,” ICLR (2018).</p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-15" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders">
		
		<li class="copy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#">Add Highlight</a></li>
		<li class="add-note"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#">
			Add Note
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch13.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">13. Loading and Preprocessing Data with TensorFlow</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">15. Processing Sequences Using RNNs and CNNs</div>
        </a>
    
  
  </div>

</section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    



        
      </div>
      



  <footer class="pagefoot">
    <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#" class="icon-up" style="display: none;"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      <li class="full-support"><a href="https://www.oreilly.com/online-learning/support/">Support</a></li>
      <li><a href="https://www.oreilly.com/online-learning/apps.html">Get the App</a></li>
      
        <li><a href="https://learning.oreilly.com/accounts/logout/">Sign Out</a></li>
      
    </ul>
    <span class="copyright">© 2019 <a href="https://learning.oreilly.com/" target="_blank">Safari</a>.</span>
    <a href="https://learning.oreilly.com/terms/">Terms of Service</a> /
    <a href="https://learning.oreilly.com/membership-agreement/">Membership Agreement</a> /
    <a href="https://www.oreilly.com/privacy.html">Privacy Policy</a>
  </footer>

<script type="text/javascript">window.NREUM||(NREUM={});NREUM.info={"applicationID":"172641827,79672898,93931619","errorBeacon":"bam.nr-data.net","agent":"","applicationTime":451,"licenseKey":"510f1a6865","transactionName":"YgdaZ0NSW0cEB0RdWltNfkZfUEFdCgofXFBHDVYdR1pQQxZeRl1QQj1aWkU=","beacon":"bam.nr-data.net","queueTime":4}</script>


    
    <script src="./Chapter14_files/saved_resource" charset="utf-8"></script><script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","1732687426968531");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=1732687426968531&amp;ev=PageView&amp;noscript=1"></noscript><script type="text/javascript" id="">(function(){window.medalliaUserIdentifier=document.documentElement.dataset.userUuid;window.medalliaUserName=document.documentElement.dataset.username})();</script>
<script type="text/javascript" id="" src="./Chapter14_files/embed.js.download"></script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script>
<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("set","agent","tmgoogletagmanager","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>

<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>
<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.09556792590964802"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.5229133033930915" width="0" height="0" alt="" src="./Chapter14_files/0"></div>
    <script src="./Chapter14_files/saved_resource(1)" charset="utf-8"></script>
  

<script src="./Chapter14_files/saved_resource(2)" type="text/javascript"></script><script type="text/javascript" id="">window._pp=window._pp||[];if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/register/")_pp.targetUrl="/confirm/trial";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/subscribe/")_pp.targetUrl="/confirm/paid";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/signup/")_pp.targetUrl="/confirm/paid";_pp.siteId="2508";
_pp.siteUId="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79";_pp.orderValue="undefined";_pp.orderId="undefined";(function(){var ppjs=document.createElement("script");ppjs.type="text/javascript";ppjs.async=true;ppjs.src=("https:"==document.location.protocol?"https:":"http:")+"//cdn.pbbl.co/r/"+_pp.siteId+".js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(ppjs,s)})();</script><div class="annotator-notice"></div><div class="font-flyout" style="top: 201px; left: 1194px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#">Reset</a>
</div>
</div><script type="text/javascript" async="" src="./Chapter14_files/generic1566415868241.js.download" charset="UTF-8"></script><div style="display: none; visibility: hidden;"><script>(function(){if(null!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')&&void 0!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')){var a=!1;window.addEventListener("blur",function(){a&&dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"facebook",eventVal:0,nonInteraction:0})});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseover",function(){window.focus();
a=!0});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseout",function(){a=!1})}try{window.twttr=function(b,a,d){var c,e=b.getElementsByTagName(a)[0];if(!b.getElementById(d))return b=b.createElement(a),b.id=d,b.src="//platform.twitter.com/widgets.js",e.parentNode.insertBefore(b,e),window.twttr||(c={_e:[],ready:function(a){c._e.push(a)}})}(document,"script","twitter-wjs"),twttr.ready(function(a){a.events.bind("tweet",trackTwitter)})}catch(b){}})();
null!==document.querySelector(".IN-widget")&&void 0!==document.querySelector(".IN-widget")&&document.querySelector(".IN-widget").addEventListener("click",function(){dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"linkedin",eventVal:0,nonInteraction:0})});
function trackTwitter(a){a&&(a.target&&"IFRAME"==a.target.nodeName&&(opt_target=extractParamFromUri(a.target.src,"url")),dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"twitter",eventVal:0,nonInteraction:0}))}function extractParamFromUri(a,b){if(a){var c=new RegExp("[\\?\x26#]"+b+"\x3d([^\x26#]*)");c=c.exec(a);if(null!=c)return unescape(c[1])}};</script></div><span><div id="KampyleAnimationContainer" style="z-index: 2147483000; border: 0px; position: fixed; display: block; width: 0px; height: 0px;"></div></span><iframe scrolling="no" frameborder="0" allowtransparency="true" src="./Chapter14_files/widget_iframe.097c1f5038f9e8a0d62a39a892838d66.html" title="Twitter settings iframe" style="display: none;"></iframe><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_AMS, sans-serif;"></div></div></body></html>