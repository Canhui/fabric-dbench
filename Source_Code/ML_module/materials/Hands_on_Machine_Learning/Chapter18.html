<!DOCTYPE html>
<!-- saved from url=(0091)https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html -->
<html class=" js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/hands-on-machine-learning/9781492032632/part01.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="4626953" data-user-uuid="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79" data-username="17481074" data-account-type="B2B" data-activated-trial-date="" data-archive="9781492032632" data-publishers="O&#39;Reilly Media, Inc." data-htmlfile-name="part01.html" data-epub-title="Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781492032632"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><script type="text/javascript" async="" src="./Chapter18_files/cool-2.1.15.min.js.download"></script><script type="text/javascript" src="./Chapter18_files/510f1a6865"></script><script id="twitter-wjs" src="./Chapter18_files/widgets.js.download"></script><script src="./Chapter18_files/nr-1130.min.js.download"></script><script type="text/javascript" async="" src="./Chapter18_files/2508.js.download"></script><script async="" src="./Chapter18_files/fbevents.js.download"></script><script type="text/javascript" async="" src="./Chapter18_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter18_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter18_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter18_files/analytics.js.download"></script><script type="text/javascript" async="" src="./Chapter18_files/ec.js.download"></script><script type="text/javascript" async="" src="./Chapter18_files/bat.js.download"></script><script type="text/javascript" async="" src="./Chapter18_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter18_files/insight.min.js.download"></script><script type="text/javascript" async="" src="./Chapter18_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter18_files/linkid.js.download"></script><script async="" src="./Chapter18_files/gtm.js.download"></script><script async="" src="./Chapter18_files/analytics.js.download"></script><script type="text/javascript">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if("function"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e("handle"),a=e(3),u=e(4),f=e("ee").get("tracer"),c=e("loader"),s=NREUM;"undefined"==typeof window.newrelic&&(newrelic=s);var p=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],d="api-",l=d+"ixn-";a(p,function(e,n){s[n]=o(d+n,!0,"api")}),s.addPageAction=o(d+"addPageAction",!0),s.setCurrentRouteName=o(d+"routeName",!0),n.exports=newrelic,s.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(e,n){var t={},r=this,o="function"==typeof n;return i(l+"tracer",[c.now(),e,t],r),function(){if(f.emit((o?"":"no-")+"fn-start",[c.now(),r,o],t),o)try{return n.apply(this,arguments)}catch(e){throw f.emit("fn-err",[arguments,this,e],t),e}finally{f.emit("fn-end",[c.now()],t)}}}};a("actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(e,n){m[n]=o(l+n)}),newrelic.noticeError=function(e,n){"string"==typeof e&&(e=new Error(e)),i("err",[e,c.now(),!1,n])}},{}],2:[function(e,n,t){function r(e,n){if(!o)return!1;if(e!==o)return!1;if(!n)return!0;if(!i)return!1;for(var t=i.split("."),r=n.split("."),a=0;a<r.length;a++)if(r[a]!==t[a])return!1;return!0}var o=null,i=null,a=/Version\/(\S+)\s+Safari/;if(navigator.userAgent){var u=navigator.userAgent,f=u.match(a);f&&u.indexOf("Chrome")===-1&&u.indexOf("Chromium")===-1&&(o="Safari",i=f[1])}n.exports={agent:o,version:i,match:r}},{}],3:[function(e,n,t){function r(e,n){var t=[],r="",i=0;for(r in e)o.call(e,r)&&(t[i]=n(r,e[r]),i+=1);return t}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],4:[function(e,n,t){function r(e,n,t){n||(n=0),"undefined"==typeof t&&(t=e?e.length:0);for(var r=-1,o=t-n||0,i=Array(o<0?0:o);++r<o;)i[r]=e[n+r];return i}n.exports=r},{}],5:[function(e,n,t){n.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(e,n,t){function r(){}function o(e){function n(e){return e&&e instanceof r?e:e?f(e,u,i):i()}function t(t,r,o,i){if(!d.aborted||i){e&&e(t,r,o);for(var a=n(o),u=v(t),f=u.length,c=0;c<f;c++)u[c].apply(a,r);var p=s[y[t]];return p&&p.push([b,t,r,a]),a}}function l(e,n){h[e]=v(e).concat(n)}function m(e,n){var t=h[e];if(t)for(var r=0;r<t.length;r++)t[r]===n&&t.splice(r,1)}function v(e){return h[e]||[]}function g(e){return p[e]=p[e]||o(t)}function w(e,n){c(e,function(e,t){n=n||"feature",y[t]=n,n in s||(s[n]=[])})}var h={},y={},b={on:l,addEventListener:l,removeEventListener:m,emit:t,get:g,listeners:v,context:n,buffer:w,abort:a,aborted:!1};return b}function i(){return new r}function a(){(s.api||s.feature)&&(d.aborted=!0,s=d.backlog={})}var u="nr@context",f=e("gos"),c=e(3),s={},p={},d=n.exports=o();d.backlog=s},{}],gos:[function(e,n,t){function r(e,n,t){if(o.call(e,n))return e[n];var r=t();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,n,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return e[n]=r,r}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],handle:[function(e,n,t){function r(e,n,t,r){o.buffer([e],r),o.emit(e,n,t)}var o=e("ee").get("handle");n.exports=r,r.ee=o},{}],id:[function(e,n,t){function r(e){var n=typeof e;return!e||"object"!==n&&"function"!==n?-1:e===window?0:a(e,i,function(){return o++})}var o=1,i="nr@id",a=e("gos");n.exports=r},{}],loader:[function(e,n,t){function r(){if(!E++){var e=x.info=NREUM.info,n=l.getElementsByTagName("script")[0];if(setTimeout(s.abort,3e4),!(e&&e.licenseKey&&e.applicationID&&n))return s.abort();c(y,function(n,t){e[n]||(e[n]=t)}),f("mark",["onload",a()+x.offset],null,"api");var t=l.createElement("script");t.src="https://"+e.agent,n.parentNode.insertBefore(t,n)}}function o(){"complete"===l.readyState&&i()}function i(){f("mark",["domContent",a()+x.offset],null,"api")}function a(){return O.exists&&performance.now?Math.round(performance.now()):(u=Math.max((new Date).getTime(),u))-x.offset}var u=(new Date).getTime(),f=e("handle"),c=e(3),s=e("ee"),p=e(2),d=window,l=d.document,m="addEventListener",v="attachEvent",g=d.XMLHttpRequest,w=g&&g.prototype;NREUM.o={ST:setTimeout,SI:d.setImmediate,CT:clearTimeout,XHR:g,REQ:d.Request,EV:d.Event,PR:d.Promise,MO:d.MutationObserver};var h=""+location,y={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-1130.min.js"},b=g&&w&&w[m]&&!/CriOS/.test(navigator.userAgent),x=n.exports={offset:u,now:a,origin:h,features:{},xhrWrappable:b,userAgent:p};e(1),l[m]?(l[m]("DOMContentLoaded",i,!1),d[m]("load",r,!1)):(l[v]("onreadystatechange",o),d[v]("onload",r)),f("mark",["firstbyte",u],null,"api");var E=0,O=e(5)},{}]},{},["loader"]);</script><link rel="apple-touch-icon" href="https://learning.oreilly.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://learning.oreilly.com/favicon.ico" type="image/x-icon"><link href="./Chapter18_files/css" rel="stylesheet" type="text/css"><title>18. Reinforcement Learning - Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition</title><link rel="stylesheet" href="./Chapter18_files/output.68851547a55f.css" type="text/css"><link rel="stylesheet" type="text/css" href="./Chapter18_files/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="./Chapter18_files/font-awesome.min.css"><style type="text/css" title="ibis-book">@charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000 !important;padding-top:10px !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:2em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content p[data-type="footnote"]{font-size:90% !important;line-height:1.2em !important;margin-left:2.5em !important;text-indent:-2.3em !important}#sbo-rt-content p[data-type="footnote"] sup{display:inline-block !important;position:static !important;width:2em !important;text-align:right !important;font-size:100% !important;padding-right:.5em !important}#sbo-rt-content p[data-type="footnote"] a[href$="-marker"]{font-family:sans-serif !important;font-size:90% !important;color:#8e0012 !important}#sbo-rt-content p[data-type="footnote"] a[data-type="xref"]{margin:0 !important;padding:0 !important;text-indent:0 !important}#sbo-rt-content a[data-type="noteref"]{font-family:sans-serif !important;color:#8e0012;margin-left:0;padding-left:0}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dd{margin-left:1.5em !important;margin-bottom:.25em}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul>li,#sbo-rt-content ol ul,#sbo-rt-content ol ul>li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul>li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul>li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul>li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol>li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol>li,#sbo-rt-content ul ol,#sbo-rt-content ul ol>li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol>li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol>li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:middle;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content table.border tbody>tr:last-child>td{border-bottom:transparent}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:1em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content .width-10{width:10vw !important}#sbo-rt-content .width-20{width:20vw !important}#sbo-rt-content .width-30{width:30vw !important}#sbo-rt-content .width-40{width:40vw !important}#sbo-rt-content .width-50{width:50vw !important}#sbo-rt-content .width-60{width:60vw !important}#sbo-rt-content .width-70{width:70vw !important}#sbo-rt-content .width-80{width:80vw !important}#sbo-rt-content .width-90{width:90vw !important}#sbo-rt-content .width-full,#sbo-rt-content .width-100{width:100vw !important}#sbo-rt-content div[data-type="equation"].fifty-percent img{width:50%}</style><script> // <![CDATA[
    var g = {
      position_cache: {
        
          "chapter": "/api/v1/book/9781492032632/chapter/part01.html",
          "book_id": "9781492032632",
          "chapter_uri": "part01.html",
          "position": 100.0,
          "user_uuid": "d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79",
          "next_chapter_uri": "/library/view/hands-on-machine-learning/9781492032632/ch01.html"
        
      },
      title: "Hands\u002Don Machine Learning with Scikit\u002DLearn, Keras, and TensorFlow, 2nd Edition",
      author_list: "Aurélien Géron",
      format: "book",
      source: "application/epub+zip",
      is_system_book: true,
      is_public: false,
      loaded_from_server: true,
      allow_scripts: false,
      has_mathml: true
    };
    // ]]></script><script src="./Chapter18_files/modernizr.8e35451ddb64.js.download"></script><script>
    
      

      
        
          window.PUBLIC_ANNOTATIONS = true;
        
      

      window.MOBILE_PUBLIC_ANNOTATIONS = false;

    

    
      window.PRIVACY_CONTROL_OVERRIDE = false;
    

      window.PRIVACY_CONTROL_SWITCH = true;

      window.PUBLISHER_PAGES = true;

      window.SBO = {
        "constants": {
          "SITB_ENDPOINT": "/api/v2/sitb/",
          "SEARCH_SELECT_ENDPOINT": "https://learning.oreilly.com/api/v2/search/select/",
          "ENABLE_ONLINE_TRAINING": false
        }
      };
  </script><link rel="canonical" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta name="description" content=" Part I. The Fundamentals of Machine Learning "><meta property="og:title" content="I. The Fundamentals of Machine Learning"><meta itemprop="isPartOf" content="/library/view/hands-on-machine-learning/9781492032632/"><meta itemprop="name" content="I. The Fundamentals of Machine Learning"><meta property="og:url" itemprop="url" content="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://learning.oreilly.com/library/cover/9781492032632/"><meta property="og:description" itemprop="description" content=" Part I. The Fundamentals of Machine Learning "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O&#39;Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781492032649"><meta property="og:book:author" itemprop="author" content="Aurélien Géron"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: &lt;%= font_size %&gt; !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: &lt;%= font_family %&gt; !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: &lt;%= column_width %&gt;% !important; margin: 0 auto !important; }"></style><noscript><meta http-equiv="refresh" content="0; url=/library/no-js/" /></noscript><script type="text/javascript">
    (function(i,s,o,g,r,a,m) {
      i['GoogleAnalyticsObject']=r;
      i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();
      a=s.createElement(o),m=s.getElementsByTagName(o)[0];
      a.async=1;
      a.src=g;
      m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    var matches = document.cookie.match(/BrowserCookie\s*=\s*([a-f0-9\-]{36})/),
      user_uuid = null;

    if (matches && matches.length === 2) {
      user_uuid = matches[1];
    }

  
    ga('create', 'UA-39299553-7', {'userId': 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79' });
  

  
    
      ga('set', 'dimension1', 'B2B');
    
  

  ga('set', 'dimension6', user_uuid);

  
    ga('set', 'dimension2', 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79');
    
      ga('set', 'dimension7', '0012M0000229keZQAQ');
    
  

  

  

  //enable enhanced link tracking
  ga('require', 'linkid', 'linkid.js');

  // reading interface will track pageviews itself
  if (document.location.pathname.indexOf("/library/view") !== 0) {
    ga('send', 'pageview');
  }
  </script><script>
    var dataLayer = window.dataLayer || [];

    
      window.medalliaVsgUserIdentifier = 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79';
      dataLayer.push({userIdentifier: 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79'});
      dataLayer.push({loggedIn: 'yes'});

      
        window.medalliaVsgAccountIdentifier = '21bed0a7-6b7b-470c-8fa0-40a52db0b491';
        
        dataLayer.push({orgID: '21bed0a7-6b7b-470c-8fa0-40a52db0b491'});
        

        window.medalliaVsgIsIndividual = false;
        
          
          dataLayer.push({learningAccountType: 'enterprise'});
          
        

        
          dataLayer.push({learningPaidAccount: 'yes'});
        
      
    

    (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-5P4V6Z');
    (function () {
      var VERSION = 'V1.1';
      var AUTHOR = 'Awwad';
      if (!window.GtmHelper)
        window.GtmHelper = function () {
          var instance = this;
          var loc = document.location;
          this.version = VERSION;
          this.author = AUTHOR;
          this.readCookie = function (name) {
            var nameEQ = name + "=";
            var ca = document.cookie.split(';');
            for (var i = 0; i < ca.length; i++) {
              var c = ca[i];
              while (c.charAt(0) == ' ') c = c.substring(1, c.length);
              if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length, c.length);
            }
            return null;
          };
          this.createCookie = function (name, value, days, cookieDomain) {
            var domain = "";
            var expires = "";

            if (days) {
              var date = new Date();
              date.setTime(date.getTime() + Math.ceil(days * 24 * 60 * 60 * 1000));
              var expires = " expires=" + date.toGMTString() + ";";
            }

            if (typeof (cookieDomain) != 'undefined')
              domain = " domain=" + cookieDomain + "; ";

            document.cookie = name + "=" + value + ";" + expires + domain + "path=/";
          };

          this.isDuplicated = function (currentTransactionId) {
            // the previous transaction id:
            var previousTransIdValue = this.readCookie("previousTransId");

            if (currentTransactionId === previousTransIdValue) {
              return true; // Duplication
            } else {
              return false;
            }
          };
        }
    })()
  </script><script defer="" src="./Chapter18_files/vendor.a48a756c5182.js.download"></script><script defer="" src="./Chapter18_files/reader.f2a0c6bd2fee.js.download"></script><script src="./Chapter18_files/f(1).txt"></script><script src="./Chapter18_files/f(2).txt"></script><script src="./Chapter18_files/f(3).txt"></script><script src="./Chapter18_files/f(4).txt"></script><script async="" src="./Chapter18_files/MathJax.js.download"></script><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 2147483020;
}
.annotator-filter {
  z-index: 2147483010;
}</style><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
</style><script async="true" type="text/javascript" src="./Chapter18_files/roundtrip.js.download"></script><style type="text/css" id="kampyleStyle">.noOutline{outline: none !important;}.wcagOutline:focus{outline: 1px dashed #595959 !important;outline-offset: 2px !important;transition: none !important;}</style><script async="true" type="text/javascript" src="./Chapter18_files/roundtrip.js.download"></script><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Math-bold-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Script; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_AMS; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf?V=2.7.1') format('opentype')}
</style></head>


<body class="reading sidenav  scalefonts library nav-collapsed"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

    
  <noscript> 
    <iframe src="//www.googletagmanager.com/ns.html?id=GTM-5P4V6Z"
            height="0" width="0"
            style="display:none;visibility:hidden">
    </iframe>
  </noscript>



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#container" class="skip">Skip to content</a><header class="topbar t-topbar"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://learning.oreilly.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M11.738 14H9.254v-3.676a.617.617 0 0 0-.621-.613H7.39a.617.617 0 0 0-.62.613V14H4.284a.617.617 0 0 1-.622-.613V10.22c0-.327.132-.64.367-.87l3.547-3.493a.627.627 0 0 1 .875 0l3.54 3.499c.234.229.366.54.367.864v3.167a.617.617 0 0 1-.62.613zM7.57 2.181a.625.625 0 0 1 .882 0l5.77 5.692-.93.92-5.28-5.209-5.28 5.208-.932-.919 5.77-5.692z"></path></svg><span>Safari Home</span></a></li><li><a href="https://learning.oreilly.com/resource-centers/" class="t-resource-centers-nav l0 nav-icn"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="16px" height="16px" viewBox="0 0 16 16" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="Topic-Page-Design" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="Heron-Button" transform="translate(-20.000000, -78.000000)" fill="#4A3A30"><g id="Group-9" transform="translate(20.000000, 78.000000)"><rect id="Rectangle" x="9.6" y="0" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="9.6" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="0" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect></g></g></g></svg><span>Resource Centers</span></a></li><li><a href="https://learning.oreilly.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://learning.oreilly.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://learning.oreilly.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://learning.oreilly.com/learning-paths/" class="l1 nav-icn t-learningpaths-nav js-toggle-menu-item"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="32px" height="32px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 52.5 (67469) - http://www.bohemiancoding.com/sketch --><title>Mask</title><desc>Created with Sketch.</desc><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><path d="M0,16.0214227 C0,15.0387209 0.796453294,14.2411658 1.77779753,14.2411658 C2.75914177,14.2411658 3.55559506,15.0387209 3.55559506,16.0214227 C3.55559506,17.0041246 2.75914177,17.8016797 1.77779753,17.8016797 C0.796453294,17.8016797 0,17.0041246 0,16.0214227 Z M9.77788642,5.22914885 C8.9280992,5.72049977 7.84008711,5.42853763 7.34941499,4.57757479 C6.85874287,3.72661195 7.15030167,2.63709467 8.00008889,2.14574375 C8.84987611,1.65439282 9.9378882,1.94635496 10.4285603,2.7973178 C10.9192324,3.64828064 10.6276736,4.73779792 9.77788642,5.22914885 Z M4.57213969,7.35869225 C5.42192691,7.85004318 5.71348571,8.93956046 5.22281359,9.79052329 C4.73214147,10.6414861 3.64412938,10.9334483 2.79434216,10.4420974 C1.94455494,9.95074642 1.65299614,8.86122915 2.14366826,8.01026631 C2.63434038,7.15930347 3.72235247,6.86734132 4.57213969,7.35869225 Z M2.79434216,21.6007481 C3.64412938,21.1093972 4.73214147,21.4013594 5.22281359,22.2523222 C5.71348571,23.103285 5.42192691,24.1928023 4.57213969,24.6841532 C3.72235247,25.1755042 2.63434038,24.883542 2.14366826,24.0325792 C1.65299614,23.1816163 1.94455494,22.0920991 2.79434216,21.6007481 Z M7.34941499,27.4652707 C7.84008711,26.6143079 8.9280992,26.3223457 9.77788642,26.8136966 C10.6276736,27.3050476 10.9192324,28.3945649 10.4285603,29.2455277 C9.9378882,30.0964905 8.84987611,30.3884527 8.00008889,29.8971017 C7.15030167,29.4057508 6.85874287,28.3162335 7.34941499,27.4652707 Z M18.7118524,11.3165596 C21.3074367,12.8173162 22.1963355,16.1392758 20.6976522,18.738451 C19.1989689,21.3358459 15.8815987,22.2259744 13.2860143,20.726998 C10.6922077,19.2262414 9.80330893,15.9042818 11.3002144,13.3051066 C12.7988978,10.7059314 16.116268,9.81580294 18.7118524,11.3165596 Z M26.7821642,27.8093944 L30.1315348,31.1633985 C30.3982044,31.4304371 30.2097579,31.8844026 29.8346426,31.8844026 L21.5945511,31.8844026 C21.1287681,31.8844026 20.751875,31.5069881 20.751875,31.0405608 L20.751875,22.7890697 C20.751875,22.4134355 21.2052134,22.2247282 21.4701052,22.4899865 L24.2843587,25.3081333 C26.8337204,23.0240636 28.4444049,19.7092251 28.4444049,16.0223129 C28.4444049,9.15052091 22.8621207,3.56051397 15.9998222,3.56051397 L15.9998222,0 C24.8230314,0 32,7.18689745 32,16.0223129 C32,20.6919269 29.9750886,24.8790914 26.7821642,27.8093944 Z" id="Mask" fill="#8B889A"></path></g></svg><span>Learning Paths</span></a></li><li class="nav-highlights"><a href="https://learning.oreilly.com/u/d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" fill="#4A3C31"><path d="M13.325 18.071H8.036c0-6.736 4.324-10.925 14.464-12.477V0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35c5.142 0 9.175-3.515 9.175-8.816 0-4.628-2.367-7.293-6.253-8.113zm27.5 0h-5.26c0-6.736 4.295-10.925 14.435-12.477V0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35c5.113 0 9.146-3.515 9.146-8.816 0-4.628-2.338-7.293-6.253-8.113z" fill-rule="evenodd"></path></svg><span>Highlights</span></a></li><li><a href="https://learning.oreilly.com/u/preferences/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l1 no-icon">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://learning.oreilly.com/u/preferences/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l2">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9781492032632/chapter/part01.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"><div data-reactroot="" class="menu-dropdown-wrapper js-menu-dropdown-wrapper align-right"><img class="hidden" src="./Chapter18_files/ajax-transp.gif" alt="loading spinner"><div class="menu-control"><div class="control "><div class="js-playlists-menu"><button class="js-playlist-icon"><svg class="icon-add-to-playlist-sml" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill-rule="nonzero" fill="#000000"><g transform="translate(-1.000000, 0.000000)"><rect x="5" y="0" width="12" height="2"></rect><title>Playlists</title><path d="M4.5,14 C6.43299662,14 8,12.4329966 8,10.5 C8,8.56700338 6.43299662,7 4.5,7 C2.56700338,7 1,8.56700338 1,10.5 C1,12.4329966 2.56700338,14 4.5,14 Z M2.5,10 L4,10 L4,8.5 L5,8.5 L5,10 L6.5,10 L6.5,11 L5,11 L5,12.5 L4,12.5 L4,11 L2.5,11 L2.5,10 Z"></path><circle cx="2" cy="5" r="1"></circle><circle cx="1.94117647" cy="1" r="1"></circle><rect x="5" y="4" width="12" height="2"></rect><rect x="9" y="8" width="8" height="2"></rect><rect x="9" y="12" width="8" height="2"></rect></g></g></g></svg><div class="js-playlist-addto-label">Add&nbsp;To</div></button></div></div></div></div></div></div></li><li class="js-font-control-panel font-control-activator"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html&amp;text=Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%20I.%20The%20Fundamentals%20of%20Machine%20Learning&amp;body=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html%0D%0Afrom%20Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    
    <section role="document">
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">17. Representation Learning and Generative Learning Using Autoencoders and GANs</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">19. Training and Deploying TensorFlow Models at Scale</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content"><div class="annotator-wrapper"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 18. Reinforcement Learning"><div class="chapter" id="rl_chapter">
<h1><span class="label">Chapter 18. </span>Reinforcement Learning</h1>


<p><em>Reinforcement Learning</em> (RL) is one of the most exciting fields of Machine Learning today, and also one of the oldest. It has been around since the 1950s, producing many interesting applications over the years,<sup><a data-type="noteref" id="idm46263487004888-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263487004888" class="totri-footnote">1</a></sup> particularly in games (e.g., <em>TD-Gammon</em>, a Backgammon-playing program) and in machine control, but seldom making the headline news. But a revolution took place in 2013, when researchers from a British startup called DeepMind <a href="https://homl.info/dqn">demonstrated a system that could learn to play just about any Atari game from scratch</a>,<sup><a data-type="noteref" id="idm46263487001944-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263487001944" class="totri-footnote">2</a></sup> eventually <a href="https://homl.info/dqn2">outperforming humans</a><sup><a data-type="noteref" id="idm46263487000568-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263487000568" class="totri-footnote">3</a></sup> in most of them, using only raw pixels as inputs and without any prior knowledge of the rules of the games.<sup><a data-type="noteref" id="idm46263486999608-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263486999608" class="totri-footnote">4</a></sup> This was the first of a series of amazing feats, culminating in March 2016 with the victory of their system AlphaGo against Lee Sedol, a legendary professional player of the game of Go, and in May 2017 against Ke Jie, the world champion. No program had ever come close to beating a master of this game, let alone the world champion. Today the whole field of RL is boiling with new ideas, with a wide range of applications. DeepMind was bought by Google for over $500 million in 2014.</p>

<p>So how did DeepMind achieve all this? With hindsight it seems rather simple: they applied the power of Deep Learning to the field of Reinforcement Learning, and it worked beyond their wildest dreams. In this chapter we will first explain what Reinforcement Learning is and what it’s good at, and then we will present two of the most important techniques in Deep Reinforcement Learning: <em>policy gradients</em> and <em>deep Q-networks</em> (DQN), including a discussion of <em>Markov decision processes</em> (MDP). We will use these techniques to train models to balance a pole on a moving cart, then we will introduce the TF-Agents library, which uses state-of-the-art algorithms that greatly simplify building powerful RL systems, and we will use the library to train an agent to play Breakout, the famous Atari game. We’ll close the chapter by taking a look at some of the latest advances in the field.</p>






<section data-type="sect1" data-pdf-bookmark="Learning to Optimize Rewards"><div class="sect1" id="idm46263486993816">
<h1>Learning to Optimize Rewards</h1>

<p>In Reinforcement Learning, a software <em>agent</em> makes <em>observations</em> and takes <em>actions</em> within an <em>environment</em>, and in return it receives <em>rewards</em>. Its objective is to learn to act in a way that will maximize its expected rewards over time. If you don’t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term “reward” is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain.</p>

<p>This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#rl_examples_diagram">Figure&nbsp;18-1</a>):</p>
<ol type="a">
<li>The agent can be the program controlling a robot. In this case, the environment is the real world, the agent observes the environment through a set of <em>sensors</em> such as cameras and touch sensors, and its actions consist of sending signals to activate motors. It may be programmed to get positive rewards whenever it approaches the target destination, and negative rewards whenever it wastes time or goes in the wrong direction.</li>
<li>The agent can be the program controlling _Ms. Pac-Man_. In this case, the environment is a simulation of the Atari game, the actions are the nine possible joystick positions (upper left, down, center, and so on), the observations are screenshots, and the rewards are just the game points.</li>
<li>Similarly, the agent can be the program playing a board game such as Go.</li>
<li>The agent does not have to control a physically (or virtually) moving thing. For example, it can be a smart thermostat, getting positive rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs.</li>
<li>The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses.</li>
</ol>

<figure><div id="rl_examples_diagram" class="figure">
<img src="./Chapter18_files/mls2_1801.png" alt="mls2 1801" width="1413" height="1153" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1801.png">
<h6><span class="label">Figure 18-1. </span>Reinforcement Learning examples: (a) robotics, (b) <em>Ms. Pac-Man</em>, (c) Go player, (d) thermostat, (e) automatic trader<sup><a data-type="noteref" id="idm46263486981432-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263486981432" class="totri-footnote">5</a></sup></h6>
</div></figure>

<p>Note that there may not be any positive rewards at all; for example, the agent may move around in a maze, getting a negative reward at every time step, so it better find the exit as quickly as possible! There are many other examples of tasks where Reinforcement Learning is well suited, such as self-driving cars, recommender systems, placing ads on a web page, or controlling where an image classification system should focus its attention.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Policy Search"><div class="sect1" id="idm46263486977912">
<h1>Policy Search</h1>

<p>The algorithm used by the software agent to determine its actions is called its <em>policy</em>. The policy could be a neural network taking observations as inputs and outputting the action to take (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#rl_with_nn_policy_diagram">Figure&nbsp;18-2</a>).</p>

<figure><div id="rl_with_nn_policy_diagram" class="figure">
<img src="./Chapter18_files/mls2_1802.png" alt="mls2 1802" width="1168" height="406" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1802.png">
<h6><span class="label">Figure 18-2. </span>Reinforcement Learning using a neural network policy</h6>
</div></figure>

<p>The policy can be any algorithm you can think of, and it does not have to be deterministic. In fact, in some cases it does not even have to observe the environment! For example, consider a robotic vacuum cleaner whose reward is the amount of dust it picks up in 30 minutes. Its policy could be to move forward with some probability <em>p</em> every second, or randomly rotate left or right with probability 1 – <em>p</em>. The rotation angle would be a random angle between –r and +r. Since this policy involves some randomness, it is called a <em>stochastic policy</em>. The robot will have an erratic trajectory, which guarantees that it will eventually get to any place it can reach and pick up all the dust. The question is, how much dust will it pick up in 30 minutes?</p>

<p>How would you train such a robot? There are just two <em>policy parameters</em> you can tweak: the probability <em>p</em> and the angle range <em>r</em>. One possible learning algorithm could be to try out many different values for these parameters, and pick the combination that performs best (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#policy_search_diagram">Figure&nbsp;18-3</a>). This is an example of <em>policy search</em>, in this case using a brute force approach. When the <em>policy space</em> is too large (which is generally the case), finding a good set of parameters this way is like searching for a needle in a gigantic haystack.</p>

<p>Another way to explore the policy space is to use <em>genetic algorithms</em>. For example, you could randomly create a first generation of 100 policies and try them out, then “kill” the 80 worst policies<sup><a data-type="noteref" id="idm46263486965656-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263486965656" class="totri-footnote">6</a></sup> and make the 20 survivors produce 4 offspring each. An offspring is a copy of its parent<sup><a data-type="noteref" id="idm46263486964712-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263486964712" class="totri-footnote">7</a></sup> plus some random variation. The surviving policies plus their offspring together constitute the second generation. You can continue to iterate through generations this way until you find a good policy.<sup><a data-type="noteref" id="idm46263486962728-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263486962728" class="totri-footnote">8</a></sup></p>

<figure><div id="policy_search_diagram" class="figure">
<img src="./Chapter18_files/mls2_1803.png" alt="mls2 1803" width="1440" height="681" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1803.png">
<h6><span class="label">Figure 18-3. </span>Four points in policy space (left) and the agent’s corresponding behavior (right)</h6>
</div></figure>

<p>Yet another approach is to use optimization techniques, by evaluating the gradients of the rewards with regard to the policy parameters, then tweaking these parameters by following the gradients toward higher rewards.<sup><a data-type="noteref" id="idm46263486826568-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263486826568" class="totri-footnote">9</a></sup> This approach is called <em>policy gradients</em> (PG), which we will discuss in more detail later in this chapter. Going back to the vacuum cleaner robot, you could slightly increase <em>p</em> and evaluate whether doing so increases the amount of dust picked up by the robot in 30 minutes; if it does, then increase <em>p</em> some more, or else reduce <em>p</em>. We will implement a popular PG algorithm using TensorFlow, but before we do, we need to create an environment for the agent to live in, so it’s time to introduce OpenAI gym.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Introduction to OpenAI gym"><div class="sect1" id="idm46263486823160">
<h1>Introduction to OpenAI gym</h1>

<p>One of the challenges of Reinforcement Learning is that in order to train an agent, you first need to have a working environment. If you want to program an agent that will learn to play an Atari game, you will need an Atari game simulator. If you want to program a walking robot, then the environment is the real world, and you can directly train your robot in that environment, but this has its limits: if the robot falls off a cliff, you can’t just click Undo. You can’t speed up time either; adding more computing power won’t make the robot move any faster. And it’s generally too expensive to train 1,000 robots in parallel. In short, training is hard and slow in the real world, so you generally need a <em>simulated environment</em> at least for bootstrap training. For example, you may use a library like <a href="https://pybullet.org/">pybullet</a> or <a href="http://www.mujoco.org/">MuJoCo</a> for 3D physics simulation.</p>

<p><a href="https://gym.openai.com/"><em>OpenAI gym</em></a><sup><a data-type="noteref" id="idm46263486817544-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263486817544">10</a></sup> is a toolkit that provides a wide variety of simulated environments (Atari games, board games, 2D and 3D physical simulations, and so on), so you can train agents, compare them, or develop new RL algorithms.</p>

<p>Let’s install OpenAI gym. If you created an isolated environment using virtualenv, you first need to activate it:</p>
<pre data-type="programlisting" data-code-language="shell-session"><code class="go">$ </code><strong><code class="go">cd $ML_PATH</code></strong><code class="go">                # Your ML working directory (e.g., $HOME/ml)
</code><code class="go">$ </code><strong><code class="go">source my_env/bin/activate</code></strong><code class="go"> # on Linux or MacOSX
</code><code class="go">$ </code><strong><code class="go">.\my_env\Scripts\activate</code></strong><code class="go">  # on Windows
</code></pre>

<p>Next, install OpenAI gym (if you are not using a virtual environment, you will need to add the <code>--user</code> option, or have administrator rights):</p>
<pre data-type="programlisting" data-code-language="shell-session"><code class="go">$ </code><strong><code class="go">python3 -m pip install --upgrade gym</code></strong><code class="go">
</code></pre>

<p>Depending on your system, you may also need to install the Mesa OpenGL Utility (GLU) library (e.g., on Ubuntu 18.04 you need to run <code>apt install libglu1-mesa</code>). This library will be needed to render the first environment. Next open up a Python shell or a Jupyter notebook and create an environment:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">gym</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">env</code> <code class="o">=</code> <code class="n">gym</code><code class="o">.</code><code class="n">make</code><code class="p">(</code><code class="s">"CartPole-v1"</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">obs</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">obs</code>
<code class="go">array([-0.01258566, -0.00156614,  0.04207708, -0.00180545])</code></pre>

<p>The <code>make()</code> function creates an environment, in this case a CartPole environment. This is a 2D simulation in which a cart can be accelerated left or right in order to balance a pole placed on top of it (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#cart_pole_diagram">Figure&nbsp;18-4</a>). You can get the list of all available environments by running <code>gym.envs.registry.all()</code>. After the environment is created, you must initialize it using the <code>reset()</code> method. This returns the first observation. Observations depend on the type of environment. For the CartPole environment, each observation is a 1D NumPy array containing four floats: these floats represent the cart’s horizontal position (<code>0.0</code> = center), its velocity (positive means right), the angle of the pole (<code>0.0</code> = vertical), and its angular velocity (positive means clockwise).</p>

<p>Now let’s display this environment by calling its <code>render()</code> method (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#cart_pole_diagram">Figure&nbsp;18-4</a>). On Windows, this requires first installing an X&nbsp;Server, such as VcXsrv or Xming:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">env</code><code class="o">.</code><code class="n">render</code><code class="p">()</code>
<code class="go">True</code></pre>

<figure class="smallerfifty"><div id="cart_pole_diagram" class="figure">
<img src="./Chapter18_files/mls2_1804.png" alt="mls2 1804" width="896" height="549" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1804.png">
<h6><span class="label">Figure 18-4. </span>The CartPole environment</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>If you are using a headless server (i.e., without a screen), such as a virtual machine on the cloud, rendering will fail. The only way to avoid this is to use a fake X server such as Xvfb or Xdummy. For example, you can install Xvfb (<code>apt install xvfb</code> on Ubuntu or Debian) and start Python using the following command: <code>xvfb-run -s "-screen 0 1400x900x24" python3</code>. Alternatively, install Xvfb and the <a href="https://homl.info/pyvd">pyvirtualdisplay library</a> (which wraps Xvfb) and run <code>pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()</code> at the beginning of your program.</p>
</div>

<p>If you want <code>render()</code> to return the rendered image as a NumPy array, you can set <code>mode="rgb_array"</code> (oddly, this environment will render the environment to screen as well):</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">img</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">render</code><code class="p">(</code><code class="n">mode</code><code class="o">=</code><code class="s">"rgb_array"</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">img</code><code class="o">.</code><code class="n">shape</code>  <code class="c"># height, width, channels (3 = Red, Green, Blue)</code>
<code class="go">(800, 1200, 3)</code></pre>

<p>Let’s ask the environment what actions are possible:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">env</code><code class="o">.</code><code class="n">action_space</code>
<code class="go">Discrete(2)</code></pre>

<p><code>Discrete(2)</code> means that the possible actions are integers 0 and 1, which represent accelerating left (0) or right (1). Other environments may have additional discrete actions, or other kinds of actions (e.g., continuous). Since the pole is leaning toward the right (<code>obs[2] &gt; 0</code>), let’s accelerate the cart toward the right:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">action</code> <code class="o">=</code> <code class="mi">1</code>  <code class="c"># accelerate right</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">obs</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">info</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">action</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">obs</code>
<code class="go">array([-0.01261699,  0.19292789,  0.04204097, -0.28092127])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">reward</code>
<code class="go">1.0</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">done</code>
<code class="go">False</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">info</code>
<code class="go">{}</code></pre>

<p>The <code>step()</code> method executes the given action and returns four values:</p>
<dl>
<dt><code>obs</code></dt>
<dd>
<p>This is the new observation. The cart is now moving toward the right (<code>obs[1] &gt; 0</code>). The pole is still tilted toward the right (<code>obs[2] &gt; 0</code>), but its angular velocity is now negative (<code>obs[3] &lt; 0</code>), so it will likely be tilted toward the left after the next step.</p>
</dd>
<dt><code>reward</code></dt>
<dd>
<p>In this environment, you get a reward of 1.0 at every step, no matter what you do, so the goal is to keep running as long as possible.</p>
</dd>
<dt><code>done</code></dt>
<dd>
<p>This value will be <code>True</code> when the <em>episode</em> is over. This will happen when the pole tilts too much, or goes off the screen, or after 200 steps (in this last case, you have won). After that, the environment must be reset before it can be used again.</p>
</dd>
<dt><code>info</code></dt>
<dd>
<p>This environment-specific dictionary can provide some extra information that you may find useful for debugging or for training. For example, in some games it may indicate how many lives the agent has.</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>Once you have finished using an environment, you should call its <code>close()</code> method to free resources.</p>
</div>

<p>Let’s hardcode a simple policy that accelerates left when the pole is leaning toward the left and accelerates right when the pole is leaning toward the right. We will run this policy to see the average rewards it gets over 500 episodes:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">basic_policy</code><code class="p">(</code><code class="n">obs</code><code class="p">):</code>
    <code class="n">angle</code> <code class="o">=</code> <code class="n">obs</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code>
    <code class="k">return</code> <code class="mi">0</code> <code class="k">if</code> <code class="n">angle</code> <code class="o">&lt;</code> <code class="mi">0</code> <code class="k">else</code> <code class="mi">1</code>

<code class="n">totals</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">episode</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">500</code><code class="p">):</code>
    <code class="n">episode_rewards</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="n">obs</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">step</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">200</code><code class="p">):</code>
        <code class="n">action</code> <code class="o">=</code> <code class="n">basic_policy</code><code class="p">(</code><code class="n">obs</code><code class="p">)</code>
        <code class="n">obs</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">info</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">action</code><code class="p">)</code>
        <code class="n">episode_rewards</code> <code class="o">+=</code> <code class="n">reward</code>
        <code class="k">if</code> <code class="n">done</code><code class="p">:</code>
            <code class="k">break</code>
    <code class="n">totals</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">episode_rewards</code><code class="p">)</code></pre>

<p>This code is hopefully self-explanatory. Let’s look at the result:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">totals</code><code class="p">),</code> <code class="n">np</code><code class="o">.</code><code class="n">std</code><code class="p">(</code><code class="n">totals</code><code class="p">),</code> <code class="n">np</code><code class="o">.</code><code class="n">min</code><code class="p">(</code><code class="n">totals</code><code class="p">),</code> <code class="n">np</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">totals</code><code class="p">)</code>
<code class="go">(41.718, 8.858356280936096, 24.0, 68.0)</code></pre>

<p>Even with 500 tries, this policy never managed to keep the pole upright for more than 68 consecutive steps. Not great. If you look at the simulation in the <a href="https://github.com/ageron/handson-ml2">Jupyter notebooks</a>, you will see that the cart oscillates left and right more and more strongly until the pole tilts too much. Let’s see if a neural network can come up with a better policy.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Neural Network Policies"><div class="sect1" id="idm46263486822248">
<h1>Neural Network Policies</h1>

<p>Let’s create a neural network policy. Just like the policy we hardcoded earlier, this neural network will take an observation as input, and it will output the action to be executed. More precisely, it will estimate a probability for each action, and then we will select an action randomly, according to the estimated probabilities (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#neural_network_policy_diagram">Figure&nbsp;18-5</a>). In the case of the CartPole environment, there are just two possible actions (left or right), so we only need one output neuron. It will output the probability <em>p</em> of action 0 (left), and of course the probability of action 1 (right) will be 1 – <em>p</em>. For example, if it outputs 0.7, then we will pick action 0 with 70% probability, or action 1 with 30% probability.</p>

<figure class="smallerseventy"><div id="neural_network_policy_diagram" class="figure">
<img src="./Chapter18_files/mls2_1805.png" alt="mls2 1805" width="1097" height="1028" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1805.png">
<h6><span class="label">Figure 18-5. </span>Neural network policy</h6>
</div></figure>

<p>You may wonder why we are picking a random action based on the probability given by the neural network, rather than just picking the action with the highest score. This approach lets the agent find the right balance between <em>exploring</em> new actions and <em>exploiting</em> the actions that are known to work well. Here’s an analogy: suppose you go to a restaurant for the first time, and all the dishes look equally appealing, so you randomly pick one. If it turns out to be good, you can increase the probability that you’ll order it next time, but you shouldn’t increase that probability up to 100%, or else you will never try out the other dishes, some of which may be even better than the one you tried.</p>

<p>Also note that in this particular environment, the past actions and observations can safely be ignored, since each observation contains the environment’s full state. If there were some hidden state, then you may need to consider past actions and observations as well. For example, if the environment only revealed the position of the cart but not its velocity, you would have to consider not only the current observation but also the previous observation in order to estimate the current velocity. Another example is when the observations are noisy; in that case, you generally want to use the past few observations to estimate the most likely current state. The CartPole problem is thus as simple as can be; the observations are noise-free, and they contain the environment’s full state.</p>

<p>Here is the code to build this neural network policy using tf.keras:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">tensorflow</code> <code class="kn">as</code> <code class="nn">tf</code>
<code class="kn">from</code> <code class="nn">tensorflow</code> <code class="kn">import</code> <code class="n">keras</code>

<code class="n">n_inputs</code> <code class="o">=</code> <code class="mi">4</code> <code class="c1"># == env.observation_space.shape[0]</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"elu"</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="n">n_inputs</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">),</code>
<code class="p">])</code></pre>

<p>After the imports, we use a simple <code>Sequential</code> model to define the policy network. The number of inputs is the size of the observation space (which in the case of the CartPole is 4), and we just have 5 hidden units because it’s a simple problem. Finally, we want to output a single probability (the probability of going left), so we have a single output neuron using the sigmoid activation function. If there were more than two possible actions, there would be one output neuron per action, and you would use the softmax activation function instead.</p>

<p>OK, we now have a neural network policy that will take observations and output action probabilities. But how do we train it?</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Evaluating Actions: The Credit Assignment Problem"><div class="sect1" id="idm46263486289064">
<h1>Evaluating Actions: The Credit Assignment Problem</h1>

<p>If we knew what the best action was at each step, we could train the neural network as usual, by minimizing the cross entropy between the estimated probability distribution and the target probability distribution. It would just be regular supervised learning. However, in Reinforcement Learning the only guidance the agent gets is through rewards, and rewards are typically sparse and delayed. For example, if the agent manages to balance the pole for 100 steps, how can it know which of the 100 actions it took were good, and which of them were bad? All it knows is that the pole fell after the last action, but surely this last action is not entirely responsible. This is called the <em>credit assignment problem</em>: when the agent gets a reward, it is hard for it to know which actions should get credited (or blamed) for it. Think of a dog that gets rewarded hours after it behaved well; will it understand what it is rewarded for?</p>

<p>To tackle this problem, a common strategy is to evaluate an action based on the sum of all the rewards that come after it, usually applying a <em>discount factor</em> <em>γ</em> (gamma) at each step. This sum of discounted rewards is called the action’s <em>return</em>. For example (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#discounted_rewards_diagram">Figure&nbsp;18-6</a>), if an agent decides to go right three times in a row and gets +10 reward after the first step, 0 after the second step, and finally –50 after the third step, then assuming we use a discount factor <em>γ</em> = 0.8, the first action will have a return of 10 + <em>γ</em> × 0 + <em>γ</em><sup>2</sup> × (–50) = –22. If the discount factor is close to 0, then future rewards won’t count for much compared to immediate rewards. Conversely, if the discount factor is close to 1, then rewards far into the future will count almost as much as immediate rewards. Typical discount factors vary from 0.9 to 0.99. With a discount factor of 0.95, rewards 13 steps into the future count roughly for half as much as immediate rewards (since 0.95<sup>13</sup> ≈ 0.5), while with a discount factor of 0.99, rewards 69 steps into the future count for half as much as immediate rewards. In the CartPole environment, actions have fairly short-term effects, so choosing a discount factor of 0.95 seems reasonable.</p>

<figure><div id="discounted_rewards_diagram" class="figure">
<img src="./Chapter18_files/mls2_1806.png" alt="mls2 1806" width="865" height="620" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1806.png">
<h6><span class="label">Figure 18-6. </span>Computing an action’s return: the sum of discounted future rewards</h6>
</div></figure>

<p>Of course, a good action may be followed by several bad actions that cause the pole to fall quickly, resulting in the good action getting a low return (similarly, a good actor may sometimes star in a terrible movie). However, if we play the game enough times, on average good actions will get a higher return than bad ones. We want to estimate how much better or worse an action is, compared to the other possible actions, on average. This is called the <em>action advantage</em>. For this, we must run many episodes and normalize all the action returns (by subtracting the mean and dividing by the standard deviation). After that, we can reasonably assume that actions with a negative advantage were bad while actions with a positive advantage were good. Perfect—now that we have a way to evaluate each action, we are ready to train our first agent using policy gradients. Let’s see how.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Policy Gradients"><div class="sect1" id="idm46263486276744">
<h1>Policy Gradients</h1>

<p>As discussed earlier, PG algorithms optimize the parameters of a policy by following the gradients toward higher rewards. One popular class of PG algorithms, called <em>REINFORCE algorithms</em>, was <a href="https://homl.info/132">introduced back in 1992</a><sup><a data-type="noteref" id="idm46263486273512-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263486273512">11</a></sup> by Ronald Williams. Here is one common variant:</p>
<ol>
<li>
<p>First, let the neural network policy play the game several times, and at each step, compute the gradients that would make the chosen action even more likely, but don’t apply these gradients yet.</p>
</li>
<li>
<p>Once you have run several episodes, compute each action’s advantage (using the method described in the previous section).</p>
</li>
<li>
<p>If an action’s advantage is positive, it means that the action was probably good, and you want to apply the gradients computed earlier to make the action even more likely to be chosen in the future. However, if the action’s advantage is negative, it means the action was probably bad, and you want to apply the opposite gradients to make this action slightly <em>less</em> likely in the future. The solution is simply to multiply each gradient vector by the corresponding action’s advantage.</p>
</li>
<li>
<p>Finally, compute the mean of all the resulting gradient vectors, and use it to perform a Gradient Descent step.</p>
</li>

</ol>

<p>Let’s use tf.keras to implement this algorithm. We will train the neural network policy we built earlier so that it learns to balance the pole on the cart. First, we need a function that will play one step. We will pretend for now that whatever action it takes is the right one so that we can compute the loss and its gradients (these gradients will just be saved for a while, and we will modify them later depending on how good or bad the action turned out to be):</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">play_one_step</code><code class="p">(</code><code class="n">env</code><code class="p">,</code> <code class="n">obs</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">loss_fn</code><code class="p">):</code>
    <code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">GradientTape</code><code class="p">()</code> <code class="k">as</code> <code class="n">tape</code><code class="p">:</code>
        <code class="n">left_proba</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">obs</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">])</code>
        <code class="n">action</code> <code class="o">=</code> <code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">])</code> <code class="o">&gt;</code> <code class="n">left_proba</code><code class="p">)</code>
        <code class="n">y_target</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">constant</code><code class="p">([[</code><code class="mf">1.</code><code class="p">]])</code> <code class="o">-</code> <code class="n">tf</code><code class="o">.</code><code class="n">cast</code><code class="p">(</code><code class="n">action</code><code class="p">,</code> <code class="n">tf</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_mean</code><code class="p">(</code><code class="n">loss_fn</code><code class="p">(</code><code class="n">y_target</code><code class="p">,</code> <code class="n">left_proba</code><code class="p">))</code>
    <code class="n">grads</code> <code class="o">=</code> <code class="n">tape</code><code class="o">.</code><code class="n">gradient</code><code class="p">(</code><code class="n">loss</code><code class="p">,</code> <code class="n">model</code><code class="o">.</code><code class="n">trainable_variables</code><code class="p">)</code>
    <code class="n">obs</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">info</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="nb">int</code><code class="p">(</code><code class="n">action</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">numpy</code><code class="p">()))</code>
    <code class="k">return</code> <code class="n">obs</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">grads</code></pre>

<p>Let’s walk though this function:</p>

<ul>
<li>
<p>Within the <code>GradientTape</code> block (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch12.html#tensorflow_chapter">Chapter&nbsp;12</a>), we start by calling the model, giving it a single observation (we reshape the observation so it becomes a batch containing a single instance, as the model expects a batch). This outputs the probability of going left.</p>
</li>
<li>
<p>Next, we sample a random float between 0 and 1, and we check whether it is greater than <code>left_proba</code>. So the <code>action</code> will be <code>False</code> with probability <code>left_proba</code>, or <code>True</code> with probability <code>1 - left_proba</code>. Once we cast this boolean to a number, the action will be 0 (left) or 1 (right) with the appropriate probabilities.</p>
</li>
<li>
<p>Next, we define the target probability of going left: it is 1 minus the action (cast to a float). Indeed, if the action is 0 (left), then the target probability of going left will be 1. If the action is 1 (right), then the target probability will be 0.</p>
</li>
<li>
<p>Then we compute the loss using the given loss function, and we use the <code>tape</code> to compute the gradient of the loss with regard to the model’s trainable variables. Again, these gradients will be tweaked later, before we apply them, depending on how good or bad the action turned out to be.</p>
</li>
<li>
<p>Finally, we play the selected action, and we return the new observation, the reward, whether the episode is ended or not, and of course the gradients that we just computed.</p>
</li>
</ul>

<p>Now let’s create another function that will rely on the <code>play_one_step()</code> function to play multiple episodes, returning all the rewards and gradients, for each episode and each step:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">play_multiple_episodes</code><code class="p">(</code><code class="n">env</code><code class="p">,</code> <code class="n">n_episodes</code><code class="p">,</code> <code class="n">n_max_steps</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">loss_fn</code><code class="p">):</code>
    <code class="n">all_rewards</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="n">all_grads</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="k">for</code> <code class="n">episode</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_episodes</code><code class="p">):</code>
        <code class="n">current_rewards</code> <code class="o">=</code> <code class="p">[]</code>
        <code class="n">current_grads</code> <code class="o">=</code> <code class="p">[]</code>
        <code class="n">obs</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">()</code>
        <code class="k">for</code> <code class="n">step</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_max_steps</code><code class="p">):</code>
            <code class="n">obs</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">grads</code> <code class="o">=</code> <code class="n">play_one_step</code><code class="p">(</code><code class="n">env</code><code class="p">,</code> <code class="n">obs</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">loss_fn</code><code class="p">)</code>
            <code class="n">current_rewards</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">reward</code><code class="p">)</code>
            <code class="n">current_grads</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">grads</code><code class="p">)</code>
            <code class="k">if</code> <code class="n">done</code><code class="p">:</code>
                <code class="k">break</code>
        <code class="n">all_rewards</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">current_rewards</code><code class="p">)</code>
        <code class="n">all_grads</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">current_grads</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">all_rewards</code><code class="p">,</code> <code class="n">all_grads</code></pre>

<p>This code returns a list of reward lists (one reward list per episode, containing one reward per step), as well as a list of gradient lists (one gradient list per episode, each containing one tuple of gradients per step, and each tuple contains one gradient tensor per trainable variable).</p>

<p>The algorithm will use the <code>play_multiple_episodes()</code> function to play the game several times (e.g., 10 times), then it will go back and look at all the rewards, discount them, and normalize them. To do that, we need a couple more functions: the first will compute the sum of future discounted rewards at each step; the second will normalize all these discounted rewards (returns) across many episodes by subtracting the mean and dividing by the standard deviation:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">discount_rewards</code><code class="p">(</code><code class="n">rewards</code><code class="p">,</code> <code class="n">discount_factor</code><code class="p">):</code>
    <code class="n">discounted</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">rewards</code><code class="p">)</code>
    <code class="k">for</code> <code class="n">step</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">rewards</code><code class="p">)</code> <code class="o">-</code> <code class="mi">2</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">):</code>
        <code class="n">discounted</code><code class="p">[</code><code class="n">step</code><code class="p">]</code> <code class="o">+=</code> <code class="n">discounted</code><code class="p">[</code><code class="n">step</code> <code class="o">+</code> <code class="mi">1</code><code class="p">]</code> <code class="o">*</code> <code class="n">discount_factor</code>
    <code class="k">return</code> <code class="n">discounted</code>

<code class="k">def</code> <code class="nf">discount_and_normalize_rewards</code><code class="p">(</code><code class="n">all_rewards</code><code class="p">,</code> <code class="n">discount_factor</code><code class="p">):</code>
    <code class="n">all_discounted_rewards</code> <code class="o">=</code> <code class="p">[</code><code class="n">discount_rewards</code><code class="p">(</code><code class="n">rewards</code><code class="p">,</code> <code class="n">discount_factor</code><code class="p">)</code>
                              <code class="k">for</code> <code class="n">rewards</code> <code class="ow">in</code> <code class="n">all_rewards</code><code class="p">]</code>
    <code class="n">flat_rewards</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">concatenate</code><code class="p">(</code><code class="n">all_discounted_rewards</code><code class="p">)</code>
    <code class="n">reward_mean</code> <code class="o">=</code> <code class="n">flat_rewards</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
    <code class="n">reward_std</code> <code class="o">=</code> <code class="n">flat_rewards</code><code class="o">.</code><code class="n">std</code><code class="p">()</code>
    <code class="k">return</code> <code class="p">[(</code><code class="n">discounted_rewards</code> <code class="o">-</code> <code class="n">reward_mean</code><code class="p">)</code> <code class="o">/</code> <code class="n">reward_std</code>
            <code class="k">for</code> <code class="n">discounted_rewards</code> <code class="ow">in</code> <code class="n">all_discounted_rewards</code><code class="p">]</code></pre>

<p>Let’s check that this works:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">discount_rewards</code><code class="p">([</code><code class="mi">10</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="o">-</code><code class="mi">50</code><code class="p">],</code> <code class="n">discount_factor</code><code class="o">=</code><code class="mf">0.8</code><code class="p">)</code>
<code class="go">array([-22, -40, -50])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">discount_and_normalize_rewards</code><code class="p">([[</code><code class="mi">10</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="o">-</code><code class="mi">50</code><code class="p">],</code> <code class="p">[</code><code class="mi">10</code><code class="p">,</code> <code class="mi">20</code><code class="p">]],</code>
<code class="gp">... </code>                               <code class="n">discount_factor</code><code class="o">=</code><code class="mf">0.8</code><code class="p">)</code>
<code class="gp">...</code>
<code class="go">[array([-0.28435071, -0.86597718, -1.18910299]),</code>
<code class="go"> array([1.26665318, 1.0727777 ])]</code></pre>

<p>The call to <code>discount_rewards()</code> returns exactly what we expect (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#discounted_rewards_diagram">Figure&nbsp;18-6</a>). You can verify that the function <code>discount_and_normalize_rewards()</code> does indeed return the normalized action advantages for each action in both episodes. Notice that the first episode was much worse than the second, so its normalized advantages are all negative; all actions from the first episode would be considered bad, and conversely all actions from the second episode would be considered good.</p>

<p>We are almost ready to run the algorithm! Now let’s define the hyperparameters. We will run 150 training iterations, playing 10 episodes per iteration, and each episode will last at most 200 steps. We will use a discount factor of 0.95:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">n_iterations</code> <code class="o">=</code> <code class="mi">150</code>
<code class="n">n_episodes_per_update</code> <code class="o">=</code> <code class="mi">10</code>
<code class="n">n_max_steps</code> <code class="o">=</code> <code class="mi">200</code>
<code class="n">discount_factor</code> <code class="o">=</code> <code class="mf">0.95</code></pre>

<p>We also need an optimizer and the loss function. A regular Adam optimizer with learning rate 0.01 will do just fine, and we will use the binary cross-entropy loss function because we are training a binary classifier (there are two possible actions: left or right):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">0.01</code><code class="p">)</code>
<code class="n">loss_fn</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">losses</code><code class="o">.</code><code class="n">binary_crossentropy</code></pre>

<p>All right, we are now ready to build and run the training loop!</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">iteration</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_iterations</code><code class="p">):</code>
    <code class="n">all_rewards</code><code class="p">,</code> <code class="n">all_grads</code> <code class="o">=</code> <code class="n">play_multiple_episodes</code><code class="p">(</code>
        <code class="n">env</code><code class="p">,</code> <code class="n">n_episodes_per_update</code><code class="p">,</code> <code class="n">n_max_steps</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">loss_fn</code><code class="p">)</code>
    <code class="n">all_final_rewards</code> <code class="o">=</code> <code class="n">discount_and_normalize_rewards</code><code class="p">(</code><code class="n">all_rewards</code><code class="p">,</code>
                                                       <code class="n">discount_factor</code><code class="p">)</code>
    <code class="n">all_mean_grads</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="k">for</code> <code class="n">var_index</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">trainable_variables</code><code class="p">)):</code>
        <code class="n">mean_grads</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_mean</code><code class="p">(</code>
            <code class="p">[</code><code class="n">final_reward</code> <code class="o">*</code> <code class="n">all_grads</code><code class="p">[</code><code class="n">episode_index</code><code class="p">][</code><code class="n">step</code><code class="p">][</code><code class="n">var_index</code><code class="p">]</code>
             <code class="k">for</code> <code class="n">episode_index</code><code class="p">,</code> <code class="n">final_rewards</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">all_final_rewards</code><code class="p">)</code>
                 <code class="k">for</code> <code class="n">step</code><code class="p">,</code> <code class="n">final_reward</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">final_rewards</code><code class="p">)],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
        <code class="n">all_mean_grads</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">mean_grads</code><code class="p">)</code>
    <code class="n">optimizer</code><code class="o">.</code><code class="n">apply_gradients</code><code class="p">(</code><code class="nb">zip</code><code class="p">(</code><code class="n">all_mean_grads</code><code class="p">,</code> <code class="n">model</code><code class="o">.</code><code class="n">trainable_variables</code><code class="p">))</code></pre>

<p>Let’s walk through this code:</p>

<ul>
<li>
<p>At each training iteration, this loop calls the <code>play_multiple_episodes()</code> function, which plays the game 10 times and returns all the rewards and gradients for every episode and step.</p>
</li>
<li>
<p>Then we call the <code>discount_and_normalize_rewards()</code> to compute each action’s normalized advantage (which in the code we call the <code>final_reward</code>). This provides a measure of how good or bad each action actually was, in hindsight.</p>
</li>
<li>
<p>Next, we go through each trainable variable, and for each of them we compute the weighted mean of the gradients for that variable over all episodes and all steps, weighted by the <code>final_reward</code>..</p>
</li>
<li>
<p>Finally, we apply these mean gradients using the optimizer: the model’s trainable variables will be tweaked, and hopefully the policy will be a bit better.</p>
</li>
</ul>

<p>And we’re done! This code will train the neural network policy, and it will successfully learn to balance the pole on the cart (you can try it out in the Jupyter notebooks). The mean reward per episode will get very close to 200 (which is the maximum by default with this environment). Success!</p>
<div data-type="tip"><h6>Tip</h6>
<p>Researchers try to find algorithms that work well even when the agent initially knows nothing about the environment. However, unless you are writing a paper, you should not hesitate to inject prior knowledge into the agent, as it will speed up training dramatically. For example, since you know that the pole should be as vertical as possible, you could add negative rewards proportional to the pole’s angle. This will make the rewards much less sparse and speed up training. Also, if you already have a reasonably good policy (e.g., hardcoded), you may want to train the neural network to imitate it before using policy gradients to improve it.</p>
</div>

<p>The simple Policy Gradients algorithm we just trained solved the CartPole task, but it would not scale well to larger and more complex tasks. Indeed, it is very <em>sample inefficient</em>, meaning it needs to explore the game for a very long time before it can make significant progress. This is due to the fact that it must run multiple episodes to estimate the advantage of each action, as we have seen. However, it is the foundation of more powerful algorithms, such as <em>Actor-Critic</em> algorithms (which we will discuss briefly at the end of this chapter).</p>

<p>We will now look at another popular family of algorithms. Whereas PG algorithms directly try to optimize the policy to increase rewards, the algorithms we will look at now are less direct: the agent learns to estimate the expected return for each state, or for each action in each state, then it uses this knowledge to decide how to act. To understand these algorithms, we must first introduce <em>Markov decision processes</em> (MDP).</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Markov Decision Processes"><div class="sect1" id="idm46263486275800">
<h1>Markov Decision Processes</h1>

<p>In the early 20th century, the mathematician Andrey Markov studied stochastic processes with no memory, called <em>Markov chains</em>. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state <em>s</em> to a state <em>s</em>′ is fixed, and it depends only on the pair (<em>s</em>,<em>s</em>′), not on past states (this is why we say that the system has no memory).</p>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#markov_chain_diagram">Figure&nbsp;18-7</a> shows an example of a Markov chain with four states. Suppose that the process starts in state <em>s</em><sub>0</sub>, and there is a 70% chance that it will remain in that state at the next step. Eventually it is bound to leave that state and never come back because no other state points back to <em>s</em><sub>0</sub>. If it goes to state <em>s</em><sub>1</sub>, it will then most likely go to state <em>s</em><sub>2</sub> (90% probability), then immediately back to state <em>s</em><sub>1</sub> (with 100% probability). It may alternate a number of times between these two states, but eventually it will fall into state <em>s</em><sub>3</sub> and remain there forever (this is a <em>terminal state</em>). Markov chains can have very different dynamics, and they are heavily used in thermodynamics, chemistry, statistics, and much more.</p>

<figure class="smallerseventy"><div id="markov_chain_diagram" class="figure">
<img src="./Chapter18_files/mls2_1807.png" alt="mls2 1807" width="895" height="570" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1807.png">
<h6><span class="label">Figure 18-7. </span>Example of a Markov chain</h6>
</div></figure>

<p>Markov decision processes were <a href="https://homl.info/133">first described in the 1950s by Richard Bellman</a>.<sup><a data-type="noteref" id="idm46263485472760-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263485472760">12</a></sup> They resemble Markov chains but with a twist: at each step, an agent can choose one of several possible actions, and the transition probabilities depend on the chosen action. Moreover, some state transitions return some reward (positive or negative), and the agent’s goal is to find a policy that will maximize rewards over time.</p>

<p>For example, the MDP represented in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#mdp_diagram">Figure&nbsp;18-8</a> has three states (represented by circles) and up to three possible discrete actions at each step (represented by diamonds). If it starts in state <em>s</em><sub>0</sub>, the agent can choose between actions <em>a</em><sub>0</sub>, <em>a</em><sub>1</sub>, or <em>a</em><sub>2</sub>. If it chooses action <em>a</em><sub>1</sub>, it just remains in state <em>s</em><sub>0</sub> with certainty, and without any reward. It can thus decide to stay there forever if it wants to. But if it chooses action <em>a</em><sub>0</sub>, it has a 70% probability of gaining a reward of +10, and remaining in state <em>s</em><sub>0</sub>. It can then try again and again to gain as much reward as possible. But at one point it is going to end up instead in state <em>s</em><sub>1</sub>. In state <em>s</em><sub>1</sub> it has only two possible actions: <em>a</em><sub>0</sub> or <em>a</em><sub>2</sub>. It can choose to stay put by repeatedly choosing action <em>a</em><sub>0</sub>, or it can choose to move on to state <em>s</em><sub>2</sub> and get a negative reward of –50 (ouch). In state <em>s</em><sub>2</sub> it has no other choice than to take action <em>a</em><sub>1</sub>, which will most likely lead it back to state <em>s</em><sub>0</sub>, gaining a reward of +40 on the way. You get the picture. By looking at this MDP, can you guess which strategy will gain the most reward over time? In state <em>s</em><sub>0</sub> it is clear that action <em>a</em><sub>0</sub> is the best option, and in state <em>s</em><sub>2</sub> the agent has no choice but to take action <em>a</em><sub>1</sub>, but in state <em>s</em><sub>1</sub> it is not obvious whether the agent should stay put (<em>a</em><sub>0</sub>) or go through the fire (<em>a</em><sub>2</sub>).</p>

<figure><div id="mdp_diagram" class="figure">
<img src="./Chapter18_files/mls2_1808.png" alt="mls2 1808" width="1280" height="659" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1808.png">
<h6><span class="label">Figure 18-8. </span>Example of a Markov decision process</h6>
</div></figure>

<p>Bellman found a way to estimate the <em>optimal state value</em> of any state <em>s</em>, noted <em>V</em>*(<em>s</em>), which is the sum of all discounted future rewards the agent can expect on average after it reaches a state <em>s</em>, assuming it acts optimally. He showed that if the agent acts optimally, then the <em>Bellman Optimality Equation</em> applies (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#bellman_optimality_equation">Equation 18-1</a>). This recursive equation says that if the agent acts optimally, then the optimal value of the current state is equal to the reward it will get on average after taking one optimal action, plus the expected optimal value of all possible next states that this action can lead to.</p>
<div data-type="equation" id="bellman_optimality_equation">
<h5><span class="label">Equation 18-1. </span>Bellman Optimality Equation</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-161-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;*&lt;/mo&gt;&lt;/msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;max&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/munder&gt;&lt;munder&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;s&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;s&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B3;&lt;/mi&gt;&lt;mo&gt;&amp;#xB7;&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;*&lt;/mo&gt;&lt;/msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;mtext&gt;for&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;all&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7022" style="width: 28.126em; display: inline-block;"><span style="display: inline-block; position: relative; width: 27.304em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.185em, 1027.26em, 2.573em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-7023"><span class="mrow" id="MathJax-Span-7024"><span class="msup" id="MathJax-Span-7025"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7026" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.208em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.877em;"><span class="mo" id="MathJax-Span-7027" style="font-size: 70.7%; font-family: MathJax_Main;">∗</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-7028" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7029" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7030" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7031" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-7032" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="munder" id="MathJax-Span-7033" style="padding-left: 0.311em; padding-right: 0.311em;"><span style="display: inline-block; position: relative; width: 2.316em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1001.86em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-7034" style="font-family: MathJax_Main;">max</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.853em;"><span class="mi" id="MathJax-Span-7035" style="font-size: 70.7%; font-family: MathJax_Math-italic;">a</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="munder" id="MathJax-Span-7036"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1000.99em, 4.424em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-7037" style="font-family: MathJax_Size1; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.699em; left: 1.082em;"><span class="mi" id="MathJax-Span-7038" style="font-size: 70.7%; font-family: MathJax_Math-italic;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-7039" style="padding-left: 0.157em;"><span class="mi" id="MathJax-Span-7040" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mrow" id="MathJax-Span-7041" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7042" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7043" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7044" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7045" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">a</span><span class="mo" id="MathJax-Span-7046" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7047" style="font-family: MathJax_Main; padding-left: 0.157em;">s’</span><span class="mo" id="MathJax-Span-7048" style="font-family: MathJax_Main;">)</span></span><span class="mrow" id="MathJax-Span-7049" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7050" style="font-family: MathJax_Main;">[</span><span class="mi" id="MathJax-Span-7051" style="font-family: MathJax_Math-italic;">R</span><span class="mrow" id="MathJax-Span-7052" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7053" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7054" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7055" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7056" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">a</span><span class="mo" id="MathJax-Span-7057" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7058" style="font-family: MathJax_Main; padding-left: 0.157em;">s’</span><span class="mo" id="MathJax-Span-7059" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-7060" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-7061" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-7062" style="font-family: MathJax_Main; padding-left: 0.208em;">⋅</span><span class="msup" id="MathJax-Span-7063" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7064" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.208em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.877em;"><span class="mo" id="MathJax-Span-7065" style="font-size: 70.7%; font-family: MathJax_Main;">∗</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-7066" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7067" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7068" style="font-family: MathJax_Main;">s’</span><span class="mo" id="MathJax-Span-7069" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-7070" style="font-family: MathJax_Main;">]</span></span></span><span class="mspace" id="MathJax-Span-7071" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-7072" style="font-family: MathJax_Main; padding-left: 0.157em;">for</span><span class="mspace" id="MathJax-Span-7073" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-7074" style="font-family: MathJax_Main;">all</span><span class="mspace" id="MathJax-Span-7075" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-7076" style="font-family: MathJax_Math-italic;">s</span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.368em; border-left: 0px solid; width: 0px; height: 1.168em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>V</mi><mo>*</mo></msup><mrow><mo>(</mo><mi>s</mi><mo>)</mo></mrow><mo>=</mo><munder><mo movablelimits="true" form="prefix">max</mo><mi>a</mi></munder><munder><mo>∑</mo><mi>s</mi></munder><mrow><mi>T</mi><mrow><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>s’</mi><mo>)</mo></mrow><mrow><mo>[</mo><mi>R</mi><mrow><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>s’</mi><mo>)</mo></mrow><mo>+</mo><mi>γ</mi><mo>·</mo><msup><mi>V</mi><mo>*</mo></msup><mrow><mo>(</mo><mi>s’</mi><mo>)</mo></mrow><mo>]</mo></mrow></mrow><mspace width="1.em"></mspace><mtext>for</mtext><mspace width="4.pt"></mspace><mtext>all</mtext><mspace width="4.pt"></mspace><mi>s</mi></mrow></math></span></span><script type="math/mml" id="MathJax-Element-161"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow>
    <msup><mi>V</mi> <mo>*</mo> </msup>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <munder><mo movablelimits="true" form="prefix">max</mo> <mi>a</mi></munder>
    <munder><mo>∑</mo><mi>s</mi></munder>
    <mrow>
      <mi>T</mi>
      <mrow>
        <mo>(</mo>
        <mi>s</mi>
        <mo>,</mo>
        <mi>a</mi>
        <mo>,</mo>
        <mi>s’</mi>
        <mo>)</mo>
      </mrow>
      <mrow>
        <mo>[</mo>
        <mi>R</mi>
        <mrow>
          <mo>(</mo>
          <mi>s</mi>
          <mo>,</mo>
          <mi>a</mi>
          <mo>,</mo>
          <mi>s’</mi>
          <mo>)</mo>
        </mrow>
        <mo>+</mo>
        <mi>γ</mi>
        <mo>·</mo>
        <msup><mi>V</mi> <mo>*</mo> </msup>
        <mrow>
          <mo>(</mo>
          <mi>s’</mi>
          <mo>)</mo>
        </mrow>
        <mo>]</mo>
      </mrow>
    </mrow>
    <mspace width="1.em"></mspace>
    <mtext>for</mtext>
    <mspace width="4.pt"></mspace>
    <mtext>all</mtext>
    <mspace width="4.pt"></mspace>
    <mi>s</mi>
  </mrow>
</math></script>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><em>T</em>(<em>s</em>, <em>a</em>, <em>s</em>′) is the transition probability from state <em>s</em> to state <em>s</em>′, given that the agent chose action <em>a</em>. For example, in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#mdp_diagram">Figure&nbsp;18-8</a>, <em>T</em>(<em>s</em><sub>2</sub>, <em>a</em><sub>1</sub>, <em>s</em><sub>0</sub>)&nbsp;=&nbsp;0.8.</p>
</li>
<li>
<p><em>R</em>(<em>s</em>, <em>a</em>, <em>s</em>′) is the reward that the agent gets when it goes from state <em>s</em> to state <em>s</em>′, given that the agent chose action <em>a</em>. For example, in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#mdp_diagram">Figure&nbsp;18-8</a>, <em>R</em>(<em>s</em><sub>2</sub>, <em>a</em><sub>1</sub>, <em>s</em><sub>0</sub>)&nbsp;=&nbsp;+40.</p>
</li>
<li>
<p><em>γ</em> is the discount factor.</p>
</li>
</ul>

<p>This equation leads directly to an algorithm that can precisely estimate the optimal state value of every possible state: you first initialize all the state value estimates to zero, and then you iteratively update them using the <em>Value Iteration</em> algorithm (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#value_iteration_equation">Equation 18-2</a>). A remarkable result is that, given enough time, these estimates are guaranteed to converge to the optimal state values, corresponding to the optimal <span class="keep-together">policy</span>.</p>
<div data-type="equation" id="value_iteration_equation">
<h5><span class="label">Equation 18-2. </span>Value Iteration algorithm</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-162-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;munder&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;max&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/munder&gt;&lt;munder&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mi&gt;s&amp;#x2019;&lt;/mi&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;s&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;s&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B3;&lt;/mi&gt;&lt;mo&gt;&amp;#xB7;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;mtext&gt;for&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;all&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7077" style="width: 28.28em; display: inline-block;"><span style="display: inline-block; position: relative; width: 27.458em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.031em, 1027.41em, 3.499em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-7078"><span class="mrow" id="MathJax-Span-7079"><span class="msub" id="MathJax-Span-7080"><span style="display: inline-block; position: relative; width: 1.956em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7081" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.208em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mrow" id="MathJax-Span-7082"><span class="mi" id="MathJax-Span-7083" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span class="mo" id="MathJax-Span-7084" style="font-size: 70.7%; font-family: MathJax_Main;">+</span><span class="mn" id="MathJax-Span-7085" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-7086" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7087" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7088" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7089" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-7090" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="munder" id="MathJax-Span-7091" style="padding-left: 0.311em; padding-right: 0.311em;"><span style="display: inline-block; position: relative; width: 1.853em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1001.86em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-7092" style="font-family: MathJax_Main;">max</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.55em, 1000.37em, 4.27em, -1000.01em); top: -3.391em; left: 0.722em;"><span class="mi" id="MathJax-Span-7093" style="font-size: 70.7%; font-family: MathJax_Math-italic;">a</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="munder" id="MathJax-Span-7094"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1001.4em, 4.63em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-7095" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.345em, 1000.42em, 4.27em, -1000.01em); top: -2.877em; left: 0.465em;"><span class="mi" id="MathJax-Span-7096" style="font-size: 70.7%; font-family: MathJax_Main;">s’</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-7097" style="padding-left: 0.157em;"><span class="mi" id="MathJax-Span-7098" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mrow" id="MathJax-Span-7099" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7100" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7101" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7102" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7103" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">a</span><span class="mo" id="MathJax-Span-7104" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7105" style="font-family: MathJax_Main; padding-left: 0.157em;">s’</span><span class="mo" id="MathJax-Span-7106" style="font-family: MathJax_Main;">)</span></span><span class="mrow" id="MathJax-Span-7107" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7108" style="font-family: MathJax_Main;">[</span><span class="mi" id="MathJax-Span-7109" style="font-family: MathJax_Math-italic;">R</span><span class="mrow" id="MathJax-Span-7110" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7111" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7112" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7113" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7114" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">a</span><span class="mo" id="MathJax-Span-7115" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7116" style="font-family: MathJax_Main; padding-left: 0.157em;">s’</span><span class="mo" id="MathJax-Span-7117" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-7118" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-7119" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-7120" style="font-family: MathJax_Main; padding-left: 0.208em;">⋅</span><span class="msub" id="MathJax-Span-7121" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7122" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.208em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mi" id="MathJax-Span-7123" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-7124" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7125" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7126" style="font-family: MathJax_Main;">s’</span><span class="mo" id="MathJax-Span-7127" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-7128" style="font-family: MathJax_Main;">]</span></span></span><span class="mspace" id="MathJax-Span-7129" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-7130" style="font-family: MathJax_Main; padding-left: 0.157em;">for</span><span class="mspace" id="MathJax-Span-7131" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-7132" style="font-family: MathJax_Main;">all</span><span class="mspace" id="MathJax-Span-7133" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-7134" style="font-family: MathJax_Math-italic;">s</span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.321em; border-left: 0px solid; width: 0px; height: 2.333em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>V</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mrow><mo>(</mo><mi>s</mi><mo>)</mo></mrow><mo>←</mo><munder><mo movablelimits="true" form="prefix">max</mo><mi>a</mi></munder><munder><mo>∑</mo><mi>s’</mi></munder><mrow><mi>T</mi><mrow><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>s’</mi><mo>)</mo></mrow><mrow><mo>[</mo><mi>R</mi><mrow><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>s’</mi><mo>)</mo></mrow><mo>+</mo><mi>γ</mi><mo>·</mo><msub><mi>V</mi><mi>k</mi></msub><mrow><mo>(</mo><mi>s’</mi><mo>)</mo></mrow><mo>]</mo></mrow></mrow><mspace width="1.em"></mspace><mtext>for</mtext><mspace width="4.pt"></mspace><mtext>all</mtext><mspace width="4.pt"></mspace><mi>s</mi></mrow></math></span></span></div><script type="math/mml" id="MathJax-Element-162"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <msub><mi>V</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow> </msub>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>)</mo>
    </mrow>
    <mo>←</mo>
    <munder><mo movablelimits="true" form="prefix">max</mo> <mi>a</mi></munder>
    <munder><mo>∑</mo> <mi>s’</mi> </munder>
    <mrow>
      <mi>T</mi>
      <mrow>
        <mo>(</mo>
        <mi>s</mi>
        <mo>,</mo>
        <mi>a</mi>
        <mo>,</mo>
        <mi>s’</mi>
        <mo>)</mo>
      </mrow>
      <mrow>
        <mo>[</mo>
        <mi>R</mi>
        <mrow>
          <mo>(</mo>
          <mi>s</mi>
          <mo>,</mo>
          <mi>a</mi>
          <mo>,</mo>
          <mi>s’</mi>
          <mo>)</mo>
        </mrow>
        <mo>+</mo>
        <mi>γ</mi>
        <mo>·</mo>
        <msub><mi>V</mi> <mi>k</mi> </msub>
        <mrow>
          <mo>(</mo>
          <mi>s’</mi>
          <mo>)</mo>
        </mrow>
        <mo>]</mo>
      </mrow>
    </mrow>
    <mspace width="1.em"></mspace>
    <mtext>for</mtext>
    <mspace width="4.pt"></mspace>
    <mtext>all</mtext>
    <mspace width="4.pt"></mspace>
    <mi>s</mi>
  </mrow>
</math></script>
</div>

<p>In this equation, <em>V</em><sub><em>k</em></sub>(<em>s</em>) is the estimated value of state <em>s</em> at the <em>k</em><sup>th</sup> iteration of the algorithm.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>This algorithm is an example of <em>Dynamic Programming</em>, which breaks down a complex problem into tractable subproblems that can be tackled iteratively.</p>
</div>

<p>Knowing the optimal state values can be useful, in particular to evaluate a policy, but it does not give us the optimal policy for the agent. Luckily, Bellman found a very similar algorithm to estimate the optimal <em>state-action values</em>, generally called <em>Q-Values</em> (Quality-Values). The optimal Q-Value of the state-action pair (<em>s</em>,<em>a</em>), noted <em>Q</em>*(<em>s</em>,<em>a</em>), is the sum of discounted future rewards the agent can expect on average after it reaches the state <em>s</em> and chooses action <em>a</em>, but before it sees the outcome of this action, assuming it acts optimally after that action.</p>

<p>Here is how it works: once again, you start by initializing all the Q-Value estimates to zero, then you update them using the <em>Q-Value Iteration</em> algorithm (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#q_value_iteration_equation">Equation 18-3</a>).</p>
<div data-type="equation" id="q_value_iteration_equation">
<h5><span class="label">Equation 18-3. </span>Q-Value Iteration algorithm</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-163-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mi&gt;s&amp;#x2019;&lt;/mi&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;s&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;[&lt;/mo&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;s&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B3;&lt;/mi&gt;&lt;mo&gt;&amp;#xB7;&lt;/mo&gt;&lt;munder&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;max&lt;/mo&gt;&lt;mi&gt;a&amp;#x2019;&lt;/mi&gt;&lt;/munder&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;mtext&gt;for&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;all&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7135" style="width: 33.628em; display: inline-block;"><span style="display: inline-block; position: relative; width: 32.651em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.751em, 1032.56em, 4.733em, -1000.01em); top: -3.339em; left: 0em;"><span class="mrow" id="MathJax-Span-7136"><span class="mrow" id="MathJax-Span-7137"><span class="msub" id="MathJax-Span-7138"><span style="display: inline-block; position: relative; width: 2.162em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.73em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7139" style="font-family: MathJax_Math-italic;">Q</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.774em;"><span class="mrow" id="MathJax-Span-7140"><span class="mi" id="MathJax-Span-7141" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span class="mo" id="MathJax-Span-7142" style="font-size: 70.7%; font-family: MathJax_Main;">+</span><span class="mn" id="MathJax-Span-7143" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-7144" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7145" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7146" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7147" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7148" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">a</span><span class="mo" id="MathJax-Span-7149" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-7150" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="munder" id="MathJax-Span-7151" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1001.4em, 4.63em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-7152" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.345em, 1000.42em, 4.27em, -1000.01em); top: -2.877em; left: 0.465em;"><span class="mi" id="MathJax-Span-7153" style="font-size: 70.7%; font-family: MathJax_Main;">s’</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-7154" style="padding-left: 0.157em;"><span class="mi" id="MathJax-Span-7155" style="font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mrow" id="MathJax-Span-7156" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7157" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7158" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7159" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7160" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">a</span><span class="mo" id="MathJax-Span-7161" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7162" style="font-family: MathJax_Main; padding-left: 0.157em;">s’</span><span class="mo" id="MathJax-Span-7163" style="font-family: MathJax_Main;">)</span></span><span class="mrow" id="MathJax-Span-7164" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7165" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">[</span></span><span class="mi" id="MathJax-Span-7166" style="font-family: MathJax_Math-italic;">R</span><span class="mrow" id="MathJax-Span-7167" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7168" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7169" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7170" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7171" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">a</span><span class="mo" id="MathJax-Span-7172" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7173" style="font-family: MathJax_Main; padding-left: 0.157em;">s’</span><span class="mo" id="MathJax-Span-7174" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-7175" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-7176" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-7177" style="font-family: MathJax_Main; padding-left: 0.208em;">⋅</span><span class="munder" id="MathJax-Span-7178" style="padding-left: 0.311em; padding-right: 0.311em;"><span style="display: inline-block; position: relative; width: 1.853em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1001.86em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-7179" style="font-family: MathJax_Main;">max</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.345em, 1000.52em, 4.27em, -1000.01em); top: -3.339em; left: 0.671em;"><span class="mi" id="MathJax-Span-7180" style="font-size: 70.7%; font-family: MathJax_Main;">a’</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mspace" id="MathJax-Span-7181" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mrow" id="MathJax-Span-7182"><span class="msub" id="MathJax-Span-7183"><span style="display: inline-block; position: relative; width: 1.237em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.73em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7184" style="font-family: MathJax_Math-italic;">Q</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.774em;"><span class="mi" id="MathJax-Span-7185" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-7186" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7187" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7188" style="font-family: MathJax_Main;">s’</span><span class="mo" id="MathJax-Span-7189" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7190" style="font-family: MathJax_Main; padding-left: 0.157em;">a’</span><span class="mo" id="MathJax-Span-7191" style="font-family: MathJax_Main;">)</span></span></span><span class="mo" id="MathJax-Span-7192" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">]</span></span></span></span><span class="mspace" id="MathJax-Span-7193" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-7194" style="font-family: MathJax_Main; padding-left: 0.157em;">for</span><span class="mspace" id="MathJax-Span-7195" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-7196" style="font-family: MathJax_Main;">all</span><span class="mspace" id="MathJax-Span-7197" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mrow" id="MathJax-Span-7198" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7199" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7200" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7201" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7202" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">a</span><span class="mo" id="MathJax-Span-7203" style="font-family: MathJax_Main;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 3.345em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.321em; border-left: 0px solid; width: 0px; height: 2.862em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>Q</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mrow><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow><mo>←</mo><munder><mo>∑</mo><mi>s’</mi></munder><mrow><mi>T</mi><mrow><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>s’</mi><mo>)</mo></mrow><mrow><mo>[</mo><mi>R</mi><mrow><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>s’</mi><mo>)</mo></mrow><mo>+</mo><mi>γ</mi><mo>·</mo><munder><mo movablelimits="true" form="prefix">max</mo><mi>a’</mi></munder><mspace width="0.166667em"></mspace><mrow><msub><mi>Q</mi><mi>k</mi></msub><mrow><mo>(</mo><mi>s’</mi><mo>,</mo><mi>a’</mi><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow><mspace width="1.em"></mspace><mtext>for</mtext><mspace width="4.pt"></mspace><mtext>all</mtext><mspace width="4.pt"></mspace><mrow><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></mrow></math></span></span></div><script type="math/mml" id="MathJax-Element-163"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <msub><mi>Q</mi> <mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow> </msub>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>,</mo>
      <mi>a</mi>
      <mo>)</mo>
    </mrow>
    <mo>←</mo>
    <munder><mo>∑</mo> <mi>s’</mi> </munder>
    <mrow>
      <mi>T</mi>
      <mrow>
        <mo>(</mo>
        <mi>s</mi>
        <mo>,</mo>
        <mi>a</mi>
        <mo>,</mo>
        <mi>s’</mi>
        <mo>)</mo>
      </mrow>
      <mrow>
        <mo>[</mo>
        <mi>R</mi>
        <mrow>
          <mo>(</mo>
          <mi>s</mi>
          <mo>,</mo>
          <mi>a</mi>
          <mo>,</mo>
          <mi>s’</mi>
          <mo>)</mo>
        </mrow>
        <mo>+</mo>
        <mi>γ</mi>
        <mo>·</mo>
        <munder><mo movablelimits="true" form="prefix">max</mo> <mi>a’</mi></munder>
        <mspace width="0.166667em"></mspace>
        <mrow>
          <msub><mi>Q</mi> <mi>k</mi> </msub>
          <mrow>
            <mo>(</mo>
            <mi>s’</mi>
            <mo>,</mo>
            <mi>a’</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>]</mo>
      </mrow>
    </mrow>
    <mspace width="1.em"></mspace>
    <mtext>for</mtext>
    <mspace width="4.pt"></mspace>
    <mtext>all</mtext>
    <mspace width="4.pt"></mspace>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>,</mo>
      <mi>a</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math></script>
</div>

<p>Once you have the optimal Q-Values, defining the optimal policy, noted <em>π</em>*(<em>s</em>), is trivial: when the agent is in state <em>s</em>, it should choose the action with the highest Q-Value for that state: <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-164-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;pi Superscript asterisk Baseline left-parenthesis s right-parenthesis equals a r g m a x Underscript a Endscripts upper Q Superscript asterisk Baseline left-parenthesis s comma a right-parenthesis&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x3C0;&lt;/mi&gt;&lt;mo&gt;*&lt;/mo&gt;&lt;/msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mo form=&quot;prefix&quot;&gt;argmax&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/munder&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;msup&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo&gt;*&lt;/mo&gt;&lt;/msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7204" aria-label="pi Superscript asterisk Baseline left-parenthesis s right-parenthesis equals a r g m a x Underscript a Endscripts upper Q Superscript asterisk Baseline left-parenthesis s comma a right-parenthesis" style="width: 11.417em; display: inline-block;"><span style="display: inline-block; position: relative; width: 11.057em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.185em, 1010.96em, 3.19em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-7205"><span class="mrow" id="MathJax-Span-7206"><span class="msup" id="MathJax-Span-7207"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.447em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7208" style="font-family: MathJax_Math-italic;">π<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.62em;"><span class="mo" id="MathJax-Span-7209" style="font-size: 70.7%; font-family: MathJax_Main;">∗</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-7210" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7211" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7212" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7213" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-7214" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="munder" id="MathJax-Span-7215" style="padding-left: 0.311em; padding-right: 0.311em;"><span style="display: inline-block; position: relative; width: 3.242em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1003.25em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-7216" style="font-family: MathJax_Main;">argmax</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.55em, 1000.37em, 4.27em, -1000.01em); top: -3.185em; left: 1.442em;"><span class="mi" id="MathJax-Span-7217" style="font-size: 70.7%; font-family: MathJax_Math-italic;">a</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mspace" id="MathJax-Span-7218" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="msup" id="MathJax-Span-7219"><span style="display: inline-block; position: relative; width: 1.237em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.73em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7220" style="font-family: MathJax_Math-italic;">Q</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.774em;"><span class="mo" id="MathJax-Span-7221" style="font-size: 70.7%; font-family: MathJax_Main;">∗</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-7222" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7223" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7224" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7225" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7226" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">a</span><span class="mo" id="MathJax-Span-7227" style="font-family: MathJax_Main;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.004em; border-left: 0px solid; width: 0px; height: 1.803em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="pi Superscript asterisk Baseline left-parenthesis s right-parenthesis equals a r g m a x Underscript a Endscripts upper Q Superscript asterisk Baseline left-parenthesis s comma a right-parenthesis"><mrow><msup><mi>π</mi><mo>*</mo></msup><mrow><mo>(</mo><mi>s</mi><mo>)</mo></mrow><mo>=</mo><munder><mo form="prefix">argmax</mo><mi>a</mi></munder><mspace width="0.166667em"></mspace><msup><mi>Q</mi><mo>*</mo></msup><mrow><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></mrow></math></span></span><script type="math/mml" id="MathJax-Element-164"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="pi Superscript asterisk Baseline left-parenthesis s right-parenthesis equals a r g m a x Underscript a Endscripts upper Q Superscript asterisk Baseline left-parenthesis s comma a right-parenthesis">
  <mrow>
    <msup><mi>π</mi> <mo>*</mo> </msup>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <munder><mo form="prefix">argmax</mo> <mi>a</mi></munder>
    <mspace width="0.166667em"></mspace>
    <msup><mi>Q</mi> <mo>*</mo> </msup>
    <mrow>
      <mo>(</mo>
      <mi>s</mi>
      <mo>,</mo>
      <mi>a</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math></script>.</p>

<p>Let’s apply this algorithm to the MDP represented in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#mdp_diagram">Figure&nbsp;18-8</a>. First, we need to define the MDP:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">transition_probabilities</code> <code class="o">=</code> <code class="p">[</code> <code class="c1"># shape=[s, a, s']</code>
        <code class="p">[[</code><code class="mf">0.7</code><code class="p">,</code> <code class="mf">0.3</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">1.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">],</code> <code class="p">[</code><code class="mf">0.8</code><code class="p">,</code> <code class="mf">0.2</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">]],</code>
        <code class="p">[[</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">],</code> <code class="bp">None</code><code class="p">,</code> <code class="p">[</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">]],</code>
        <code class="p">[</code><code class="bp">None</code><code class="p">,</code> <code class="p">[</code><code class="mf">0.8</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">],</code> <code class="bp">None</code><code class="p">]]</code>
<code class="n">rewards</code> <code class="o">=</code> <code class="p">[</code> <code class="c1"># shape=[s, a, s']</code>
        <code class="p">[[</code><code class="o">+</code><code class="mi">10</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]],</code>
        <code class="p">[[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="o">-</code><code class="mi">50</code><code class="p">]],</code>
        <code class="p">[[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="o">+</code><code class="mi">40</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]]]</code>
<code class="n">possible_actions</code> <code class="o">=</code> <code class="p">[[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">1</code><code class="p">]]</code></pre>

<p>For example, to know the transition probability from <em>s</em><sub>2</sub> to <em>s</em><sub>0</sub> after playing action <em>a</em><sub>1</sub>, we will look up <code>transition_probabilities[2][1][0]</code> (which is 0.8). Similarly, to get the corresponding reward, we will look up <code>rewards[2][1][0]</code> (which is +40). And to get the list of possible actions in <em>s</em><sub>2</sub>, we will look up <code>possible_actions[2]</code> (in this case, only action <em>a</em><sub>1</sub> is possible). Next, we must initialize all the Q-Values to 0 (except for the the impossible actions, for which we set the Q-Values to –∞):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Q_values</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">full</code><code class="p">((</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code> <code class="o">-</code><code class="n">np</code><code class="o">.</code><code class="n">inf</code><code class="p">)</code> <code class="c1"># -np.inf for impossible actions</code>
<code class="k">for</code> <code class="n">state</code><code class="p">,</code> <code class="n">actions</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">possible_actions</code><code class="p">):</code>
    <code class="n">Q_values</code><code class="p">[</code><code class="n">state</code><code class="p">,</code> <code class="n">actions</code><code class="p">]</code> <code class="o">=</code> <code class="mf">0.0</code>  <code class="c1"># for all possible actions</code></pre>

<p>Now let’s run the Q-Value Iteration algorithm. It applies <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#q_value_iteration_equation">Equation 18-3</a> repeatedly, to all Q-Values, for every state and every possible action:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">gamma</code> <code class="o">=</code> <code class="mf">0.90</code> <code class="c1"># the discount factor</code>

<code class="k">for</code> <code class="n">iteration</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">50</code><code class="p">):</code>
    <code class="n">Q_prev</code> <code class="o">=</code> <code class="n">Q_values</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">s</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">3</code><code class="p">):</code>
        <code class="k">for</code> <code class="n">a</code> <code class="ow">in</code> <code class="n">possible_actions</code><code class="p">[</code><code class="n">s</code><code class="p">]:</code>
            <code class="n">Q_values</code><code class="p">[</code><code class="n">s</code><code class="p">,</code> <code class="n">a</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">([</code>
                    <code class="n">transition_probabilities</code><code class="p">[</code><code class="n">s</code><code class="p">][</code><code class="n">a</code><code class="p">][</code><code class="n">sp</code><code class="p">]</code>
                    <code class="o">*</code> <code class="p">(</code><code class="n">rewards</code><code class="p">[</code><code class="n">s</code><code class="p">][</code><code class="n">a</code><code class="p">][</code><code class="n">sp</code><code class="p">]</code> <code class="o">+</code> <code class="n">gamma</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">Q_prev</code><code class="p">[</code><code class="n">sp</code><code class="p">]))</code>
                <code class="k">for</code> <code class="n">sp</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">3</code><code class="p">)])</code></pre>

<p>That’s it! The resulting Q-Values look like this:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">Q_values</code>
<code class="go">array([[18.91891892, 17.02702702, 13.62162162],</code>
<code class="go">       [ 0.        ,        -inf, -4.87971488],</code>
<code class="go">       [       -inf, 50.13365013,        -inf]])</code></pre>

<p>For example, when the agent is in state <em>s</em><sub>0</sub> and it chooses action <em>a</em><sub>1</sub>, the expected sum of discounted future rewards is approximately 17.0. For each state, let’s look at the action that has the highest Q-Value:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">Q_values</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code> <code class="c"># optimal action for each state</code>
<code class="go">array([0, 0, 1])</code></pre>

<p>This gives us the optimal policy for this MDP, when using a discount factor of 0.90: in state <em>s</em><sub>0</sub> choose action <em>a</em><sub>0</sub>; in state <em>s</em><sub>1</sub> choose action <em>a</em><sub>0</sub> (i.e., stay put); and in state <em>s</em><sub>2</sub> choose action <em>a</em><sub>1</sub> (the only possible action). Interestingly, if you increase the discount factor to 0.95, the optimal policy changes: in state <em>s</em><sub>1</sub> the best action becomes <em>a</em><sub>2</sub> (go through the fire!). It makes sense because the more you value future rewards, the more you are willing to put up with some pain now for the promise of future bliss.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Temporal Difference Learning"><div class="sect1" id="idm46263484910616">
<h1>Temporal Difference Learning</h1>

<p>Reinforcement Learning problems with discrete actions can often be modeled as Markov decision processes, but the agent initially has no idea what the transition probabilities are (it does not know <em>T</em>(<em>s</em>, <em>a</em>, <em>s</em>′)), and it does not know what the rewards are going to be either (it does not know <em>R</em>(<em>s</em>, <em>a</em>, <em>s</em>′)). It must experience each state and each transition at least once to know the rewards, and it must experience them multiple times if it is to have a reasonable estimate of the transition probabilities.</p>

<p>The <em>Temporal Difference Learning</em> (TD Learning) algorithm is very similar to the Value Iteration algorithm, but tweaked to take into account the fact that the agent has only partial knowledge of the MDP. In general we assume that the agent initially knows only the possible states and actions, and nothing more. The agent uses an <em>exploration policy</em>—for example, a purely random policy—to explore the MDP, and as it progresses, the TD Learning algorithm updates the estimates of the state values based on the transitions and rewards that are actually observed (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#td_learning_equation">Equation 18-4</a>).</p>
<div data-type="equation" id="td_learning_equation">
<h5><span class="label">Equation 18-4. </span>TD Learning algorithm</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display"><span class="MathJax MathJax_FullWidth" id="MathJax-Element-165-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mi&gt;&amp;#x3B1;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B1;&lt;/mi&gt;&lt;mfenced&gt;&lt;mrow&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B3;&lt;/mi&gt;&lt;mo&gt;&amp;#xB7;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mspace linebreak=&quot;newline&quot; /&gt;&lt;mtext&gt;or,&amp;#xA0;equivalently:&amp;#xA0;&lt;/mtext&gt;&lt;mspace linebreak=&quot;newline&quot; /&gt;&lt;msub&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B1;&lt;/mi&gt;&lt;mo&gt;&amp;#xB7;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B4;&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;&#39;&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mspace linebreak=&quot;newline&quot; /&gt;&lt;mtext&gt;with&amp;#xA0;&lt;/mtext&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B4;&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;&#39;&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B3;&lt;/mi&gt;&lt;mo&gt;&amp;#xB7;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;&#39;&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7228" style="width: 100%; display: inline-block; min-width: 18.769em;"><span style="display: inline-block; position: relative; width: 100%; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(5.247em, 1018.11em, 10.748em, -1000.01em); top: -6.167em; left: 0em; width: 100%;"><span class="mrow" id="MathJax-Span-7229"><span style="display: inline-block; position: relative; width: 100%; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1018.11em, 4.424em, -1000.01em); top: -4.008em; left: 50%; margin-left: -9.098em;"><span class="msub" id="MathJax-Span-7230"><span style="display: inline-block; position: relative; width: 1.956em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7231" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.208em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mrow" id="MathJax-Span-7232"><span class="mi" id="MathJax-Span-7233" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span class="mo" id="MathJax-Span-7234" style="font-size: 70.7%; font-family: MathJax_Main;">+</span><span class="mn" id="MathJax-Span-7235" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-7236" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7237" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7238" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-7239" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="mo" id="MathJax-Span-7240" style="font-family: MathJax_Main; padding-left: 0.26em;">(</span><span class="mn" id="MathJax-Span-7241" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-7242" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mi" id="MathJax-Span-7243" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">α</span><span class="mo" id="MathJax-Span-7244" style="font-family: MathJax_Main;">)</span><span class="msub" id="MathJax-Span-7245"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7246" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.208em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mi" id="MathJax-Span-7247" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-7248" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7249" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7250" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-7251" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-7252" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">α</span><span class="mfenced" id="MathJax-Span-7253" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7332" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mrow" id="MathJax-Span-7255"><span class="mi" id="MathJax-Span-7256" style="font-family: MathJax_Math-italic;">r</span><span class="mo" id="MathJax-Span-7257" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-7258" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-7259" style="font-family: MathJax_Main; padding-left: 0.208em;">⋅</span><span class="msub" id="MathJax-Span-7260" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7261" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.208em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mi" id="MathJax-Span-7262" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-7263" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7264" style="font-family: MathJax_Main;">s’</span><span class="mo" id="MathJax-Span-7265" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-7333" style=""><span style="font-family: MathJax_Main;">)</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1007em, 4.373em, -1000.01em); top: -2.62em; left: 50%; margin-left: -3.648em;"><span class="mspace" id="MathJax-Span-7267" style="height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-7268" style="font-family: MathJax_Main;">or,&nbsp;equivalently:&nbsp;</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1013.59em, 4.424em, -1000.01em); top: -1.231em; left: 50%; margin-left: -6.836em;"><span class="mspace" id="MathJax-Span-7269" style="height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;"></span><span class="msub" id="MathJax-Span-7270"><span style="display: inline-block; position: relative; width: 1.956em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7271" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.208em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mrow" id="MathJax-Span-7272"><span class="mi" id="MathJax-Span-7273" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span class="mo" id="MathJax-Span-7274" style="font-size: 70.7%; font-family: MathJax_Main;">+</span><span class="mn" id="MathJax-Span-7275" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-7276" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7277" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7278" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-7279" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="msub" id="MathJax-Span-7280" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7281" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.208em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mi" id="MathJax-Span-7282" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-7283" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7284" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7285" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-7286" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-7287" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">α</span><span class="mo" id="MathJax-Span-7288" style="font-family: MathJax_Main; padding-left: 0.208em;">⋅</span><span class="msub" id="MathJax-Span-7289" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 0.877em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.47em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7290" style="font-family: MathJax_Math-italic;">δ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.465em;"><span class="mi" id="MathJax-Span-7291" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-7292" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7293" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7294" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7295" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">r</span><span class="mo" id="MathJax-Span-7296" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7297" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">s</span><span class="mo" id="MathJax-Span-7298" style="font-family: MathJax_Main;">'</span><span class="mo" id="MathJax-Span-7299" style="font-family: MathJax_Main;">)</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1016.51em, 4.424em, -1000.01em); top: 0.157em; left: 50%; margin-left: -8.275em;"><span class="mspace" id="MathJax-Span-7300" style="height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-7301" style="font-family: MathJax_Main;">with&nbsp;</span><span class="msub" id="MathJax-Span-7302"><span style="display: inline-block; position: relative; width: 0.877em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.47em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7303" style="font-family: MathJax_Math-italic;">δ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.465em;"><span class="mi" id="MathJax-Span-7304" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-7305" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7306" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7307" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7308" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">r</span><span class="mo" id="MathJax-Span-7309" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7310" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">s</span><span class="mo" id="MathJax-Span-7311" style="font-family: MathJax_Main;">'</span><span class="mo" id="MathJax-Span-7312" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-7313" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mi" id="MathJax-Span-7314" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">r</span><span class="mo" id="MathJax-Span-7315" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-7316" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-7317" style="font-family: MathJax_Main; padding-left: 0.208em;">⋅</span><span class="msub" id="MathJax-Span-7318" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7319" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.208em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mi" id="MathJax-Span-7320" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-7321" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7322" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7323" style="font-family: MathJax_Main;">'</span><span class="mo" id="MathJax-Span-7324" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-7325" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="msub" id="MathJax-Span-7326" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7327" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.208em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mi" id="MathJax-Span-7328" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-7329" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7330" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7331" style="font-family: MathJax_Main;">)</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 6.172em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -4.605em; border-left: 0px solid; width: 0px; height: 5.457em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>V</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>(</mo><mi>s</mi><mo>)</mo><mo>←</mo><mo>(</mo><mn>1</mn><mo>-</mo><mi>α</mi><mo>)</mo><msub><mi>V</mi><mi>k</mi></msub><mo>(</mo><mi>s</mi><mo>)</mo><mo>+</mo><mi>α</mi><mfenced><mrow><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><msub><mi>V</mi><mi>k</mi></msub><mo>(</mo><mi>s’</mi><mo>)</mo></mrow></mfenced><mspace linebreak="newline"></mspace><mtext>or,&nbsp;equivalently:&nbsp;</mtext><mspace linebreak="newline"></mspace><msub><mi>V</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>(</mo><mi>s</mi><mo>)</mo><mo>←</mo><msub><mi>V</mi><mi>k</mi></msub><mo>(</mo><mi>s</mi><mo>)</mo><mo>+</mo><mi>α</mi><mo>·</mo><msub><mi>δ</mi><mi>k</mi></msub><mo>(</mo><mi>s</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>s</mi><mo>'</mo><mo>)</mo><mspace linebreak="newline"></mspace><mtext>with&nbsp;</mtext><msub><mi>δ</mi><mi>k</mi></msub><mo>(</mo><mi>s</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>s</mi><mo>'</mo><mo>)</mo><mo>=</mo><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><msub><mi>V</mi><mi>k</mi></msub><mo>(</mo><mi>s</mi><mo>'</mo><mo>)</mo><mo>-</mo><msub><mi>V</mi><mi>k</mi></msub><mo>(</mo><mi>s</mi><mo>)</mo></math></span></span></div><script type="math/mml" id="MathJax-Element-165"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <msub>
    <mi>V</mi>
    <mrow>
      <mi>k</mi><mo>+</mo><mn>1</mn>
    </mrow>
  </msub>
  <mo>(</mo><mi>s</mi><mo>)</mo><mo>←</mo><mo>(</mo><mn>1</mn><mo>-</mo><mi>α</mi><mo>)</mo>
  <msub><mi>V</mi><mi>k</mi></msub>
  <mo>(</mo><mi>s</mi><mo>)</mo><mo>+</mo><mi>α</mi>
  <mfenced>
    <mrow>
      <mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo>
      <msub><mi>V</mi><mi>k</mi></msub>
      <mo>(</mo><mi>s’</mi><mo>)</mo>
    </mrow>
  </mfenced>
  <mspace linebreak="newline"></mspace>
  <mtext>or, equivalently: </mtext><mspace linebreak="newline"></mspace>
  <msub><mi>V</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub>
  <mo>(</mo><mi>s</mi><mo>)</mo><mo>←</mo><msub><mi>V</mi><mi>k</mi></msub>
  <mo>(</mo><mi>s</mi><mo>)</mo><mo>+</mo><mi>α</mi><mo>·</mo>
  <msub><mi>δ</mi><mi>k</mi></msub>
  <mo>(</mo><mi>s</mi><mo>,</mo><mi>r</mi><mo>,</mo>
  <mi>s</mi><mo>'</mo><mo>)</mo>
  <mspace linebreak="newline"></mspace>
  <mtext>with </mtext><msub><mi>δ</mi><mi>k</mi></msub>
  <mo>(</mo><mi>s</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>s</mi><mo>'</mo>
  <mo>)</mo><mo>=</mo><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo>
  <msub><mi>V</mi><mi>k</mi></msub>
  <mo>(</mo><mi>s</mi><mo>'</mo><mo>)</mo><mo>-</mo>
  <msub><mi>V</mi><mi>k</mi></msub><mo>(</mo><mi>s</mi><mo>)</mo>
</math></script>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><em>α</em> is the learning rate (e.g., 0.01).</p>
</li>
<li>
<p><em>r</em> + <em>γ</em>·<em>V</em><sub><em>k</em></sub>(<em>s</em>′) is called the <em>TD target</em>.</p>
</li>
<li>
<p><em>δ</em><sub>k</sub>(<em>s</em>, <em>r</em>, <em>s</em>′) is called the <em>TD error</em>.</p>
</li>
</ul>

<p>A more concise way of writing the first form of this equation is to use the notation <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-166-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;munder&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mi&gt;&amp;#x3B1;&lt;/mi&gt;&lt;/munder&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7334" style="width: 2.573em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.47em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.288em, 1002.48em, 2.985em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-7335"><span class="mi" id="MathJax-Span-7336" style="font-family: MathJax_Math-italic;">a</span><span class="munder" id="MathJax-Span-7337" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.345em, 1000.94em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-7338" style=""><span style="font-family: MathJax_Main;">←</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.55em, 1000.42em, 4.27em, -1000.01em); top: -3.391em; left: 0.26em;"><span class="mi" id="MathJax-Span-7339" style="font-size: 70.7%; font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-7340" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">b</span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.792em; border-left: 0px solid; width: 0px; height: 1.538em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>a</mi><munder><mo>←</mo><mi>α</mi></munder><mi>b</mi></math></span></span><script type="math/mml" id="MathJax-Element-166"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>a</mi><munder><mo>←</mo><mi>α</mi></munder><mi>b</mi></math></script>, which means <em>a</em><sub><em>k</em>+1</sub> ← (1 – <em>α</em>)·<em>a</em><sub><em>k</em></sub> + <em>α</em>·<em>b</em><sub><em>k</em></sub>. So the first line of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#td_learning_equation">Equation 18-4</a> can be rewritten like this: <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-167-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mi&gt;&amp;#x3B1;&lt;/mi&gt;&lt;/munder&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B3;&lt;/mi&gt;&lt;mo&gt;&amp;#xB7;&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7341" style="width: 9.771em; display: inline-block;"><span style="display: inline-block; position: relative; width: 9.463em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.905em, 1009.27em, 4.064em, -1000.01em); top: -3.185em; left: 0em;"><span class="mrow" id="MathJax-Span-7342"><span class="mi" id="MathJax-Span-7343" style="font-family: MathJax_Math-italic;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.208em;"></span></span><span class="mo" id="MathJax-Span-7344" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">(</span></span><span class="mi" id="MathJax-Span-7345" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7346" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">)</span></span><span class="munder" id="MathJax-Span-7347" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.345em, 1000.94em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-7348" style=""><span style="font-family: MathJax_Main;">←</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.55em, 1000.42em, 4.27em, -1000.01em); top: -3.391em; left: 0.26em;"><span class="mi" id="MathJax-Span-7349" style="font-size: 70.7%; font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-7350" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">r</span><span class="mo" id="MathJax-Span-7351" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-7352" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-7353" style="font-family: MathJax_Main; padding-left: 0.208em;">⋅</span><span class="mi" id="MathJax-Span-7354" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">V<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.208em;"></span></span><span class="mo" id="MathJax-Span-7355" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">(</span></span><span class="mi" id="MathJax-Span-7356" style="font-family: MathJax_Main;">s’</span><span class="mo" id="MathJax-Span-7357" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">)</span></span></span><span style="display: inline-block; width: 0px; height: 3.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.792em; border-left: 0px solid; width: 0px; height: 2.015em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>V</mi><mo>(</mo><mi>s</mi><mo>)</mo><munder><mo>←</mo><mi>α</mi></munder><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><mi>V</mi><mo>(</mo><mi>s’</mi><mo>)</mo></math></span></span><script type="math/mml" id="MathJax-Element-167"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>V</mi><mo>(</mo><mi>s</mi><mo>)</mo><munder><mo>←</mo><mi>α</mi></munder><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><mi>V</mi><mo>(</mo><mi>s’</mi><mo>)</mo></math></script>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>TD Learning has many similarities with Stochastic Gradient Descent, in particular the fact that it handles one sample at a time. Moreover, just like SGD, it can only truly converge if you gradually reduce the learning rate (otherwise it will keep bouncing around the optimum Q-Values).</p>
</div>

<p>For each state <em>s</em>, this algorithm simply keeps track of a running average of the immediate rewards the agent gets upon leaving that state, plus the rewards it expects to get later (assuming it acts optimally).</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Q-Learning"><div class="sect1" id="idm46263484755032">
<h1>Q-Learning</h1>

<p>Similarly, the Q-Learning algorithm is an adaptation of the Q-Value Iteration algorithm to the situation where the transition probabilities and the rewards are initially unknown (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#q_learning_equation">Equation 18-5</a>). Q-Learning works by watching an agent play (e.g., randomly) and gradually improving its estimates of the Q-Values. Once it has accurate Q-Value estimates (or close enough), then the optimal policy is choosing the action that has the highest Q-Value (i.e., the greedy policy).</p>
<div data-type="equation" id="q_learning_equation">
<h5><span class="label">Equation 18-5. </span>Q-Learning algorithm</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-168-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mi&gt;&amp;#x3B1;&lt;/mi&gt;&lt;/munder&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B3;&lt;/mi&gt;&lt;mo&gt;&amp;#xB7;&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;max&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;a&amp;#x2019;&lt;/mi&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mo&gt;&amp;#xA0;&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7358" style="width: 14.913em; display: inline-block;"><span style="display: inline-block; position: relative; width: 14.45em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.751em, 1014.25em, 4.424em, -1000.01em); top: -3.339em; left: 0em;"><span class="mrow" id="MathJax-Span-7359"><span class="mi" id="MathJax-Span-7360" style="font-family: MathJax_Math-italic;">Q</span><span class="mo" id="MathJax-Span-7361" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">(</span></span><span class="mi" id="MathJax-Span-7362" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7363" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7364" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">a</span><span class="mo" id="MathJax-Span-7365" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">)</span></span><span class="munder" id="MathJax-Span-7366" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.345em, 1000.94em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-7367" style=""><span style="font-family: MathJax_Main;">←</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.55em, 1000.42em, 4.27em, -1000.01em); top: -3.391em; left: 0.26em;"><span class="mi" id="MathJax-Span-7368" style="font-size: 70.7%; font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-7369" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">r</span><span class="mo" id="MathJax-Span-7370" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-7371" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-7372" style="font-family: MathJax_Main; padding-left: 0.208em;">⋅</span><span class="munder" id="MathJax-Span-7373" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.853em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1001.86em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7374" style="font-family: MathJax_Main;">max</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.345em, 1000.52em, 4.27em, -1000.01em); top: -3.339em; left: 0.671em;"><span class="mrow" id="MathJax-Span-7375"><span class="mi" id="MathJax-Span-7376" style="font-size: 70.7%; font-family: MathJax_Main;">a’</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-7377" style="font-family: MathJax_Main; padding-left: 0.157em;">&nbsp;</span><span class="mi" id="MathJax-Span-7378" style="font-family: MathJax_Math-italic;">Q</span><span class="mo" id="MathJax-Span-7379" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">(</span></span><span class="mi" id="MathJax-Span-7380" style="font-family: MathJax_Main;">s’</span><span class="mo" id="MathJax-Span-7381" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7382" style="font-family: MathJax_Main; padding-left: 0.157em;">a’</span><span class="mo" id="MathJax-Span-7383" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">)</span></span></span><span style="display: inline-block; width: 0px; height: 3.345em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.004em; border-left: 0px solid; width: 0px; height: 2.598em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>Q</mi><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo><munder><mo>←</mo><mi>α</mi></munder><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><munder><mi>max</mi><mrow><mi>a’</mi></mrow></munder><mo>&nbsp;</mo><mi>Q</mi><mo>(</mo><mi>s’</mi><mo>,</mo><mi>a’</mi><mo>)</mo></math></span></span></div><script type="math/mml" id="MathJax-Element-168"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>Q</mi><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo>
  <munder>
    <mo>←</mo><mi>α</mi>
  </munder>
  <mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo>
  <munder>
    <mi>max</mi><mrow><mi>a’</mi></mrow>
  </munder>
  <mo> </mo><mi>Q</mi><mo>(</mo><mi>s’</mi><mo>,</mo>
  <mi>a’</mi><mo>)</mo>
</math></script>
</div>

<p>For each state-action pair (<em>s</em>, <em>a</em>), this algorithm keeps track of a running average of the rewards <em>r</em> the agent gets upon leaving the state <em>s</em> with action <em>a</em>, plus the sum of discounted future rewards it expects to get. To estimate this sum, we take the maximum of the Q-Value estimates for the next state <em>s</em>′, since we assume that the target policy would act optimally from then on.</p>

<p>Let’s implement the Q-Learning algorithm. First, we will need to make an agent explore the environment. For this, we need a step function so that the agent can execute one action and get the resulting state and reward:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">action</code><code class="p">):</code>
    <code class="n">probas</code> <code class="o">=</code> <code class="n">transition_probabilities</code><code class="p">[</code><code class="n">state</code><code class="p">][</code><code class="n">action</code><code class="p">]</code>
    <code class="n">next_state</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="n">p</code><code class="o">=</code><code class="n">probas</code><code class="p">)</code>
    <code class="n">reward</code> <code class="o">=</code> <code class="n">rewards</code><code class="p">[</code><code class="n">state</code><code class="p">][</code><code class="n">action</code><code class="p">][</code><code class="n">next_state</code><code class="p">]</code>
    <code class="k">return</code> <code class="n">next_state</code><code class="p">,</code> <code class="n">reward</code></pre>

<p>Now let’s implement the agent’s exploration policy. Since the state space is pretty small, a simple random policy will be sufficient. If we run the algorithm for long enough, the agent will visit every state many times, and it will also try every possible action many times:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">exploration_policy</code><code class="p">(</code><code class="n">state</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">possible_actions</code><code class="p">[</code><code class="n">state</code><code class="p">])</code></pre>

<p>Next, after we initialize the Q-Values just like earlier, we are ready to run the Q-Learning algorithm with learning rate decay (using power scheduling, introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">alpha0</code> <code class="o">=</code> <code class="mf">0.05</code> <code class="c1"># initial learning rate</code>
<code class="n">decay</code> <code class="o">=</code> <code class="mf">0.005</code> <code class="c1"># learning rate decay</code>
<code class="n">gamma</code> <code class="o">=</code> <code class="mf">0.90</code> <code class="c1"># discount factor</code>
<code class="n">state</code> <code class="o">=</code> <code class="mi">0</code> <code class="c1"># initial state</code>

<code class="k">for</code> <code class="n">iteration</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">10000</code><code class="p">):</code>
    <code class="n">action</code> <code class="o">=</code> <code class="n">exploration_policy</code><code class="p">(</code><code class="n">state</code><code class="p">)</code>
    <code class="n">next_state</code><code class="p">,</code> <code class="n">reward</code> <code class="o">=</code> <code class="n">step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">action</code><code class="p">)</code>
    <code class="n">next_value</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">Q_values</code><code class="p">[</code><code class="n">next_state</code><code class="p">])</code>
    <code class="n">alpha</code> <code class="o">=</code> <code class="n">alpha0</code> <code class="o">/</code> <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="n">iteration</code> <code class="o">*</code> <code class="n">decay</code><code class="p">)</code>
    <code class="n">Q_values</code><code class="p">[</code><code class="n">state</code><code class="p">,</code> <code class="n">action</code><code class="p">]</code> <code class="o">*=</code> <code class="mi">1</code> <code class="o">-</code> <code class="n">alpha</code>
    <code class="n">Q_values</code><code class="p">[</code><code class="n">state</code><code class="p">,</code> <code class="n">action</code><code class="p">]</code> <code class="o">+=</code> <code class="n">alpha</code> <code class="o">*</code> <code class="p">(</code><code class="n">reward</code> <code class="o">+</code> <code class="n">gamma</code> <code class="o">*</code> <code class="n">next_value</code><code class="p">)</code>
    <code class="n">state</code> <code class="o">=</code> <code class="n">next_state</code></pre>

<p>This algorithm will converge to the optimal Q-Values, but it will take many iterations, and possibly quite a lot of hyperparameter tuning. As you can see in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#q_value_plot">Figure&nbsp;18-9</a>, the Q-Value iteration algorithm (left) converges very quickly, in fewer than 20 iterations, while the Q-Learning algorithm (right) takes about 8,000 iterations to converge. Obviously, not knowing the transition probabilities or the rewards makes finding the optimal policy significantly harder!</p>

<figure><div id="q_value_plot" class="figure">
<img src="./Chapter18_files/mls2_1809.png" alt="mls2 1809" width="1440" height="542" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1809.png">
<h6><span class="label">Figure 18-9. </span>The Q-Value Iteration algorithm (left) <em>vs</em> the Q-Learning algorithm (right)</h6>
</div></figure>

<p>The Q-Learning algorithm is called an <em>off-policy</em> algorithm because the policy being trained is not necessarily the one being executed: in the previous code example, the policy being executed (the exploration policy) is completely random, while the policy being trained will always choose the actions with the highest Q-Values. Conversely, the Policy Gradients algorithm is an <em>on-policy</em> algorithm: it explores the world using the policy being trained. It is somewhat surprising that Q-Learning is capable of learning the optimal policy by just watching an agent act randomly (imagine learning to play golf when your teacher is a drunk monkey). Can we do better?</p>








<section data-type="sect2" data-pdf-bookmark="Exploration Policies"><div class="sect2" id="idm46263484495128">
<h2>Exploration Policies</h2>

<p>Of course Q-Learning can work only if the exploration policy explores the MDP thoroughly enough. Although a purely random policy is guaranteed to eventually visit every state and every transition many times, it may take an extremely long time to do so. Therefore, a better option is to use the <em>ε-greedy policy</em> (ε is epsilon): at each step it acts randomly with probability <em>ε</em>, or greedily with probability 1–<em>ε</em> (i.e., choosing the action with the highest Q-Value). The advantage of the <em>ε</em>-greedy policy (compared to a completely random policy) is that it will spend more and more time exploring the interesting parts of the environment, as the Q-Value estimates get better and better, while still spending some time visiting unknown regions of the MDP. It is quite common to start with a high value for <em>ε</em> (e.g., 1.0) and then gradually reduce it (e.g., down to 0.05).</p>

<p>Alternatively, rather than relying only on chance for exploration, another approach is to encourage the exploration policy to try actions that it has not tried much before. This can be implemented as a bonus added to the Q-Value estimates, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#exploration_function_equation">Equation 18-6</a>.</p>
<div data-type="equation" id="exploration_function_equation">
<h5><span class="label">Equation 18-6. </span>Q-Learning using an exploration function</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-169-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mi&gt;&amp;#x3B1;&lt;/mi&gt;&lt;/munder&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B3;&lt;/mi&gt;&lt;mo&gt;&amp;#xB7;&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;max&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;a&amp;#x2019;&lt;/mi&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mo&gt;&amp;#xA0;&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&amp;#x2019;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7384" style="width: 19.746em; display: inline-block;"><span style="display: inline-block; position: relative; width: 19.18em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.751em, 1019.09em, 4.424em, -1000.01em); top: -3.339em; left: 0em;"><span class="mrow" id="MathJax-Span-7385"><span class="mi" id="MathJax-Span-7386" style="font-family: MathJax_Math-italic;">Q</span><span class="mo" id="MathJax-Span-7387" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">(</span></span><span class="mi" id="MathJax-Span-7388" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7389" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7390" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">a</span><span class="mo" id="MathJax-Span-7391" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">)</span></span><span class="munder" id="MathJax-Span-7392" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.345em, 1000.94em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-7393" style=""><span style="font-family: MathJax_Main;">←</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.55em, 1000.42em, 4.27em, -1000.01em); top: -3.391em; left: 0.26em;"><span class="mi" id="MathJax-Span-7394" style="font-size: 70.7%; font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-7395" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">r</span><span class="mo" id="MathJax-Span-7396" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-7397" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-7398" style="font-family: MathJax_Main; padding-left: 0.208em;">⋅</span><span class="munder" id="MathJax-Span-7399" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.853em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1001.86em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7400" style="font-family: MathJax_Main;">max</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.345em, 1000.52em, 4.27em, -1000.01em); top: -3.339em; left: 0.671em;"><span class="mrow" id="MathJax-Span-7401"><span class="mi" id="MathJax-Span-7402" style="font-size: 70.7%; font-family: MathJax_Main;">a’</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-7403" style="font-family: MathJax_Main; padding-left: 0.157em;">&nbsp;</span><span class="mi" id="MathJax-Span-7404" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mfenced" id="MathJax-Span-7405" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-7406" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mrow" id="MathJax-Span-7407"><span class="mi" id="MathJax-Span-7408" style="font-family: MathJax_Math-italic;">Q</span><span class="mo" id="MathJax-Span-7409" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7410" style="font-family: MathJax_Main;">s’</span><span class="mo" id="MathJax-Span-7411" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7412" style="font-family: MathJax_Main; padding-left: 0.157em;">a’</span><span class="mo" id="MathJax-Span-7413" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-7414" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7415" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mo" id="MathJax-Span-7416" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7417" style="font-family: MathJax_Main;">s’</span><span class="mo" id="MathJax-Span-7418" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7419" style="font-family: MathJax_Main; padding-left: 0.157em;">a’</span><span class="mo" id="MathJax-Span-7420" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-7421" style=""><span style="font-family: MathJax_Main;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 3.345em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.004em; border-left: 0px solid; width: 0px; height: 2.598em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>Q</mi><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo><munder><mo>←</mo><mi>α</mi></munder><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><munder><mi>max</mi><mrow><mi>a’</mi></mrow></munder><mo>&nbsp;</mo><mi>f</mi><mfenced><mrow><mi>Q</mi><mo>(</mo><mi>s’</mi><mo>,</mo><mi>a’</mi><mo>)</mo><mo>,</mo><mi>N</mi><mo>(</mo><mi>s’</mi><mo>,</mo><mi>a’</mi><mo>)</mo></mrow></mfenced></math></span></span></div><script type="math/mml" id="MathJax-Element-169"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
<mi>Q</mi><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo>
<munder>
  <mo>←</mo><mi>α</mi>
</munder>
<mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><munder><mi>max</mi>
<mrow>
  <mi>a’</mi>
</mrow>
</munder><mo> </mo><mi>f</mi>
<mfenced>
  <mrow>
    <mi>Q</mi><mo>(</mo><mi>s’</mi><mo>,</mo><mi>a’</mi>
    <mo>)</mo><mo>,</mo><mi>N</mi><mo>(</mo><mi>s’</mi><mo>,</mo>
    <mi>a’</mi><mo>)</mo>
  </mrow>
</mfenced>
</math></script>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><em>N</em>(<em>s</em>′, <em>a</em>′) counts the number of times the action <em>a</em>′ was chosen in state <em>s</em>′.</p>
</li>
<li>
<p><em>f</em>(<em>q</em>, <em>n</em>) is an <em>exploration function</em>, such as <em>f</em>(<em>q</em>, <em>n</em>) = <em>q</em> + <em>κ</em>/(1 + <em>n</em>), where <em>κ</em> is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown.</p>
</li>
</ul>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Approximate Q-Learning and Deep Q-Learning"><div class="sect2" id="idm46263484465896">
<h2>Approximate Q-Learning and Deep Q-Learning</h2>

<p>The main problem with Q-Learning is that it does not scale well to large (or even medium) MDPs with many states and actions. For example, suppose you wanted to use Q-Learning to train an agent to play Ms. Pac-Man (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#rl_examples_diagram">Figure&nbsp;18-1</a>). There are about 150 pellets that Ms. Pac-Man can eat, each of which can be present or absent (i.e., already eaten). So the number of possible states is greater than 2<sup>150</sup> ≈ 10<sup>45</sup>. And if you add all the possible combinations of positions for all the ghosts and Ms. Pac-Man, the number of possible states becomes larger than the number of atoms in our planet, so there’s absolutely no way you can keep track of an estimate for every single Q-Value.</p>

<p>The solution is to find a function <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-170-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7422" style="width: 3.602em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.499em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.185em, 1003.4em, 2.522em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-7423"><span class="msub" id="MathJax-Span-7424"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.73em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7425" style="font-family: MathJax_Math-italic;">Q</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.774em;"><span class="mi" id="MathJax-Span-7426" style="font-size: 70.7%; font-family: MathJax_Math-bold-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-7427" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-7428" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7429" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7430" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">a</span><span class="mo" id="MathJax-Span-7431" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.315em; border-left: 0px solid; width: 0px; height: 1.115em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>Q</mi><mi mathvariant="bold">θ</mi></msub><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></math></span></span><script type="math/mml" id="MathJax-Element-170"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>Q</mi><mi mathvariant="bold">θ</mi></msub><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></math></script> that approximates the Q-Value of any state-action pair (<em>s</em>,<em>a</em>) using a manageable number of parameters (given by the parameter vector <strong>θ</strong>). This is called <em>Approximate Q-Learning</em>. For years it was recommended to use linear combinations of hand-crafted features extracted from the state (e.g., distance of the closest ghosts, their directions, and so on) to estimate Q-Values, but in 2013, DeepMind showed that using deep neural networks can work much better, especially for complex problems, and it does not require any feature engineering. A DNN used to estimate Q-Values is called a <em>deep Q-network</em> (DQN), and using a DQN for Approximate Q-Learning is called <em>Deep Q-Learning</em>.</p>

<p>Now, how can we train a DQN? Well, consider the approximate Q-Value computed by the DQN for a given state-action pair (<em>s</em>,<em>a</em>). Thanks to Bellman, we know we want this approximate Q-Value to be as close as possible to the reward <em>r</em> that we actually observe after playing action <em>a</em> in state <em>s</em>, plus the discounted value of playing optimally from then on. To estimate this sum of future discounted rewards, we can simply execute the DQN on the next state <em>s</em>′ and for all possible actions <em>a</em>′. We get an approximate future Q-Value for each possible action. We then pick the highest (since we assume we will be playing optimally), we discount it, and this gives us an estimate of the sum of future discounted rewards. By summing the reward <em>r</em> and the future discounted value estimate, we get a target Q-Value <em>y</em>(<em>s</em>, <em>a</em>) for the state-action pair (<em>s</em>, <em>a</em>), as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#target_q_value_equation">Equation 18-7</a>.</p>
<div id="target_q_value_equation" data-type="equation"><h5><span class="label">Equation 18-7. </span>Target Q-Value</h5><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-171-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mtext&gt;target&lt;/mtext&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B3;&lt;/mi&gt;&lt;mo&gt;&amp;#xB7;&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;max&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;&#39;&lt;/mo&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mo&gt;&amp;#xA0;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;&#39;&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;&#39;&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7432" style="width: 17.278em; display: inline-block;"><span style="display: inline-block; position: relative; width: 16.764em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.751em, 1016.57em, 4.424em, -1000.01em); top: -3.339em; left: 0em;"><span class="mrow" id="MathJax-Span-7433"><span class="msub" id="MathJax-Span-7434"><span style="display: inline-block; position: relative; width: 2.728em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.73em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7435" style="font-family: MathJax_Math-italic;">Q</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.774em;"><span class="mtext" id="MathJax-Span-7436" style="font-size: 70.7%; font-family: MathJax_Main;">target</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-7437" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">(</span></span><span class="mi" id="MathJax-Span-7438" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7439" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7440" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">a</span><span class="mo" id="MathJax-Span-7441" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">)</span></span><span class="mo" id="MathJax-Span-7442" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mi" id="MathJax-Span-7443" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">r</span><span class="mo" id="MathJax-Span-7444" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-7445" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-7446" style="font-family: MathJax_Main; padding-left: 0.208em;">⋅</span><span class="munder" id="MathJax-Span-7447" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.853em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1001.86em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7448" style="font-family: MathJax_Main;">max</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.345em, 1000.52em, 4.27em, -1000.01em); top: -3.339em; left: 0.671em;"><span class="mrow" id="MathJax-Span-7449"><span class="mi" id="MathJax-Span-7450" style="font-size: 70.7%; font-family: MathJax_Math-italic;">a</span><span class="mo" id="MathJax-Span-7451" style="font-size: 70.7%; font-family: MathJax_Main;">'</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-7452" style="font-family: MathJax_Main; padding-left: 0.157em;">&nbsp;</span><span class="msub" id="MathJax-Span-7453"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.73em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7454" style="font-family: MathJax_Math-italic;">Q</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.774em;"><span class="mi" id="MathJax-Span-7455" style="font-size: 70.7%; font-family: MathJax_Math-bold-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-7456" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">(</span></span><span class="mi" id="MathJax-Span-7457" style="font-family: MathJax_Math-italic;">s</span><span class="mo" id="MathJax-Span-7458" style="font-family: MathJax_Main;">'</span><span class="mo" id="MathJax-Span-7459" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-7460" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">a</span><span class="mo" id="MathJax-Span-7461" style="font-family: MathJax_Main;">'</span><span class="mo" id="MathJax-Span-7462" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">)</span></span></span><span style="display: inline-block; width: 0px; height: 3.345em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.004em; border-left: 0px solid; width: 0px; height: 2.598em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>Q</mi><mtext>target</mtext></msub><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo><mo>=</mo><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><munder><mi>max</mi><mrow><mi>a</mi><mo>'</mo></mrow></munder><mo>&nbsp;</mo><msub><mi>Q</mi><mi mathvariant="bold">θ</mi></msub><mo>(</mo><mi>s</mi><mo>'</mo><mo>,</mo><mi>a</mi><mo>'</mo><mo>)</mo></math></span></span><script type="math/mml" id="MathJax-Element-171"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>Q</mi><mtext>target</mtext></msub><mo>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo><mo>=</mo><mi>r</mi><mo>+</mo><mi>γ</mi><mo>·</mo><munder><mi>max</mi><mrow><mi>a</mi><mo>'</mo></mrow></munder><mo> </mo><msub><mi>Q</mi><mi mathvariant="bold">θ</mi></msub><mo>(</mo><mi>s</mi><mo>'</mo><mo>,</mo><mi>a</mi><mo>'</mo><mo>)</mo></math></script></div>

<p>With this target Q-Value, we can run a training step using any Gradient Descent algorithm. Specifically, we generally try to minimize the squared error between the estimated Q-Value <em>Q</em>(<em>s</em>, <em>a</em>) and the target Q-Value (or the Huber loss to reduce the algorithm’s sensitivity to large errors). And that’s all for the basic Deep Q-Learning algorithm! Let’s see how to implement it to solve the CartPole environment.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Implementing Deep Q-Learning"><div class="sect1" id="idm46263484754440">
<h1>Implementing Deep Q-Learning</h1>

<p>The first thing we need is a Deep Q-Network. In theory, you need a neural net that takes a state-action pair and outputs an approximate Q-Value, but in practice it’s much more efficient to use a neural net that takes a state and outputs one approximate Q-Value for each possible action. To solve the CartPole environment, we do not need a very complicated neural net; a couple hidden layers will do:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">env</code> <code class="o">=</code> <code class="n">gym</code><code class="o">.</code><code class="n">make</code><code class="p">(</code><code class="s2">"CartPole-v0"</code><code class="p">)</code>
<code class="n">input_shape</code> <code class="o">=</code> <code class="p">[</code><code class="mi">4</code><code class="p">]</code> <code class="c1"># == env.observation_space.shape</code>
<code class="n">n_outputs</code> <code class="o">=</code> <code class="mi">2</code> <code class="c1"># == env.action_space.n</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"elu"</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="n">input_shape</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"elu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="n">n_outputs</code><code class="p">)</code>
<code class="p">])</code></pre>

<p>To select an action using this DQN, we pick the action with the largest predicted Q-value. To ensure that the agent explores the environment, we will use an <em>ε</em>-greedy policy (i.e., we will choose a random action with probability <em>ε</em>):</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">epsilon_greedy_policy</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">epsilon</code><code class="o">=</code><code class="mi">0</code><code class="p">):</code>
    <code class="k">if</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">rand</code><code class="p">()</code> <code class="o">&lt;</code> <code class="n">epsilon</code><code class="p">:</code>
        <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>
    <code class="k">else</code><code class="p">:</code>
        <code class="n">Q_values</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">state</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">])</code>
        <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">Q_values</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code></pre>

<p>Instead of training the DQN based only on the latest experiences, we will store all experiences in a <em>replay buffer</em> (or <em>replay memory</em>), and we will sample a random training batch from it at each training iteration. This helps reduce the correlations between the experiences in a training batch, which tremendously helps training. For this, we will just use a deque list:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">collections</code> <code class="kn">import</code> <code class="n">deque</code>

<code class="n">replay_buffer</code> <code class="o">=</code> <code class="n">deque</code><code class="p">(</code><code class="n">maxlen</code><code class="o">=</code><code class="mi">2000</code><code class="p">)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>A deque is a linked list, where each element points to the next one and to the previous one. It makes inserting and deleting items very fast, but the longer the deque is, the slower random access will be. If you need a very large replay buffer, use a circular buffer; see the notebook for an implementation.</p>
</div>

<p>Each experience will be composed of five elements: a state, the action the agent took, the resulting reward, the next state it reached, and finally a boolean indicating whether the episode ended at that point (<code>done</code>). We will need a small function to sample a random batch of experiences from the replay buffer. It will return five NumPy arrays corresponding to the five experience elements:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">sample_experiences</code><code class="p">(</code><code class="n">batch_size</code><code class="p">):</code>
    <code class="n">indices</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">replay_buffer</code><code class="p">),</code> <code class="n">size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">)</code>
    <code class="n">batch</code> <code class="o">=</code> <code class="p">[</code><code class="n">replay_buffer</code><code class="p">[</code><code class="n">index</code><code class="p">]</code> <code class="k">for</code> <code class="n">index</code> <code class="ow">in</code> <code class="n">indices</code><code class="p">]</code>
    <code class="n">states</code><code class="p">,</code> <code class="n">actions</code><code class="p">,</code> <code class="n">rewards</code><code class="p">,</code> <code class="n">next_states</code><code class="p">,</code> <code class="n">dones</code> <code class="o">=</code> <code class="p">[</code>
        <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">experience</code><code class="p">[</code><code class="n">field_index</code><code class="p">]</code> <code class="k">for</code> <code class="n">experience</code> <code class="ow">in</code> <code class="n">batch</code><code class="p">])</code>
        <code class="k">for</code> <code class="n">field_index</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">5</code><code class="p">)]</code>
    <code class="k">return</code> <code class="n">states</code><code class="p">,</code> <code class="n">actions</code><code class="p">,</code> <code class="n">rewards</code><code class="p">,</code> <code class="n">next_states</code><code class="p">,</code> <code class="n">dones</code></pre>

<p>Let’s also create a function that will play a single step using the <em>ε</em>-greedy policy, then store the resulting experience in the replay buffer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">play_one_step</code><code class="p">(</code><code class="n">env</code><code class="p">,</code> <code class="n">state</code><code class="p">,</code> <code class="n">epsilon</code><code class="p">):</code>
    <code class="n">action</code> <code class="o">=</code> <code class="n">epsilon_greedy_policy</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">epsilon</code><code class="p">)</code>
    <code class="n">next_state</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">info</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">action</code><code class="p">)</code>
    <code class="n">replay_buffer</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="n">state</code><code class="p">,</code> <code class="n">action</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">next_state</code><code class="p">,</code> <code class="n">done</code><code class="p">))</code>
    <code class="k">return</code> <code class="n">next_state</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">info</code></pre>

<p>Finally, let’s create one last function that will sample a batch of experiences from the replay buffer and train the DQN by performing a single Gradient Descent on this batch:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">batch_size</code> <code class="o">=</code> <code class="mi">32</code>
<code class="n">discount_factor</code> <code class="o">=</code> <code class="mf">0.95</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">1e-3</code><code class="p">)</code>
<code class="n">loss_fn</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">losses</code><code class="o">.</code><code class="n">mean_squared_error</code>

<code class="k">def</code> <code class="nf">training_step</code><code class="p">(</code><code class="n">batch_size</code><code class="p">):</code>
    <code class="n">experiences</code> <code class="o">=</code> <code class="n">sample_experiences</code><code class="p">(</code><code class="n">batch_size</code><code class="p">)</code>
    <code class="n">states</code><code class="p">,</code> <code class="n">actions</code><code class="p">,</code> <code class="n">rewards</code><code class="p">,</code> <code class="n">next_states</code><code class="p">,</code> <code class="n">dones</code> <code class="o">=</code> <code class="n">experiences</code>
    <code class="n">next_Q_values</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">next_states</code><code class="p">)</code>
    <code class="n">max_next_Q_values</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">next_Q_values</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
    <code class="n">target_Q_values</code> <code class="o">=</code> <code class="p">(</code><code class="n">rewards</code> <code class="o">+</code>
                       <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">dones</code><code class="p">)</code> <code class="o">*</code> <code class="n">discount_factor</code> <code class="o">*</code> <code class="n">max_next_Q_values</code><code class="p">)</code>
    <code class="n">mask</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">one_hot</code><code class="p">(</code><code class="n">actions</code><code class="p">,</code> <code class="n">n_outputs</code><code class="p">)</code>
    <code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">GradientTape</code><code class="p">()</code> <code class="k">as</code> <code class="n">tape</code><code class="p">:</code>
        <code class="n">all_Q_values</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">states</code><code class="p">)</code>
        <code class="n">Q_values</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_sum</code><code class="p">(</code><code class="n">all_Q_values</code> <code class="o">*</code> <code class="n">mask</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">keepdims</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_mean</code><code class="p">(</code><code class="n">loss_fn</code><code class="p">(</code><code class="n">target_Q_values</code><code class="p">,</code> <code class="n">Q_values</code><code class="p">))</code>
    <code class="n">grads</code> <code class="o">=</code> <code class="n">tape</code><code class="o">.</code><code class="n">gradient</code><code class="p">(</code><code class="n">loss</code><code class="p">,</code> <code class="n">model</code><code class="o">.</code><code class="n">trainable_variables</code><code class="p">)</code>
    <code class="n">optimizer</code><code class="o">.</code><code class="n">apply_gradients</code><code class="p">(</code><code class="nb">zip</code><code class="p">(</code><code class="n">grads</code><code class="p">,</code> <code class="n">model</code><code class="o">.</code><code class="n">trainable_variables</code><code class="p">))</code></pre>

<p>Let’s go through this code:</p>

<ul>
<li>
<p>First we define some hyperparameters, and we create the optimizer and the loss function.</p>
</li>
<li>
<p>Then we create the <code>training_step()</code> function. It starts by sampling a batch of experiences, then it uses the DQN to predict the Q-Value for each possible action in each experience’s next state. Since we assume that the agent will be playing optimally, we only keep the maximum Q-Value for each next state. Next, we use <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#target_q_value_equation">Equation 18-7</a> to compute the target Q-Value for each experience’s state-action pair.</p>
</li>
<li>
<p>Next, we want to use the DQN to compute the Q-Value for each experienced state-action pair. However, the DQN will also output the Q-Values for the other possible actions, not just for the action that was actually chosen by the agent. So we need to mask out all the Q-Values we do not need. The <code>tf.one_hot()</code> function makes it easy to convert an array of action indices into such a mask. For example, if the first three experiences contain actions 1, 1, 0, respectively, then the mask will start with <code>[[0, 1], [0, 1], [1, 0],...]</code>. We can then multiply the DQN’s output with this mask, and this will zero out all the Q-Values we do not want. We then sum over axis 1 to get rid of all the zeros, keeping only the Q-Values of the experienced state-action pairs. This gives us the <code>Q_values</code> tensor, containing one predicted Q-Value for each experience in the batch.</p>
</li>
<li>
<p>Then we compute the loss: it is the mean squared error between the target and predicted Q-Values for the experienced state-action pairs.</p>
</li>
<li>
<p>Finally, we perform a Gradient Descent step to minimize the loss with regard to the model’s trainable variables.</p>
</li>
</ul>

<p>This was the hardest part. Now training the model is straightforward:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">episode</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">600</code><code class="p">):</code>
    <code class="n">obs</code> <code class="o">=</code> <code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">step</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">200</code><code class="p">):</code>
        <code class="n">epsilon</code> <code class="o">=</code> <code class="nb">max</code><code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">episode</code> <code class="o">/</code> <code class="mi">500</code><code class="p">,</code> <code class="mf">0.01</code><code class="p">)</code>
        <code class="n">obs</code><code class="p">,</code> <code class="n">reward</code><code class="p">,</code> <code class="n">done</code><code class="p">,</code> <code class="n">info</code> <code class="o">=</code> <code class="n">play_one_step</code><code class="p">(</code><code class="n">env</code><code class="p">,</code> <code class="n">obs</code><code class="p">,</code> <code class="n">epsilon</code><code class="p">)</code>
        <code class="k">if</code> <code class="n">done</code><code class="p">:</code>
            <code class="k">break</code>
    <code class="k">if</code> <code class="n">episode</code> <code class="o">&gt;</code> <code class="mi">50</code><code class="p">:</code>
        <code class="n">training_step</code><code class="p">(</code><code class="n">batch_size</code><code class="p">)</code></pre>

<p>We run 600 episodes, each for a maximum of 200 steps. At each step, we first compute the <code>epsilon</code> value for the <em>ε</em>-greedy policy: it will go from 1 down to 0.01, linearly, in a bit under 500 episodes. Then we call the <code>play_one_step()</code> function, which will use the <em>ε</em>-greedy policy to pick an action, then execute it and record the experience in the replay buffer. If the episode is done, we exit the loop. Finally, if we are past the 50th episode, we call the <code>training_step()</code> function to train the model on one batch sampled from the replay buffer. The reason we play 50 episodes without training is to give the replay buffer some time to fill up (if we don’t wait enough, then there will not be enough diversity in the replay buffer). And that’s it, we just implemented the Deep Q-Learning algorithm! <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#dqn_rewards_plot">Figure&nbsp;18-10</a> shows the total rewards the agent got during each episode.</p>

<figure><div id="dqn_rewards_plot" class="figure">
<img src="./Chapter18_files/mls2_1810.png" alt="mls2 1810" width="1440" height="683" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1810.png">
<h6><span class="label">Figure 18-10. </span>Learning curve of the Deep Q-Learning algorithm</h6>
</div></figure>

<p>As you can see, the algorithm made no apparent progress at all for almost 300 episodes (in part because <em>ε</em> was very high at the beginning), then its performance suddenly sky-rocketed up to 200 (which is the maximum possible performance in this environment). That’s great news, the algorithm worked fine, and it actually ran much faster than the Policy Gradient algorithm! But wait… just a few episodes later, it forgot everything it knew, and its performance dropped below 25! This is called <em>catastrophic forgetting</em>, and it is one of the big problems facing virtually all RL algorithms: as the agent explores the environment, it updates its policy, but what it learns in one part of the environment may break what it learned earlier in other parts of the environment. The experiences are quite correlated, and the learning environment keeps changing: this is not ideal for Gradient Descent! If you increase the size of the replay buffer, the algorithm will be less subject to this problem. And reducing the learning rate may also help. But the truth is, Reinforcement Learning is hard: training is often unstable, and you may need to try many hyperparameter values and random seeds before you find a combination that works well. For example, if you try changing the number of neurons per layer in the code above from 32 to 30 or 34, the performance will never go above 100 (the DQN may be more stable with 1 hidden layer instead of 2).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Reinforcement Learning is notoriously difficult, largely because of the training instabilities and the huge sensitivity to the choice of hyperparameter values and random seeds.<sup><a data-type="noteref" id="idm46263483703656-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263483703656">13</a></sup> As the researcher Andrej Karpathy put it: “[supervised learning] wants to work. […] RL must be forced to work”. You will need time, patience, perseverance and perhaps a bit of luck too. This is a major reason RL is not as widely adopted as regular Deep Learning (e.g., ConvNets). But there are a few real-world applications, beyond AlphaGo and Atari Games: for example, Google uses RL to optimize its datacenter costs, it is used in some robotics applications, for hyperparameter tuning, and in recommender systems.</p>
</div>

<p>You might wonder why we didn’t plot the loss. It turns out that loss is a poor indicator of the model’s performance: the loss can go down and yet the agent might perform worse (e.g., this can happen when the agent gets stuck in one small region of the environment, and the DQN starts overfitting this region). Conversely, the loss can go up and yet the agent might perform better (e.g., if the DQN was underestimating the Q-Values, and it starts correctly increasing its predictions, the agent will likely perform better, getting more rewards, and yet the loss might increase because the DQN also sets the targets, which will be larger too).</p>

<p>The basic Deep Q-Learning algorithm we used so far would be too unstable to learn to play Atari games. So how did DeepMind do it? Well, they tweaked the algorithm!</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Deep Q-Learning Variants"><div class="sect1" id="idm46263484435480">
<h1>Deep Q-Learning Variants</h1>

<p>Let’s look at a few variants of the Deep Q-Learning algorithm that can stabilize and speed up training.</p>








<section data-type="sect2" data-pdf-bookmark="Fixed Q-Value Targets"><div class="sect2" id="idm46263483698568">
<h2>Fixed Q-Value Targets</h2>

<p>In the basic Deep Q-Learning algorithm, the model is used both to make predictions and to set its own targets. This can lead to a situation analog to a dog chasing its own tail. This feedback loop can make the network unstable: it can diverge, oscillate, freeze, and so on. To solve this problem, in their 2013 paper, researchers used two DQNs instead of one: the first is the <em>online model</em>, that learns at each step and is used to move the agent around. The other is the <em>target model</em> used only to define the targets. The target model is just a clone of the online model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">target</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">clone_model</code><code class="p">(</code><code class="n">model</code><code class="p">)</code>
<code class="n">target</code><code class="o">.</code><code class="n">set_weights</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">get_weights</code><code class="p">())</code></pre>

<p>Then in the <code>training_step()</code> function, we just need to change one line to use the target model instead of the online model when computing the Q-Values of the next states:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">next_Q_values</code> <code class="o">=</code> <code class="n">target</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">next_states</code><code class="p">)</code></pre>

<p>Finally, in the training loop, we must copy the weights of the online model to the target model, at regular intervals (e.g., every 50 episodes):</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">if</code> <code class="n">episode</code> <code class="o">%</code> <code class="mi">50</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
    <code class="n">target</code><code class="o">.</code><code class="n">set_weights</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">get_weights</code><code class="p">())</code></pre>

<p>Since the target model is updated much less often than the online model, the Q-Value targets are more stable, the feedback loop we discussed earlier is dampened, and its effects are less severe. This approach was one of DeepMind researchers’ main contributions in their 2013 paper, allowing agents to learn to play Atari games from raw pixels. To stabilize training, they used a tiny learning rate of 0.00025, they updated the target model only every 10,000 steps (instead of 50 in the previous code example), and they used a very large replay buffer of 1 million experiences. They decreased <code>epsilon</code> very slowly, from 1 to 0.1 in 1 million steps, and they let the algorithm run for 50 million steps.</p>

<p>Later in this chapter, we will use the TF-Agents library to train a DQN agent to play Breakout using these hyperparameters, but before we get there, let’s take a look at another DQN variant that managed to beat the state of the art once more.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Double DQN"><div class="sect2" id="idm46263483544648">
<h2>Double DQN</h2>

<p>In a <a href="https://homl.info/doubledqn">2015 paper</a>,<sup><a data-type="noteref" id="idm46263483542728-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263483542728">14</a></sup> DeepMind researchers tweaked their DQN algorithm, increasing its performance and somewhat stabilizing training. They called this variant <em>Double DQN</em>. Here is what they did: first, they noted that the target network is prone to overestimating Q-Values. Indeed, suppose all actions are equally good: the Q-Values estimated by the target model should be identical, but since they are approximations, some may be slightly greater than the others, by pure chance. The target model will always select the largest Q-Value, which will be slightly greater than the mean Q-Value, most likely overestimating the true Q-Value (a bit like counting the height of the tallest random wave when measuring the depth of a pool). To fix this, they proposed using the online model instead of the target model when selecting the best actions for the next states, and using the target model only to estimate the Q-Values for these best actions. Here is the updated <code>training_step()</code> function:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">training_step</code><code class="p">(</code><code class="n">batch_size</code><code class="p">):</code>
    <code class="n">experiences</code> <code class="o">=</code> <code class="n">sample_experiences</code><code class="p">(</code><code class="n">batch_size</code><code class="p">)</code>
    <code class="n">states</code><code class="p">,</code> <code class="n">actions</code><code class="p">,</code> <code class="n">rewards</code><code class="p">,</code> <code class="n">next_states</code><code class="p">,</code> <code class="n">dones</code> <code class="o">=</code> <code class="n">experiences</code>
    <code class="n">next_Q_values</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">next_states</code><code class="p">)</code>
    <code class="n">best_next_actions</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">next_Q_values</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
    <code class="n">next_mask</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">one_hot</code><code class="p">(</code><code class="n">best_next_actions</code><code class="p">,</code> <code class="n">n_outputs</code><code class="p">)</code><code class="o">.</code><code class="n">numpy</code><code class="p">()</code>
    <code class="n">next_best_Q_values</code> <code class="o">=</code> <code class="p">(</code><code class="n">target</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">next_states</code><code class="p">)</code> <code class="o">*</code> <code class="n">next_mask</code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
    <code class="n">target_Q_values</code> <code class="o">=</code> <code class="p">(</code><code class="n">rewards</code> <code class="o">+</code>
                       <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">dones</code><code class="p">)</code> <code class="o">*</code> <code class="n">discount_factor</code> <code class="o">*</code> <code class="n">next_best_Q_values</code><code class="p">)</code>
    <code class="n">mask</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">one_hot</code><code class="p">(</code><code class="n">actions</code><code class="p">,</code> <code class="n">n_outputs</code><code class="p">)</code>
    <code class="p">[</code><code class="o">...</code><code class="p">]</code> <code class="c1"># the rest is the same as earlier</code></pre>

<p>Just a few months later, another improvement to the DQN algorithm was proposed.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Prioritized Experience Replay"><div class="sect2" id="idm46263483537256">
<h2>Prioritized Experience Replay</h2>

<p>Instead of sampling experiences <em>uniformly</em> from the replay buffer, why not sample important experiences more frequently? This is called <em>importance sampling</em> (IS) or <em>prioritized experience replay</em> (PER), and it was introduced in a <a href="https://homl.info/prioreplay">2015 paper</a><sup><a data-type="noteref" id="idm46263483419400-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263483419400">15</a></sup> by DeepMind researchers (once again!).</p>

<p>More specifically, experiences are considered “important” if they are likely to lead to fast learning progress. But how can we estimate this? One reasonable approach is to measure the magnitude of the TD error <em>δ</em> = <em>r</em> + <em>γ</em>·<em>V</em>(<em>s</em>′) - <em>V</em>(<em>s</em>). A large TD error indicates that a transition (<em>s</em>, <em>r</em>, <em>s</em>′) is very surprising, and thus probably worth learning from.<sup><a data-type="noteref" id="idm46263483412952-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263483412952">16</a></sup> When an experience is recorded in the replay buffer, its priority is set to a very large value, to ensure that it gets sampled at least once. However, once it is sampled (and every time it is sampled), the TD error <em>δ</em> is computed, and this experience’s priority is set to <em>p</em> = |<em>δ</em>| (plus a small constant to ensure that every experience has a non-zero probability of being sampled). The probability <em>P</em> of sampling an experience with priority <em>p</em> is proportional to <em>p</em><sup><em>ζ</em></sup>, where <em>ζ</em> is a hyperparameter that controls how greedy we want importance sampling to be: when <em>ζ</em> = 0, we just get uniform sampling, and when <em>ζ</em> = 1, we get full-blown importance sampling. In the paper, the authors used <em>ζ</em> = 0.6, but the optimal value will depend on the task.</p>

<p>There’s one catch, though: since the samples will be biased toward important experiences, we must compensate for this bias during training by downweighting the experiences according to their importance, or else the model will just overfit the important experiences. To be clear, we want important experiences to be sampled more often, but this also means we must give them a lower weight during training. To do this, we define each experience’s training weight as <em>w</em> = (<em>n</em> <em>P</em>)<sup>–β</sup>, where <em>n</em> is the number of experiences in the replay buffer, and <em>β</em> is a hyperparameter that controls how much we want to compensate for the importance sampling bias (0 means not at all, while 1 means entirely). In the paper, the authors used <em>β</em> = 0.4 at the beginning of training and linearly increased it to <em>β</em> = 1 by the end of training. Again, the optimal value will depend on the task, but if you increase one, you will usually want to increase the other as well.</p>

<p>Now let’s look at one last important variant of the DQN algorithm.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Dueling DQN"><div class="sect2" id="idm46263483402168">
<h2>Dueling DQN</h2>

<p>The <em>Dueling DQN</em> algorithm (DDQN, not to be confused with Double DQN, although both techniques can easily be combined) was introduced in yet another <a href="https://homl.info/ddqn">2015 paper</a><sup><a data-type="noteref" id="idm46263483398872-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263483398872">17</a></sup> by DeepMind researchers. To understand how it works, we must first note that the Q-Value of a state-action pair (<em>s</em>, <em>a</em>) can be expressed as <em>Q</em>(<em>s</em>, <em>a</em>) = <em>V</em>(<em>s</em>) + <em>A</em>(<em>s</em>, <em>a</em>), where <em>V</em>(<em>s</em>) is the value of state <em>s</em> and <em>A</em>(<em>s</em>, <em>a</em>) is the <em>advantage</em> of taking the action <em>a</em> in state <em>s</em>, compared to all other possible actions in that state. Moreover, the value of a state is equal to the Q-Value of the best action <em>a</em><sup>*</sup> for that state (since we assume the optimal policy will pick the best action), so <em>V</em>(<em>s</em>) = <em>Q</em>(<em>s</em>, <em>a</em><sup>*</sup>), which implies that <em>A</em>(<em>s</em>, <em>a</em><sup>*</sup>) = 0. In a Dueling DQN, the model estimates both the value of the state and the advantage of each possible action. Since the best action should have an advantage of 0, the model subtracts the maximum predicted advantage from all predicted advantages. Here is a simple Dueling DQN model, implemented using the Functional API:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">K</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">backend</code>
<code class="n">input_states</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Input</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="mi">4</code><code class="p">])</code>
<code class="n">hidden1</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"elu"</code><code class="p">)(</code><code class="n">input_states</code><code class="p">)</code>
<code class="n">hidden2</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"elu"</code><code class="p">)(</code><code class="n">hidden1</code><code class="p">)</code>
<code class="n">state_values</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">)(</code><code class="n">hidden2</code><code class="p">)</code>
<code class="n">raw_advantages</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="n">n_outputs</code><code class="p">)(</code><code class="n">hidden2</code><code class="p">)</code>
<code class="n">advantages</code> <code class="o">=</code> <code class="n">raw_advantages</code> <code class="o">-</code> <code class="n">K</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">raw_advantages</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">keepdims</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">Q_values</code> <code class="o">=</code> <code class="n">state_values</code> <code class="o">+</code> <code class="n">advantages</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">Model</code><code class="p">(</code><code class="n">inputs</code><code class="o">=</code><code class="p">[</code><code class="n">input_states</code><code class="p">],</code> <code class="n">outputs</code><code class="o">=</code><code class="p">[</code><code class="n">Q_values</code><code class="p">])</code></pre>

<p>The rest of the algorithm is just the same as earlier. In fact, you can build a Double Dueling DQN and combine it with Prioritized Experience Replay! More generally, many RL techniques can be combined, as DeepMind demonstrated in a <a href="https://homl.info/rainbow">2017 paper</a>.<sup><a data-type="noteref" id="idm46263483381784-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263483381784">18</a></sup> The authors of the paper combined six different techniques into an agent they called <em>Rainbow</em>, which largely outperformed the state of the art.</p>

<p>Unfortunately, implementing all of these techniques, debugging them, fine-tuning them, and of course training the models can require a huge amount of work. So instead of reinventing the wheel, it is often best to reuse scalable and well tested libraries, such as TF-Agents.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="The TF-Agents Library"><div class="sect1" id="idm46263483218072">
<h1>The TF-Agents Library</h1>

<p>The <a href="https://github.com/tensorflow/agents">TF-Agents library</a> is a Reinforcement Learning library based on TensorFlow, developed at Google and open sourced in 2018. Just like OpenAI gym, it provides many off-the-shelf environments, including wrappers for all OpenAI gym environments, plus it supports the pybullet library (3D physics simulation), DeepMind’s DM-control library (based on MuJoCo’s physics engine), and Unity’s ML-Agents library (simulating many 3D environments). It also implements many RL algorithms, including REINFORCE, DQN, and DDQN, as well as various RL components such as efficient replay buffers and metrics. It is fast, scalable, easy to use, and customizable: you can create your own environments and neural nets, and you can customize pretty much any component. In this section we will use TF-Agents to train an agent to play Breakout, the famous Atari game (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#breakout_plot">Figure&nbsp;18-11</a><sup><a data-type="noteref" id="idm46263483214264-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263483214264">19</a></sup>), using the DQN algorithm (you can easily switch to another algorithm if you prefer).</p>

<figure class="smallerfourty"><div id="breakout_plot" class="figure">
<img src="./Chapter18_files/mls2_1811.png" alt="mls2 1811" width="1440" height="1719" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1811.png">
<h6><span class="label">Figure 18-11. </span>The famous Breakout game</h6>
</div></figure>








<section data-type="sect2" data-pdf-bookmark="Installing TF-Agents"><div class="sect2" id="idm46263483210952">
<h2>Installing TF-Agents</h2>

<p>Let’s start by installing TF-Agents. This can be done using pip (as always, if you are using a virtual environment, make sure to activate it first; if not, you will need to use the <code>--user</code> option, or have administrator rights):</p>
<pre data-type="programlisting" data-code-language="shell-session"><code class="go">$ </code><strong><code class="go">python3 -m pip install --upgrade tf-agents</code></strong><code class="go">
</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>At the time of this writing, TF-Agents is still quite new and improving every day, so the API may change a bit by the time you read this but the big picture should remain the same, as well as most of the code. If anything breaks, I will update the Jupyter notebook accordingly, so make sure to check it out.</p>
</div>

<p>Next, let’s create a TF-Agents environment that will just wrap OpenAI gym’s Breakout environment. For this, you must first install OpenAI gym’s Atari dependencies:</p>
<pre data-type="programlisting" data-code-language="shell-session"><code class="go">$ </code><strong><code class="go">python3 -m pip install --upgrade 'gym[atari]'</code></strong><code class="go">
</code></pre>

<p>Among other libraries, this command will install atari-py, which is a Python interface for the Arcade Learning Environment (ALE), a framework built on top of the Atari 2600 emulator Stella.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="TF-Agents Environments"><div class="sect2" id="idm46263483177640">
<h2>TF-Agents Environments</h2>

<p>If everything went well, you should be able to import TF-Agents and create a Breakout environment:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">tf_agents.environments</code> <code class="kn">import</code> <code class="n">suite_gym</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">env</code> <code class="o">=</code> <code class="n">suite_gym</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s">"Breakout-v4"</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">env</code>
<code class="go">&lt;tf_agents.environments.wrappers.TimeLimit at 0x10c523c18&gt;</code></pre>

<p>This is just a wrapper around an OpenAI gym environment, which you can access through the <code>gym</code> attribute:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">env</code><code class="o">.</code><code class="n">gym</code>
<code class="go">&lt;gym.envs.atari.atari_env.AtariEnv at 0x24dcab940&gt;</code></pre>

<p>TF-Agents environments are very similar to OpenAI gym environments, but there are a few differences. First, the <code>reset()</code> method does not return an observation; instead it returns a <code>TimeStep</code> object that wraps the observation, as well as some extra information:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">env</code><code class="o">.</code><code class="n">reset</code><code class="p">()</code>
<code class="go">TimeStep(step_type=array(0, dtype=int32),</code>
<code class="go">         reward=array(0., dtype=float32),</code>
<code class="go">         discount=array(1., dtype=float32),</code>
<code class="go">         observation=array([[[0., 0., 0.], [0., 0., 0.],...]]], dtype=float32))</code></pre>

<p>Moreover, the <code>step()</code> method returns a <code>TimeStep</code> object as well:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">env</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code> <code class="c"># Fire</code>
<code class="go">TimeStep(step_type=array(1, dtype=int32),</code>
<code class="go">         reward=array(0., dtype=float32),</code>
<code class="go">         discount=array(1., dtype=float32),</code>
<code class="go">         observation=array([[[0., 0., 0.], [0., 0., 0.],...]]], dtype=float32))</code></pre>

<p>The <code>reward</code> and the <code>observation</code> attributes are self-explanatory, and they are the same as for OpenAI gym (except the <code>reward</code> is represented as a NumPy array). The <code>step_type</code> attribute is equal to 0 for the first time step in the episode, 1 for intermediate time steps, and 2 for the final time step. You can call the time step’s <code>is_last()</code> method to check whether it is the final one or not. Lasly, the <code>discount</code> attribute indicates the discount factor to use at this time step. In this example, it is equal to 1, so there will be no discount at all. You can define the discount factor by setting the <code>discount</code> parameter when loading the environment.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>At any time, you can access the environment’s current time step by calling its <code>current_time_step()</code> method.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Environment Specifications"><div class="sect2" id="idm46263483177048">
<h2>Environment Specifications</h2>

<p>Conveniently, a TF-Agents environment provides the specifications of the observations, actions and time steps, including their shapes, data types, and names, as well as their minimum and maximum values:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">env</code><code class="o">.</code><code class="n">observation_spec</code><code class="p">()</code>
<code class="go">BoundedArraySpec(shape=(210, 160, 3), dtype=dtype('float32'), name=None,</code>
<code class="go">                 minimum=[[[0. 0. 0.], [0. 0. 0.],...]],</code>
<code class="go">                 maximum=[[[255., 255., 255.], [255., 255., 255.], ...]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">env</code><code class="o">.</code><code class="n">action_spec</code><code class="p">()</code>
<code class="go">BoundedArraySpec(shape=(), dtype=dtype('int64'), name=None,</code>
<code class="go">                 minimum=0, maximum=3)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">env</code><code class="o">.</code><code class="n">time_step_spec</code><code class="p">()</code>
<code class="go">TimeStep(step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'),</code>
<code class="go">         reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),</code>
<code class="go">         discount=BoundedArraySpec(shape=(), ..., minimum=0.0, maximum=1.0),</code>
<code class="go">         observation=BoundedArraySpec(shape=(210, 160, 3), ...))</code></pre>

<p>As you can see, the observations are simply screenshots of the Atari screen, represented as NumPy arrays of shape <code>[210, 160, 3]</code>. To render an environment, you can call <code>env.render(mode="human")</code>, and if you want to get back the image in the form of a NumPy array, just call <code>env.render(mode="rgb_array")</code> (contrary to OpenAI gym, this is the default mode).</p>

<p>There are four actions available. Gym’s Atari environments have an extra method that you can call to know what each action corresponds to:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">env</code><code class="o">.</code><code class="n">gym</code><code class="o">.</code><code class="n">get_action_meanings</code><code class="p">()</code>
<code class="go">['NOOP', 'FIRE', 'RIGHT', 'LEFT']</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>Specs can be instances of a specification class, nested lists, or dictionaries of specs. If the specification is nested, then the specified object must match the specification’s nested structure. For example, if the observation spec is <code>{"sensors": ArraySpec(shape=[2]), "camera": ArraySpec(shape=[100, 100])}</code>, then a valid observation would be <code>{"sensors": np.array([1.5, 3.5]), "camera": np.array(...)}</code>. The <code>tf.nest</code> package provides tools to handle such nested structures (a.k.a. <em>nests</em>).</p>
</div>

<p>The observations are quite large, so we will downsample them and also convert them to grayscale. This will speed up training and use less RAM. For this, we can use an environment wrapper.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Environment Wrappers and Atari Preprocessing"><div class="sect2" id="idm46263482891368">
<h2>Environment Wrappers and Atari Preprocessing</h2>

<p>TF-Agents provides several environment wrappers in the <code>tf_agents.environments.wrappers</code> package. As their name suggests, they wrap an environment, forwarding every call to it, but also adding some extra functionality. Here are some of the available wrappers:</p>
<dl>
<dt><code>ActionClipWrapper</code></dt>
<dd>
<p>Clips the actions to the action spec.</p>
</dd>
<dt><code>ActionDiscretizeWrapper</code></dt>
<dd>
<p>Quantizes a continuous action space to a discrete action space. For example, if the original environment’s action space is the continuous range from -1.0 to +1.0, but you want to use an algorithm that only supports discrete action spaces, such as a DQN, then you can wrap the environment using <code>discrete_env = ActionDiscretizeWrapper(env, num_actions=5)</code>, and the new <code>discrete_env</code> will have a discrete action space with five possible actions: 0, 1, 2, 3, 4. These actions correspond to the actions -1.0, -0.5, 0.0, 0.5, and 1.0 in the original environment.</p>
</dd>
<dt><code>ActionRepeat</code></dt>
<dd>
<p>Repeats each action over <em>n</em> steps, while accumulating the rewards. In many environments, this can speed up training significantly.</p>
</dd>
<dt><code>RunStats</code></dt>
<dd>
<p>Records environment statistics such as the number of steps and the number of episodes.</p>
</dd>
<dt><code>TimeLimit</code></dt>
<dd>
<p>Interrupts the environment if it runs for longer than a maximum number of steps.</p>
</dd>
<dt><code>VideoWrapper</code></dt>
<dd>
<p>Records a video of the environment.</p>
</dd>
</dl>

<p>To create a wrapped environment, you must create a wrapper, passing the wrapped environment to the constructor. That’s all! For example, the following code will wrap our environment in an <code>ActionRepeat</code> wrapper so that every action is repeated four times:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tf_agents.environments.wrappers</code> <code class="kn">import</code> <code class="n">ActionRepeat</code>

<code class="n">repeating_env</code> <code class="o">=</code> <code class="n">ActionRepeat</code><code class="p">(</code><code class="n">env</code><code class="p">,</code> <code class="n">times</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code></pre>

<p>OpenAI gym has some environment wrappers of its own in the <code>gym.wrappers</code> package. They are meant to wrap Gym environments, not TF-Agents environments, so to use them, you must first wrap the Gym environment with the Gym wrappers before you wrap the resulting environment with a TF-Agents wrapper. The <code>suite_gym.wrap_env()</code> function will do this for you, provided you give it a Gym environment and a list of Gym wrappers and/or a list of TF-Agents wrappers. Alternatively, the <code>suite_gym.load()</code> function will both create the Gym environment and wrap it for you, if you give it some wrappers. Each wrapper will be created without any arguments, so if you want to set some arguments, you must pass a <code>lambda</code>. For example, the following code creates a Breakout environment that will run for a maximum of 10,000 steps during each episode, and each action will be repeated four times:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">gym.wrappers</code> <code class="kn">import</code> <code class="n">TimeLimit</code>

<code class="n">limited_repeating_env</code> <code class="o">=</code> <code class="n">suite_gym</code><code class="o">.</code><code class="n">load</code><code class="p">(</code>
    <code class="s2">"Breakout-v4"</code><code class="p">,</code>
    <code class="n">gym_env_wrappers</code><code class="o">=</code><code class="p">[</code><code class="k">lambda</code> <code class="n">env</code><code class="p">:</code> <code class="n">TimeLimit</code><code class="p">(</code><code class="n">env</code><code class="p">,</code> <code class="n">max_episode_steps</code><code class="o">=</code><code class="mi">10000</code><code class="p">)],</code>
    <code class="n">env_wrappers</code><code class="o">=</code><code class="p">[</code><code class="k">lambda</code> <code class="n">env</code><code class="p">:</code> <code class="n">ActionRepeat</code><code class="p">(</code><code class="n">env</code><code class="p">,</code> <code class="n">times</code><code class="o">=</code><code class="mi">4</code><code class="p">)])</code></pre>

<p>For Atari environments, some standard preprocessing steps are applied in most papers that use them, so TF-Agents provides a handy <code>AtariPreprocessing</code> wrapper that implements them. Here is the list of preprocessing steps it supports:</p>
<dl>
<dt>Grayscale and downsampling</dt>
<dd>
<p>Observations are converted to grayscale and downsampled (by default to 84 × 84 pixels).</p>
</dd>
<dt>Max-pooling</dt>
<dd>
<p>The last two frames of the game are max-pooled using a 1 × 1 filter. This is to remove the flickering that occurs in some Atari games due to the limited number of sprites that the Atari 2600 could display in each frame.</p>
</dd>
<dt>Frame skipping</dt>
<dd>
<p>The agent only gets to see every <em>n</em> frames of the game (by default <em>n</em> = 4), and its actions are repeated for each frame, collecting all the rewards. This effectively speeds up the game from the perspective of the agent, and it also speeds up training because rewards are less delayed.</p>
</dd>
<dt>End on life lost</dt>
<dd>
<p>In some games, the rewards are just based on the score, so the agent gets no immediate penalty for losing a life. One solution is to end the game immediately whenever a life is lost. There is some debate over the actual benefits of this strategy, so it is off by default.</p>
</dd>
</dl>

<p>Since the default Atari environment already applies random frame skipping and max-pooling, we will need to load the raw, nonskipping variant called <code>"BreakoutNoFrameskip-v4"</code>. Moreover, a single frame from the Breakout game is insufficient to know the direction and speed of the ball, which will make it very difficult for the Agent to play the game properly (unless it is an RNN agent, which preserves some internal state between steps). One way to handle this is to use an environment wrapper that will output observations composed of multiple frames stacked on top of each other along the channels dimension. This strategy is implemented by the <code>FrameStack4</code> wrapper, which returns stacks of four frames. Let’s create the wrapped Atari environment!</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tf_agents.environments</code> <code class="kn">import</code> <code class="n">suite_atari</code>
<code class="kn">from</code> <code class="nn">tf_agents.environments.atari_preprocessing</code> <code class="kn">import</code> <code class="n">AtariPreprocessing</code>
<code class="kn">from</code> <code class="nn">tf_agents.environments.atari_wrappers</code> <code class="kn">import</code> <code class="n">FrameStack4</code>

<code class="n">max_episode_steps</code> <code class="o">=</code> <code class="mi">27000</code> <code class="c1"># &lt;=&gt; 108k ALE frames since 1 step = 4 frames</code>
<code class="n">environment_name</code> <code class="o">=</code> <code class="s2">"BreakoutNoFrameskip-v4"</code>

<code class="n">env</code> <code class="o">=</code> <code class="n">suite_atari</code><code class="o">.</code><code class="n">load</code><code class="p">(</code>
    <code class="n">environment_name</code><code class="p">,</code>
    <code class="n">max_episode_steps</code><code class="o">=</code><code class="n">max_episode_steps</code><code class="p">,</code>
    <code class="n">gym_env_wrappers</code><code class="o">=</code><code class="p">[</code><code class="n">AtariPreprocessing</code><code class="p">,</code> <code class="n">FrameStack4</code><code class="p">])</code></pre>

<p>The result of all this preprocessing is shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#preprocessed_breakout_plot">Figure&nbsp;18-12</a>. You can see that the resolution is much lower, but sufficient to play the game. Moreover, frames are stacked along the channels dimension, so red represents the frame from three steps ago, green is two steps ago, blue is the previous frame, and pink is the current frame.<sup><a data-type="noteref" id="idm46263482701496-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263482701496">20</a></sup> From this single observation, the agent can see that the ball is going toward the lower-left corner, and that it should continue to move the paddle to the left (as it did in the previous steps).</p>

<figure class="smallerfourty"><div id="preprocessed_breakout_plot" class="figure">
<img src="./Chapter18_files/mls2_1812.png" alt="mls2 1812" width="1440" height="1440" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1812.png">
<h6><span class="label">Figure 18-12. </span>Preprocessed Breakout observation</h6>
</div></figure>

<p>Lastly we can wrap the environment inside a <code>TFPyEnvironment</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tf_agents.environments.tf_py_environment</code> <code class="kn">import</code> <code class="n">TFPyEnvironment</code>

<code class="n">tf_env</code> <code class="o">=</code> <code class="n">TFPyEnvironment</code><code class="p">(</code><code class="n">env</code><code class="p">)</code></pre>

<p>This will make the environment usable from within a TensorFlow graph (under the hood, this class relies on <code>tf.py_function()</code>, which allows a graph to call arbitrary Python code). Thanks to the TFPyEnvironment class, TF-Agents supports both pure-Python environments and TensorFlow-based environments. More generally, TF-Agents supports and provides both pure-Python and TensorFlow-based components (agents, replay buffers, metrics, and so on).</p>

<p>Now that we have a nice Breakout environment, with all the appropriate preprocessing and TensorFlow support, we must create the DQN agent and the other components we will need to train it. Let’s look at the architecture of the system we will build.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Training Architecture"><div class="sect2" id="idm46263482928984">
<h2>Training Architecture</h2>

<p>A TF-Agents training program is usually split in two parts that run in parallel, as you can see in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#tf_agents_architecture_diagram">???</a>: on the left, a <em>driver</em> explores the <em>environment</em> using a <em>collect policy</em> to choose actions, and it collects <em>trajectories</em> (i.e., experiences), sending them to an <em>observer</em>, which saves them to a <em>replay buffer</em>; on the right, an <em>agent</em> pulls batches of trajectories from the replay buffer, trains some <em>networks</em>, which the collect policy uses. In short, the left part explores the environment and collects trajectories, while the right part learns and updates the collect policy.</p>

<p id="tf_agents_architecture_diagram">A typical TF-Agents training architecture
image::images/mls2_1813.png[]</p>

<p>This figure begs a few questions:</p>

<ul>
<li>
<p>Why are there multiple environments? Well, instead of exploring a single environment, you generally want the driver to explore multiple copies of the environment in parallel, taking advantage of the power of all your CPU cores, keeping the training GPUs busy, and providing less correlated trajectories to the training algorithm.</p>
</li>
<li>
<p>What is a <em>trajectory</em>? It is a concise representation of a <em>transition</em> from one time step to the next, or a sequence of consecutive transitions from time step <em>n</em> to time step <em>n</em> + <em>t</em>. The trajectories collected by the driver are passed to the observer, which saves them in the replay buffer, and they are later sampled by the agent and used for training.</p>
</li>
<li>
<p>Why do we need an observer? Can’t the driver save the trajectories directly? Indeed, it could, but this would make the architecture less flexible. For example, what if you don’t want to use a replay buffer? What if you want to use the trajectories for something else, like computing metrics? In fact, an observer is just any function that takes a trajectory as an argument. You can use an observer to save the trajectories to a replay buffer, or to save them to a TFRecord file (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch13.html#data_chapter">Chapter&nbsp;13</a>), or to compute metrics, or for anything else. Moreover, you can pass multiple observers to the driver, and it will broadcast the trajectories to all of them.</p>
</li>
</ul>
<div data-type="tip"><h6>Tip</h6>
<p>Although this architecture is the most common, you can customize it as you please, and even replace some components with your own. In fact, unless you are researching new RL algorithms, you will most likely want to use a custom environment for your task. For this, you just need to create a custom class that inherits from the <code>PyEnvironment</code> class in the <code>tf_agents.environments.py_environment</code> package and overrides the appropriate methods, such as <code>action_spec()</code>, <code>observation_spec()</code>, <code>_reset()</code>, and <code>_step()</code> (see the notebook for an example).</p>
</div>

<p>Now we will create all these components: first the Deep Q-Network, then the DQN agent (which will take care of creating the collect policy), then the replay buffer and the observer to write to it, then a few training metrics, then the driver, and finally the dataset. Once we have all the components in place, we will populate the replay buffer with some initial trajectories, then we will run the main training loop. So let’s start by creating the Deep Q-Network.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Creating the Deep Q-Network"><div class="sect2" id="idm46263482663992">
<h2>Creating the Deep Q-Network</h2>

<p>The TF-Agents library provides many networks in the <code>tf_agents.networks</code> package and its subpackages. We will use the <code>tf_agents.networks.q_network.QNetwork</code> class:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tf_agents.networks.q_network</code> <code class="kn">import</code> <code class="n">QNetwork</code>

<code class="n">preprocessing_layer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Lambda</code><code class="p">(</code>
                          <code class="k">lambda</code> <code class="n">obs</code><code class="p">:</code> <code class="n">tf</code><code class="o">.</code><code class="n">cast</code><code class="p">(</code><code class="n">obs</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code> <code class="o">/</code> <code class="mf">255.</code><code class="p">)</code>
<code class="n">conv_layer_params</code><code class="o">=</code><code class="p">[(</code><code class="mi">32</code><code class="p">,</code> <code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mi">8</code><code class="p">),</code> <code class="mi">4</code><code class="p">),</code> <code class="p">(</code><code class="mi">64</code><code class="p">,</code> <code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">),</code> <code class="mi">2</code><code class="p">),</code> <code class="p">(</code><code class="mi">64</code><code class="p">,</code> <code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code> <code class="mi">1</code><code class="p">)]</code>
<code class="n">fc_layer_params</code><code class="o">=</code><code class="p">[</code><code class="mi">512</code><code class="p">]</code>

<code class="n">q_net</code> <code class="o">=</code> <code class="n">QNetwork</code><code class="p">(</code>
    <code class="n">tf_env</code><code class="o">.</code><code class="n">observation_spec</code><code class="p">(),</code>
    <code class="n">tf_env</code><code class="o">.</code><code class="n">action_spec</code><code class="p">(),</code>
    <code class="n">preprocessing_layers</code><code class="o">=</code><code class="n">preprocessing_layer</code><code class="p">,</code>
    <code class="n">conv_layer_params</code><code class="o">=</code><code class="n">conv_layer_params</code><code class="p">,</code>
    <code class="n">fc_layer_params</code><code class="o">=</code><code class="n">fc_layer_params</code><code class="p">)</code></pre>

<p>This <code>QNetwork</code> takes an observation as input and outputs one Q-Value per action, so we must give it the specifications of the observations and the actions. Moreover, it starts with a preprocessing layer: a simple <code>Lambda</code> layer that casts the observations to 32-bit floats and normalizes them (the values will range from 0.0 to 1.0). Indeed, the observations contain unsigned bytes, which use 4 times less space than 32-bit floats, which is why we did not cast the observations to 32-bit floats earlier; we want to save RAM in the replay buffer. Next, the network applies 3 convolutional layers: the first has 32 8 × 8 filters and uses a stride of 4, the second has 64 4 × 4 filters and a stride of 2, and the third has 64 3 × 3 filters and a stride of 1. Lastly, it applies a dense layer with 512 units, followed by a dense output layer with 4 units, one per Q-Value to output (i.e., one per action). All convolutional layers and all dense layers except the output layer use the ReLU activation function by default (you can change this by setting the <code>activation_fn</code> argument). The output layer does not use any activation function.</p>

<p>Under the hood, a <code>QNetwork</code> is composed of two parts: an encoding network that processes the observations, followed by a dense output layer that outputs one Q-Value per action. TF-Agent’s <code>EncodingNetwork</code> class implements a neural network architecture found in various agents (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#encoding_network_diagram">Figure&nbsp;18-13</a>). It may have one or more inputs. For example, if each observation is composed of some sensor data plus an image from a camera, you will have two inputs. Each input may require some preprocessing steps, in which case you can specify a list of Keras layers via the <code>preprocessing_layers</code> argument, with one preprocessing layer per input, and the network will apply each layer to the corresponding input (if an input requires multiple layers of preprocessing, you can pass a whole model, since a Keras model can always be used as a layer). If there are two inputs or more, you must also pass an extra layer via the <code>preprocessing_combiner</code> argument, to combine the outputs from the preprocessing layers into a single output. Next, the encoding network will optionally apply a list of convolutions sequentially, provided you specify their parameters via the <code>conv_layer_params</code> argument. This must be a list composed of 3-tuples (one per convolutional layer) indicating the number of filters, the kernel size, and the stride. After these convolutional layers, the encoding network will optionally apply a sequence of dense layers, if you set the <code>fc_layer_params</code> argument: it must be a list containing the number of neurons for each dense layer. Optionally, you can also pass a list of dropout rates (one per dense layer) via the <code>dropout_layer_params</code> argument if you want to apply dropout after each dense layer. The <code>QNetwork</code> takes the output of this encoding network and passes it to the dense output layer (with one unit per action).</p>

<figure class="smallereighty"><div id="encoding_network_diagram" class="figure">
<img src="./Chapter18_files/mls2_1814.png" alt="mls2 1814" width="1440" height="688" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1814.png">
<h6><span class="label">Figure 18-13. </span>Architecture of an encoding network</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The <code>QNetwork</code> class is flexible enough to build many different architectures, but you can always build your own network class if you need extra flexibility: extend the <code>tf_agents.networks.Network</code> class and implement it like a regular custom Keras layer. The <code>tf_agents.networks.Network</code> class is a subclass of the <code>keras.layers.Layer</code> class that adds some functionality required by some agents, such as the possibility to easily create shallow copies of the network (i.e., copying the network’s architecture, but not its weights). For example, the <code>DQNAgent</code> uses this to create a copy of the online model</p>
</div>

<p>Now that we have the DQN, we are ready to build the DQN Agent.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Creating the DQN Agent"><div class="sect2" id="idm46263482513224">
<h2>Creating the DQN Agent</h2>

<p>The TF-Agents library implements many types of agents, located in the <code>tf_agents.agents</code> package and its subpackages. We will use the <code>tf_agents.agents.dqn.dqn_agent.DqnAgent</code> class:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tf_agents.agents.dqn.dqn_agent</code> <code class="kn">import</code> <code class="n">DqnAgent</code>

<code class="n">train_step</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">Variable</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
<code class="n">update_period</code> <code class="o">=</code> <code class="mi">4</code> <code class="c1"># train the model every 4 steps</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">RMSprop</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">2.5e-4</code><code class="p">,</code> <code class="n">rho</code><code class="o">=</code><code class="mf">0.95</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="mf">0.0</code><code class="p">,</code>
                                     <code class="n">epsilon</code><code class="o">=</code><code class="mf">0.00001</code><code class="p">,</code> <code class="n">centered</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">epsilon_fn</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">schedules</code><code class="o">.</code><code class="n">PolynomialDecay</code><code class="p">(</code>
    <code class="n">initial_learning_rate</code><code class="o">=</code><code class="mf">1.0</code><code class="p">,</code> <code class="c1"># initial ε</code>
    <code class="n">decay_steps</code><code class="o">=</code><code class="mi">250000</code> <code class="o">//</code> <code class="n">update_period</code><code class="p">,</code> <code class="c1"># &lt;=&gt; 1,000,000 ALE frames</code>
    <code class="n">end_learning_rate</code><code class="o">=</code><code class="mf">0.01</code><code class="p">)</code> <code class="c1"># final ε</code>
<code class="n">agent</code> <code class="o">=</code> <code class="n">DqnAgent</code><code class="p">(</code><code class="n">tf_env</code><code class="o">.</code><code class="n">time_step_spec</code><code class="p">(),</code>
                 <code class="n">tf_env</code><code class="o">.</code><code class="n">action_spec</code><code class="p">(),</code>
                 <code class="n">q_network</code><code class="o">=</code><code class="n">q_net</code><code class="p">,</code>
                 <code class="n">optimizer</code><code class="o">=</code><code class="n">optimizer</code><code class="p">,</code>
                 <code class="n">target_update_period</code><code class="o">=</code><code class="mi">2000</code><code class="p">,</code> <code class="c1"># &lt;=&gt; 32,000 ALE frames</code>
                 <code class="n">td_errors_loss_fn</code><code class="o">=</code><code class="n">keras</code><code class="o">.</code><code class="n">losses</code><code class="o">.</code><code class="n">Huber</code><code class="p">(</code><code class="n">reduction</code><code class="o">=</code><code class="s2">"none"</code><code class="p">),</code>
                 <code class="n">gamma</code><code class="o">=</code><code class="mf">0.99</code><code class="p">,</code> <code class="c1"># discount factor</code>
                 <code class="n">train_step_counter</code><code class="o">=</code><code class="n">train_step</code><code class="p">,</code>
                 <code class="n">epsilon_greedy</code><code class="o">=</code><code class="k">lambda</code><code class="p">:</code> <code class="n">epsilon_fn</code><code class="p">(</code><code class="n">train_step</code><code class="p">))</code>
<code class="n">agent</code><code class="o">.</code><code class="n">initialize</code><code class="p">()</code></pre>

<p>Let’s walk through this code:</p>

<ul>
<li>
<p>We first create a variable that will count the number of training steps.</p>
</li>
<li>
<p>Then we build the optimizer, using the same hyperparameters as in the 2015 DQN paper.</p>
</li>
<li>
<p>Next, we create a <code>PolynomialDecay</code> object that will compute the <em>ε</em> value for the <em>ε</em>-greedy collect policy, given the current training step (it is normally used to decay the learning rate, hence the names of the arguments, but it will work just fine to decay any other value). It will go from 1.0 down to 0.01 in 1 million ALE frames (as was used in the 2015 DQN paper), which corresponds to 250,000 steps (since we use frame skipping with a period of 4). Moreover, we will train the agent every 4 steps (i.e., 16 ALE frames), so <em>ε</em> will actually decay over 62,500 <em>training</em> steps.</p>
</li>
<li>
<p>We then build the <code>DQNAgent</code>, passing it the time step and action specs, the <code>QNetwork</code> to train, the optimizer, the number of training steps between target model updates, the loss function to use, the discount factor, the <code>train_step</code> variable, and a function that returns the <em>ε</em> value (it must take no argument, which is why we need a lambda to pass the <code>train_step</code>).</p>
</li>
<li>
<p>Note that the loss function must return an error per instance, not the mean error, which is why we set <code>reduction="none"</code>.</p>
</li>
<li>
<p>Lastly, we initialize the agent.</p>
</li>
</ul>

<p>Next, let’s build the replay buffer and the observer that will write to it.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Creating the Replay Buffer and the Corresponding Observer"><div class="sect2" id="idm46263482325272">
<h2>Creating the Replay Buffer and the Corresponding Observer</h2>

<p>The TF-Agents library provides various replay buffer implementations in the <code>tf_agents.replay_buffers</code> package. Some are purely written in Python (their module name starts with <code>py_</code>), and others are written based on TensorFlow (their module name starts with <code>tf_</code>). We will use the <code>TFUniformReplayBuffer</code> class in the <code>tf_agents.replay_buffers.tf_uniform_replay_buffer</code> package. It provides a high-performance implementation of a replay buffer with uniform sampling:<sup><a data-type="noteref" id="idm46263482321224-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263482321224">21</a></sup></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tf_agents.replay_buffers</code> <code class="kn">import</code> <code class="n">tf_uniform_replay_buffer</code>

<code class="n">replay_buffer</code> <code class="o">=</code> <code class="n">tf_uniform_replay_buffer</code><code class="o">.</code><code class="n">TFUniformReplayBuffer</code><code class="p">(</code>
    <code class="n">data_spec</code><code class="o">=</code><code class="n">agent</code><code class="o">.</code><code class="n">collect_data_spec</code><code class="p">,</code>
    <code class="n">batch_size</code><code class="o">=</code><code class="n">tf_env</code><code class="o">.</code><code class="n">batch_size</code><code class="p">,</code>
    <code class="n">max_length</code><code class="o">=</code><code class="mi">1000000</code><code class="p">)</code></pre>

<p>Let’s look at each of these arguments:</p>
<dl>
<dt><code>data_spec</code></dt>
<dd>
<p>The specification of the data that will be saved in the replay buffer. The DQN agent knowns what the collected data will look like, and it makes the data spec available via its <code>collect_data_spec</code> attribute, so that’s what we give the replay buffer.</p>
</dd>
<dt><code>batch_size</code></dt>
<dd>
<p>The number of trajectories that will be added at each step. In our case, it will be one, since the driver will just execute one action per step and collect one trajectory. HIf the environment were a <em>batched environment</em>, meaning an environment that takes a batch of actions at each step and returns a batch of observations, then the driver would have to save a batch of trajectories at each step. Since we are using a TensorFlow replay buffer, it needs to know the size of the batches it will handle (to build the computation graph). An example of a batched environment is the <code>ParallelPyEnvironment</code> (from the <code>tf_agents.environments.parallel_py_environment</code> package): it runs multiple environments in parallel in separate processes (they can be different as long as they have the same action and observation specs), and at each step it takes a batch of actions and executes them in the environments (one action per environment), then it returns all the resulting observations.</p>
</dd>
<dt><code>max_length</code></dt>
<dd>
<p>The maximum size of the replay buffer. We created a large replay buffer that can store one million trajectories (as was used in the 2015 DQN paper). This will require a lot of RAM.</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>When we store two consecutive trajectories, they contain two consecutive observations with four frames each (since we used the <code>FrameStack4</code> wrapper), and unfortunately three out of four frames in the second observation are redundant (they are already present in the first observation). In other words, we are using about four times more RAM than necessary. To avoid this, you can use instead a <code>PyHashedReplayBuffer</code> from the <code>tf_agents.replay_buffers.py_hashed_replay_buffer</code> package: it deduplicates data in the stored trajectories along the last axis of the observations.</p>
</div>

<p>Now we can create the observer that will write the trajectories to the replay buffer. An observer is just a function (or a callable object) that takes a trajectory argument, so we can directly use the <code>add_method()</code> method (bound to the <code>replay_buffer</code> object) as our observer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">replay_buffer_observer</code> <code class="o">=</code> <code class="n">replay_buffer</code><code class="o">.</code><code class="n">add_batch</code></pre>

<p>If you wanted to create your own observer, you could write any function with a <code>trajectory</code> argument. If it must have a state, you can write a class with a <code>__call__(self, trajectory)</code> method. For example, here is a simple observer that will increment a counter every time it is called (except when the trajectory represents a boundary between two episodes, which does not count as a step), and every 100 increments it displays the progress up to a given total (the carriage return <code>\r</code> along with <code>end=""</code> ensures that the displayed counter remains on the same line):</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">ShowProgress</code><code class="p">:</code>
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">total</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">counter</code> <code class="o">=</code> <code class="mi">0</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">total</code> <code class="o">=</code> <code class="n">total</code>
    <code class="k">def</code> <code class="nf-Magic">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">trajectory</code><code class="p">):</code>
        <code class="k">if</code> <code class="ow">not</code> <code class="n">trajectory</code><code class="o">.</code><code class="n">is_boundary</code><code class="p">():</code>
            <code class="bp">self</code><code class="o">.</code><code class="n">counter</code> <code class="o">+=</code> <code class="mi">1</code>
        <code class="k">if</code> <code class="bp">self</code><code class="o">.</code><code class="n">counter</code> <code class="o">%</code> <code class="mi">100</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
            <code class="k">print</code><code class="p">(</code><code class="s2">"</code><code class="se">\r</code><code class="s2">{}/{}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">counter</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">total</code><code class="p">),</code> <code class="n">end</code><code class="o">=</code><code class="s2">""</code><code class="p">)</code></pre>

<p>Now let’s create a few training metrics.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Creating Training Metrics"><div class="sect2" id="idm46263482324648">
<h2>Creating Training Metrics</h2>

<p>TF-Agents implements several RL metrics in the <code>tf_agents.metrics</code> package, some implemented purely in Python and some based on TensorFlow. Let’s create a few of them in order to count the number of episodes, the number of steps taken, and most importantly the average return per episode and the average episode length:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tf_agents.metrics</code> <code class="kn">import</code> <code class="n">tf_metrics</code>

<code class="n">train_metrics</code> <code class="o">=</code> <code class="p">[</code>
    <code class="n">tf_metrics</code><code class="o">.</code><code class="n">NumberOfEpisodes</code><code class="p">(),</code>
    <code class="n">tf_metrics</code><code class="o">.</code><code class="n">EnvironmentSteps</code><code class="p">(),</code>
    <code class="n">tf_metrics</code><code class="o">.</code><code class="n">AverageReturnMetric</code><code class="p">(),</code>
    <code class="n">tf_metrics</code><code class="o">.</code><code class="n">AverageEpisodeLengthMetric</code><code class="p">(),</code>
<code class="p">]</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Discounting the rewards makes sense for training or to implement a policy, as it makes it possible to balance the importance of immediate rewards with future rewards. However, once an episode is over, we can evaluate how good it was overall by summing the <em>undiscounted</em> rewards. For this reason, the <code>AverageReturnMetric</code> computes the sum of undiscounted rewards for each episode, and it keeps track of the streaming mean of these sums over all the episodes it encounters.</p>
</div>

<p>At any time, you can get the value of each of these metrics by calling its <code>result()</code> method (e.g., <code>train_metrics[0].result()</code>). Alternatively, you can log all metrics by calling <code>log_metrics(train_metrics)</code> (this function is located in the <code>tf_agents.eval.metric_utils</code> package):</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">tf_agents.eval.metric_utils</code> <code class="kn">import</code> <code class="n">log_metrics</code>
<code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">logging</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">logging</code><code class="o">.</code><code class="n">get_logger</code><code class="p">()</code><code class="o">.</code><code class="n">set_level</code><code class="p">(</code><code class="n">logging</code><code class="o">.</code><code class="n">INFO</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_metrics</code><code class="p">(</code><code class="n">train_metrics</code><code class="p">)</code>
<code class="go">[...]</code>
<code class="go">NumberOfEpisodes = 0</code>
<code class="go">EnvironmentSteps = 0</code>
<code class="go">AverageReturn = 0.0</code>
<code class="go">AverageEpisodeLength = 0.0</code></pre>

<p>Next, let’s create the collect driver.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Creating the Collect Driver"><div class="sect2" id="idm46263481934840">
<h2>Creating the Collect Driver</h2>

<p>As we explored in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#tf_agents_architecture_diagram">???</a>, a driver is an object that explores an environment using a given policy, collects experiences, and broadcasts them to some observers. At each step, each of the following things happen:</p>

<ul>
<li>
<p>The driver passes the current time step to the collect policy, which uses this time step to choose an action and returns an <em>action step</em> object containing the action.</p>
</li>
<li>
<p>The driver then passes the action to the environment, which returns the next time step.</p>
</li>
<li>
<p>Finally, the driver creates a trajectory object to represent this transition and broadcasts it to all the observers.</p>
</li>
</ul>

<p>Some policies, such as RNN policies, are stateful: they choose an action based on both the given time step and their own internal state. Stateful policies return their own state in the action step, along with the chosen action. The driver will then pass this state back to the policy at the next time step. Moreover, the driver saves the policy state to the trajectory (in the <code>policy_info</code> field), so it ends up in the replay buffer. This is essential when training a stateful policy: indeed, when the agent samples a trajectory, it must set the policy’s state to the state it was in at the time of the sampled time step.</p>

<p>Also, as discussed earlier, the environment may be a batched environment, in which case the driver passes a <em>batched time step</em> to the policy (i.e., a time step object containing a batch of observations, a batch of step types, a batch of rewards, and a batch of discounts, all four batches of the same size. The driver also passes a batch of previous policy states. The policy then returns a <em>batched action step</em> containing a batch of actions and a batch of policy states. Finally, the driver creates a <em>batched trajectory</em> (i.e., a trajectory containing a batch of step types, a batch of observations, a batch of actions, a batch of rewards, and more generally a batch for each trajectory attribute, with all batches of the same size.</p>

<p>There are two main driver classes: <code>DynamicStepDriver</code> and <code>DynamicEpisodeDriver</code>. The first one collects experiences for a given number of steps, while the second collects experiences for a given number of episodes. We want to collect experiences for four steps for each training iteration (as was done in the 2015 DQN paper), so let’s create a <code>DynamicStepDriver</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tf_agents.drivers.dynamic_step_driver</code> <code class="kn">import</code> <code class="n">DynamicStepDriver</code>

<code class="n">collect_driver</code> <code class="o">=</code> <code class="n">DynamicStepDriver</code><code class="p">(</code>
    <code class="n">tf_env</code><code class="p">,</code>
    <code class="n">agent</code><code class="o">.</code><code class="n">collect_policy</code><code class="p">,</code>
    <code class="n">observers</code><code class="o">=</code><code class="p">[</code><code class="n">replay_buffer_observer</code><code class="p">]</code> <code class="o">+</code> <code class="n">training_metrics</code><code class="p">,</code>
    <code class="n">num_steps</code><code class="o">=</code><code class="n">update_period</code><code class="p">)</code> <code class="c1"># collect 4 steps for each training iteration</code></pre>

<p>We give it the environment to play with, the agent’s collect policy, a list of observers (including the replay buffer observer and the training metrics), and finally the number of steps to run (in this case, four). We could now run it by calling its <code>run()</code> method, but it’s best to warm up the replay buffer with experiences collected using a purely random policy. For this, we can use the <code>RandomTFPolicy</code> class, and create a second driver that will run this policy for 20,000 steps (which is equivalent to 80,000 simulator frames, as was done in the 2015 DQN paper). We can use our <code>ShowProgress</code> observer to display the progress:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tf_agents.policies.random_tf_policy</code> <code class="kn">import</code> <code class="n">RandomTFPolicy</code>

<code class="n">initial_collect_policy</code> <code class="o">=</code> <code class="n">RandomTFPolicy</code><code class="p">(</code><code class="n">tf_env</code><code class="o">.</code><code class="n">time_step_spec</code><code class="p">(),</code>
                                        <code class="n">tf_env</code><code class="o">.</code><code class="n">action_spec</code><code class="p">())</code>
<code class="n">init_driver</code> <code class="o">=</code> <code class="n">DynamicStepDriver</code><code class="p">(</code>
    <code class="n">tf_env</code><code class="p">,</code>
    <code class="n">initial_collect_policy</code><code class="p">,</code>
    <code class="n">observers</code><code class="o">=</code><code class="p">[</code><code class="n">replay_buffer</code><code class="o">.</code><code class="n">add_batch</code><code class="p">,</code> <code class="n">ShowProgress</code><code class="p">(</code><code class="mi">20000</code><code class="p">)],</code>
    <code class="n">num_steps</code><code class="o">=</code><code class="mi">20000</code><code class="p">)</code> <code class="c1"># &lt;=&gt; 80,000 ALE frames</code>
<code class="n">final_time_step</code><code class="p">,</code> <code class="n">final_policy_state</code> <code class="o">=</code> <code class="n">init_driver</code><code class="o">.</code><code class="n">run</code><code class="p">()</code></pre>

<p>We’re almost ready to run the training loop! We just need one last component: the dataset.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Creating the Dataset"><div class="sect2" id="idm46263481976536">
<h2>Creating the Dataset</h2>

<p>To sample a batch of trajectories from the replay buffer, call its <code>get_next()</code> method. This returns the batch of trajectories plus a <code>BufferInfo</code> object that contains the sample identifiers and their sampling probabilities (this may be useful for some algorithms, such as Prioritized Experience Replay). For example, the following code will sample a small batch of two trajectories (subepisodes), each containing three consecutive steps. These subepisodes are shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#sub_episodes_plot">Figure&nbsp;18-14</a> (each row contains three consecutive steps from an episode):</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">trajectories</code><code class="p">,</code> <code class="n">buffer_info</code> <code class="o">=</code> <code class="n">replay_buffer</code><code class="o">.</code><code class="n">get_next</code><code class="p">(</code>
<code class="gp">... </code>    <code class="n">sample_batch_size</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">num_steps</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
<code class="gp">...</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">trajectories</code><code class="o">.</code><code class="n">_fields</code>
<code class="go">('step_type', 'observation', 'action', 'policy_info',</code>
<code class="go"> 'next_step_type', 'reward', 'discount')</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">trajectories</code><code class="o">.</code><code class="n">observation</code><code class="o">.</code><code class="n">shape</code>
<code class="go">TensorShape([2, 3, 84, 84, 4])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">trajectories</code><code class="o">.</code><code class="n">step_type</code><code class="o">.</code><code class="n">numpy</code><code class="p">()</code>
<code class="go">array([[1, 1, 1],</code>
<code class="go">       [1, 1, 1]], dtype=int32)</code></pre>

<p>The <code>trajectories</code> object is a named tuple, with seven fields. Each field contains a tensor whose first two dimensions are 2 and 3 (since there are two trajectories, each with three steps). This explains why the shape of the <code>observation</code> field is <code>[2, 3, 84, 84, 4]</code>: that’s two trajectories, each with three steps, and each step’s observation is 84 × 84 × 4. Similarly, the <code>step_type</code> tensor has a shape of <code>[2, 3]</code>: in this example, both trajectories contain three consecutive steps in the middle on an episode (types 1, 1, 1). In the second trajectory, you can barely see the ball at the lower left of the first observation, and it disappears in the next two observations, so the agent is about to lose a life, but the episode will not end immediately because it still has several lives left.</p>

<figure><div id="sub_episodes_plot" class="figure">
<img src="./Chapter18_files/mls2_1815.png" alt="mls2 1815" width="1440" height="942" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1815.png">
<h6><span class="label">Figure 18-14. </span>Two trajectories containing three consecutive steps each</h6>
</div></figure>

<p>Each trajectory is a concise representation of a sequence of consecutive time steps and action steps, designed to avoid redundancy. How so? Well, as you can see in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#trajectory_transition_diagram">Figure&nbsp;18-15</a>, transition <em>n</em> is composed of time step <em>n</em>, action step <em>n</em>, and time step <em>n</em> + 1; while transition <em>n</em> + 1 is composed of time step <em>n</em> + 1, action step <em>n</em> + 1, and time step <em>n</em> + 2. If we just stored these two transitions directly in the replay buffer, the time step <em>n</em> + 1 would be duplicated. To avoid this duplication, the <em>n</em><sup>th</sup> trajectory step includes only the type and observation from time step <em>n</em> (but not its reward and discount), and it does not contain the observation from time step <em>n</em> + 1 (however, it does contain a copy of the next time step’s type; that’s the only duplication).</p>

<figure><div id="trajectory_transition_diagram" class="figure">
<img src="./Chapter18_files/mls2_1816.png" alt="mls2 1816" width="1440" height="826" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1816.png">
<h6><span class="label">Figure 18-15. </span>Trajectories, transitions, time steps, and action steps</h6>
</div></figure>

<p>So if you have a batch of trajectories where each trajectory has <em>t</em> + 1 steps (from time step <em>n</em> to time step <em>n</em> + <em>t</em>), then it contains all the data from time step <em>n</em> to time step <em>n</em> + <em>t</em>, except for the reward and discount from time step <em>n</em> (but it contains the reward and discount of time step <em>n</em> + <em>t</em> + 1). This represents <em>t</em> transitions (<em>n</em> to <em>n</em> + 1, <em>n</em> + 1 to <em>n</em> + 2, …, <em>n</em> + <em>t</em> - 1 to <em>n</em> + <em>t</em>). The <code>to_transition()</code> function in the <code>tf_agents.trajectories.trajectory</code> module converts a batched trajectory into a list containing a batched <code>time_step</code>, a batched <code>action_step</code>, and a batched <code>next_time_step</code>. Notice that the second dimension is 2 instead of 3, since there are <em>t</em> transitions between <em>t</em> + 1 time steps (don’t worry if you’re a bit confused; you’ll get the hang of it):</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">tf_agents.trajectories.trajectory</code> <code class="kn">import</code> <code class="n">to_transition</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">time_steps</code><code class="p">,</code> <code class="n">action_steps</code><code class="p">,</code> <code class="n">next_time_steps</code> <code class="o">=</code> <code class="n">to_transition</code><code class="p">(</code><code class="n">trajectories</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">time_steps</code><code class="o">.</code><code class="n">observation</code><code class="o">.</code><code class="n">shape</code>
<code class="go">TensorShape([2, 2, 84, 84, 4]) # 3 time steps = 2 transitions</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>A sampled trajectory may actually overlap two (or more) episodes! In this case, it will contain <em>boundary transitions</em>, meaning transitions with a <code>step_type</code> equal to 2 (end) and a <code>next_step_type</code> equal to 0 (start). Of course, TF-Agents properly handles such trajectories (e.g., by resetting the policy state when encountering a boundary). The trajectory’s <code>is_boundary()</code> method returns a tensor indicating whether each step is a boundary or not.</p>
</div>

<p>For our main training loop, instead of calling the <code>get_next()</code> method, we will use a <code>tf.data.Dataset</code>. This way, we can benefit from the power of the Data API (e.g., parallelism and prefetching). For this, call the replay buffer’s <code>as_dataset()</code> method:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">dataset</code> <code class="o">=</code> <code class="n">replay_buffer</code><code class="o">.</code><code class="n">as_dataset</code><code class="p">(</code>
    <code class="n">sample_batch_size</code><code class="o">=</code><code class="mi">64</code><code class="p">,</code>
    <code class="n">num_steps</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
    <code class="n">num_parallel_calls</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code><code class="o">.</code><code class="n">prefetch</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code></pre>

<p>We will sample batches of 64 trajectories at each training step (as in the 2015 DQN paper), each with 2 steps (i.e., 2 steps = 1 full transition, including the next step’s observation). This dataset will process three elements in parallel, and prefetch three batches.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>For on-policy algorithms such as Policy Gradients, each experience should be sampled once, used from training, and then discarded. In this case, you can still use a replay buffer, but instead of using a <code>Dataset</code>, you would call the replay buffer’s <code>gather_all()</code> method at each training iteration to get a tensor containing all the trajectories recorded so far, then use them to perform a training step, and finally clear the replay buffer by calling its <code>clear()</code> method.</p>
</div>

<p>And now we have all the components in place, we are ready to train the model!</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Creating the Training Loop"><div class="sect2" id="idm46263481836008">
<h2>Creating the Training Loop</h2>

<p>To speed up training, we will convert the main functions to TensorFlow functions. For this we will use the <code>tf_agents.utils.common.function()</code> function, which wraps <code>tf.function()</code>, with some extra experimental options:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tf_agents.utils.common</code> <code class="kn">import</code> <code class="n">function</code>

<code class="n">collect_driver</code><code class="o">.</code><code class="n">run</code> <code class="o">=</code> <code class="n">function</code><code class="p">(</code><code class="n">collect_driver</code><code class="o">.</code><code class="n">run</code><code class="p">)</code>
<code class="n">agent</code><code class="o">.</code><code class="n">train</code> <code class="o">=</code> <code class="n">function</code><code class="p">(</code><code class="n">agent</code><code class="o">.</code><code class="n">train</code><code class="p">)</code></pre>

<p>Now let’s create a small function that will run the main training loop for <code>n_iterations</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">train_agent</code><code class="p">(</code><code class="n">n_iterations</code><code class="p">):</code>
    <code class="n">time_step</code> <code class="o">=</code> <code class="bp">None</code>
    <code class="n">policy_state</code> <code class="o">=</code> <code class="n">agent</code><code class="o">.</code><code class="n">collect_policy</code><code class="o">.</code><code class="n">get_initial_state</code><code class="p">(</code><code class="n">tf_env</code><code class="o">.</code><code class="n">batch_size</code><code class="p">)</code>
    <code class="n">iterator</code> <code class="o">=</code> <code class="nb">iter</code><code class="p">(</code><code class="n">dataset</code><code class="p">)</code>
    <code class="k">for</code> <code class="n">iteration</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_iterations</code><code class="p">):</code>
        <code class="n">time_step</code><code class="p">,</code> <code class="n">policy_state</code> <code class="o">=</code> <code class="n">collect_driver</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">time_step</code><code class="p">,</code> <code class="n">policy_state</code><code class="p">)</code>
        <code class="n">trajectories</code><code class="p">,</code> <code class="n">buffer_info</code> <code class="o">=</code> <code class="nb">next</code><code class="p">(</code><code class="n">iterator</code><code class="p">)</code>
        <code class="n">train_loss</code> <code class="o">=</code> <code class="n">agent</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">trajectories</code><code class="p">)</code>
        <code class="k">print</code><code class="p">(</code><code class="s2">"</code><code class="se">\r</code><code class="s2">{} loss:{:.5f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code>
            <code class="n">iteration</code><code class="p">,</code> <code class="n">train_loss</code><code class="o">.</code><code class="n">loss</code><code class="o">.</code><code class="n">numpy</code><code class="p">()),</code> <code class="n">end</code><code class="o">=</code><code class="s2">""</code><code class="p">)</code>
        <code class="k">if</code> <code class="n">iteration</code> <code class="o">%</code> <code class="mi">1000</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
            <code class="n">log_metrics</code><code class="p">(</code><code class="n">train_metrics</code><code class="p">)</code></pre>

<p>The function first asks the collect policy for its initial state (given the environment batch size, which is 1 in this case). Since the policy is stateless, this returns an empty tuple (so we could have written <code>policy_state = ()</code>). Next, we create an iterator over the dataset, and we run the training loop. At each iteration, we call the driver’s <code>run()</code> method, passing it the current time step (initially <code>None</code>) and the current policy state. It will run the collect policy and collect experience for four steps (as we configured earlier), broadcasting the collected trajectories to the replay buffer and the metrics. Next, we sample one batch of trajectories from the dataset, and we pass it to the agent’s <code>train()</code> method. It returns a <code>train_loss</code> object which may vary depending on the type of agent. Next, we display the iteration number and the training loss, and every 1,000 iterations we log all the metrics. Now just call <code>train_agent()</code> for some number of iterations, and see the agent gradually learn to play Breakout!</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">train_agent</code><code class="p">(</code><code class="mi">10000000</code><code class="p">)</code></pre>

<p>This will take a lot of computing power and a lot of patience (it may take hours, or even days, depending on your hardware), plus you may need to run the algorithm several times with different random seeds to get good results, but once it’s done, the agent will be superhuman (at least at Breakout). You can also try training this DQN agent on other Atari games: it can achieve superhuman skill at most action games, but it is not so good at games with long-running storylines.<sup><a data-type="noteref" id="idm46263481421784-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263481421784">22</a></sup></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Overview of Some Popular RL Algorithms"><div class="sect1" id="idm46263483217480">
<h1>Overview of Some Popular RL Algorithms</h1>

<p>Before we finish this chapter, let’s take a quick look at a few popular RL algorithms:</p>
<dl>
<dt>Actor-Critic algorithms</dt>
<dd>
<p>A family of RL algorithms that combine Policy Gradients with Deep Q-Networks. An Actor-Critic agent contains two neural networks: a policy net and a DQN. The DQN is trained normally, by learning from the agent’s experiences. The policy net learns differently (and much faster) than in regular PG: instead of estimating the value of each action by going through multiple episodes, then summing the future discounted rewards for each action, and finally normalizing them, the agent (actor) relies on the action values estimated by the DQN (critic). It’s a bit like an athlete (the agent) learning with the help of a coach (the DQN).</p>
</dd>
<dt><a href="https://homl.info/a3c"><em>Asynchronous Advantage Actor-Critic</em></a><sup><a data-type="noteref" id="idm46263481415000-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263481415000">23</a></sup> (A3C)</dt>
<dd>
<p>An important Actor-Critic variant introduced by DeepMind researchers in 2016, where multiple agents learn in parallel, exploring different copies of the environment. At regular intervals, but asynchronously (hence the name), each agent pushes some weight updates to a master network, then it pulls the latest weights from that network. Each agent thus contributes to improving the master network and benefits from what the other agents have learned. Moreover, instead of estimating the Q-Values, the DQN estimates the advantage of each action (hence the second A in the name), which stabilizes training.</p>
</dd>
<dt><a href="https://homl.info/a2c"><em>Advantage Actor-Critic</em></a> (A2C)</dt>
<dd>
<p>A variant of the A3C algorithm that removes the asynchronicity. All model updates are synchronous, so gradient updates are performed over larger batches, which allows the model to better utilize the power of the GPU.</p>
</dd>
<dt><a href="https://homl.info/sac"><em>Soft Actor-Critic</em></a><sup><a data-type="noteref" id="idm46263481408264-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263481408264">24</a></sup> (SAC)</dt>
<dd>
<p>An Actor-Critic variant proposed in 2018 by Tuomas Haarnoja and other UC Berkeley researchers. It learns not only rewards, but also to maximize the entropy of its actions. In other words, it tries to be as unpredictable as possible while still getting as many rewards as possible. This encourages the agent to explore the environment, which speeds up training, and makes it less likely to repeatedly execute the same action when the DQN produces imperfect estimates. This algorithm has demonstrated an amazing sample efficiency (contrary to all the previous algorithms which learn very slowly). SAC is available in TF-Agents.</p>
</dd>
<dt><a href="https://homl.info/ppo"><em>Proximal Policy Optimization</em></a><sup><a data-type="noteref" id="idm46263481404920-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263481404920">25</a></sup></dt>
<dd>
<p>The PPO algorithm is based on the A2C algorithm. It clips the loss function to avoid excessively large weight updates (which often lead to training instabilities). This algorithm is a simplification of the previous <a href="https://homl.info/trpo"><em>Trust Region Policy Optimization</em></a><sup><a data-type="noteref" id="idm46263481402776-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263481402776">26</a></sup> (TRPO) algorithm, also by John Schulman and other OpenAI researchers. OpenAI made the news in April 2019 with their AI called OpenAI Five, based on the PPO algorithm, which defeated the world champions at the multiplayer game Dota&nbsp;2. PPO is also available in TF-Agents.</p>
</dd>
<dt><a href="https://homl.info/curiosity"><em>Curiosity-based exploration</em></a><sup><a data-type="noteref" id="idm46263481400792-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263481400792">27</a></sup></dt>
<dd>
<p>A recurring problem in RL is the sparsity of the rewards, which makes learning very slow and inefficient. Deepak Pathak and other UC Berkeley researchers have proposed an exciting way to tackle this issue: why not ignore the rewards, and just make the agent extremely curious to explore the environment? The rewards thus become intrinsic to the agent, rather than coming from the environment. Similarly, stimulating curiosity in a child is more likely to give good results than purely rewarding the child for getting good grades. How does this work? The agent continuously tries to predict the outcome of its actions, and it seeks situations where the outcome does not match its predictions. In other words, it wants to be surprised. If the outcome is predictable (boring), it goes elsewhere. However, if the outcome is unpredictable but the agent notices that it has no control over it, it also gets bored after a while. With only curiosity, the authors succeeded in training an agent at many video games: even though the agent gets no penalty for losing, the game starts over, which is boring so it learns to avoid it.</p>
</dd>
</dl>

<p>We covered many topics in this chapter: Policy Gradients, Markov Chains, Markov decision processes, Q-Learning, Approximate Q-Learning, and Deep Q-Learning and its main variants (fixed Q-Value targets, Double DQN, Dueling DQN, and Prioritized Experience Replay). We discussed how to use TF-Agents to train agents at scale, and finally we took a quick look at a few other popular algorithms. Reinforcement Learning is a huge and exciting field, with new ideas and algorithms popping out every day, so I hope this chapter sparked your curiosity: there is a whole world to explore!</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="idm46263481376488">
<h1>Exercises</h1>
<ol>
<li>
<p>How would you define Reinforcement Learning? How is it different from regular supervised or unsupervised learning?</p>
</li>
<li>
<p>Can you think of three possible applications of RL that were not mentioned in this chapter? For each of them, what is the environment? What is the agent? What are possible actions? What are the rewards?</p>
</li>
<li>
<p>What is the discount factor? Can the optimal policy change if you modify the discount factor?</p>
</li>
<li>
<p>How do you measure the performance of a Reinforcement Learning agent?</p>
</li>
<li>
<p>What is the credit assignment problem? When does it occur? How can you alleviate it?</p>
</li>
<li>
<p>What is the point of using a replay memory?</p>
</li>
<li>
<p>What is an off-policy RL algorithm?</p>
</li>
<li>
<p>Use policy gradients to solve OpenAI gym’s "<code>LunarLander-v2</code>“. You will need to install the Box2D dependencies (<code>python3 -m pip install gym[box2d]</code>).</p>
</li>
<li>
<p>Use TF-Agents to train an agent that can achieve a superhuman level at "<code>SpaceInvaders-v4</code>" using any of the available algorithms.</p>
</li>
<li>
<p>If you have about $100 to spare, you can purchase a Raspberry Pi 3 plus some cheap robotics components, install TensorFlow on the Pi, and go wild! For an example, check out this <a href="https://homl.info/2">fun post</a> by Lukas Biewald, or take a look at GoPiGo or BrickPi. Start with simple goals, like making the robot turn around to find the brightest angle (if it has a light sensor) or the closest object (if it has a sonar sensor), and move in that direction. Then you can start using Deep Learning: for example, if the robot has a camera, you can try to implement an object detection algorithm so it detects people and moves toward them. You can also try to use RL to make the agent learn on its own how to use the motors to achieve that goal. Have fun!</p>
</li>

</ol>

<p>Solutions to these exercises are available in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/app01.html#solutions_appendix">Appendix&nbsp;A</a>.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46263487004888"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263487004888-marker" class="totri-footnote">1</a></sup> For more details, be sure to check out Richard Sutton and Andrew Barto’s book on RL, <a href="https://homl.info/126"><em>Reinforcement Learning: An Introduction</em> </a>(MIT Press).</p><p data-type="footnote" id="idm46263487001944"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263487001944-marker" class="totri-footnote">2</a></sup> Volodymyr Mnih et al., Playing Atari with Deep Reinforcement Learning (DeepMind Technologies, December 2013).</p><p data-type="footnote" id="idm46263487000568"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263487000568-marker" class="totri-footnote">3</a></sup> Volodymyr Mnih et al., “Human-level Control Through Deep Reinforcement Learning,” Nature 518 (February 2015): 529–533.</p><p data-type="footnote" id="idm46263486999608"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263486999608-marker" class="totri-footnote">4</a></sup> Check out the videos of DeepMind’s system learning to play <em>Space Invaders</em>, <em>Breakout</em>, and other video games at <a href="https://homl.info/dqn3"><em class="hyperlink">https://homl.info/dqn3</em></a>.</p><p data-type="footnote" id="idm46263486981432"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263486981432-marker" class="totri-footnote">5</a></sup> Image (a) is from NASA (public domain). (b) is a screenshot from the Ms. Pac-Man game, copyright Atari (fair use in this chapter). Image (c) and (d) are reproduced from Wikipedia. (c) was created by user Stevertigo and released under <a href="https://creativecommons.org/licenses/by-sa/2.0/">Creative Commons BY-SA 2.0</a>. (d) is in the public domain. (e) was reproduced from Pixabay, released under <a href="https://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0</a>.</p><p data-type="footnote" id="idm46263486965656"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263486965656-marker" class="totri-footnote">6</a></sup> It is often better to give the poor performers a slight chance of survival, to preserve some diversity in the “gene pool.”</p><p data-type="footnote" id="idm46263486964712"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263486964712-marker" class="totri-footnote">7</a></sup> If there is a single parent, this is called <em>asexual reproduction</em>. With two (or more) parents, it is called <em>sexual reproduction</em>. An offspring’s genome (in this case a set of policy parameters) is randomly composed of parts of its parents’ genomes.</p><p data-type="footnote" id="idm46263486962728"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263486962728-marker" class="totri-footnote">8</a></sup> One interesting example of a genetic algorithm used for Reinforcement Learning is the <a href="https://homl.info/neat"><em>NeuroEvolution of Augmenting Topologies</em></a> (NEAT) algorithm.</p><p data-type="footnote" id="idm46263486826568"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263486826568-marker" class="totri-footnote">9</a></sup> This is called <em>gradient ascent</em>. It’s just like gradient descent but in the opposite direction: maximizing instead of minimizing.</p><p data-type="footnote" id="idm46263486817544"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263486817544-marker">10</a></sup> OpenAI is an artificial intelligence research company, funded in part by Elon Musk. Its stated goal is to promote and develop friendly AIs that will benefit humanity (rather than exterminate it).</p><p data-type="footnote" id="idm46263486273512"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263486273512-marker">11</a></sup> Ronald J. Williams, “Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Leaning,” Machine Learning 8 (1992) : 229–256.</p><p data-type="footnote" id="idm46263485472760"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263485472760-marker">12</a></sup> Richard Bellman, “A Markovian Decision Process,” Journal of Mathematics and Mechanics 6, no. 5 (1957): 679–684.</p><p data-type="footnote" id="idm46263483703656"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263483703656-marker">13</a></sup> This great <a href="https://homl.info/rlhard">2018 post</a> by Alex Irpan nicely lays out RL’s biggest difficulties and limitations.</p><p data-type="footnote" id="idm46263483542728"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263483542728-marker">14</a></sup> Hado van Hasselt et al., “Deep Reinforcement Learning with Double Q-Learning” in proceedings of Association for the Advancement of Artificial Intelligence 2015: 2094–2100.</p><p data-type="footnote" id="idm46263483419400"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263483419400-marker">15</a></sup> Tom Schaul et al., “Prioritized Experience Replay,” ICLR (2016).</p><p data-type="footnote" id="idm46263483412952"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263483412952-marker">16</a></sup> It could also just be that the rewards are noisy, in which case there are better methods for estimating an experience’s importance (see the paper for some examples).</p><p data-type="footnote" id="idm46263483398872"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263483398872-marker">17</a></sup> Ziyu Wang et al., Dueling Network Architectures for Deep Reinforcement Learning (London: Google DeepMind, 2015).</p><p data-type="footnote" id="idm46263483381784"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263483381784-marker">18</a></sup> Matteo Hessel et al., “Rainbow: Combining Improvements in Deep Reinforcement Learning” in Association for the Advancement of Artificial Intelligence 2017: 3215–3222.</p><p data-type="footnote" id="idm46263483214264"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263483214264-marker">19</a></sup> If you don’t know this game, it’s simple: a ball bounces around and breaks bricks when it touches them. You control a paddle near the bottom of the screen. The paddle can go left or right, and you must get the ball to break every brick, while preventing it from touching the bottom of the screen.</p><p data-type="footnote" id="idm46263482701496"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263482701496-marker">20</a></sup> Since there are just 3 primary colors, you cannot just display an image with four color channels. For this reason, I combined the last channel with the first three to get the RGB image represented here. Pink is actually a mix of blue and red, but the agent sees four independent channels.</p><p data-type="footnote" id="idm46263482321224"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263482321224-marker">21</a></sup> At the time of this writing, there is no Prioritized Experience Replay buffer yet, but one will likely be open sourced soon.</p><p data-type="footnote" id="idm46263481421784"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263481421784-marker">22</a></sup> For a comparison of this algorithm’s performance on various Atari games, see Figure 3 in DeepMind’s <a href="https://homl.info/dqn2">2015 paper</a>.</p><p data-type="footnote" id="idm46263481415000"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263481415000-marker">23</a></sup> Volodymyr Mnih et al., “Asynchonous Methods for Deep Reinforcement Learning,” JMLR 48 (2016).</p><p data-type="footnote" id="idm46263481408264"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263481408264-marker">24</a></sup> Tuomas Haarnoja et al., “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor” in ICML 2018: 1856–1865.</p><p data-type="footnote" id="idm46263481404920"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263481404920-marker">25</a></sup> John Schulman et al., Proximal Policy Optimization Algorithms (OpenAI, 2017).</p><p data-type="footnote" id="idm46263481402776"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263481402776-marker">26</a></sup> John Schulman et al., “Trust Region Policy Optimization,” JMLR 37 (2015).</p><p data-type="footnote" id="idm46263481400792"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#idm46263481400792-marker">27</a></sup> Deepak Pathak et al., “Curiosity-Driven Exploration by Self-Supervised Prediction,” JMLR (2017).</p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-19" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders">
		
		<li class="copy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#">Add Highlight</a></li>
		<li class="add-note"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#">
			Add Note
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">17. Representation Learning and Generative Learning Using Autoencoders and GANs</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">19. Training and Deploying TensorFlow Models at Scale</div>
        </a>
    
  
  </div>

</section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    



        
      </div>
      



  <footer class="pagefoot">
    <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#" class="icon-up" style="display: none;"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      <li class="full-support"><a href="https://www.oreilly.com/online-learning/support/">Support</a></li>
      <li><a href="https://www.oreilly.com/online-learning/apps.html">Get the App</a></li>
      
        <li><a href="https://learning.oreilly.com/accounts/logout/">Sign Out</a></li>
      
    </ul>
    <span class="copyright">© 2019 <a href="https://learning.oreilly.com/" target="_blank">Safari</a>.</span>
    <a href="https://learning.oreilly.com/terms/">Terms of Service</a> /
    <a href="https://learning.oreilly.com/membership-agreement/">Membership Agreement</a> /
    <a href="https://www.oreilly.com/privacy.html">Privacy Policy</a>
  </footer>

<script type="text/javascript">window.NREUM||(NREUM={});NREUM.info={"applicationID":"172641827,79672898,93931619","errorBeacon":"bam.nr-data.net","agent":"","applicationTime":451,"licenseKey":"510f1a6865","transactionName":"YgdaZ0NSW0cEB0RdWltNfkZfUEFdCgofXFBHDVYdR1pQQxZeRl1QQj1aWkU=","beacon":"bam.nr-data.net","queueTime":4}</script>


    
    <script src="./Chapter18_files/saved_resource" charset="utf-8"></script><script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","1732687426968531");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=1732687426968531&amp;ev=PageView&amp;noscript=1"></noscript><script type="text/javascript" id="">(function(){window.medalliaUserIdentifier=document.documentElement.dataset.userUuid;window.medalliaUserName=document.documentElement.dataset.username})();</script>
<script type="text/javascript" id="" src="./Chapter18_files/embed.js.download"></script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script>
<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("set","agent","tmgoogletagmanager","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>

<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>
<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.09556792590964802"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.5229133033930915" width="0" height="0" alt="" src="./Chapter18_files/0"></div>
    <script src="./Chapter18_files/saved_resource(1)" charset="utf-8"></script>
  

<script src="./Chapter18_files/saved_resource(2)" type="text/javascript"></script><script type="text/javascript" id="">window._pp=window._pp||[];if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/register/")_pp.targetUrl="/confirm/trial";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/subscribe/")_pp.targetUrl="/confirm/paid";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/signup/")_pp.targetUrl="/confirm/paid";_pp.siteId="2508";
_pp.siteUId="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79";_pp.orderValue="undefined";_pp.orderId="undefined";(function(){var ppjs=document.createElement("script");ppjs.type="text/javascript";ppjs.async=true;ppjs.src=("https:"==document.location.protocol?"https:":"http:")+"//cdn.pbbl.co/r/"+_pp.siteId+".js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(ppjs,s)})();</script><div class="annotator-notice"></div><div class="font-flyout" style="top: 201px; left: 1194px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html#">Reset</a>
</div>
</div><script type="text/javascript" async="" src="./Chapter18_files/generic1566415868241.js.download" charset="UTF-8"></script><div style="display: none; visibility: hidden;"><script>(function(){if(null!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')&&void 0!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')){var a=!1;window.addEventListener("blur",function(){a&&dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"facebook",eventVal:0,nonInteraction:0})});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseover",function(){window.focus();
a=!0});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseout",function(){a=!1})}try{window.twttr=function(b,a,d){var c,e=b.getElementsByTagName(a)[0];if(!b.getElementById(d))return b=b.createElement(a),b.id=d,b.src="//platform.twitter.com/widgets.js",e.parentNode.insertBefore(b,e),window.twttr||(c={_e:[],ready:function(a){c._e.push(a)}})}(document,"script","twitter-wjs"),twttr.ready(function(a){a.events.bind("tweet",trackTwitter)})}catch(b){}})();
null!==document.querySelector(".IN-widget")&&void 0!==document.querySelector(".IN-widget")&&document.querySelector(".IN-widget").addEventListener("click",function(){dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"linkedin",eventVal:0,nonInteraction:0})});
function trackTwitter(a){a&&(a.target&&"IFRAME"==a.target.nodeName&&(opt_target=extractParamFromUri(a.target.src,"url")),dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"twitter",eventVal:0,nonInteraction:0}))}function extractParamFromUri(a,b){if(a){var c=new RegExp("[\\?\x26#]"+b+"\x3d([^\x26#]*)");c=c.exec(a);if(null!=c)return unescape(c[1])}};</script></div><span><div id="KampyleAnimationContainer" style="z-index: 2147483000; border: 0px; position: fixed; display: block; width: 0px; height: 0px;"></div></span><iframe scrolling="no" frameborder="0" allowtransparency="true" src="./Chapter18_files/widget_iframe.097c1f5038f9e8a0d62a39a892838d66.html" title="Twitter settings iframe" style="display: none;"></iframe><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_AMS, sans-serif;"></div></div></body></html>