<!DOCTYPE html>
<!-- saved from url=(0091)https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html -->
<html class=" js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/hands-on-machine-learning/9781492032632/part01.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="4626953" data-user-uuid="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79" data-username="17481074" data-account-type="B2B" data-activated-trial-date="" data-archive="9781492032632" data-publishers="O&#39;Reilly Media, Inc." data-htmlfile-name="part01.html" data-epub-title="Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781492032632"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><script type="text/javascript" async="" src="./Chapter19_files/cool-2.1.15.min.js.download"></script><script type="text/javascript" src="./Chapter19_files/510f1a6865"></script><script id="twitter-wjs" src="./Chapter19_files/widgets.js.download"></script><script src="./Chapter19_files/nr-1130.min.js.download"></script><script type="text/javascript" async="" src="./Chapter19_files/2508.js.download"></script><script async="" src="./Chapter19_files/fbevents.js.download"></script><script type="text/javascript" async="" src="./Chapter19_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter19_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter19_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter19_files/analytics.js.download"></script><script type="text/javascript" async="" src="./Chapter19_files/ec.js.download"></script><script type="text/javascript" async="" src="./Chapter19_files/bat.js.download"></script><script type="text/javascript" async="" src="./Chapter19_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter19_files/insight.min.js.download"></script><script type="text/javascript" async="" src="./Chapter19_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter19_files/linkid.js.download"></script><script async="" src="./Chapter19_files/gtm.js.download"></script><script async="" src="./Chapter19_files/analytics.js.download"></script><script type="text/javascript">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if("function"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e("handle"),a=e(3),u=e(4),f=e("ee").get("tracer"),c=e("loader"),s=NREUM;"undefined"==typeof window.newrelic&&(newrelic=s);var p=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],d="api-",l=d+"ixn-";a(p,function(e,n){s[n]=o(d+n,!0,"api")}),s.addPageAction=o(d+"addPageAction",!0),s.setCurrentRouteName=o(d+"routeName",!0),n.exports=newrelic,s.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(e,n){var t={},r=this,o="function"==typeof n;return i(l+"tracer",[c.now(),e,t],r),function(){if(f.emit((o?"":"no-")+"fn-start",[c.now(),r,o],t),o)try{return n.apply(this,arguments)}catch(e){throw f.emit("fn-err",[arguments,this,e],t),e}finally{f.emit("fn-end",[c.now()],t)}}}};a("actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(e,n){m[n]=o(l+n)}),newrelic.noticeError=function(e,n){"string"==typeof e&&(e=new Error(e)),i("err",[e,c.now(),!1,n])}},{}],2:[function(e,n,t){function r(e,n){if(!o)return!1;if(e!==o)return!1;if(!n)return!0;if(!i)return!1;for(var t=i.split("."),r=n.split("."),a=0;a<r.length;a++)if(r[a]!==t[a])return!1;return!0}var o=null,i=null,a=/Version\/(\S+)\s+Safari/;if(navigator.userAgent){var u=navigator.userAgent,f=u.match(a);f&&u.indexOf("Chrome")===-1&&u.indexOf("Chromium")===-1&&(o="Safari",i=f[1])}n.exports={agent:o,version:i,match:r}},{}],3:[function(e,n,t){function r(e,n){var t=[],r="",i=0;for(r in e)o.call(e,r)&&(t[i]=n(r,e[r]),i+=1);return t}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],4:[function(e,n,t){function r(e,n,t){n||(n=0),"undefined"==typeof t&&(t=e?e.length:0);for(var r=-1,o=t-n||0,i=Array(o<0?0:o);++r<o;)i[r]=e[n+r];return i}n.exports=r},{}],5:[function(e,n,t){n.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(e,n,t){function r(){}function o(e){function n(e){return e&&e instanceof r?e:e?f(e,u,i):i()}function t(t,r,o,i){if(!d.aborted||i){e&&e(t,r,o);for(var a=n(o),u=v(t),f=u.length,c=0;c<f;c++)u[c].apply(a,r);var p=s[y[t]];return p&&p.push([b,t,r,a]),a}}function l(e,n){h[e]=v(e).concat(n)}function m(e,n){var t=h[e];if(t)for(var r=0;r<t.length;r++)t[r]===n&&t.splice(r,1)}function v(e){return h[e]||[]}function g(e){return p[e]=p[e]||o(t)}function w(e,n){c(e,function(e,t){n=n||"feature",y[t]=n,n in s||(s[n]=[])})}var h={},y={},b={on:l,addEventListener:l,removeEventListener:m,emit:t,get:g,listeners:v,context:n,buffer:w,abort:a,aborted:!1};return b}function i(){return new r}function a(){(s.api||s.feature)&&(d.aborted=!0,s=d.backlog={})}var u="nr@context",f=e("gos"),c=e(3),s={},p={},d=n.exports=o();d.backlog=s},{}],gos:[function(e,n,t){function r(e,n,t){if(o.call(e,n))return e[n];var r=t();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,n,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return e[n]=r,r}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],handle:[function(e,n,t){function r(e,n,t,r){o.buffer([e],r),o.emit(e,n,t)}var o=e("ee").get("handle");n.exports=r,r.ee=o},{}],id:[function(e,n,t){function r(e){var n=typeof e;return!e||"object"!==n&&"function"!==n?-1:e===window?0:a(e,i,function(){return o++})}var o=1,i="nr@id",a=e("gos");n.exports=r},{}],loader:[function(e,n,t){function r(){if(!E++){var e=x.info=NREUM.info,n=l.getElementsByTagName("script")[0];if(setTimeout(s.abort,3e4),!(e&&e.licenseKey&&e.applicationID&&n))return s.abort();c(y,function(n,t){e[n]||(e[n]=t)}),f("mark",["onload",a()+x.offset],null,"api");var t=l.createElement("script");t.src="https://"+e.agent,n.parentNode.insertBefore(t,n)}}function o(){"complete"===l.readyState&&i()}function i(){f("mark",["domContent",a()+x.offset],null,"api")}function a(){return O.exists&&performance.now?Math.round(performance.now()):(u=Math.max((new Date).getTime(),u))-x.offset}var u=(new Date).getTime(),f=e("handle"),c=e(3),s=e("ee"),p=e(2),d=window,l=d.document,m="addEventListener",v="attachEvent",g=d.XMLHttpRequest,w=g&&g.prototype;NREUM.o={ST:setTimeout,SI:d.setImmediate,CT:clearTimeout,XHR:g,REQ:d.Request,EV:d.Event,PR:d.Promise,MO:d.MutationObserver};var h=""+location,y={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-1130.min.js"},b=g&&w&&w[m]&&!/CriOS/.test(navigator.userAgent),x=n.exports={offset:u,now:a,origin:h,features:{},xhrWrappable:b,userAgent:p};e(1),l[m]?(l[m]("DOMContentLoaded",i,!1),d[m]("load",r,!1)):(l[v]("onreadystatechange",o),d[v]("onload",r)),f("mark",["firstbyte",u],null,"api");var E=0,O=e(5)},{}]},{},["loader"]);</script><link rel="apple-touch-icon" href="https://learning.oreilly.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://learning.oreilly.com/favicon.ico" type="image/x-icon"><link href="./Chapter19_files/css" rel="stylesheet" type="text/css"><title>19. Training and Deploying TensorFlow Models at Scale - Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition</title><link rel="stylesheet" href="./Chapter19_files/output.68851547a55f.css" type="text/css"><link rel="stylesheet" type="text/css" href="./Chapter19_files/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="./Chapter19_files/font-awesome.min.css"><style type="text/css" title="ibis-book">@charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000 !important;padding-top:10px !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:2em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content p[data-type="footnote"]{font-size:90% !important;line-height:1.2em !important;margin-left:2.5em !important;text-indent:-2.3em !important}#sbo-rt-content p[data-type="footnote"] sup{display:inline-block !important;position:static !important;width:2em !important;text-align:right !important;font-size:100% !important;padding-right:.5em !important}#sbo-rt-content p[data-type="footnote"] a[href$="-marker"]{font-family:sans-serif !important;font-size:90% !important;color:#8e0012 !important}#sbo-rt-content p[data-type="footnote"] a[data-type="xref"]{margin:0 !important;padding:0 !important;text-indent:0 !important}#sbo-rt-content a[data-type="noteref"]{font-family:sans-serif !important;color:#8e0012;margin-left:0;padding-left:0}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dd{margin-left:1.5em !important;margin-bottom:.25em}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul>li,#sbo-rt-content ol ul,#sbo-rt-content ol ul>li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul>li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul>li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul>li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol>li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol>li,#sbo-rt-content ul ol,#sbo-rt-content ul ol>li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol>li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol>li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:middle;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content table.border tbody>tr:last-child>td{border-bottom:transparent}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:1em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content .width-10{width:10vw !important}#sbo-rt-content .width-20{width:20vw !important}#sbo-rt-content .width-30{width:30vw !important}#sbo-rt-content .width-40{width:40vw !important}#sbo-rt-content .width-50{width:50vw !important}#sbo-rt-content .width-60{width:60vw !important}#sbo-rt-content .width-70{width:70vw !important}#sbo-rt-content .width-80{width:80vw !important}#sbo-rt-content .width-90{width:90vw !important}#sbo-rt-content .width-full,#sbo-rt-content .width-100{width:100vw !important}#sbo-rt-content div[data-type="equation"].fifty-percent img{width:50%}</style><script> // <![CDATA[
    var g = {
      position_cache: {
        
          "chapter": "/api/v1/book/9781492032632/chapter/part01.html",
          "book_id": "9781492032632",
          "chapter_uri": "part01.html",
          "position": 100.0,
          "user_uuid": "d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79",
          "next_chapter_uri": "/library/view/hands-on-machine-learning/9781492032632/ch01.html"
        
      },
      title: "Hands\u002Don Machine Learning with Scikit\u002DLearn, Keras, and TensorFlow, 2nd Edition",
      author_list: "Aurélien Géron",
      format: "book",
      source: "application/epub+zip",
      is_system_book: true,
      is_public: false,
      loaded_from_server: true,
      allow_scripts: false,
      has_mathml: true
    };
    // ]]></script><script src="./Chapter19_files/modernizr.8e35451ddb64.js.download"></script><script>
    
      

      
        
          window.PUBLIC_ANNOTATIONS = true;
        
      

      window.MOBILE_PUBLIC_ANNOTATIONS = false;

    

    
      window.PRIVACY_CONTROL_OVERRIDE = false;
    

      window.PRIVACY_CONTROL_SWITCH = true;

      window.PUBLISHER_PAGES = true;

      window.SBO = {
        "constants": {
          "SITB_ENDPOINT": "/api/v2/sitb/",
          "SEARCH_SELECT_ENDPOINT": "https://learning.oreilly.com/api/v2/search/select/",
          "ENABLE_ONLINE_TRAINING": false
        }
      };
  </script><link rel="canonical" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta name="description" content=" Part I. The Fundamentals of Machine Learning "><meta property="og:title" content="I. The Fundamentals of Machine Learning"><meta itemprop="isPartOf" content="/library/view/hands-on-machine-learning/9781492032632/"><meta itemprop="name" content="I. The Fundamentals of Machine Learning"><meta property="og:url" itemprop="url" content="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://learning.oreilly.com/library/cover/9781492032632/"><meta property="og:description" itemprop="description" content=" Part I. The Fundamentals of Machine Learning "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O&#39;Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781492032649"><meta property="og:book:author" itemprop="author" content="Aurélien Géron"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: &lt;%= font_size %&gt; !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: &lt;%= font_family %&gt; !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: &lt;%= column_width %&gt;% !important; margin: 0 auto !important; }"></style><noscript><meta http-equiv="refresh" content="0; url=/library/no-js/" /></noscript><script type="text/javascript">
    (function(i,s,o,g,r,a,m) {
      i['GoogleAnalyticsObject']=r;
      i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();
      a=s.createElement(o),m=s.getElementsByTagName(o)[0];
      a.async=1;
      a.src=g;
      m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    var matches = document.cookie.match(/BrowserCookie\s*=\s*([a-f0-9\-]{36})/),
      user_uuid = null;

    if (matches && matches.length === 2) {
      user_uuid = matches[1];
    }

  
    ga('create', 'UA-39299553-7', {'userId': 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79' });
  

  
    
      ga('set', 'dimension1', 'B2B');
    
  

  ga('set', 'dimension6', user_uuid);

  
    ga('set', 'dimension2', 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79');
    
      ga('set', 'dimension7', '0012M0000229keZQAQ');
    
  

  

  

  //enable enhanced link tracking
  ga('require', 'linkid', 'linkid.js');

  // reading interface will track pageviews itself
  if (document.location.pathname.indexOf("/library/view") !== 0) {
    ga('send', 'pageview');
  }
  </script><script>
    var dataLayer = window.dataLayer || [];

    
      window.medalliaVsgUserIdentifier = 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79';
      dataLayer.push({userIdentifier: 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79'});
      dataLayer.push({loggedIn: 'yes'});

      
        window.medalliaVsgAccountIdentifier = '21bed0a7-6b7b-470c-8fa0-40a52db0b491';
        
        dataLayer.push({orgID: '21bed0a7-6b7b-470c-8fa0-40a52db0b491'});
        

        window.medalliaVsgIsIndividual = false;
        
          
          dataLayer.push({learningAccountType: 'enterprise'});
          
        

        
          dataLayer.push({learningPaidAccount: 'yes'});
        
      
    

    (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-5P4V6Z');
    (function () {
      var VERSION = 'V1.1';
      var AUTHOR = 'Awwad';
      if (!window.GtmHelper)
        window.GtmHelper = function () {
          var instance = this;
          var loc = document.location;
          this.version = VERSION;
          this.author = AUTHOR;
          this.readCookie = function (name) {
            var nameEQ = name + "=";
            var ca = document.cookie.split(';');
            for (var i = 0; i < ca.length; i++) {
              var c = ca[i];
              while (c.charAt(0) == ' ') c = c.substring(1, c.length);
              if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length, c.length);
            }
            return null;
          };
          this.createCookie = function (name, value, days, cookieDomain) {
            var domain = "";
            var expires = "";

            if (days) {
              var date = new Date();
              date.setTime(date.getTime() + Math.ceil(days * 24 * 60 * 60 * 1000));
              var expires = " expires=" + date.toGMTString() + ";";
            }

            if (typeof (cookieDomain) != 'undefined')
              domain = " domain=" + cookieDomain + "; ";

            document.cookie = name + "=" + value + ";" + expires + domain + "path=/";
          };

          this.isDuplicated = function (currentTransactionId) {
            // the previous transaction id:
            var previousTransIdValue = this.readCookie("previousTransId");

            if (currentTransactionId === previousTransIdValue) {
              return true; // Duplication
            } else {
              return false;
            }
          };
        }
    })()
  </script><script defer="" src="./Chapter19_files/vendor.a48a756c5182.js.download"></script><script defer="" src="./Chapter19_files/reader.f2a0c6bd2fee.js.download"></script><script src="./Chapter19_files/f(1).txt"></script><script src="./Chapter19_files/f(2).txt"></script><script src="./Chapter19_files/f(3).txt"></script><script src="./Chapter19_files/f(4).txt"></script><script async="" src="./Chapter19_files/MathJax.js.download"></script><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 2147483020;
}
.annotator-filter {
  z-index: 2147483010;
}</style><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
</style><script async="true" type="text/javascript" src="./Chapter19_files/roundtrip.js.download"></script><style type="text/css" id="kampyleStyle">.noOutline{outline: none !important;}.wcagOutline:focus{outline: 1px dashed #595959 !important;outline-offset: 2px !important;transition: none !important;}</style><script async="true" type="text/javascript" src="./Chapter19_files/roundtrip.js.download"></script><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Math-bold-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Script; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_AMS; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf?V=2.7.1') format('opentype')}
</style></head>


<body class="reading sidenav  scalefonts library nav-collapsed"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

    
  <noscript> 
    <iframe src="//www.googletagmanager.com/ns.html?id=GTM-5P4V6Z"
            height="0" width="0"
            style="display:none;visibility:hidden">
    </iframe>
  </noscript>



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#container" class="skip">Skip to content</a><header class="topbar t-topbar"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://learning.oreilly.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M11.738 14H9.254v-3.676a.617.617 0 0 0-.621-.613H7.39a.617.617 0 0 0-.62.613V14H4.284a.617.617 0 0 1-.622-.613V10.22c0-.327.132-.64.367-.87l3.547-3.493a.627.627 0 0 1 .875 0l3.54 3.499c.234.229.366.54.367.864v3.167a.617.617 0 0 1-.62.613zM7.57 2.181a.625.625 0 0 1 .882 0l5.77 5.692-.93.92-5.28-5.209-5.28 5.208-.932-.919 5.77-5.692z"></path></svg><span>Safari Home</span></a></li><li><a href="https://learning.oreilly.com/resource-centers/" class="t-resource-centers-nav l0 nav-icn"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="16px" height="16px" viewBox="0 0 16 16" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="Topic-Page-Design" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="Heron-Button" transform="translate(-20.000000, -78.000000)" fill="#4A3A30"><g id="Group-9" transform="translate(20.000000, 78.000000)"><rect id="Rectangle" x="9.6" y="0" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="9.6" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="0" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect></g></g></g></svg><span>Resource Centers</span></a></li><li><a href="https://learning.oreilly.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://learning.oreilly.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://learning.oreilly.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://learning.oreilly.com/learning-paths/" class="l1 nav-icn t-learningpaths-nav js-toggle-menu-item"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="32px" height="32px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 52.5 (67469) - http://www.bohemiancoding.com/sketch --><title>Mask</title><desc>Created with Sketch.</desc><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><path d="M0,16.0214227 C0,15.0387209 0.796453294,14.2411658 1.77779753,14.2411658 C2.75914177,14.2411658 3.55559506,15.0387209 3.55559506,16.0214227 C3.55559506,17.0041246 2.75914177,17.8016797 1.77779753,17.8016797 C0.796453294,17.8016797 0,17.0041246 0,16.0214227 Z M9.77788642,5.22914885 C8.9280992,5.72049977 7.84008711,5.42853763 7.34941499,4.57757479 C6.85874287,3.72661195 7.15030167,2.63709467 8.00008889,2.14574375 C8.84987611,1.65439282 9.9378882,1.94635496 10.4285603,2.7973178 C10.9192324,3.64828064 10.6276736,4.73779792 9.77788642,5.22914885 Z M4.57213969,7.35869225 C5.42192691,7.85004318 5.71348571,8.93956046 5.22281359,9.79052329 C4.73214147,10.6414861 3.64412938,10.9334483 2.79434216,10.4420974 C1.94455494,9.95074642 1.65299614,8.86122915 2.14366826,8.01026631 C2.63434038,7.15930347 3.72235247,6.86734132 4.57213969,7.35869225 Z M2.79434216,21.6007481 C3.64412938,21.1093972 4.73214147,21.4013594 5.22281359,22.2523222 C5.71348571,23.103285 5.42192691,24.1928023 4.57213969,24.6841532 C3.72235247,25.1755042 2.63434038,24.883542 2.14366826,24.0325792 C1.65299614,23.1816163 1.94455494,22.0920991 2.79434216,21.6007481 Z M7.34941499,27.4652707 C7.84008711,26.6143079 8.9280992,26.3223457 9.77788642,26.8136966 C10.6276736,27.3050476 10.9192324,28.3945649 10.4285603,29.2455277 C9.9378882,30.0964905 8.84987611,30.3884527 8.00008889,29.8971017 C7.15030167,29.4057508 6.85874287,28.3162335 7.34941499,27.4652707 Z M18.7118524,11.3165596 C21.3074367,12.8173162 22.1963355,16.1392758 20.6976522,18.738451 C19.1989689,21.3358459 15.8815987,22.2259744 13.2860143,20.726998 C10.6922077,19.2262414 9.80330893,15.9042818 11.3002144,13.3051066 C12.7988978,10.7059314 16.116268,9.81580294 18.7118524,11.3165596 Z M26.7821642,27.8093944 L30.1315348,31.1633985 C30.3982044,31.4304371 30.2097579,31.8844026 29.8346426,31.8844026 L21.5945511,31.8844026 C21.1287681,31.8844026 20.751875,31.5069881 20.751875,31.0405608 L20.751875,22.7890697 C20.751875,22.4134355 21.2052134,22.2247282 21.4701052,22.4899865 L24.2843587,25.3081333 C26.8337204,23.0240636 28.4444049,19.7092251 28.4444049,16.0223129 C28.4444049,9.15052091 22.8621207,3.56051397 15.9998222,3.56051397 L15.9998222,0 C24.8230314,0 32,7.18689745 32,16.0223129 C32,20.6919269 29.9750886,24.8790914 26.7821642,27.8093944 Z" id="Mask" fill="#8B889A"></path></g></svg><span>Learning Paths</span></a></li><li class="nav-highlights"><a href="https://learning.oreilly.com/u/d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" fill="#4A3C31"><path d="M13.325 18.071H8.036c0-6.736 4.324-10.925 14.464-12.477V0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35c5.142 0 9.175-3.515 9.175-8.816 0-4.628-2.367-7.293-6.253-8.113zm27.5 0h-5.26c0-6.736 4.295-10.925 14.435-12.477V0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35c5.113 0 9.146-3.515 9.146-8.816 0-4.628-2.338-7.293-6.253-8.113z" fill-rule="evenodd"></path></svg><span>Highlights</span></a></li><li><a href="https://learning.oreilly.com/u/preferences/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l1 no-icon">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://learning.oreilly.com/u/preferences/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l2">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9781492032632/chapter/part01.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"><div data-reactroot="" class="menu-dropdown-wrapper js-menu-dropdown-wrapper align-right"><img class="hidden" src="./Chapter19_files/ajax-transp.gif" alt="loading spinner"><div class="menu-control"><div class="control "><div class="js-playlists-menu"><button class="js-playlist-icon"><svg class="icon-add-to-playlist-sml" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill-rule="nonzero" fill="#000000"><g transform="translate(-1.000000, 0.000000)"><rect x="5" y="0" width="12" height="2"></rect><title>Playlists</title><path d="M4.5,14 C6.43299662,14 8,12.4329966 8,10.5 C8,8.56700338 6.43299662,7 4.5,7 C2.56700338,7 1,8.56700338 1,10.5 C1,12.4329966 2.56700338,14 4.5,14 Z M2.5,10 L4,10 L4,8.5 L5,8.5 L5,10 L6.5,10 L6.5,11 L5,11 L5,12.5 L4,12.5 L4,11 L2.5,11 L2.5,10 Z"></path><circle cx="2" cy="5" r="1"></circle><circle cx="1.94117647" cy="1" r="1"></circle><rect x="5" y="4" width="12" height="2"></rect><rect x="9" y="8" width="8" height="2"></rect><rect x="9" y="12" width="8" height="2"></rect></g></g></g></svg><div class="js-playlist-addto-label">Add&nbsp;To</div></button></div></div></div></div></div></div></li><li class="js-font-control-panel font-control-activator"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html&amp;text=Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%20I.%20The%20Fundamentals%20of%20Machine%20Learning&amp;body=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html%0D%0Afrom%20Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    
    <section role="document">
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">18. Reinforcement Learning</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/app01.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">A. Exercise Solutions</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content"><div class="annotator-wrapper"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 19. Training and Deploying TensorFlow Models at Scale"><div class="chapter" id="deployment_chapter">
<h1><span class="label">Chapter 19. </span>Training and Deploying TensorFlow <span class="keep-together">Models at Scale</span></h1>


<p>Once you have a beautiful model that makes amazing predictions, what do you do with it? Well, you need to put it in production! This could be as simple as running the model on a batch of data and perhaps writing a script that runs this model every night. However, it is often much more involved. Various parts of your infrastructure may need to use this model on live data, in which case you probably want to wrap your model in a webservice: this way, any part of your infrastructure can query your model at any time using a simple REST API (or some other protocol), as we discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch02.html#project_chapter">Chapter&nbsp;2</a>. But as time passes, you need to regularly retrain your model on fresh data and push the updated version to production. You must handle model versioning, gracefully transition from one model to the next, possibly roll back to the previous model in case of problems, and perhaps run multiple different models in parallel to perform <em>A/B experiments</em>.<sup><a data-type="noteref" id="idm46263481357064-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263481357064" class="totri-footnote">1</a></sup> If your product becomes successful, your service may start to get plenty of <em>queries per second</em> (QPS), and it must scale up to support the load. A great solution to scape up your service, as we will see in this chapter, is to use TF Serving, either on your own hardware infrastructure or via a cloud service such as Google Cloud AI Platform. It will take care of efficiently serving your model, handle graceful model transitions, and more. If you use the cloud platform, you will also get many extra features, such as powerful monitoring tools.</p>

<p>Moreover, if you have a lot of training data, and compute-intensive models, then training time may be prohibitively long. If your product needs to adapt to changes quickly, then a long training time can be a show-stopper (e.g., think of a news recommendation system promoting news from last week). Perhaps even more importantly, a long training time will prevent you from experimenting with new ideas. In Machine Learning (as in many other fields), it is hard to know in advance which ideas will work, so you should try out as many as possible, as fast as possible. One way to speed up training is to use hardware accelerators such as GPUs or TPUs. To go even faster, you can train a model across multiple machines, each equipped with multiple hardware accelerators. TensorFlow’s simple yet powerful <em>Distribution Strategies</em> API makes this easy, as we will see.</p>

<p>In this chapter we will look at how to deploy models, first to TF Serving, then to Google Cloud AI Platform. We will also take a quick look at deploying models to mobile apps, embedded devices, and web apps. Lastly, we will discuss how to speed up computations using GPUs and how to train models across multiple devices and servers using the Distribution Strategies API. That’s a lot of topics to discuss, so let’s get started!</p>






<section data-type="sect1" data-pdf-bookmark="Serving a TensorFlow Model"><div class="sect1" id="idm46263481352616">
<h1>Serving a TensorFlow Model</h1>

<p>Once you have trained a TensorFlow model, you can easily use it in any Python code: if it’s a tf.keras model, just call its <code>predict()</code> method! But as your infrastructure grows, there comes a point where it is preferable to wrap your model in a small service whose sole role is to make predictions and have the rest of the infrastructure query it (e.g., via a REST or gRPC API).<sup><a data-type="noteref" id="idm46263481350216-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263481350216" class="totri-footnote">2</a></sup> This decouples your model from the rest of the infrastructure, making it possible to easily switch model versions or scale the service up as needed (independently from the rest of your infrastructure), perform A/B experiments easily, ensure that all your software components rely on the same model versions, simplify testing and development, and more. You could create your own microservice using any technology you want (e.g., using the Flask library), but why reinvent the wheel when you can just use TF Serving?</p>








<section data-type="sect2" data-pdf-bookmark="Using TensorFlow Serving"><div class="sect2" id="idm46263481347784">
<h2>Using TensorFlow Serving</h2>

<p>TF Serving is a very efficient, battle-tested model server that’s written in C++. It can sustain a high load, serve multiple versions of your models, automatically watch a model repository to deploy the latest version of your models, and more (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#tf_serving_diagram">Figure&nbsp;19-1</a>).</p>

<figure class="smallerseventy"><div id="tf_serving_diagram" class="figure">
<img src="./Chapter19_files/mls2_1901.png" alt="mls2 1901" width="1274" height="625" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1901.png">
<h6><span class="label">Figure 19-1. </span>TF Serving can serve multiple models and automatically deploy the latest version of each model</h6>
</div></figure>

<p>So let’s suppose you have trained an MNIST model using tf.keras, and you want to deploy it to TF Serving. The first thing you have to do is export this model to TensorFlow’s <em>SavedModel format</em>.</p>










<section data-type="sect3" data-pdf-bookmark="Exporting SavedModels"><div class="sect3" id="idm46263481325176">
<h3>Exporting SavedModels</h3>

<p>TensorFlow provides a simple <code>tf.saved_model.save()</code> function to export models to the SavedModel format. All you need to do is give it the model, specify the model’s name and version number, and that’s it! The function will save the model’s computation graph and its weights:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="o">...</code><code class="p">])</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">([</code><code class="o">...</code><code class="p">])</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">([</code><code class="o">...</code><code class="p">])</code>

<code class="n">model_version</code> <code class="o">=</code> <code class="s2">"0001"</code>
<code class="n">model_name</code> <code class="o">=</code> <code class="s2">"my_mnist_model"</code>
<code class="n">model_path</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">model_name</code><code class="p">,</code> <code class="n">model_version</code><code class="p">)</code>
<code class="n">tf</code><code class="o">.</code><code class="n">saved_model</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">model_path</code><code class="p">)</code></pre>

<p>It’s usually a good idea to include all preprocessing layers in the final model you export so that it can ingest data in its natural form once it is deployed to production. This avoids having to take care of preprocessing separately within the application that uses the model. Bundling the preprocessing steps within the model also makes it simpler to update them later on and limits the risk of mismatch between a model and the preprocessing steps it requires.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Since a SavedModel saves the computation graph, it can only be used with models that are based exclusively on TensorFlow operations, excluding the <code>tf.py_function()</code> operation (which wraps arbitrary Python code). It also excludes dynamic tf.keras models (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/app07.html#tffunctions_appendix">Appendix&nbsp;G</a>), since these models cannot be converted to computation graphs. Dynamic models need to be served using other tools (e.g., Flask).</p>
</div>

<p>A SavedModel represents a version of your model. It is stored as a directory containing a <em>saved_model.pb</em> file, which defines the computation graph (represented as a serialized Protocol Buffer), and a <code>variables</code> subdirectory containing the variable values. For models containing a large number of weights, these variable values may be split across multiple files. A SavedModel also includes an <code>assets</code> subdirectory that may contain additional data, such as vocabulary files, class names, or some example instances for this model (in the following example, we don’t use assets):</p>
<pre>my_mnist_model
└── 0001
    ├── assets
    ├── saved_model.pb
    └── variables
        ├── variables.data-00000-of-00001
        └── variables.index</pre>

<p>As you might expect, you can load a SavedModel using the <code>tf.saved_model.load()</code> function. However, the returned object is not a Keras model: it represents the SavedModel, including its computation graph and variable values. You can use it like a function, and it will make predictions (make sure to pass the inputs as tensors, and you must also set the <code>training</code> argument, generally to <code>False</code>):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">saved_model</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">saved_model</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">model_path</code><code class="p">)</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">saved_model</code><code class="p">(</code><code class="n">X_new</code><code class="p">,</code> <code class="n">training</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code></pre>

<p>Alternatively, you can wrap this SavedModel’s prediction function in a Keras model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">inputs</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Input</code><code class="p">(</code><code class="n">shape</code><code class="o">=...</code><code class="p">)</code>
<code class="n">outputs</code> <code class="o">=</code> <code class="n">saved_model</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">training</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Model</code><code class="p">(</code><code class="n">inputs</code><code class="o">=</code><code class="p">[</code><code class="n">inputs</code><code class="p">],</code> <code class="n">outputs</code><code class="o">=</code><code class="p">[</code><code class="n">outputs</code><code class="p">])</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code></pre>

<p>TensorFlow also comes with a small <code>saved_model_cli</code> command-line tool to inspect SavedModels:</p>
<pre data-type="programlisting" data-code-language="shell-session"><code class="go">$ </code><strong><code class="go">export ML_PATH="$HOME/ml"</code></strong><code class="go"> # point to this project, wherever it is
</code><code class="go">$ </code><strong><code class="go">cd $ML_PATH</code></strong><code class="go">
</code><code class="go">$ </code><strong><code class="go">saved_model_cli show --dir my_mnist_model/0001 --all</code></strong><code class="go">
</code><code class="go">MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:
</code><code class="go">
</code><code class="go">signature_def['__saved_model_init_op']:
</code><code class="go">  [...]
</code><code class="go">
</code><code class="go">signature_def['serving_default']:
</code><code class="go">  The given SavedModel SignatureDef contains the following input(s):
</code><code class="go">    inputs['flatten_input'] tensor_info:
</code><code class="go">        dtype: DT_FLOAT
</code><code class="go">        shape: (-1, 28, 28)
</code><code class="go">        name: serving_default_flatten_input:0
</code><code class="go">  The given SavedModel SignatureDef contains the following output(s):
</code><code class="go">    outputs['dense_1'] tensor_info:
</code><code class="go">        dtype: DT_FLOAT
</code><code class="go">        shape: (-1, 10)
</code><code class="go">        name: StatefulPartitionedCall:0
</code><code class="go">  Method name is: tensorflow/serving/predict
</code></pre>

<p>A SavedModel contains one or more <em>metagraphs</em>. A metagraph is a computation graph plus some function signature definitions (including their input and output names, types, and shapes). Each metagraph is identified by a set of tags. For example, you may want to have a metagraph containing the full computation graph, including the training operations (this one may be tagged <code>"train"</code>, for example), and another metagraph containing a pruned computation graph with only the prediction operations, including some GPU-specific operations (this metagraph may be tagged <code>"serve", "gpu"</code>). However, when you pass a tf.keras model to the <code>tf.saved_model.save()</code> function, by default the function saves a much simpler SavedModel: it saves a single metagraph tagged <code>"serve"</code>, which contains two signature definitions, an initialization function (called <code>"__saved_model_init_op"</code>, which you do not need to worry about) and a default serving function (called <code>serving_default</code>). When saving a tf.keras model, the default serving function corresponds to the model’s <code>call()</code> function, which of course makes predictions.</p>

<p>The <code>saved_model_cli</code> tool can also be used to make predictions (for testing, not really for production). Suppose you have a NumPy array (<code>X_new</code>) containing three images of handwritten digits that you want to make predictions for. You first need to export them to NumPy’s <code>npy</code> format:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">np</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="s2">"my_mnist_tests.npy"</code><code class="p">,</code> <code class="n">X_new</code><code class="p">)</code></pre>

<p>Next, use the <code>saved_model_cli</code> command like this:</p>
<pre data-type="programlisting" data-code-language="shell-session"><code class="go">$ </code><strong><code class="go">saved_model_cli run --dir my_mnist_model/0001 --tag_set serve</code></strong><code class="go"> \
</code><code class="go">                      </code><strong><code class="go">--signature_def serving_default</code></strong><code class="go"> \
</code><code class="go">                      </code><strong><code class="go">--inputs flatten_input=my_mnist_tests.npy</code></strong><code class="go">
</code><code class="go">[...] Result for output key dense_1:
</code><code class="go">[[1.1739199e-04 1.1239604e-07 6.0210604e-04 [...] 3.9471846e-04]
</code><code class="go"> [1.2294615e-03 2.9207937e-05 9.8599273e-01 [...] 1.1113169e-07]
</code><code class="go"> [6.4066830e-05 9.6359509e-01 9.0598064e-03 [...] 4.2495009e-04]]
</code></pre>

<p>The tool’s output contains the 10 class probabilities of each of the 3 instances. Great! Now that you have a working SavedModel, the next step is to install TF Serving.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Installing TensorFlow Serving"><div class="sect3" id="idm46263481324584">
<h3>Installing TensorFlow Serving</h3>

<p>There are many ways to install TF Serving: using a Docker image,<sup><a data-type="noteref" id="idm46263480999928-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263480999928" class="totri-footnote">3</a></sup> using the system’s package manager, installing from source, and more. Let’s use the Docker option, which is highly recommended by the TensorFlow team, as it is simple to install, it will not mess with your system, and it offers high performance. You first need to install <a href="https://docker.com/">Docker</a>. Then download the official TF Serving Docker image:</p>
<pre data-type="programlisting" data-code-language="shell-session"><code class="go">$ </code><strong><code class="go">docker pull tensorflow/serving</code></strong><code class="go">
</code></pre>

<p>Now you can create a Docker container to run this image:</p>
<pre data-type="programlisting" data-code-language="shell-session"><code class="go">$ </code><strong><code class="go">docker run -it --rm -p 8500:8500 -p 8501:8501</code></strong><code class="go"> \
</code><code class="go">             </code><strong><code class="go">-v "$ML_PATH/my_mnist_model:/models/my_mnist_model"</code></strong><code class="go"> \
</code><code class="go">             </code><strong><code class="go">-e MODEL_NAME=my_mnist_model</code></strong><code class="go"> \
</code><code class="go">             </code><strong><code class="go">tensorflow/serving</code></strong><code class="go">
</code><code class="go">[...]
</code><code class="go">2019-06-01 [...] loaded servable version {name: my_mnist_model version: 1}
</code><code class="go">2019-06-01 [...] Running gRPC ModelServer at 0.0.0.0:8500 ...
</code><code class="go">2019-06-01 [...] Exporting HTTP/REST API at:localhost:8501 ...
</code><code class="go">[evhttp_server.cc : 237] RAW: Entering the event loop ...
</code></pre>

<p>That’s it! TF Serving is running. It loaded our MNIST model version 1, and it is serving it both through gRPC (on port 8500) and REST (on port 8501). Here is what all the command-line options mean:</p>

<ul>
<li>
<p><code>-it</code>:: Makes the container interactive (so you can press Ctrl-C to stop it) and displays the server’s output.</p>
</li>
<li>
<p><code>--rm</code>:: Deletes the container when you stop it (no need to clutter your machine with interrupted containers). However, it does not delete the image.</p>
</li>
<li>
<p><code>-p 8500:8500</code>:: Makes the Docker engine forward the host’s TCP port 8500 to the container’s TCP port 8500. By default, TF Serving uses this port to serve the gRPC API.</p>
</li>
<li>
<p><code>-p 8501:8501</code>:: Forwards the hosts TCP port 8501 to the container’s TCP port 8501. By default, TF Serving uses it to serve the REST API.</p>
</li>
<li>
<p><code>-v "$ML_PATH/my_mnist_model:/models/my_mnist_model"</code>:: Makes the host’s <code>$ML_PATH/my_mnist_model</code> directory available to the container at the path <em>/models/mnist_model</em>. On Windows, you may need to replace <code>/</code> with <code>\</code> in the host path (but not in the container path).</p>
</li>
<li>
<p><code>-e MODEL_NAME=my_mnist_model</code>:: Sets the container’s <code>MODEL_NAME</code> environment variable, so TF Serving knows which model to serve. By default, it will look for models in the <em>/models</em> directory, and it will automatically serve the latest version it finds.</p>
</li>
<li>
<p><code>tensorflow/serving</code>:: This is the name of the image to run.</p>
</li>
</ul>

<p>Now let’s go back to Python and query this server, first using the REST API, then the gRPC API.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Querying TF Serving Through the REST API"><div class="sect3" id="idm46263480909864">
<h3>Querying TF Serving Through the REST API</h3>

<p>Let’s start by creating the query. It must contain the name of the function signature you want to call, and of course the input data:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">json</code>

<code class="n">input_data_json</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">dumps</code><code class="p">({</code>
    <code class="s2">"signature_name"</code><code class="p">:</code> <code class="s2">"serving_default"</code><code class="p">,</code>
    <code class="s2">"instances"</code><code class="p">:</code> <code class="n">X_new</code><code class="o">.</code><code class="n">tolist</code><code class="p">(),</code>
<code class="p">})</code></pre>

<p>Note that the JSON format is 100% text-based, so the <code>X_new</code> NumPy array had to be converted to a Python list and then formatted as JSON:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">input_data_json</code>
<code class="go">'{"signature_name": "serving_default", "instances": [[[0.0, 0.0, 0.0, [...]</code>
<code class="go">0.3294117647058824, 0.725490196078431, [...very long], 0.0, 0.0, 0.0, 0.0]]]}'</code></pre>

<p>Now let’s send the input data to TF Serving by sending an HTTP POST request. This can be done easily using the <code>requests</code> library (it is not part of Python’s standard library, so you will need to install it first, e.g., using pip):</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">requests</code>

<code class="n">SERVER_URL</code> <code class="o">=</code> <code class="s1">'http://localhost:8501/v1/models/my_mnist_model:predict'</code>
<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="n">SERVER_URL</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">input_data_json</code><code class="p">)</code>
<code class="n">response</code><code class="o">.</code><code class="n">raise_for_status</code><code class="p">()</code> <code class="c1"># raise an exception in case of error</code>
<code class="n">response</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">()</code></pre>

<p>The response is a dictionary containing a single <code>"predictions"</code> key. The corresponding value is the list of predictions. This list is a Python list, so let’s convert it to a NumPy array and round the floats it contains to the second decimal:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_proba</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">response</code><code class="p">[</code><code class="s">"predictions"</code><code class="p">])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_proba</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>
<code class="go">array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],</code>
<code class="go">       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],</code>
<code class="go">       [0.  , 0.96, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.  ]])</code></pre>

<p>Hurray, we have the predictions! The model is close to 100% confident that the first image is a 7, it is 99% confident that the second image is a 2, and 96% confident that the third image is a 1.</p>

<p>The REST API is nice and simple, and it works well when the input and output data are not too large. Moreover, just about any client application can make REST queries without additional dependencies, whereas other protocols are not always so readily available. However, it is based on JSON, which is text-based and fairly verbose. For example, we had to convert the NumPy array to a Python list, and every float ended up represented as a string. This is very inefficient, both in terms of serialization/deserialization time (to convert all the floats to strings and back) and also in terms of payload size: many floats end up being represented using over 15 characters, which translates to over 120 bits for 32-bit floats! This will result in high latency and bandwidth usage when transferring large NumPy arrays.<sup><a data-type="noteref" id="idm46263480801160-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263480801160" class="totri-footnote">4</a></sup> So let’s use gRPC instead.</p>
<div data-type="tip"><h6>Tip</h6>
<p>When transferring large amounts of data, it is much better to use the gRPC API (if the client supports it), as it is based on a compact binary format and on an efficient communication protocol (based on HTTP/2 framing).</p>
</div>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Querying TF Serving Through the gRPC API"><div class="sect3" id="idm46263480909240">
<h3>Querying TF Serving Through the gRPC API</h3>

<p>The gRPC API expects a serialized <code>PredictRequest</code> Protocol Buffer as input, and it outputs a serialized <code>PredictResponse</code> Protocol Buffer. These protos are part of the <code>tensorflow-serving-api</code> library, which you must install (e.g., using pip). So first let’s create the request:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tensorflow_serving.apis.predict_pb2</code> <code class="kn">import</code> <code class="n">PredictRequest</code>

<code class="n">request</code> <code class="o">=</code> <code class="n">PredictRequest</code><code class="p">()</code>
<code class="n">request</code><code class="o">.</code><code class="n">model_spec</code><code class="o">.</code><code class="n">name</code> <code class="o">=</code> <code class="n">model_name</code>
<code class="n">request</code><code class="o">.</code><code class="n">model_spec</code><code class="o">.</code><code class="n">signature_name</code> <code class="o">=</code> <code class="s2">"serving_default"</code>
<code class="n">input_name</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">input_names</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
<code class="n">request</code><code class="o">.</code><code class="n">inputs</code><code class="p">[</code><code class="n">input_name</code><code class="p">]</code><code class="o">.</code><code class="n">CopyFrom</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">make_tensor_proto</code><code class="p">(</code><code class="n">X_new</code><code class="p">))</code></pre>

<p>The code creates a <code>PredictRequest</code> Protocol Buffer and fills in the required fields, including the model name (defined earlier), the signature name of the function we want to call, and finally the input data, in the form of a <code>Tensor</code> Protocol Buffer. The <code>tf.make_tensor_proto()</code> function creates a <code>Tensor</code> Protocol Buffer based on the given tensor or NumPy array, in this case <code>X_new</code>.</p>

<p>Next, you send the request to the server and get its response (for this, you will need the <code>grpcio</code> library, which you can install using pip):</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">grpc</code>
<code class="kn">from</code> <code class="nn">tensorflow_serving.apis</code> <code class="kn">import</code> <code class="n">prediction_service_pb2_grpc</code>

<code class="n">channel</code> <code class="o">=</code> <code class="n">grpc</code><code class="o">.</code><code class="n">insecure_channel</code><code class="p">(</code><code class="s1">'localhost:8500'</code><code class="p">)</code>
<code class="n">predict_service</code> <code class="o">=</code> <code class="n">prediction_service_pb2_grpc</code><code class="o">.</code><code class="n">PredictionServiceStub</code><code class="p">(</code><code class="n">channel</code><code class="p">)</code>
<code class="n">response</code> <code class="o">=</code> <code class="n">predict_service</code><code class="o">.</code><code class="n">Predict</code><code class="p">(</code><code class="n">request</code><code class="p">,</code> <code class="n">timeout</code><code class="o">=</code><code class="mf">10.0</code><code class="p">)</code></pre>

<p>The code is quite straightforward: after the imports, we create a gRPC communication channel to localhost on TCP port 8500, then we create a gRPC service over this channel, and we use it to send a request, with a 10-second timeout (not that the call is synchronous: it will block until it receives the response or the timeout period expires). In this example the channel is insecure (no encryption, no authentication), but gRPC and TensorFlow Serving also support secure channels over SSL/TLS.</p>

<p>Next, let’s convert the <code>PredictResponse</code> Protocol Buffer to a tensor:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">output_name</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">output_names</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
<code class="n">outputs_proto</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">outputs</code><code class="p">[</code><code class="n">output_name</code><code class="p">]</code>
<code class="n">y_proba</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">make_ndarray</code><code class="p">(</code><code class="n">outputs_proto</code><code class="p">)</code></pre>

<p>If you run this code and print <code>y_proba.numpy().round(2)</code>, you will get the exact same estimated class probabilities as earlier. And that’s all there is to it: in just a few lines of code, you can now access your TensorFlow model remotely, either using REST or gRPC.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Deploying a new model version"><div class="sect3" id="idm46263480752328">
<h3>Deploying a new model version</h3>

<p>Now let’s create a new model version and export a SavedModel to the <em>my_mnist_model/0002</em> directory, just like earlier:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="o">...</code><code class="p">])</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">([</code><code class="o">...</code><code class="p">])</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">([</code><code class="o">...</code><code class="p">])</code>

<code class="n">model_version</code> <code class="o">=</code> <code class="s2">"0002"</code>
<code class="n">model_name</code> <code class="o">=</code> <code class="s2">"my_mnist_model"</code>
<code class="n">model_path</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">model_name</code><code class="p">,</code> <code class="n">model_version</code><code class="p">)</code>
<code class="n">tf</code><code class="o">.</code><code class="n">saved_model</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">model_path</code><code class="p">)</code></pre>

<p>At regular intervals (the delay is configurable), TensorFlow Serving checks for new model versions. If it finds one, it will automatically handle the transition gracefully: by default, it will answer pending requests (if any) with the previous model version, while handling new requests with the new version.<sup><a data-type="noteref" id="idm46263480466632-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263480466632" class="totri-footnote">5</a></sup> As soon as every pending request has been answered, the previous model version is unloaded. You can see this at work in the TensorFlow Serving logs:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[</code><code class="o">...</code><code class="p">]</code>
<code class="n">reserved</code> <code class="n">resources</code> <code class="n">to</code> <code class="n">load</code> <code class="n">servable</code> <code class="p">{</code><code class="n">name</code><code class="p">:</code> <code class="n">my_mnist_model</code> <code class="n">version</code><code class="p">:</code> <code class="mi">2</code><code class="p">}</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>
<code class="n">Reading</code> <code class="n">SavedModel</code> <code class="n">from</code><code class="p">:</code> <code class="o">/</code><code class="n">models</code><code class="o">/</code><code class="n">my_mnist_model</code><code class="o">/</code><code class="mo">0002</code>
<code class="n">Reading</code> <code class="n">meta</code> <code class="n">graph</code> <code class="k">with</code> <code class="n">tags</code> <code class="p">{</code> <code class="n">serve</code> <code class="p">}</code>
<code class="n">Successfully</code> <code class="n">loaded</code> <code class="n">servable</code> <code class="n">version</code> <code class="p">{</code><code class="n">name</code><code class="p">:</code> <code class="n">my_mnist_model</code> <code class="n">version</code><code class="p">:</code> <code class="mi">2</code><code class="p">}</code>
<code class="n">Quiescing</code> <code class="n">servable</code> <code class="n">version</code> <code class="p">{</code><code class="n">name</code><code class="p">:</code> <code class="n">my_mnist_model</code> <code class="n">version</code><code class="p">:</code> <code class="mi">1</code><code class="p">}</code>
<code class="n">Done</code> <code class="n">quiescing</code> <code class="n">servable</code> <code class="n">version</code> <code class="p">{</code><code class="n">name</code><code class="p">:</code> <code class="n">my_mnist_model</code> <code class="n">version</code><code class="p">:</code> <code class="mi">1</code><code class="p">}</code>
<code class="n">Unloading</code> <code class="n">servable</code> <code class="n">version</code> <code class="p">{</code><code class="n">name</code><code class="p">:</code> <code class="n">my_mnist_model</code> <code class="n">version</code><code class="p">:</code> <code class="mi">1</code><code class="p">}</code></pre>

<p>This approach offers a smooth transition, but it may use too much RAM (especially GPU RAM, which is generally the most limited). In this case, you can configure TF Serving so that it handles all pending requests with the previous model version and unloads it before loading and using the new model version. This configuration will avoid having two model versions loaded at the same time, but the service will be unavailable for a short period.</p>

<p>As you can see, TF Serving makes it quite simple to deploy new models. Moreover, if you discover that version 2 does not work as well as you expected, then rolling back to version 1 is as simple as removing the <em>my_mnist_model/0002</em> directory.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Another great feature of TF Serving is its automatic batching capability, which you can activate using the <code>--enable_batching</code> option upon startup. When TF Serving receives multiple requests within a short period of time (the delay is configurable), it will automatically batch them together before using the model. This offers a significant performance boost by leveraging the power of the GPU. Once the model returns the predictions, TF Serving dispatches each prediction to the right client. You can trade a bit of latency for a greater throughput by increasing the batching delay (see the <code>--batching_parameters_file</code> option).</p>
</div>

<p>If you expect to get many queries per second, you will want to deploy TF Serving on multiple servers and load balance the queries (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#tf_serving_load_balancing_diagram">Figure&nbsp;19-2</a>). This will require deploying and managing many TF Serving containers across these servers. One way to handle that is to use an orchestration tool such as <a href="https://kubernetes.io/">Kubernetes</a>, which is an open source system for simplifying container orchestration across many servers. If you do not want to purchase, maintain, and upgrade all the hardware infrastructure, you will want to use virtual machines on cloud platforms such as Amazon AWS, Microsoft Azure, Google Cloud Platform, IBM Cloud, Alibaba Cloud, Oracle Cloud, or other Platform-as-a-Service (PaaS). Managing all the virtual machines, handling container orchestration (even with the help of Kubernetes), taking care of TF Serving configuration, tuning and monitoring—all of this can be a full-time job. Fortunately, some service providers can take care of this for you. In this chapter we will use Google Cloud AI Platform because it’s the only platform with TPUs today, it supports TensorFlow 2, it offers a nice suite of AI services (e.g., AutoML, Vision API, Natural Language API), and it is the one I have the most experience with. But there are several other providers in this space, such as Amazon AWS SageMaker and Microsoft AI Platform, which are also capable of serving TensorFlow models.</p>

<figure class="smallerseventy"><div id="tf_serving_load_balancing_diagram" class="figure">
<img src="./Chapter19_files/mls2_1902.png" alt="mls2 1902" width="1299" height="451" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1902.png">
<h6><span class="label">Figure 19-2. </span>Scaling up TF Serving with load balancing</h6>
</div></figure>

<p>Now let’s see how to serve our wonderful MNIST model on the cloud!</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Creating a Prediction Service on GCP AI Platform"><div class="sect2" id="idm46263480572824">
<h2>Creating a Prediction Service on GCP AI Platform</h2>

<p>Before you can deploy a model, there’s a little bit of setup to take care of:</p>
<ol>
<li>
<p>Log in to your Google account, and then go to the <a href="https://console.cloud.google.com/"><em>Google Cloud Platform</em> (GCP) console</a> (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#gcp_screenshot">Figure&nbsp;19-3</a>). If you don’t have a Google account, you’ll have to create one.</p>
</li>
<li>
<p>If it is your first time using GCP, you will have to read and accept the terms and conditions. Click Tour Console if you want. At the time of this writing, new users are offered a free trial, including $300 worth of GCP credit that you can use over the course of 12 months. You will only need a small portion of that to pay for the services you will use in this chapter. Upon signing up for the free trial, you will still need to create a payment profile and enter your credit card number: it is used for verification purposes (probably to avoid people using the free trial multiple times), but you will not be billed. Activate and upgrade your account if requested.</p>
</li>
<li>
<p>If you have used GCP before and your free trial has expired, then the services you will use in this chapter will cost you some money. It should not be too much, especially if you remember to turn off the services when you do not need them anymore. Make sure you understand and agree to the pricing conditions before you run any service. I hereby decline any responsibility if services end up costing more than you expected! Also make sure your billing account is active. To check, open the navigation menu on the left and click Billing, and make sure you have set up a payment method and that the billing account is active.</p>
</li>
<li>
<p>Every resource in GCP belongs to a project. This includes all the virtual machines you may use, the files you store, and the training jobs you run. When you create an account, GCP automatically creates a project for you, called “My First Project”. If you want, you can change its display name by going to the project settings: in the navigation menu (on the left of the screen), select IAM &amp; Admin &gt; Settings, change the project’s display name, and click Save. Note that the project also has a unique ID and number. You can choose the project ID when you create a project, but you cannot change it later. The project number is automatically generated and cannot be changed. If you want to create a new project: click the project name at the top of the page, then click New Project and enter the project ID. Make sure billing is active for this new project.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Always set an alarm to remind yourself to turn services off when you know you will only need them for a few hours, or else you might leave them running for days or months, incurring potentially significant costs.</p>
</div>

<figure class="smallerseventy"><div id="gcp_screenshot" class="figure">
<img src="./Chapter19_files/mls2_1903.png" alt="mls2 1903" width="1440" height="842" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1903.png">
<h6><span class="label">Figure 19-3. </span>Google Cloud Platform console</h6>
</div></figure>
</li>
<li>
<p>Now that you have a GCP account with billing activated, you can start using the services. The first one you will need is <em>Google Cloud Storage</em> (GCS): this is where you will put the SavedModels, the training data, and more. In the navigation menu, scroll down to the Storage section, and click Storage &gt; Browser. All your files will go in one or more <em>buckets</em>. Click “Create Bucket” and choose the bucket name (you may need to activate the Storage API first). GCS uses a single worldwide namespace for buckets, therefore simple names like “machine-learning” will most likely not be available. Make sure the bucket name conforms to DNS naming conventions, as it may be used in DNS records. Moreover, bucket names are public, so do not put anything private in there. It is common to use your domain name or your company name as a prefix to ensure uniqueness, or simply use a random number as part of the name. Choose the location where you want the bucket to be hosted, and the rest of the options should be fine by default. Then click Create.</p>
</li>
<li>
<p>Upload the <code>my_mnist_model</code> folder you created earlier (including one or more versions) to your bucket. Just go to the GCS Browser, click the bucket, then drag and drop the <code>my_mnist_model</code> folder from your system to the bucket (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#upload_model_screenshot">Figure&nbsp;19-4</a>). Or you can click “Upload folder” and select the <em>my_mnist_model</em> folder to upload. By default, the maximum size for a SavedModel is 250&nbsp;MB, but it is possible to request a higher quota.</p>

<figure class="smallerseventy"><div id="upload_model_screenshot" class="figure">
<img src="./Chapter19_files/mls2_1904.png" alt="mls2 1904" width="1440" height="636" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1904.png">
<h6><span class="label">Figure 19-4. </span>Uploading a saved model to Google Cloud Storage</h6>
</div></figure>
</li>
<li>
<p>Now you need to configure the <em>AI Platform</em> (formerly known as <em>ML Engine</em>) so that it knows which models and versions you want to use. In the navigation menu, scroll down to the “Artificial Intelligence” section, and click AI Platform &gt; Models. Click Activate API (it takes a few minutes), then click “Create model”. Fill in the model details (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#create_model_screenshot">Figure&nbsp;19-5</a>) and click Create.</p>

<figure class="smallerseventy"><div id="create_model_screenshot" class="figure">
<img src="./Chapter19_files/mls2_1905.png" alt="mls2 1905" width="1440" height="879" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1905.png">
<h6><span class="label">Figure 19-5. </span>Creating a new model on Google Cloud AI Platform</h6>
</div></figure>
</li>
<li>
<p>Now that you have a model on AI Platform, you need to create a model version. In the list of models, click the model you just created, then click “Create version” and fill in the version details (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#create_version_screenshot">Figure&nbsp;19-6</a>): set the name, description, Python version (3.5 or above), framework (TensorFlow), framework version (2.0 if available, or 1.13),<sup><a data-type="noteref" id="idm46263480317896-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263480317896" class="totri-footnote">6</a></sup> ML runtime version (2.0, if available or 1.13), machine type (choose “Single core CPU” for now), model path on GCS (this is the full path to the actual version folder, e.g., <em>gs://my-mnist-model-bucket/my_mnist_model/0002/</em>), scaling (choose automatic), and the minimum number of TF Serving containers running at all times (leave this field empty). Then click Save.</p>
</li>

</ol>

<figure class="smallerseventy"><div id="create_version_screenshot" class="figure">
<img src="./Chapter19_files/mls2_1906.png" alt="mls2 1906" width="1076" height="779" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1906.png">
<h6><span class="label">Figure 19-6. </span>Creating a new model version on Google Cloud AI Platform</h6>
</div></figure>

<p>Congratulations, you have deployed your first model on the cloud! Because you selected automatic scaling, AI Platform will start more TF Serving containers when the number of queries per second increases, and it will load balance the queries between them. If the QPS goes down, it will stop containers automatically. The cost is therefore directly linked to the QPS (as well as the type of machine you choose and the amount of data you store on GCS). This pricing model is particularly useful for occasional users and for services with important usage spikes, as well as for startups: the price remains low until the startup actually starts up.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>If you do not use the prediction service, AI Platform will stop all containers. This means you will only pay for the amount of storage you use (a few cents per gigabyte per month). Note that when you query the service, AI Platform will need to start up a TF Serving container, which will take a few seconds. If this delay is unacceptable, you will have to set the minimum number of TF Serving containers to 1 when creating the model version. Of course, this means at least one machine will run constantly, so the monthly fee will be higher.</p>
</div>

<p>Now let’s query this prediction service!</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Using the Prediction Service"><div class="sect2" id="idm46263480310568">
<h2>Using the Prediction Service</h2>

<p>Under the hood, AI Platform just runs TF Serving, so in principle you could use the same code as earlier, if you knew which URL to query. Except for one thing: GCP also takes care of encryption and authentication. Encryption is based on SSL/TLS, and authentication is token-based: a secret authentication token must be sent to the server in every request. So before your code can use the prediction service (or any other GCP service), it must obtain a token. We will see how to do this shortly, but first you need to configure authentication and give your application the appropriate access rights on GCP. You have two options for authentication:</p>

<ul>
<li>
<p>Your application (i.e., the client code that will query the prediction service) could authenticate using user credentials with your own Google login and password. Using user credentials would give your application the exact same rights as you on GCP, which is certainly way more than it needs. Moreover, you would have to deploy your credentials in your application, so anyone with access could steal your credentials and fully access your GCP account. In short, do not choose this option, it is only needed in very rare cases (e.g., when your application needs to access its user’s GCP account).</p>
</li>
<li>
<p>The client code can authenticate with a <em>service account</em>. This is an account that represents an application, not a user. It is generally given very restricted access rights: strictly what it needs, and no more. This is the recommended option.</p>
</li>
</ul>

<p>So let’s create a service account for your application: in the navigation menu, go to IAM &amp; admin &gt; Service accounts, then click Create Service Account, fill in the form (service account name, ID, description) and click Create (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#create_service_account_screenshot">Figure&nbsp;19-7</a>). Next, you must give this account some access rights. Select the ML Engine Developer role: this will allow the service account to make predictions, and not much more. Optionally, you can grant some users access to the service account (this is useful when your GCP user account is part of an organization, and you wish to authorize other users in the organization to deploy applications that will be based on this service account or to manage the service account itself). Next, click Create Key to export the service account’s private key, choose JSON, and click Create. This will download the private key in the form of a JSON file. Make sure to keep it private!</p>

<figure class="smallerseventy"><div id="create_service_account_screenshot" class="figure">
<img src="./Chapter19_files/mls2_1907.png" alt="mls2 1907" width="1322" height="983" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1907.png">
<h6><span class="label">Figure 19-7. </span>Creating a new service account in Google IAM</h6>
</div></figure>

<p>Great! Now let’s write a small script that will query the prediction service. Google provides several libraries to simplify access to its services:</p>
<dl>
<dt>Google API Client Library</dt>
<dd>
<p>This is a fairly thin layer on top of <a href="https://oauth.net/"><em>OAuth 2.0</em></a> (for the authentication) and REST. You can use it with all GCP services, including AI Platfom. You can install it using pip: the library is called <code>google-api-python-client</code>.</p>
</dd>
<dt>Google Cloud Client Libraries</dt>
<dd>
<p>These are a bit higher level: each one is dedicated to a particular service, such as GCS, Google BigQuery, Google Cloud Natural Language, and Google Cloud Vision. All these libraries can be installed using pip (e.g., the GCS Client Library is called <code>google-cloud-storage</code>). When a client library is available for a given service, it is recommended to use it rather than the Google API Client Library, as it implements all the best practices, and it often uses gRPC rather than REST, for better performance.</p>
</dd>
</dl>

<p>At the time of this writing there is no client library for AI Platform, so we will use the Google API Client Library. First, it will need to use the service account’s private key, so you need to tell it where it is. This can be done by setting the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable, either before starting the script, or within the script like this:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">os</code>

<code class="n">os</code><code class="o">.</code><code class="n">environ</code><code class="p">[</code><code class="s2">"GOOGLE_APPLICATION_CREDENTIALS"</code><code class="p">]</code> <code class="o">=</code> <code class="s2">"my_service_account_key.json"</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>If you deploy your application to a virtual machine on Google Cloud Engine (GCE), or within a container using Google Cloud Kubernetes Engine, or as a web application on Google Cloud App Engine, or as a microservice on Google Cloud Functions, and if the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable is not set, then the library will use the default service account for the host service (e.g., the default GCE service account, if your application runs on GCE).</p>
</div>

<p>Next, you must create a resource object that wraps access to the prediction service:<sup><a data-type="noteref" id="idm46263480283592-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263480283592" class="totri-footnote">7</a></sup></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">googleapiclient.discovery</code>

<code class="n">project_id</code> <code class="o">=</code> <code class="s2">"onyx-smoke-242003"</code> <code class="c1"># change this to your project ID</code>
<code class="n">model_id</code> <code class="o">=</code> <code class="s2">"my_mnist_model"</code>
<code class="n">model_path</code> <code class="o">=</code> <code class="s2">"projects/{}/models/{}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">project_id</code><code class="p">,</code> <code class="n">model_id</code><code class="p">)</code>
<code class="n">ml_resource</code> <code class="o">=</code> <code class="n">googleapiclient</code><code class="o">.</code><code class="n">discovery</code><code class="o">.</code><code class="n">build</code><code class="p">(</code><code class="s2">"ml"</code><code class="p">,</code> <code class="s2">"v1"</code><code class="p">)</code><code class="o">.</code><code class="n">projects</code><code class="p">()</code></pre>

<p>Note that you can append <code>"/versions/0001"</code> (or any other version number) to the <code>model_path</code> to specify the version you want to query: this can be useful for A/B testing or for testing a new version on a small group of users before releasing it widely (this is called a <em>canary</em>). Next, let’s write a small function that will use the resource object to call the prediction service and get the predictions back:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">predict</code><code class="p">(</code><code class="n">X</code><code class="p">):</code>
    <code class="n">input_data_json</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"signature_name"</code><code class="p">:</code> <code class="s2">"serving_default"</code><code class="p">,</code>
                       <code class="s2">"instances"</code><code class="p">:</code> <code class="n">X</code><code class="o">.</code><code class="n">tolist</code><code class="p">()}</code>
    <code class="n">request</code> <code class="o">=</code> <code class="n">ml_resource</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="n">model_path</code><code class="p">,</code> <code class="n">body</code><code class="o">=</code><code class="n">input_data_json</code><code class="p">)</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">request</code><code class="o">.</code><code class="n">execute</code><code class="p">()</code>
    <code class="k">if</code> <code class="s2">"error"</code> <code class="ow">in</code> <code class="n">response</code><code class="p">:</code>
        <code class="k">raise</code> <code class="ne">RuntimeError</code><code class="p">(</code><code class="n">response</code><code class="p">[</code><code class="s2">"error"</code><code class="p">])</code>
    <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">pred</code><code class="p">[</code><code class="n">output_name</code><code class="p">]</code> <code class="k">for</code> <code class="n">pred</code> <code class="ow">in</code> <code class="n">response</code><code class="p">[</code><code class="s2">"predictions"</code><code class="p">]])</code></pre>

<p>The function takes a NumPy array containing the input images; it prepares a dictionary that the client library will convert to the JSON format (as we did earlier); then it prepares a prediction request, executes it, raises an exception if the response contains an error, or else it extracts the predictions for each instance and bundles them in a NumPy array. Let’s see if it works:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">Y_probas</code> <code class="o">=</code> <code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">np</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="n">Y_probas</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
<code class="go">array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],</code>
<code class="go">       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],</code>
<code class="go">       [0.  , 0.96, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.  ]])</code></pre>

<p>Yes! You now have a nice prediction service running on the cloud that can automatically scale up to any number of QPS, plus you can query it from anywhere securely. Moreover, it costs you close to nothing when you don’t use it: just a few cents per month per gigabyte used on GCS. And you can also get detailed logs and metrics using <a href="https://cloud.google.com/stackdriver/">Google Stackdriver</a>.</p>

<p>But what if you want to deploy your model to a mobile app? Or to an embedded device?</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Deploying a Model to a Mobile or Embedded Device"><div class="sect1" id="idm46263480309944">
<h1>Deploying a Model to a Mobile or Embedded Device</h1>

<p>If you need to deploy your model to a mobile or embedded device, a large model may simply take too long to download and use too much RAM and too much compute, all of which will make your app unresponsive, heat the device, and drain its battery. To avoid this, you need to make a mobile-friendly, lightweight, and efficient model, without sacrificing too much of its accuracy. The <a href="https://tensorflow.org/lite">TFLite</a> library provides several tools<sup><a data-type="noteref" id="idm46263480044792-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263480044792" class="totri-footnote">8</a></sup> to help you deploy your models to mobile and embedded devices, with three main objectives:</p>

<ul>
<li>
<p>Reduce the model size, to shorten download time and reduce RAM usage</p>
</li>
<li>
<p>Reduce the amount of computations needed for each prediction, to reduce latency, battery usage, and heating</p>
</li>
<li>
<p>Adapt the model to device-specific constraints</p>
</li>
</ul>

<p>To reduce the model size, TFLite’s model converter can take a SavedModel and compress it to a much lighter <a href="https://google.github.io/flatbuffers/">FlatBuffers</a>-based format. This is an efficient cross-platform serialization library (a bit like Protocol Buffers) initially created by Google for gaming. It is designed so you can load FlatBuffers straight to RAM without any preprocessing: this reduces loading time and memory footprint. Once the model is loaded into a mobile or embedded device, the TFLite interpreter will execute it to make predictions. Here is how you can convert a SavedModel to a FlatBuffer and save it to a <code>.tflite</code> file:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">converter</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">lite</code><code class="o">.</code><code class="n">TFLiteConverter</code><code class="o">.</code><code class="n">from_saved_model</code><code class="p">(</code><code class="n">saved_model_path</code><code class="p">)</code>
<code class="n">tflite_model</code> <code class="o">=</code> <code class="n">converter</code><code class="o">.</code><code class="n">convert</code><code class="p">()</code>
<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s2">"converted_model.tflite"</code><code class="p">,</code> <code class="s2">"wb"</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
    <code class="n">f</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">tflite_model</code><code class="p">)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>You can also save a tf.keras model directly to a FlatBuffer using <code>from_keras_model()</code>.</p>
</div>

<p>The converter also optimizes the model, both to shrink it and to reduce its latency. It prunes all the operations that are not needed to make predictions (such as training operations), and it optimizes computations whenever possible, for example, 3×<em>a</em> + 4×<em>a</em> + 5×<em>a</em> will be converted to (3+4+5)×<em>a</em>. It also tries to fuse operations whenever possible. For example, Batch Normalization layers end up folded into the previous layer’s addition and multiplication operations, whenever possible. To get a good idea of how much TFLite can optimize a model, try downloading one of the <a href="https://homl.info/litemodels">pretrained TFLite models</a>, unzip the archive, then open the excellent <a href="https://lutzroeder.github.io/netron/">Netron graph visualization tool</a> and upload the <code>.pb</code> file to view the original model. It’s a big, complex graph, right? Next, open the optimized <code>.tflite</code> model and marvel at its beauty!</p>

<p>Another way you can reduce the model size (other than simply using smaller neural network architectures) is by using smaller bit-widths: for example, if you use half-floats (16 bits) rather than regular floats (32 bits), the model size will shrink by a factor of 2, at the cost of a (generally small) accuracy drop. Moreover, training will be faster, and you will use roughly half the amount of GPU RAM.</p>

<p>TFLite’s converter can go further than that, by quantizing the model weights down to fixed-point, 8-bit integers! This leads to a fourfold size reduction compared to using 32-bit floats. The simplest approach is called <em>post-training quantization</em>: it just quantizes the weights after training, using a fairly basic but efficient symmetrical quantization technique. It finds the maximum absolute weight value, <em>m</em>, then it maps the floating-point range –<em>m</em> to +<em>m</em> to the fixed-point (integer) range –127 to +127. For example (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#quantization_diagram">Figure&nbsp;19-8</a>), if the weights range from –1.5 to +0.8, then the bytes –127, 0, and +127 will correspond to the floats –1.5, 0.0, and +1.5, respectively. Note that 0.0 always maps to 0 when using symmetrical quantization (also note that the byte values +68 to +127 will not be used, since they map to floats greater than +0.8).</p>

<figure class="smallerseventy"><div id="quantization_diagram" class="figure">
<img src="./Chapter19_files/mls2_1908.png" alt="mls2 1908" width="1404" height="433" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1908.png">
<h6><span class="label">Figure 19-8. </span>From 32-bit floats to 8-bit integers, using symmetrical quantization</h6>
</div></figure>

<p>To perform this post-training quantization, simply add <code>OPTIMIZE_FOR_SIZE</code> to the list of converter optimizations before calling the <code>convert()</code> method:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">converter</code><code class="o">.</code><code class="n">optimizations</code> <code class="o">=</code> <code class="p">[</code><code class="n">tf</code><code class="o">.</code><code class="n">lite</code><code class="o">.</code><code class="n">Optimize</code><code class="o">.</code><code class="n">OPTIMIZE_FOR_SIZE</code><code class="p">]</code></pre>

<p>This technique dramatically reduces the model’s size, so it’s much faster to download and store. However, at runtime the quantized weights get converted back to floats before they are used: these recovered floats are not perfectly identical to the original floats, but not too far off, so the accuracy loss is usually acceptable. To avoid recomputing them all the time, the recovered floats are cached, so there is no reduction of RAM usage. And there is no reduction either in compute speed.</p>

<p>The most effective way to reduce latency and power consumption is to also quantize the activations so that the computations can be done entirely with integers, without the need for any floating-point operation. Even when using the same bit-width (e.g., 32-bit integers instead of 32-bit floats), integer computations use less CPU cycles, consume less energy, and produce less heat. And if you also reduce the bit-width (e.g., down to 8-bit integers), you can get huge speedups. Moreover, some neural network accelerator devices (such as the EdgeTPU) can only process integers, so full quantization of both weights and activations is compulsory. This can be done post-training: it requires a calibration step to find the maximum absolute value of the activations, so you need to provide a representative sample of training data to TFLite (it does not need to be huge), and it will process the data through the model and measure the activation statistics required for quantization (this step is typically fast).</p>

<p>The main problem with quantization is that it loses a bit of accuracy: it is equivalent to adding noise to the weights and activations. If the accuracy drop is too severe, then you may need to use <em>quantization-aware training</em>. This means adding <em>fake quantization</em> operations to the model so it can learn to ignore the quantization noise during training: the final weights will be more robust to quantization. Moreover, the calibration step can be taken care of automatically during training, which simplifies the whole process.</p>

<p>I have explained the core concepts of TFLite, but going all the way to coding a mobile app or an embedded program would require a whole other book. So if you want to learn more about building TensorFlow applications for mobile and embedded devices, check out the O’Reilly book <a href="https://homl.info/tinyml">“TinyML: Machine Learning with TensorFlow on Arduino and Ultra-Low Power Micro-Controllers,”</a> by Pete Warden (who leads the TFLite team) and Daniel Situnayake.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46263479920856">
<h5>TensorFlow in the Browser</h5>
<p>What if you want to use your model in a website, running directly in the user’s browser? This can be useful for many reasons:</p>

<ul>
<li>
<p>Perhaps your web application is often used in situations where the user’s connectivity is intermittent or slow (e.g., a website for hikers), so running the model directly on the client side is the only way to make your website reliable.</p>
</li>
<li>
<p>Or you need the model’s responses to be as fast as possible (e.g., for an online game): removing the need to query the server to make predictions will definitely reduce the latency and make the website much more responsive.</p>
</li>
<li>
<p>Or maybe your web service makes predictions based on some private user data, and you want to protect the user’s privacy by making the predictions on the client side so that the private data never has to leave the user’s machine.<sup><a data-type="noteref" id="idm46263479888952-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263479888952" class="totri-footnote">9</a></sup></p>
</li>
</ul>

<p>For all these scenarios, you can export your model to a special format that can be loaded by the <a href="https://tensorflow.org/js">TensorFlow.js Javascript library</a>: this library can then use your model to make predictions directly in the user’s browser! The TensorFlow.js project includes a <code>tensorflowjs_converter</code> tool that can convert a TensorFlow SavedModel or a Keras model file to the <em>TensorFlow.js Layers</em> format: this is a directory containing a set of sharded weight files in binary format and a <em>model.json</em> file that describes the model’s architecture and links to the weight files. This format is optimized to be downloaded efficiently on the web. You can then download the model and run predictions in the browser using the TensorFlow.js library. Here is a code snippet to give you an idea of what the Javascript API looks like:</p>

<pre data-type="programlisting" data-code-language="javascript"><code class="kr">import</code> <code class="o">*</code> <code class="nx">as</code> <code class="nx">tf</code> <code class="nx">from</code> <code class="s1">'@tensorflow/tfjs'</code><code class="p">;</code>
<code class="kr">const</code> <code class="nx">model</code> <code class="o">=</code> <code class="nx">await</code> <code class="nx">tf</code><code class="p">.</code><code class="nx">loadLayersModel</code><code class="p">(</code><code class="s1">'https://example.com/tfjs/model.json'</code><code class="p">);</code>
<code class="kr">const</code> <code class="nx">image</code> <code class="o">=</code> <code class="nx">tf</code><code class="p">.</code><code class="nx">fromPixels</code><code class="p">(</code><code class="nx">webcamElement</code><code class="p">);</code>
<code class="kr">const</code> <code class="nx">prediction</code> <code class="o">=</code> <code class="nx">model</code><code class="p">.</code><code class="nx">predict</code><code class="p">(</code><code class="nx">image</code><code class="p">);</code></pre>

<p>Once again, to do justice to this topic requires a whole book, so if you want to learn more about TensorFlow.js, check out the O’Reilly book <a href="https://homl.info/tfjsbook"><em>Practical Deep Learning for Cloud and Mobile: Hands-On Computer Vision Projects Using Python, Keras, and TensorFlow</em></a>, by Anirudh Koul, Siddha Ganju, and Meher Kasam.</p>
</div></aside>

<p>Next, we will see how to use GPUs to speed up computations!</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Using GPUs to Speed Up Computations"><div class="sect1" id="idm46263480062872">
<h1>Using GPUs to Speed Up Computations</h1>

<p>In <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a> we discussed several techniques that can considerably speed up training: better weight initialization, Batch Normalization, sophisticated optimizers, and so on. But even with all of these techniques, training a large neural network on a single machine with a single CPU can take days or even weeks.</p>

<p>In this section we will look at how to speed up your models by using GPUs. We will also see how to split the computations across multiple devices, including the CPU and multiple GPU devices (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#multiple_devices_diagram">Figure&nbsp;19-9</a>). For now we will run everything on a single machine, but later in this chapter we will discuss how to distribute computations across multiple servers.</p>

<p>Thanks to GPUs, instead of waiting for days or weeks for a training algorithm to complete, you may end up waiting for just a few minutes or hours. Not only does this save an enormous amount of time, it also means that you can experiment with various models much more easily and frequently retrain your models on fresh data.</p>

<figure class="smallerseventy"><div id="multiple_devices_diagram" class="figure">
<img src="./Chapter19_files/mls2_1909.png" alt="mls2 1909" width="1423" height="935" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1909.png">
<h6><span class="label">Figure 19-9. </span>Executing a TensorFlow graph across multiple devices in parallel</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>You can often get a major performance boost simply by adding GPU cards to a single machine. In fact, in many cases this will suffice; you won’t need to use multiple machines at all. For example, you can typically train a neural network just as fast using 4 GPUs on a single machine rather than 8 GPUs across multiple machines, due to the extra delay imposed by network communications in a distributed setup. Similarly, using a single powerful GPU is often preferable to using multiple slower GPUs.</p>
</div>

<p>The first step is to get your hands on a GPU. There are two options for this: you can either purchase your own GPU(s), or you can use GPU-equipped virtual machines on the cloud. Let’s start with the first option.</p>








<section data-type="sect2" data-pdf-bookmark="Getting Your Own GPU"><div class="sect2" id="idm46263479611592">
<h2>Getting Your Own GPU</h2>

<p>If you choose to purchase a GPU card, then take some time to make the right choice. Tim Dettmers wrote an <a href="https://homl.info/66">excellent blog post</a> to help you choose, and he updates it regularly: I encourage you to read it carefully. At the time of this writing, TensorFlow only supports <a href="https://homl.info/cudagpus">Nvidia cards with CUDA Compute Capability 3.5+</a> (as well as Google’s TPUs of course), but it may extend its support to other manufacturers. Moreover, although TPUs are only available on GCP, it is highly likely that TPU-like cards will be available for sale in the near future, and TensorFlow may support them. In short, make sure to check <a href="https://tensorflow.org/install">TensorFlow’s documentation</a> to see what devices are supported at this point.</p>

<p>If you go for an Nvidia GPU card, you will need to install the appropriate Nvidia drivers and several Nvidia libraries, including CUDA and cuDNN.<sup><a data-type="noteref" id="idm46263479606472-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263479606472">10</a></sup> Nvidia’s <em>Compute Unified Device Architecture</em> library (CUDA) allows developers to use CUDA-enabled GPUs for all sorts of computations (not just graphics acceleration). Nvidia’s <em>CUDA Deep Neural Network</em> library (cuDNN) is a GPU-accelerated library of primitives for DNNs. It provides optimized implementations of common DNN computations such as activation layers, normalization, forward and backward convolutions, and pooling (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cnn_chapter">Chapter&nbsp;14</a>). It is part of Nvidia’s Deep Learning SDK (note that it requires creating an Nvidia developer account in order to download it). TensorFlow uses CUDA and cuDNN to control the GPU cards and accelerate computations (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#cuda_cudnn_diagram">Figure&nbsp;19-10</a>).</p>

<figure class="smallerfourty"><div id="cuda_cudnn_diagram" class="figure">
<img src="./Chapter19_files/mls2_1910.png" alt="mls2 1910" width="1038" height="764" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1910.png">
<h6><span class="label">Figure 19-10. </span>TensorFlow uses CUDA and cuDNN to control GPUs and boost DNNs</h6>
</div></figure>

<p>Once you have installed the GPU card(s) and all the drivers and libraries, you can use the <code>nvidia-smi</code> command to check that CUDA is properly installed. It lists the available GPU cards, as well as processes running on each card:</p>
<pre data-type="programlisting" data-code-language="shell-session"><code class="go">$ </code><strong><code class="go">nvidia-smi</code></strong><code class="go">
</code><code class="go">Sun Jun  2 10:05:22 2019
</code><code class="go">+-----------------------------------------------------------------------------+
</code><code class="go">| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |
</code><code class="go">|-------------------------------+----------------------+----------------------+
</code><code class="go">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
</code><code class="go">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
</code><code class="go">|===============================+======================+======================|
</code><code class="go">|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
</code><code class="go">| N/A   61C    P8    17W /  70W |      0MiB / 15079MiB |      0%      Default |
</code><code class="go">+-------------------------------+----------------------+----------------------+
</code><code class="go">
</code><code class="go">+-----------------------------------------------------------------------------+
</code><code class="go">| Processes:                                                       GPU Memory |
</code><code class="go">|  GPU       PID   Type   Process name                             Usage      |
</code><code class="go">|=============================================================================|
</code><code class="go">|  No running processes found                                                 |
</code><code class="go">+-----------------------------------------------------------------------------+
</code></pre>

<p>At the time of this writing, you need to install the GPU version of TensorFlow (i.e., the <code>tensorflow-gpu</code> library), but there is ongoing work to have a unified installation procedure for both CPU-only and GPU machines. So please check the installation documentation to see which library you should install. In any case, since installing every required library correctly is a bit long and tricky (all hell breaks loose if you do not install the correct library versions), TensorFlow provides a docker image with everything you need inside. However, in order for the Docker container to have access to the GPU, you will still need to install the Nvidia drivers on the host machine.</p>

<p>To check that TensorFlow actually sees the GPUs, run the following tests:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">tensorflow</code> <code class="kn">as</code> <code class="nn">tf</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">tf</code><code class="o">.</code><code class="n">test</code><code class="o">.</code><code class="n">is_gpu_available</code><code class="p">()</code>
<code class="go">True</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">tf</code><code class="o">.</code><code class="n">test</code><code class="o">.</code><code class="n">gpu_device_name</code><code class="p">()</code>
<code class="go">'/device:GPU:0'</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">tf</code><code class="o">.</code><code class="n">config</code><code class="o">.</code><code class="n">experimental</code><code class="o">.</code><code class="n">list_physical_devices</code><code class="p">(</code><code class="n">device_type</code><code class="o">=</code><code class="s">'GPU'</code><code class="p">)</code>
<code class="go">[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]</code></pre>

<p>The <code>is_gpu_available()</code> function checks whether at least one GPU is available. The <code>gpu_device_name()</code> function gives the first GPU’s name: by default, operations will run on this GPU. The <code>list_physical_devices()</code> function returns the list of all available GPU devices (just one in this example):<sup><a data-type="noteref" id="idm46263478674040-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263478674040">11</a></sup></p>

<p>Now what if you don’t want to invest time and money in getting your own GPU card? Just use a GPU VM on the cloud!</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Using a GPU-Equipped Virtual Machine"><div class="sect2" id="idm46263479610968">
<h2>Using a GPU-Equipped Virtual Machine</h2>

<p>All major cloud platforms now offer GPU VMs, some preconfigured with all the drivers and libraries you need, including TensorFlow. Google Cloud enforces various GPU quotas, both worldwide and per region: you cannot just create thousands of GPU VMs without prior authorization from Google.<sup><a data-type="noteref" id="idm46263478670664-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263478670664">12</a></sup> By default, the wordwide GPU quota is zero, so you cannot use any GPU VM. Therefore, the very first thing you need to do is to request a higher worldwide quota. In the GCP Console, open the navigation menu and go to IAM &amp; admin &gt; Quotas, then click Metric, click None to uncheck all metrics, then search for “GPU” and select “GPUs (all regions)” to see the corresponding quota. If this quota’s value is zero (or just insufficient for your needs), then check the box next to this quota (it should be the only selected one), then click “Edit quotas”. Fill in the requested information, then click “Submit request”. It may take a few hours (or up to a few days) for your quota request to be processed and (generally) accepted. By default, there is also a quota of one GPU per region and per GPU type. You can request to increase these quotas: click Metric, select None to uncheck all metrics, search for GPU, select the type of GPU you want (e.g., NVIDIA P4 GPUs), then click the Location drop-down menu, click None to uncheck all metrics, then click the Location you want, check the boxes next to the quota(s) you want to change, and click “Edit quotas” to file a request.</p>

<p>Once your GPU quota requests are approved, you can in no time create a VM equipped with one or more GPUs by using Google Cloud AI Platform’s <em>Deep Learning VM Images</em>: go to <a href="https://homl.info/dlvm"><em class="hyperlink">https://homl.info/dlvm</em></a>, click View Console, then click “Launch on Compute Engine” and fill in the VM configuration form. Note that some locations do not have all types of GPUs, and some have no GPUs at all (change the location to see the types of GPUs available, if any). Make sure to select “TensorFlow 2.0” as the framework, and check “Install NVIDIA GPU driver automatically on first startup”. It is also a good idea to check “Enable access to JupyterLab via URL instead of SSH”: this will make it very easy to start a Jupyter notebook running on this GPU VM, powered by <em>JupyterLab</em> (this is an alternative web interface to run Jupyter notebooks). Once the VM is created, scroll down the navigation menu to the Artificial Intelligence section, then click AI Platform &gt; Notebooks. Once the Notebook instance appears in the list (this may take a few minutes, so click Refresh once in a while until it appears), click its Open JupyterLab link. This will run JupyterLab on the VM and connect your browser to it. You can create notebooks, run any code you want on this VM, and benefit from its GPUs!</p>

<p>But if you just want to run some quick tests or easily share notebooks with your colleagues, then you should try Colaboratory.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Colaboratory"><div class="sect2" id="idm46263478664312">
<h2>Colaboratory</h2>

<p>The simplest and cheapest way to access a GPU VM is to use <em>Colaboratory</em> (or <em>Colab</em>, for short). It’s free! Just go to <a href="https://colab.research.google.com/"><em class="hyperlink">https://colab.research.google.com/</em></a> and create a new Python 3 notebook: this will create a Jupyter notebook on your Google Drive (alternatively, you can open any notebook on GitHub, or on Google Drive, or you can even upload your own notebooks). Colab’s user interface is similar to Jupyter’s, except you can share and use the notebooks like regular Google Docs, and there are a few other minor differences (e.g., you can create handy widgets using special comments in your code).</p>

<p>When you open a Colab notebook, it runs on a free Google VM dedicated to you, called a <em>Colab Runtime</em>. By default, the Runtime is CPU-only, but you can change this by going to Runtime &gt; Change runtime type and selecting GPU in the “Hardware accelerator” drop-down menu, then clicking Save. In fact, you could even select TPU! Yes, you can actually use a TPU for free (we will talk about TPUs later in this chapter, so for now just select GPU).</p>

<p>If you run multiple Colab notebooks using the same runtime type (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#colab_diagram">Figure&nbsp;19-11</a>), they will use the same Colab Runtime. So if one writes to a file, the others will be able to read that file. It’s important to understand the security implications of this: if you run an untrusted Colab notebook written by a nasty hacker, it may read private data produced by the other notebooks and then leak this data back to the hacker. If this includes private access keys for some resources, the hacker will gain access to those resources. Moreover, if you install a library on the Colab Runtime, the other notebooks will also have that library. Depending on what you want to do, this might be great or annoying (e.g., you cannot easily use different versions of the same library in different Colab notebooks).</p>

<figure class="smallerseventy"><div id="colab_diagram" class="figure">
<img src="./Chapter19_files/mls2_1911.png" alt="mls2 1911" width="1412" height="774" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1911.png">
<h6><span class="label">Figure 19-11. </span>Colab notebooks and runtimes</h6>
</div></figure>

<p>Colab does have some restrictions: as the FAQ states, “Colaboratory is intended for interactive use. Long-running background computations, particularly on GPUs, may be stopped. Please do not use Colaboratory for cryptocurrency mining”. The Web interface will automatically disconnect from the Colab Runtime if you leave it unattended for a while (~30 minutes). When you reconnect to the Colab Runtime, it may have been reset. So make sure you always download any data you care about. Even if you never disconnect, the Colab Runtime will automatically shut down after 12 hours, as it is not meant for long-running computations. Despite these limitations, it’s a fantastic tool to run tests easily, get quick results, and collaborate with your colleagues.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Managing the GPU RAM"><div class="sect2" id="idm46263478653640">
<h2>Managing the GPU RAM</h2>

<p>By default TensorFlow automatically grabs all the RAM in all available GPUs the first time you run a computation. TensorFlow does this to limit GPU RAM fragmentation. This means that if you try to start a second TensorFlow program (or any program that requires the GPU), it will quickly run out of RAM. This does not happen as often as you might think, as you will most often have a single TensorFlow program running on a machine: usually a training script, a TF Serving node or a Jupyter notebook. If you need to run multiple programs for some reason (e.g., to train two different models in parallel on the same machine), then you will need to split the GPU RAM between these processes more evenly.</p>

<p>If you have multiple GPU cards on your machine, a simple solution is to assign each of them to a single process. To do this, the simplest option is to set the <code>CUDA_VISIBLE_DEVICES</code> environment variable so that each process only sees the appropriate GPU cards. Also set the <code>CUDA_DEVICE_ORDER</code> environment variable to <code>PCI_BUS_ID</code> to ensure that each ID always refers to the same GPU card. For example, if you have four GPU cards, here is how you would start two programs, assigning two GPUs to each of them:</p>
<pre data-type="programlisting" data-code-language="shell-session"><code class="go">$ </code><strong><code class="go">CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0,1 python3 program_1.py</code></strong><code class="go">
</code><code class="go"># and in another terminal:
</code><code class="go">$ </code><strong><code class="go">CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=3,2 python3 program_2.py</code></strong><code class="go">
</code></pre>

<p>Program 1 will only see GPU cards 0 and 1, named <code>/gpu:0</code> and <code>/gpu:1</code> respectively, and program 2 will only see GPU cards 2 and 3, named <code>/gpu:1</code> and <code>/gpu:0</code> respectively (note the order). Everything will work fine (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#splitting_gpus_diagram">Figure&nbsp;19-12</a>). Of course, you can also define these environment variables in Python by setting <code>os.environ["CUDA_DEVICE_ORDER"]</code> and <code>os.environ["CUDA_VISIBLE_DEVICES"]</code>, as long as you do so before using TensorFlow.</p>

<figure class="smallerseventy"><div id="splitting_gpus_diagram" class="figure">
<img src="./Chapter19_files/mls2_1912.png" alt="mls2 1912" width="1440" height="510" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1912.png">
<h6><span class="label">Figure 19-12. </span>Each program gets two GPUs</h6>
</div></figure>

<p>Another option is to tell TensorFlow to grab only a specific amount of GPU RAM. This must be done immediately after importing TensorFlow. For example, to make TensorFlow grab only 2&nbsp;GiB of RAM on each GPU, you must create a <em>virtual GPU device</em> (also called a <em>logical GPU device</em>) for each physical GPU device and set its memory limit to 2&nbsp;GiB (i.e., 2,048&nbsp;MiB):</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">gpu</code> <code class="ow">in</code> <code class="n">tf</code><code class="o">.</code><code class="n">config</code><code class="o">.</code><code class="n">experimental</code><code class="o">.</code><code class="n">list_physical_devices</code><code class="p">(</code><code class="s2">"GPU"</code><code class="p">):</code>
    <code class="n">tf</code><code class="o">.</code><code class="n">config</code><code class="o">.</code><code class="n">experimental</code><code class="o">.</code><code class="n">set_virtual_device_configuration</code><code class="p">(</code>
        <code class="n">gpu</code><code class="p">,</code>
        <code class="p">[</code><code class="n">tf</code><code class="o">.</code><code class="n">config</code><code class="o">.</code><code class="n">experimental</code><code class="o">.</code><code class="n">VirtualDeviceConfiguration</code><code class="p">(</code><code class="n">memory_limit</code><code class="o">=</code><code class="mi">2048</code><code class="p">)])</code></pre>

<p>Now suppose you have GPUs, each with at least 4&nbsp;GiB of RAM, then two programs like this one can run in parallel, each using all four GPU cards (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#sharing_gpus_diagram">Figure&nbsp;19-13</a>).</p>

<figure class="smallerseventy"><div id="sharing_gpus_diagram" class="figure">
<img src="./Chapter19_files/mls2_1913.png" alt="mls2 1913" width="1350" height="523" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1913.png">
<h6><span class="label">Figure 19-13. </span>Each program gets all four GPUs, but with only 2&nbsp;GiB of RAM on each GPU</h6>
</div></figure>

<p>If you run the <code>nvidia-smi</code> command while both programs are running, you should see that each process holds 2&nbsp;GiB of RAM on each card:</p>
<pre data-type="programlisting" data-code-language="shell-session"><code class="go">$ </code><strong><code class="go">nvidia-smi</code></strong><code class="go">
</code><code class="go">[...]
</code><code class="go">+-----------------------------------------------------------------------------+
</code><code class="go">| Processes:                                                       GPU Memory |
</code><code class="go">|  GPU       PID   Type   Process name                             Usage      |
</code><code class="go">|=============================================================================|
</code><code class="go">|    0      2373      C   /usr/bin/python3                            2241MiB |
</code><code class="go">|    0      2533      C   /usr/bin/python3                            2241MiB |
</code><code class="go">|    1      2373      C   /usr/bin/python3                            2241MiB |
</code><code class="go">|    1      2533      C   /usr/bin/python3                            2241MiB |
</code><code class="go">[...]</code></pre>

<p>Yet another option is to tell TensorFlow to grab memory only when it needs it (this must be done immediately after importing TensorFlow):</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">gpu</code> <code class="ow">in</code> <code class="n">tf</code><code class="o">.</code><code class="n">config</code><code class="o">.</code><code class="n">experimental</code><code class="o">.</code><code class="n">list_physical_devices</code><code class="p">(</code><code class="s2">"GPU"</code><code class="p">):</code>
    <code class="n">tf</code><code class="o">.</code><code class="n">config</code><code class="o">.</code><code class="n">experimental</code><code class="o">.</code><code class="n">set_memory_growth</code><code class="p">(</code><code class="n">gpu</code><code class="p">,</code> <code class="bp">True</code><code class="p">)</code></pre>

<p>Another way to do this is to set the <code>TF_FORCE_GPU_ALLOW_GROWTH</code> environment variable to <code>true</code>. With this option, TensorFlow will never release memory once it has grabbed it (again, to avoid memory fragmentation), except of course when the program ends. It may be harder to guarantee a deterministic behavior using this option (e.g., one program may crash because another program’s memory usage went through the roof), so in production you probably want to stick with one of the previous options. However, there are some cases where it is very useful, for example when you use a machine to run multiple Jupyter notebooks, several of which use TensorFlow. This is why the <code>TF_FORCE_GPU_ALLOW_GROWTH</code> environment variable is set to <code>true</code> on Colab Runtimes.</p>

<p>Lastly, in some cases you may want to split a GPU into two or more virtual GPUs, for example if you want to test a distribution algorithm (this is handy to try out the code examples in the rest of this chapter even if you have a single GPU, such as on a Colab Runtime). The following code splits the first GPU into 2 virtual devices, with 2&nbsp;GiB of RAM each (again, it must be done immediately after importing TensorFlow):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">physical_gpus</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">config</code><code class="o">.</code><code class="n">experimental</code><code class="o">.</code><code class="n">list_physical_devices</code><code class="p">(</code><code class="s2">"GPU"</code><code class="p">)</code>
<code class="n">tf</code><code class="o">.</code><code class="n">config</code><code class="o">.</code><code class="n">experimental</code><code class="o">.</code><code class="n">set_virtual_device_configuration</code><code class="p">(</code>
    <code class="n">physical_gpus</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code>
    <code class="p">[</code><code class="n">tf</code><code class="o">.</code><code class="n">config</code><code class="o">.</code><code class="n">experimental</code><code class="o">.</code><code class="n">VirtualDeviceConfiguration</code><code class="p">(</code><code class="n">memory_limit</code><code class="o">=</code><code class="mi">2048</code><code class="p">),</code>
     <code class="n">tf</code><code class="o">.</code><code class="n">config</code><code class="o">.</code><code class="n">experimental</code><code class="o">.</code><code class="n">VirtualDeviceConfiguration</code><code class="p">(</code><code class="n">memory_limit</code><code class="o">=</code><code class="mi">2048</code><code class="p">)])</code></pre>

<p>These two virtual devices will then be called <code>/gpu:0</code> and <code>/gpu:1</code>, and you can place operations and variables on each of them as if they were really two independent GPUs. Now let’s see how TensorFlow decides which devices it should place variables and execute operations on.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Placing Operations and Variables on Devices"><div class="sect2" id="idm46263478653016">
<h2>Placing Operations and Variables on Devices</h2>

<p>The TensorFlow <a href="https://homl.info/67">whitepaper</a><sup><a data-type="noteref" id="idm46263478393640-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263478393640">13</a></sup> presents a friendly <em>dynamic placer</em> algorithm that automagically distributes operations across all available devices, taking into account things like the measured computation time in previous runs of the graph, estimations of the size of the input and output tensors to each operation, the amount of RAM available in each device, communication delay when transferring data in and out of devices, and hints and constraints from the user. In practice this algorithm turned out to be less efficient than a small set of placement rules specified by the user, so the TensorFlow team ended up dropping the dynamic placer.</p>

<p>That said, tf.keras and tf.data generally do a good job placing operations and variables where they belong (e.g., heavy computations on the GPU, and data preprocessing on the CPU). But you can also place operations and variables manually on each device, if you want more control:</p>

<ul>
<li>
<p>As just mentioned, you generally want to place the data preprocessing operations on the CPU, and place the neural network operations on the GPUs.</p>
</li>
<li>
<p>GPUs usually have a fairly limited communication bandwidth, so it is important to avoid unnecessary data transfers in and out of the GPUs.</p>
</li>
<li>
<p>Adding more CPU RAM to a machine is simple and fairly cheap, so there’s usually plenty of it, whereas the GPU RAM is baked into the GPU: it is an expensive and thus limited resource, so if a variable is not needed in the next few training steps, it should probably be placed on the CPU (e.g., datasets generally belong on the CPU).</p>
</li>
</ul>

<p>By default, all variables and all operations will be placed on the first GPU (named <code>/gpu:0</code>), except for variables and operations that don’t have a GPU kernel:<sup><a data-type="noteref" id="idm46263478086920-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263478086920">14</a></sup> these are placed on the CPU (named <code>/cpu:0</code>). A tensor or variable’s <code>device</code> attribute tells you which device it was placed on:<sup><a data-type="noteref" id="idm46263478082504-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263478082504">15</a></sup></p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">a</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">Variable</code><code class="p">(</code><code class="mf">42.0</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">a</code><code class="o">.</code><code class="n">device</code>
<code class="go">'/job:localhost/replica:0/task:0/device:GPU:0'</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">b</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">Variable</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">b</code><code class="o">.</code><code class="n">device</code>
<code class="go">'/job:localhost/replica:0/task:0/device:CPU:0'</code></pre>

<p>You can safely ignore the prefix <code>/job:localhost/replica:0/task:0</code> for now (it allows you to place operations on other machines when using a TensorFlow cluster; we will talk about jobs, replicas, and tasks later in this chapter). As you can see, the first variable was placed on GPU 0, which is the default device. However, the second variable was placed on the CPU: this is because there are no GPU kernels for integer variables (or for operations involving integer tensors), so TensorFlow fell back to the CPU.</p>

<p>Now if you want to place an operation on a different device than the default one, use a <code>tf.device()</code> context:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">device</code><code class="p">(</code><code class="s">"/cpu:0"</code><code class="p">):</code>
<code class="gp">... </code>    <code class="n">c</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">Variable</code><code class="p">(</code><code class="mf">42.0</code><code class="p">)</code>
<code class="gp">...</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">c</code><code class="o">.</code><code class="n">device</code>
<code class="go">'/job:localhost/replica:0/task:0/device:CPU:0'</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The CPU is always treated as a single device (<code>/cpu:0</code>), even if your machine has multiple CPU cores. Any operation placed on the CPU may run in parallel across multiple cores if it has a multithreaded kernel.</p>
</div>

<p>If you explicitly try to place an operation or variable on a device that does not exist or for which there is no kernel, then you will get an exception. However, in some cases you may prefer to fall back to the CPU; for example, if your program may run both on CPU-only machines and on GPU machines, you may want TensorFlow to ignore your <code>tf.device("/gpu:*")</code> on CPU-only machines. To do this, you can call <code>tf.config.set_soft_device_placement(True)</code> just after importing TensorFlow: when a placement request fails, TensorFlow will fall back to its default placement rules (i.e., GPU 0 by default if it exists and there is a GPU kernel, and CPU 0 otherwise).</p>

<p>Now how exactly will TensorFlow execute all these operations across multiple devices?</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Parallel Execution Across Multiple Devices"><div class="sect2" id="idm46263478395368">
<h2>Parallel Execution Across Multiple Devices</h2>

<p>As we saw in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch12.html#tensorflow_chapter">Chapter&nbsp;12</a>, one of the benefits of using TF Functions is parallelism. Let’s look at this a bit more closely. When TensorFlow runs a TF Function, it starts by analyzing its graph to find the list of operations that need to be evaluated, and it counts how many dependencies each of them has. TensorFlow then adds each operation with zero dependencies (i.e., each source operation) to the evaluation queue of this operation’s device (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#parallelization_diagram">Figure&nbsp;19-14</a>). Once an operation has been evaluated, the dependency counter of each operation that depends on it is decremented. Once an operation’s dependency counter reaches zero, it is pushed to the evaluation queue of its device. And once all the nodes that TensorFlow needs have been evaluated, it returns their outputs.</p>

<p>Operations in the CPU’s evaluation queue are dispatched to a thread pool called the <em>inter-op thread pool</em>. If the CPU has multiple cores, then these operations will effectively be evaluated in parallel. Some operations have multithreaded CPU kernels: these kernels split their task into multiple suboperations which are placed in another evaluation queue and dispatched to a second thread pool called the <em>intra-op thread pool</em> (shared by all multithreaded CPU kernels). In short, multiple operations and suboperations may be evaluated in parallel on different CPU cores.</p>

<p>For the GPU, things are a bit simpler: operations in a GPU’s evaluation queue are evaluated sequentially. However, most operations have multithreaded GPU kernels, typically implemented by libraries that TensorFlow depends on, such as CUDA and cuDNN. These implementations have their own thread pools, and they typically exploit as many GPU threads as they can (which is the reason why there is no need for an inter-op thread pool in GPUs, since each operation already floods most GPU threads).</p>

<figure class="smallerseventy"><div id="parallelization_diagram" class="figure">
<img src="./Chapter19_files/mls2_1914.png" alt="mls2 1914" width="1418" height="1163" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1914.png">
<h6><span class="label">Figure 19-14. </span>Parallelized execution of a TensorFlow graph</h6>
</div></figure>

<p>For example, in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#parallelization_diagram">Figure&nbsp;19-14</a>, operations A, B, and C are source ops, so they can immediately be evaluated. Operations A and B are placed on the CPU, so they are sent to the CPU’s evaluation queue, then they are dispatched to the inter-op thread pool and immediately evaluated in parallel. Operation A happens to have a multithreaded kernel; its computations are split into three parts, which are executed in parallel by the intra-op thread pool. Operation C goes to GPU 0’s evaluation queue, and in this example its GPU kernel happens to use cuDNN, which manages its own intra-op thread pool and runs the operation across many GPU threads in parallel. Suppose C finishes first. The dependency counter of D and E are decremented, and they reach zero, so both operations are pushed to GPU 0’s evaluation queue, and they are executed sequentially. Note that C only gets evaluated once, even though both D and E depend on it. Suppose B finishes next. Then F’s dependency counter is decremented from 4 to 3, and since that’s not 0, it does not run yet. Once A, D, and E are finished, then F’s dependency counter reaches 0, and it is pushed to the CPU’s evaluation queue and evaluated. Finally, TensorFlow returns the requested outputs.</p>

<p>An extra bit of magic that TensorFlow performs is when the TF Function modifies a stateful resource, such as a variable: it ensures that the order of execution matches the order in the code, even if there is no explicit dependency between the statements. For example, if your TF Function contains <code>v.assign_add(1)</code> followed by <code>v.assign(v * 2)</code>, TensorFlow will ensure that these operations are executed in that order.</p>
<div data-type="tip"><h6>Tip</h6>
<p>You can control the number of threads in the inter-op thread pool by calling <code>tf.config.threading.set_inter_op_parallelism_threads()</code>. To set the number of intra-op threads, use <code>tf.config.threading.set_intra_op_parallelism_threads()</code>. This is useful if you want do not want TensorFlow to use all the CPU cores or if you want it to be single-threaded.<sup><a data-type="noteref" id="idm46263477966424-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263477966424">16</a></sup></p>
</div>

<p>With that, you have all you need to run any operation on any device, and exploit the power of your GPUs! Here are some of the things you could do:</p>

<ul>
<li>
<p>You could train several models in parallel, each on its own GPU: just write a training script for each model and run them in parallel, setting <code>CUDA_DEVICE_ORDER</code> and <code>CUDA_VISIBLE_DEVICES</code> so that each script only sees a single GPU device. This is great for hyperparameter tuning, as you can train in parallel multiple models with different hyperparameters. If you have a single machine with two GPUs, and it takes one hour to train one model on one GPU, then training two models in parallel, each on its dedicated GPU, will take just one hour. Simple!</p>
</li>
<li>
<p>You could train a model on a single GPU and perform all the preprocessing in parallel on the CPU, using the dataset’s <code>prefetch()</code> method<sup><a data-type="noteref" id="idm46263477960424-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263477960424">17</a></sup> to prepare the next few batches in advance so that they are ready when the GPU needs them (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch13.html#data_chapter">Chapter&nbsp;13</a>).</p>
</li>
<li>
<p>If your model takes two images as input and processes them using two CNNs before joining their outputs, then it will probably run much faster if you place each CNN on a different GPU.</p>
</li>
<li>
<p>You can create an efficient ensemble: just place a different trained model on each GPU so that you can get all predictions much faster to produce the ensemble’s final prediction.</p>
</li>
</ul>

<p>But what if you want to <em>train</em> a single model across multiple GPUs?</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Training Models Across Multiple Devices"><div class="sect1" id="idm46263478201256">
<h1>Training Models Across Multiple Devices</h1>

<p>There are two main approaches to training a single model across multiple devices: <em>model parallelism</em>, where the model is split across the devices, and <em>data parallelism</em>, where the model is replicated across every device, and each replica is trained on a subset of the data. Let’s look at these two options closely before we train a model on multiple GPUs.</p>








<section data-type="sect2" data-pdf-bookmark="Model Parallelism"><div class="sect2" id="idm46263477924136">
<h2>Model Parallelism</h2>

<p>So far we have trained each neural network on a single device. What if we want to train a single neural network across multiple devices? This requires chopping your model into separate chunks and running each chunk on a different device. Unfortunately, such model parallelism turns out to be pretty tricky, and it really depends on the architecture of your neural network. For fully connected networks, there is generally not much to be gained from this approach (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#split_fully_connected_diagram">Figure&nbsp;19-15</a>). Intuitively, it may seem that an easy way to split the model is to place each layer on a different device, but this does not work because each layer needs to wait for the output of the previous layer before it can do anything. So perhaps you can slice it vertically—for example, with the left half of each layer on one device, and the right part on another device? This is slightly better, since both halves of each layer can indeed work in parallel, but the problem is that each half of the next layer requires the output of both halves, so there will be a lot of cross-device communication (represented by the dashed arrows). This is likely to completely cancel out the benefit of the parallel computation, since cross-device communication is slow (especially when the devices are located on different machines).</p>

<figure class="smallerseventy"><div id="split_fully_connected_diagram" class="figure">
<img src="./Chapter19_files/mls2_1915.png" alt="mls2 1915" width="1440" height="618" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1915.png">
<h6><span class="label">Figure 19-15. </span>Splitting a fully connected neural network</h6>
</div></figure>

<p>Some neural network architectures, such as convolutional neural networks (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cnn_chapter">Chapter&nbsp;14</a>), contain layers that are only partially connected to the lower layers, so it is much easier to distribute chunks across devices in an efficient way (<a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#split_partially_connected_diagram">Figure&nbsp;19-16</a>).</p>

<figure class="smallerfiftyfive"><div id="split_partially_connected_diagram" class="figure">
<img src="./Chapter19_files/mls2_1916.png" alt="mls2 1916" width="1250" height="815" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1916.png">
<h6><span class="label">Figure 19-16. </span>Splitting a partially connected neural network</h6>
</div></figure>

<p>Deep recurrent neural networks (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#rnn_chapter">Chapter&nbsp;15</a>) can be split a bit more efficiently across multiple GPUs. Indeed, if you split the network horizontally by placing each layer on a different device, and you feed the network with an input sequence to process, then at the first time step only one device will be active (working on the sequence’s first value), at the second step two will be active (the second layer will be handling the output of the first layer for the first value, while the first layer will be handling the second value), and by the time the signal propagates to the output layer, all devices will be active simultaneously (<a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#split_rnn_network_diagram">Figure&nbsp;19-17</a>). There is still a lot of cross-device communication going on, but since each cell may be fairly complex, the benefit of running multiple cells in parallel may (in theory) outweigh the communication penalty. However, in practice a regular stack of <code>LSTM</code> layers running on a single GPU actually runs much faster.</p>

<figure class="smallerfiftyfive"><div id="split_rnn_network_diagram" class="figure">
<img src="./Chapter19_files/mls2_1917.png" alt="mls2 1917" width="1440" height="955" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1917.png">
<h6><span class="label">Figure 19-17. </span>Splitting a deep recurrent neural network</h6>
</div></figure>

<p>In short, model parallelism may speed up running or training some types of neural networks, but not all, and it requires special care and tuning, such as making sure that devices that need to communicate the most run on the same machine.<sup><a data-type="noteref" id="idm46263477907496-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263477907496">18</a></sup> Let’s look at a much simpler and generally more efficient option: data parallelism.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Data Parallelism"><div class="sect2" id="idm46263477905880">
<h2>Data Parallelism</h2>

<p>Another way to parallelize the training of a neural network is to replicate it on every device and run each training step simultaneously on all <em>replicas</em> using a different mini-batch for each. The gradients computed by each replica are then averaged, and the result is used to update the model parameters. This is called <em>data parallelism</em>. There are many variants of this idea, so let’s look at the most important ones.</p>










<section data-type="sect3" data-pdf-bookmark="Data parallelism using the mirrored strategy"><div class="sect3" id="idm46263477903064">
<h3>Data parallelism using the mirrored strategy</h3>

<p>Arguably the simplest approach is to completely mirror all the model parameters across all the GPUs and always apply the exact same parameter updates on every GPU. This way, all replicas always remain perfectly identical. This is called the <em>mirrored strategy</em>, and it turns out to be quite efficient, especially when using a single machine (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#mirrored_strategy_diagram">Figure&nbsp;19-18</a>).</p>

<figure class="smallerseventy"><div id="mirrored_strategy_diagram" class="figure">
<img src="./Chapter19_files/mls2_1918.png" alt="mls2 1918" width="1440" height="1074" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1918.png">
<h6><span class="label">Figure 19-18. </span>Data parallelism using the mirrored strategy</h6>
</div></figure>

<p>The tricky part when using this approach is to efficiently compute the mean of all the gradients from all the GPUs and distribute the result across all the GPUs. This can be done using an <em>AllReduce</em> algorithm, a class of algorithms where multiple nodes collaborate to efficiently perform a reduce operation (such as computing the mean, sum, and max), while ensuring that all nodes obtain the same final result. Fortunately, there are off-the-shelf implementations of such algorithms, as we will see.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Data parallelism with centralized parameters"><div class="sect3" id="idm46263477896008">
<h3>Data parallelism with centralized parameters</h3>

<p>Another approach is to store the model parameters outside of the GPU devices performing the computations (called <em>workers</em>), for example on the CPU (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#data_parallelism_diagram">Figure&nbsp;19-19</a>). In a distributed setup, you may place all the parameters on one or more CPU-only servers called <em>parameter servers</em>, whose only role is to host and update the parameters.</p>

<figure class="smallerseventy"><div id="data_parallelism_diagram" class="figure">
<img src="./Chapter19_files/mls2_1919.png" alt="mls2 1919" width="1401" height="949" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1919.png">
<h6><span class="label">Figure 19-19. </span>Data parallelism with centralized parameters</h6>
</div></figure>

<p>Whereas the mirrored strategy imposes synchronous weight updates across all GPUs, this centralized approach allows either synchronous or asynchronous updates. Let’s see the pros and cons of both options.</p>












<section data-type="sect4" data-pdf-bookmark="Synchronous updates"><div class="sect4" id="idm46263477889384">
<h4>Synchronous updates</h4>

<p>With <em>synchronous updates</em>, the aggregator waits until all gradients are available before it computes the average gradients and passes them to the optimizer, which will update the model parameters. Once a replica has finished computing its gradients, it must wait for the parameters to be updated before it can proceed to the next mini-batch. The downside is that some devices may be slower than others, so all other devices will have to wait for them at every step. Moreover, the parameters will be copied to every device almost at the same time (immediately after the gradients are applied), which may saturate the parameter servers’ bandwidth.</p>
<div data-type="tip"><h6>Tip</h6>
<p>To reduce the waiting time at each step, you could ignore the gradients from the slowest few replicas (typically ~10%). For example, you could run 20 replicas, but only aggregate the gradients from the fastest 18 replicas at each step, and just ignore the gradients from the last&nbsp;2. As soon as the parameters are updated, the first 18 replicas can start working again immediately, without having to wait for the&nbsp;2 slowest replicas. This setup is generally described as having 18 replicas plus&nbsp;2 <em>spare replicas</em>.<sup><a data-type="noteref" id="idm46263477884936-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263477884936">19</a></sup></p>
</div>
</div></section>













<section data-type="sect4" data-pdf-bookmark="Asynchronous updates"><div class="sect4" id="idm46263477883576">
<h4>Asynchronous updates</h4>

<p>With asynchronous updates, whenever a replica has finished computing the gradients, it immediately uses them to update the model parameters. There is no aggregation (it removes the “mean” step in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#data_parallelism_diagram">Figure&nbsp;19-19</a>) and no synchronization. Replicas work independently of the other replicas. Since there is no waiting for the other replicas, this approach runs more training steps per minute. Moreover, although the parameters still need to be copied to every device at every step, this happens at different times for each replica, so the risk of bandwidth saturation is reduced.</p>

<p>Data parallelism with asynchronous updates is an attractive choice because of its simplicity, the absence of synchronization delay, and a better use of the bandwidth. However, although it works reasonably well in practice, it is almost surprising that it works at all! Indeed, by the time a replica has finished computing the gradients based on some parameter values, these parameters will have been updated several times by other replicas (on average <em>N</em> – 1 times, if there are <em>N</em> replicas) and there is no guarantee that the computed gradients will still be pointing in the right direction (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#stale_gradients_diagram">Figure&nbsp;19-20</a>). When gradients are severely out-of-date, they are called <em>stale gradients</em>: they can slow down convergence, introducing noise and wobble effects (the learning curve may contain temporary oscillations), or they can even make the training algorithm diverge.</p>

<figure class="smallereighty"><div id="stale_gradients_diagram" class="figure">
<img src="./Chapter19_files/mls2_1920.png" alt="mls2 1920" width="1440" height="800" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1920.png">
<h6><span class="label">Figure 19-20. </span>Stale gradients when using asynchronous updates</h6>
</div></figure>

<p>There are a few ways you can reduce the effect of stale gradients:</p>

<ul>
<li>
<p>Reduce the learning rate.</p>
</li>
<li>
<p>Drop stale gradients or scale them down.</p>
</li>
<li>
<p>Adjust the mini-batch size.</p>
</li>
<li>
<p>Start the first few epochs using just one replica (this is called the <em>warmup phase</em>). Stale gradients tend to be more damaging at the beginning of training, when gradients are typically large and the parameters have not settled into a valley of the cost function yet, so different replicas may push the parameters in quite different directions.</p>
</li>
</ul>

<p>A <a href="https://homl.info/68">paper published by the Google Brain team in April 2016</a> benchmarked various approaches and found that using synchronous updates with a few spare replicas was more efficient than using asynchronous updates, not only converging faster but also producing a better model. However, this is still an active area of research, so you should not rule out asynchronous updates just yet.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Bandwidth saturation"><div class="sect3" id="idm46263477868040">
<h3>Bandwidth saturation</h3>

<p>Whether you use synchronous or asynchronous updates, data parallelism with centralized parameters still requires communicating the model parameters from the parameter servers to every replica at the beginning of each training step, and the gradients in the other direction at the end of each training step. Similarly, when using the mirrored strategy, the gradients produced by each GPU will need to be shared with every other GPU. Unfortunately, there always comes a point where adding an extra GPU will not improve performance at all because the time spent moving the data in and out of GPU RAM (and across the network in a distributed setup) will outweigh the speedup obtained by splitting the computation load. At that point, adding more GPUs will just worsen the bandwidth saturation and actually slow down training.</p>
<div data-type="tip"><h6>Tip</h6>
<p>For some models, typically relatively small and trained on a very large training set, you are often better off training the model on a single machine with a single powerful GPU with a large memory bandwidth.</p>
</div>

<p>Saturation is more severe for large dense models, since they have a lot of parameters and gradients to transfer. It is less severe for small models (but the parallelization gain is limited) and also for large sparse models because the gradients are typically mostly zeros, so they can be communicated efficiently. Jeff Dean, initiator and lead of the Google Brain project, <a href="https://homl.info/69">reported</a> typical speedups of 25–40× when distributing computations across 50 GPUs for dense models, and 300× speedup for sparser models trained across 500 GPUs. As you can see, sparse models really do scale better. Here are a few concrete examples:</p>

<ul>
<li>
<p>Neural Machine Translation: 6× speedup on 8 GPUs</p>
</li>
<li>
<p>Inception/ImageNet: 32× speedup on 50 GPUs</p>
</li>
<li>
<p>RankBrain: 300× speedup on 500 GPUs</p>
</li>
</ul>

<p>Beyond a few dozen GPUs for a dense model or few hundred GPUs for a sparse model, saturation kicks in and performance degrades. There is plenty of research going on to solve this problem (exploring peer-to-peer architectures rather than centralized parameter servers, using lossy model compression, optimizing when and what the replicas need to communicate, and so on), so there will likely be a lot of progress in parallelizing neural networks in the next few years.</p>

<p>In the meantime, to reduce the saturation problem, you probably want to use a few powerful GPUs rather than plenty of weak GPUs, and you should also group your GPUs on few and very well interconnected servers. Moreover, you can also try dropping the float precision from 32 bits (<code>tf.float32</code>) to 16 bits (<code>tf.bfloat16</code>). This will cut in half the amount of data to transfer, often without much impact on the convergence rate or the model’s performance. Lastly, if you are using centralized parameters, you can shard (split) the parameters across multiple parameter servers: adding more parameter servers will reduce the network load on each server and limit the risk of bandwidth saturation.</p>

<p>OK, now let’s train a model across multiple GPUs!</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Training at Scale Using the Distribution Strategies API"><div class="sect2" id="idm46263477856024">
<h2>Training at Scale Using the Distribution Strategies API</h2>

<p>Many models can be trained quite well on a single GPU, or even on a CPU. But if training is too slow, you can try distributing it across multiple GPUs on the same machine. If that’s still too slow, try using more powerful GPUs, or add more GPUs to the machine. If your model performs heavy computations (such as large matrix multiplications), then it will run much faster on powerful GPUs, and you could even try to use TPUs on Google Cloud AI Platform, which will usually run even faster for such models. But if you can’t fit any more GPUs on the same machine, and if TPUs aren’t for you (e.g., perhaps your model doesn’t benefit much from TPUs, or perhaps you want to use your own hardware infrastructure), then you can try training it across several servers, each with multiple GPUs (if this is still not enough, as a last resort you can try adding some model parallelism, but this requires a lot more effort). In this section we will see how to train models at scale, starting with multiple GPUs on the same machine (or TPUs) and then moving on to multiple GPUs across multiple machines.</p>

<p>Luckily, TensorFlow comes with a very simple API that takes care of all the complexity for you: the <em>Distribution Strategies API</em>. To train a Keras model across all available GPUs (on a single machine, for now) using data parallelism with the mirrored strategy, create a <code>MirroredStrategy</code> object, call its <code>scope()</code> method to get a distribution context, and wrap the creation and compilation of your model inside that context. Then call the model’s <code>fit()</code> method normally:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">distribution</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">distribute</code><code class="o">.</code><code class="n">MirroredStrategy</code><code class="p">()</code>

<code class="k">with</code> <code class="n">distribution</code><code class="o">.</code><code class="n">scope</code><code class="p">():</code>
    <code class="n">mirrored_model</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="o">...</code><code class="p">])</code>
    <code class="n">mirrored_model</code><code class="o">.</code><code class="n">compile</code><code class="p">([</code><code class="o">...</code><code class="p">])</code>

<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">100</code> <code class="c1"># must be divisible by the number of replicas</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">mirrored_model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code></pre>

<p>Under the hood, tf.keras is distribution-aware, so in this <code>MirroredStrategy</code> context, it knows that it must replicate all variables and operations across all available GPU devices. By default, it uses the <em>NVIDIA Collective Communications Library</em> (NCCL) for the AllReduce mean operation. Moreover, the <code>fit()</code> method will automatically split each training batch across all the replicas, so it’s important that the batch size be divisible by the number of replicas. And that’s all! Training will generally be significantly faster than using a single device, and the code change was really minimal.</p>

<p>Once you have finished training your model, you can use it to make predictions efficiently: call the <code>predict()</code> method, and it will automatically split the batch across all replicas, making predictions in parallel (again, the batch size must be divisible by the number of replicas). If you call the model’s <code>save()</code> method, it will be saved as a regular model, <em>not</em> as a mirrored model with multiple replicas. So when you load it, it will run like a regular model, on a single device (by default GPU 0, or the CPU if there are no GPUs). If you want to load a model and run it on all available devices, you must call <code>keras.models.load_model()</code> within a distribution context:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">with</code> <code class="n">distribution</code><code class="o">.</code><code class="n">scope</code><code class="p">():</code>
    <code class="n">mirrored_model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">load_model</code><code class="p">(</code><code class="s2">"my_mnist_model.h5"</code><code class="p">)</code></pre>

<p>If you only want to use a subset of all the available GPU devices, you can pass the list to the <code>MirroredStrategy</code>’s constructor:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">distribution</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">distribute</code><code class="o">.</code><code class="n">MirroredStrategy</code><code class="p">([</code><code class="s2">"/gpu:0"</code><code class="p">,</code> <code class="s2">"/gpu:1"</code><code class="p">])</code></pre>

<p>By defaut, the <code>MirroredStrategy</code> class uses NCCL for the AllReduce mean operation, but you can change it by setting the <code>cross_device_ops</code> argument to an instance of the <code>tf.distribute.HierarchicalCopyAllReduce</code> class, or an instance of the <code>tf.distribute.ReductionToOneDevice</code> class. The default NCCL option is based on the <code>tf.distribute.NcclAllReduce</code> class, which is usually faster, but this depends on the number and types of GPUs, so you may want to give the alternatives a try.<sup><a data-type="noteref" id="idm46263477722424-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263477722424">20</a></sup></p>

<p>If you want to try using data parallelism with centralized parameters, replace the <code>MirroredStrategy</code> with the <code>CentralStorageStrategy</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">distribution</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">distribute</code><code class="o">.</code><code class="n">experimental</code><code class="o">.</code><code class="n">CentralStorageStrategy</code><code class="p">()</code></pre>

<p>You can optionally set the <code>compute_devices</code> argument to specify the list of devices you want to use as workers (by default it will use all available GPUs), and you can optionally set the <code>parameter_device</code> argument to specify the device you want to store the parameters on (by default it will use the CPU, or the GPU if there is just one).</p>

<p>Now let’s see how to train a model across a cluster of TensorFlow servers!</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Training a Model on a TensorFlow Cluster"><div class="sect2" id="idm46263477855432">
<h2>Training a Model on a TensorFlow Cluster</h2>

<p>A <em>TensorFlow cluster</em> is a group of TensorFlow processes running in parallel, usually on different machines, and talking to each other to complete some work, for example training or executing a neural network. Each TF process in the cluster is called a <em>task</em>, or a <em>TF server</em>. It has an IP address, a port, and a type (also called its <em>role</em> or its <em>job</em>). The type can be either <code>"worker"</code>, <code>"chief"</code>, <code>"ps"</code> (parameter server), or <code>"evaluator"</code>:</p>

<ul>
<li>
<p>Each <em>worker</em> performs computations, usually on a machine with one or more GPUs.</p>
</li>
<li>
<p>The <em>chief</em> performs computations as well (it is a worker), but it also handles extra work such as writing TensorBoard logs or saving checkpoints. There is a single chief in a cluster. If no chief is specified, then the first worker is the chief.</p>
</li>
<li>
<p>A <em>parameter server</em> (ps) only keeps track of variable values, and it is usually on a CPU-only machine. This type of task is only used with the <code>ParameterServerStrategy</code>.</p>
</li>
<li>
<p>An <em>evaluator</em> obviously takes care of evaluation.</p>
</li>
</ul>

<p>To start a TensorFlow cluster, you must first specify it. This means defining each task’s IP address, TCP port, and type. For example, the following <em>cluster specification</em> defines a cluster with three tasks (two workers and one parameter server, see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#cluster_diagram">Figure&nbsp;19-21</a>). The cluster spec is dictionary with one key per job, and the values are lists of task addresses (IP:port):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">cluster_spec</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"worker"</code><code class="p">:</code> <code class="p">[</code>
        <code class="s2">"machine-a.example.com:2222"</code><code class="p">,</code>  <code class="c1"># /job:worker/task:0</code>
        <code class="s2">"machine-b.example.com:2222"</code>   <code class="c1"># /job:worker/task:1</code>
    <code class="p">],</code>
    <code class="s2">"ps"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"machine-a.example.com:2221"</code><code class="p">]</code> <code class="c1"># /job:ps/task:0</code>
<code class="p">}</code></pre>

<figure class="smallereighty"><div id="cluster_diagram" class="figure">
<img src="./Chapter19_files/mls2_1921.png" alt="mls2 1921" width="1440" height="863" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1921.png">
<h6><span class="label">Figure 19-21. </span>TensorFlow cluster</h6>
</div></figure>

<p>In general there will be a single task per machine, but as this example shows, you can configure multiple tasks on the same machine if you want (but if they share the same GPUs, make sure the RAM is split appropriately, as discussed earlier).</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>By default, every task in the cluster may communicate with every other task, so make sure to configure your firewall to authorize all communications between these machines on these ports (it’s usually simpler if you use the same port on every machine).</p>
</div>

<p>When you start a task, you must give it the cluster spec, and you must also tell it what its type and index are (e.g., worker 0). The simplest way to specify everything at once (both the cluster spec and the current task’s type and index) is to set the <code>TF_CONFIG</code> environment variable before starting TensorFlow. It must be a JSON-encoded dictionary containing a cluster specification (under the <code>"cluster"</code> key) and the type and index of the current task (under the <code>"task"</code> key). For example, the following <code>TF_CONFIG</code> environment variable uses the cluster we just defined and specifies that the task to start is the first worker:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">os</code>
<code class="kn">import</code> <code class="nn">json</code>

<code class="n">os</code><code class="o">.</code><code class="n">environ</code><code class="p">[</code><code class="s2">"TF_CONFIG"</code><code class="p">]</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">dumps</code><code class="p">({</code>
    <code class="s2">"cluster"</code><code class="p">:</code> <code class="n">cluster_spec</code><code class="p">,</code>
    <code class="s2">"task"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"type"</code><code class="p">:</code> <code class="s2">"worker"</code><code class="p">,</code> <code class="s2">"index"</code><code class="p">:</code> <code class="mi">0</code><code class="p">}</code>
<code class="p">})</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>In general you want to define the <code>TF_CONFIG</code> environment variable outside of Python, so the code does not need to include the current task’s type and index (this makes it possible to use the same code across all workers).</p>
</div>

<p>Now let’s train a model on this cluster! We will start with the mirrored strategy, so the cluster should not contain any parameter server. You must define the <code>TF_CONFIG</code> appropriately for each task (e.g., try running a cluster containing two workers on two machines). It’s surprisingly simple: just run the following training code on every worker:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">distribution</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">distribute</code><code class="o">.</code><code class="n">experimental</code><code class="o">.</code><code class="n">MultiWorkerMirroredStrategy</code><code class="p">()</code>

<code class="k">with</code> <code class="n">distribution</code><code class="o">.</code><code class="n">scope</code><code class="p">():</code>
    <code class="n">mirrored_model</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">keras</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="o">...</code><code class="p">])</code>
    <code class="n">mirrored_model</code><code class="o">.</code><code class="n">compile</code><code class="p">([</code><code class="o">...</code><code class="p">])</code>

<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">100</code> <code class="c1"># must be divisible by the number of replicas</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">mirrored_model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code></pre>

<p>Yes, that’s exactly the same code as earlier, except we are using the <code>MultiWorkerMirroredStrategy</code> (in future versions, the <code>MirroredStrategy</code> will probably handle both the single machine and multimachine cases). When you start this script on the first workers, they will remain blocked at the AllReduce step, but as soon as the last worker starts up, training will really begin, and you will see them all advancing at exactly the same rate (since they synchronize at each step).</p>

<p>You can choose from two AllReduce implementations for this distribution strategy: a ring AllReduce algorithm based on gRPC for the network communications, and NCCL’s implementation. The best algorithm to use depends on the number of workers, the numbers and types of GPUs, and the network. By default, TensorFlow will apply some heuristics to select the right algorithm for you, but if you want to force one algorithm, pass <code>CollectiveCommunication.RING</code> or <code>CollectiveCommunication.NCCL</code> (from <code>tf.distribute.experimental</code>) to the strategy’s constructor.</p>

<p>If you prefer to use parameter servers, change the strategy to <code>ParameterServerStrategy</code>, add one or more parameter servers, and configure <code>TF_CONFIG</code> appropriately for every task.</p>

<p>Lastly, if you have access to <a href="https://cloud.google.com/tpu/">TPUs on Google Cloud</a>, you can create a <code>TPUStrategy</code> like this (then use it like the other strategies):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">resolver</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">distribute</code><code class="o">.</code><code class="n">cluster_resolver</code><code class="o">.</code><code class="n">TPUClusterResolver</code><code class="p">()</code>
<code class="n">tf</code><code class="o">.</code><code class="n">tpu</code><code class="o">.</code><code class="n">experimental</code><code class="o">.</code><code class="n">initialize_tpu_system</code><code class="p">(</code><code class="n">resolver</code><code class="p">)</code>
<code class="n">tpu_strategy</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">distribute</code><code class="o">.</code><code class="n">experimental</code><code class="o">.</code><code class="n">TPUStrategy</code><code class="p">(</code><code class="n">resolver</code><code class="p">)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>If you are a researcher, you may be eligible to use TPUs for free; see <a href="https://tensorflow.org/tfrc"><em class="hyperlink">https://tensorflow.org/tfrc</em></a> for more details.</p>
</div>

<p>You can now train models across multiple GPUs and multiple servers: give yourself a pat on the back! Now if you want to train a large model, you will need many GPUs, across many servers, which will require either buying a lot of hardware or managing a lot cloud VMs. In many cases, it’s going to be less hassle and less expensive to use a cloud service that takes care of provisioning and managing all this infrastructure for you, just when you need it. Let’s see how to do that on GCP.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Running Large Training Jobs on Google Cloud AI Platform"><div class="sect2" id="idm46263477697320">
<h2>Running Large Training Jobs on Google Cloud AI Platform</h2>

<p>If you decide to use Google AI Platform, you can deploy a training job with the same training code as you would run on your own TF cluster, and the platform will take care of provisioning and configuring as many GPU VMs as you desire (within your quotas), and it will simply run your training code.</p>

<p>To start the job, you will need the <code>gcloud</code> command-line tool, which is part of the <a href="https://cloud.google.com/sdk/">Google Cloud SDK</a>. You can either install the SDK on your own machine, or just use the Google Cloud Shell on GCP: this is a terminal you can use directly in your web browser, and it runs on a free Linux VM (Debian), with the SDK already installed and preconfigured for you. The Cloud Shell is available anywhere in GCP: just click the Activate Cloud Shell icon at the top right of the page (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#cloud_shell_screenshot">Figure&nbsp;19-22</a>).</p>

<figure class="smallerfourty"><div id="cloud_shell_screenshot" class="figure">
<img src="./Chapter19_files/mls2_1922.png" alt="mls2 1922" width="934" height="224" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1922.png">
<h6><span class="label">Figure 19-22. </span>Activate the Google Cloud Shell</h6>
</div></figure>

<p>If you prefer to install the SDK on your machine, once you have installed it, you need to initialize it by running <code>gcloud init</code>: you will need to log in to GCP and grant access to your GCP resources, then select the GCP project you want to use (if you have more than one), as well as the region where you want the job to run. The <code>gcloud</code> command gives you access to every GCP feature, including the ones we used earlier. You don’t have to go through the web interface every time; you can write scripts that start or stop VMs for you, deploy models, or perform any other GCP action.</p>

<p>Before you can run the training job, you need to write the training code, exactly like you did earlier for a distributed setup (e.g., using the <code>ParameterServerStrategy</code>). The AI Platform will take care of setting <code>TF_CONFIG</code> for you on each VM. Once that’s done, you can deploy it and run it on a TF cluster with a command line like this:</p>
<pre data-type="programlisting" data-code-language="shell-session"><code class="go">$ </code><strong><code class="go">gcloud ai-platform jobs submit training my_job_20190531_164700</code></strong><code class="go"> \
</code><code class="go">    </code><strong><code class="go">--region asia-southeast1</code></strong><code class="go"> \
</code><code class="go">    </code><strong><code class="go">--scale-tier PREMIUM_1</code></strong><code class="go"> \
</code><code class="go">    </code><strong><code class="go">--runtime-version 2.0</code></strong><code class="go"> \
</code><code class="go">    </code><strong><code class="go">--python-version 3.5</code></strong><code class="go"> \
</code><code class="go">    </code><strong><code class="go">--package-path /my_project/src/trainer</code></strong><code class="go"> \
</code><code class="go">    </code><strong><code class="go">--module-name trainer.task</code></strong><code class="go"> \
</code><code class="go">    </code><strong><code class="go">--staging-bucket gs://my-staging-bucket</code></strong><code class="go"> \
</code><code class="go">    </code><strong><code class="go">--job-dir gs://my-mnist-model-bucket/trained_model</code></strong><code class="go"> \
</code><code class="go">    </code><strong><code class="go">--</code></strong><code class="go">
</code><code class="go">    </code><strong><code class="go">--my-extra-argument1 foo --my-extra-argument2 bar</code></strong><code class="go">
</code></pre>

<p>Let’s go through these options: the command will start a training job named <code>my_job_20190531_164700</code>, in the <code>asia-southeast1</code> region, using a <code>PREMIUM_1</code> <em>scale-tier</em>: this corresponds to 20 workers (including a chief) and 11 parameter servers (check out the other <a href="https://homl.info/scaletiers">available scale tiers</a>). All these VMs will be based on AI Platform’s 2.0 runtime (a VM configuration which includes TensorFlow 2.0 and many other packages)<sup><a data-type="noteref" id="idm46263477334296-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263477334296">21</a></sup> and Python 3.5. The training code is located in the <em>/my_project/src/trainer</em> directory, and the <code>gcloud</code> command will automatically bundle it into a pip package and upload it to GCS at <code>gs://my-staging-bucket</code>. Next, AI Platform will start several VMs, deploy the package to them, and run the <code>trainer.task</code> module. Lastly, the <code>--job-dir</code> argument and the extra arguments will be passed to the training program: the chief task will usually use the <code>--job-dir</code> argument to find out where to save the final model on GCS, in this case at <code>gs://my-mnist-model-bucket/trained_model</code>. And that’s it! In the GCP Console, you can then open the navigation menu, scroll down to the “Artificial Intelligence” section, and open AI Platform &gt; Jobs. You should see your job running, and if you click it you will see graphs showing the CPU, GPU and RAM utilization for every task, and you can click “View Logs” to access the detailed logs using Stackdriver.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>If you place the training data on GCS, you can create a <code>tf.data.TextLineDataset</code> or <code>tf.data.TFRecordDataset</code> to access it: just use the GCS paths as the filenames (e.g., <code>gs://my-data-bucket/my_data_001.csv</code>). These datasets rely on the <code>tf.io.gfile</code> package to access files: it supports both local files and GCS files (but make sure the service account you use has access to GCS).</p>
</div>

<p>If you want to explore a few hyperparameter values, you can simply run multiple jobs and specify the hyperparameter values using the extra arguments for your tasks. However, if you want to explore many hyperparameters efficiently, it’s a good idea to use AI Platform’s hyperparameter tuning service instead.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Black-Box Hyperparameter Tuning on AI Platform"><div class="sect2" id="idm46263477395544">
<h2>Black-Box Hyperparameter Tuning on AI Platform</h2>

<p>AI Platform provides a powerful Bayesian Optimization hyperparameter tuning service, <a href="https://homl.info/vizier">Google Vizier</a>.<sup><a data-type="noteref" id="idm46263477298088-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263477298088">22</a></sup> To use it, you need to pass a YAML configuration file when creating the job (<code>--config tuning.yaml</code>). For example, it may look like this:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">trainingInput</code><code class="p">:</code>
  <code class="nt">hyperparameters</code><code class="p">:</code>
    <code class="nt">goal</code><code class="p">:</code> <code class="l-Scalar-Plain">MAXIMIZE</code>
    <code class="nt">hyperparameterMetricTag</code><code class="p">:</code> <code class="l-Scalar-Plain">accuracy</code>
    <code class="nt">maxTrials</code><code class="p">:</code> <code class="l-Scalar-Plain">10</code>
    <code class="nt">maxParallelTrials</code><code class="p">:</code> <code class="l-Scalar-Plain">2</code>
    <code class="nt">params</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="nt">parameterName</code><code class="p">:</code> <code class="l-Scalar-Plain">n_layers</code>
        <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">INTEGER</code>
        <code class="nt">minValue</code><code class="p">:</code> <code class="l-Scalar-Plain">10</code>
        <code class="nt">maxValue</code><code class="p">:</code> <code class="l-Scalar-Plain">100</code>
        <code class="nt">scaleType</code><code class="p">:</code> <code class="l-Scalar-Plain">UNIT_LINEAR_SCALE</code>
      <code class="p-Indicator">-</code> <code class="nt">parameterName</code><code class="p">:</code> <code class="l-Scalar-Plain">momentum</code>
        <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">DOUBLE</code>
        <code class="nt">minValue</code><code class="p">:</code> <code class="l-Scalar-Plain">0.1</code>
        <code class="nt">maxValue</code><code class="p">:</code> <code class="l-Scalar-Plain">1.0</code>
        <code class="nt">scaleType</code><code class="p">:</code> <code class="l-Scalar-Plain">UNIT_LOG_SCALE</code></pre>

<p>This tells AI Platform that we want to maximize the metric named <code>"accuracy"</code>, the job will run a maximum of 10 trials (each trial will run our training code to train the model from scratch), and it will run a maximum of 2 trials in parallel. We want it to tune two hyperparameters: the <code>n_layers</code> hyperparameter (an integer between 10 and 100) and the <code>momentum</code> hyperparameter (a float between 0.1 and 1.0). The <code>scaleType</code> argument specifies the prior for the hyperparameter value: <code>UNIT_LINEAR_SCALE</code> means a flat prior (i.e., no a priori preference), while <code>UNIT_LOG_SCALE</code> says we have a prior belief that the optimal value lies closer to the max value (the other possible prior is <code>UNIT_REVERSE_LOG_SCALE</code>, when we believe the optimal value to be close to the min value).</p>

<p>The <code>n_layers</code> and <code>momentum</code> arguments will be passed as command-line arguments to the training code, and of course it is expected to use them. The question is, how will the training code communicate the metric back to the AI Platform so that it can decide which hyperparameter values to use during the next trial? Well, AI Platform just monitors the output directory (specified via <code>--job-dir</code>) for any event file (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#ann_chapter">Chapter&nbsp;10</a>) containing summaries for a metric named <code>"accuracy"</code> (or whatever metric name is specified as the <code>hyperparameterMetricTag</code>), and it reads those values. So your training code simply has to use the <code>TensorBoard</code> callback (which you will want to do anyway for monitoring) and you’re good to go!</p>

<p>Once the job is finished, all the hyperparameter values used in each trial and the resulting accuracy will be available in the job’s output (available via the AI Platform &gt; Jobs page).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>AI Platform Jobs can also be used to efficiently execute your model on large amounts of data: each worker can read part of the data from GCS, make predictions, and save them to GCS.</p>
</div>

<p>Now you have all the tools and knowledge you need to create state-of-the-art neural net architectures, train them at scale using various distribution strategies, on your own infrastructure or on the cloud, and you can even perform powerful Bayesian Optimization to fine-tune the hyperparameters!</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="idm46263477225000">
<h1>Exercises</h1>
<ol>
<li>
<p>What does a SavedModel contain? How do you inspect its content?</p>
</li>
<li>
<p>When should you use TF Serving? What are its main features? What are some tools you can use to deploy it?</p>
</li>
<li>
<p>How do you deploy a model across multiple TF Serving instances?</p>
</li>
<li>
<p>When should you use the gRPC API rather than the REST API to query a model served by TF Serving?</p>
</li>
<li>
<p>What are the different ways TFLite reduces a model’s size to make it run on a mobile or embedded device?</p>
</li>
<li>
<p>What is quantization-aware training, and why would you need it?</p>
</li>
<li>
<p>What is model parallelism and data parallelism? Why is the latter generally recommended?</p>
</li>
<li>
<p>When training a model across multiple servers, what distribution strategies can you use? How do you choose which one to use?</p>
</li>
<li>
<p>Train a model (any model you like) and deploy it to TF Serving or Google Cloud AI Platform. Write the client code to query it using the REST API or the gRPC API. Update the model and deploy the new version. Your client code will now query the new version. Roll back to the first version.</p>
</li>
<li>
<p>Train any model across multiple GPUs on the same machine using the <code>MirroredStrategy</code> (if you do not have access to GPUs, you can use Colaboratory with a GPU Runtime and create two virtual GPUs). Train the model again using the <code>CentralStorageStrategy</code> and compare the training time.</p>
</li>
<li>
<p>Train a small model on Google Cloud AI Platform, using Black-Box Hyperparameter Tuning.</p>
</li>

</ol>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Thank You!"><div class="sect1" id="idm46263477223256">
<h1>Thank You!</h1>

<p>Before we close the last chapter of this book, I would like to thank you for reading it up to the last paragraph. I truly hope that you had as much pleasure reading this book as I had writing it, and that it will be useful for your projects, big or small.</p>

<p>If you find errors, please send feedback. More generally, I would love to know what you think, so please don’t hesitate to contact me via O’Reilly, or through the <em>ageron/handson-ml2</em> GitHub project, or on Twitter at @aureliengeron.</p>

<p>Going forward, my best advice to you is to practice and practice: try going through all the exercises, if you have not done so already, play with the Jupyter notebooks, join Kaggle.com or some other ML community, watch ML courses, read papers, attend conferences, and meet experts. It also helps tremendously to have a concrete project to work on, whether it is for work or for fun (ideally for both), so if there’s anything you have always dreamt of building, give it a shot! Work incrementally; don’t shoot for the Moon right away, but stay focused on your project and build it piece by piece: it will require patience and perseverance, but once you have a walking robot, or a chat-bot, or whatever else you fancy to build, it will be immensely rewarding.</p>

<p>My greatest hope is that this book will inspire you to build a wonderful ML application that will benefit all of us! What will it be?</p>
<p><em>Aurélien Géron, June 17, 2019</em></p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46263481357064"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263481357064-marker" class="totri-footnote">1</a></sup> An A/B experiment consists in testing two different versions of your product on different subsets of users in order to check which version works best and get other insights.</p><p data-type="footnote" id="idm46263481350216"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263481350216-marker" class="totri-footnote">2</a></sup> A REST (or RESTful) API is an API that uses standard HTTP verbs, such as GET, POST, PUT, and DELETE, and uses JSON inputs and outputs. The gRPC protocol is more complex but more efficient. Data is exchanged using Protocol Buffers (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch13.html#data_chapter">Chapter&nbsp;13</a>).</p><p data-type="footnote" id="idm46263480999928"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263480999928-marker" class="totri-footnote">3</a></sup> If you are not familiar with Docker, it allows you to easily download a set of applications packaged in a <em>Docker image</em> (including all their dependencies and usually some good default configuration) and then run them on your system using a <em>Docker engine</em>. When you run an image, the engine creates a <em>Docker container</em> that keeps the applications well isolated from your own system (but you can give it some limited access if you want). It is similar to a virtual machine, but much faster and lighter weight, as the container relies directly on the host’s kernel. This means that the image does not need to include or run its own kernel.</p><p data-type="footnote" id="idm46263480801160"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263480801160-marker" class="totri-footnote">4</a></sup> To be fair, this can be mitigated by serializing the data first and encoding it to Base64 before creating the REST request. Moreover, REST requests can be compressed using gzip, which reduces the payload size significantly.</p><p data-type="footnote" id="idm46263480466632"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263480466632-marker" class="totri-footnote">5</a></sup> If the SavedModel contains some example instances in the <em>assets/extra</em> directory, you can configure TF Serving to execute the model on these instances before starting to serve new requests with it. This is called <em>model warmup</em>: it will ensure that everything is properly loaded, avoiding long response time for the first requests.</p><p data-type="footnote" id="idm46263480317896"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263480317896-marker" class="totri-footnote">6</a></sup> At the time of this writing, TensorFlow version 2 is not available yet on AI Platform, but that’s OK: you can use 1.13, and it will run your TF&nbsp;2 SavedModels just fine.</p><p data-type="footnote" id="idm46263480283592"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263480283592-marker" class="totri-footnote">7</a></sup> If you get an error saying that module <code>google.appengine</code> was not found, then set <code>cache_discovery=False</code> in the call to the <code>build()</code> method: <a href="https://stackoverflow.com/q/55561354"><em class="hyperlink">https://stackoverflow.com/q/55561354</em></a>.</p><p data-type="footnote" id="idm46263480044792"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263480044792-marker" class="totri-footnote">8</a></sup> Also check out TensorFlow’s <a href="https://homl.info/tfgtt">Graph Transform Tools</a> for modifying and optimizing computational graphs.</p><p data-type="footnote" id="idm46263479888952"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263479888952-marker" class="totri-footnote">9</a></sup> If you’re interested in this topic, check out <a href="https://tensorflow.org/federated"><em>federated learning</em></a>.</p><p data-type="footnote" id="idm46263479606472"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263479606472-marker">10</a></sup> Please check the docs for detailed and up-to-date installation instructions, as they change quite often.</p><p data-type="footnote" id="idm46263478674040"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263478674040-marker">11</a></sup> Many code examples in this chapter use experimental APIs. They are very likely to be moved to the core API in future versions. So if an experimental function fails, try simply removing the word <code>experimental</code>, and hopefully it will work. If not, then perhaps the API has changed a bit: please check the Jupyter notebook; I will ensure it contains the correct code.</p><p data-type="footnote" id="idm46263478670664"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263478670664-marker">12</a></sup> Presumably, these quotas are meant to stop bad guys who might be tempted to use GCP with stolen credit cards to mine cryptocurrencies.</p><p data-type="footnote" id="idm46263478393640"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263478393640-marker">13</a></sup> Google Research, <em>TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</em>, November 2015.</p><p data-type="footnote" id="idm46263478086920"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263478086920-marker">14</a></sup> As we saw in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch12.html#tensorflow_chapter">Chapter&nbsp;12</a>, a kernel is a variable or operation’s implementation for a specific data type and device type. For example, there is a GPU kernel for the <code>float32</code> <code>tf.matmul()</code> operation, but there is no GPU kernel for <code>int32</code> <code>tf.matmul()</code> (only a CPU kernel).</p><p data-type="footnote" id="idm46263478082504"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263478082504-marker">15</a></sup> You can also use <code>tf.debugging.set_log_device_placement(True)</code> to log all device placements.</p><p data-type="footnote" id="idm46263477966424"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263477966424-marker">16</a></sup> This can be useful if you want to guarantee perfect reproducibility, as I explain in <a href="https://homl.info/repro">this video</a>, based on TF&nbsp;1.</p><p data-type="footnote" id="idm46263477960424"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263477960424-marker">17</a></sup> At the time of this writing, it only prefetches the data to the CPU RAM, but you can use <code>tf.data.experimental.prefetch_to_device()</code> to make it prefetch the data and push it to the device of your choice so that the GPU does not waste time waiting for the data to be transferred.</p><p data-type="footnote" id="idm46263477907496"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263477907496-marker">18</a></sup> If you are interested in going further with model parallelism, check out <a href="https://github.com/tensorflow/mesh">Mesh TensorFlow</a>.</p><p data-type="footnote" id="idm46263477884936"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263477884936-marker">19</a></sup> This name is slightly confusing because it sounds like some replicas are special, doing nothing. In reality, all replicas are equivalent: they all work hard to be among the fastest at each training step, and the losers vary at every step (unless some devices are really slower than others). However, it does mean that if a server crashes, training will continue just fine.</p><p data-type="footnote" id="idm46263477722424"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263477722424-marker">20</a></sup> For more details on AllReduce algorithms, read this <a href="https://homl.info/uenopost">great post</a> by Yuichiro Ueno, and this page on <a href="https://homl.info/ncclalgo">scaling with NCCL</a>.</p><p data-type="footnote" id="idm46263477334296"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263477334296-marker">21</a></sup> At the time of this writing, the 2.0 runtime is not yet available, but it should be ready by the time your read this. Check out the <a href="https://homl.info/runtimes">list of available runtimes</a>.</p><p data-type="footnote" id="idm46263477298088"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#idm46263477298088-marker">22</a></sup> Daniel Golovin et al., <em>Google Vizier: A Service for Black-Box Optimization</em> (Pittsburgh, PA: Google Research, 2017).</p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-20" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders">
		
		<li class="copy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#">Add Highlight</a></li>
		<li class="add-note"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#">
			Add Note
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">18. Reinforcement Learning</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/app01.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">A. Exercise Solutions</div>
        </a>
    
  
  </div>

</section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    



        
      </div>
      



  <footer class="pagefoot">
    <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#" class="icon-up" style="display: none;"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      <li class="full-support"><a href="https://www.oreilly.com/online-learning/support/">Support</a></li>
      <li><a href="https://www.oreilly.com/online-learning/apps.html">Get the App</a></li>
      
        <li><a href="https://learning.oreilly.com/accounts/logout/">Sign Out</a></li>
      
    </ul>
    <span class="copyright">© 2019 <a href="https://learning.oreilly.com/" target="_blank">Safari</a>.</span>
    <a href="https://learning.oreilly.com/terms/">Terms of Service</a> /
    <a href="https://learning.oreilly.com/membership-agreement/">Membership Agreement</a> /
    <a href="https://www.oreilly.com/privacy.html">Privacy Policy</a>
  </footer>

<script type="text/javascript">window.NREUM||(NREUM={});NREUM.info={"applicationID":"172641827,79672898,93931619","errorBeacon":"bam.nr-data.net","agent":"","applicationTime":451,"licenseKey":"510f1a6865","transactionName":"YgdaZ0NSW0cEB0RdWltNfkZfUEFdCgofXFBHDVYdR1pQQxZeRl1QQj1aWkU=","beacon":"bam.nr-data.net","queueTime":4}</script>


    
    <script src="./Chapter19_files/saved_resource" charset="utf-8"></script><script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","1732687426968531");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=1732687426968531&amp;ev=PageView&amp;noscript=1"></noscript><script type="text/javascript" id="">(function(){window.medalliaUserIdentifier=document.documentElement.dataset.userUuid;window.medalliaUserName=document.documentElement.dataset.username})();</script>
<script type="text/javascript" id="" src="./Chapter19_files/embed.js.download"></script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script>
<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("set","agent","tmgoogletagmanager","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>

<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>
<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.09556792590964802"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.5229133033930915" width="0" height="0" alt="" src="./Chapter19_files/0"></div>
    <script src="./Chapter19_files/saved_resource(1)" charset="utf-8"></script>
  

<script src="./Chapter19_files/saved_resource(2)" type="text/javascript"></script><script type="text/javascript" id="">window._pp=window._pp||[];if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/register/")_pp.targetUrl="/confirm/trial";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/subscribe/")_pp.targetUrl="/confirm/paid";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/signup/")_pp.targetUrl="/confirm/paid";_pp.siteId="2508";
_pp.siteUId="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79";_pp.orderValue="undefined";_pp.orderId="undefined";(function(){var ppjs=document.createElement("script");ppjs.type="text/javascript";ppjs.async=true;ppjs.src=("https:"==document.location.protocol?"https:":"http:")+"//cdn.pbbl.co/r/"+_pp.siteId+".js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(ppjs,s)})();</script><div class="annotator-notice"></div><div class="font-flyout" style="top: 201px; left: 1194px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#">Reset</a>
</div>
</div><script type="text/javascript" async="" src="./Chapter19_files/generic1566415868241.js.download" charset="UTF-8"></script><div style="display: none; visibility: hidden;"><script>(function(){if(null!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')&&void 0!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')){var a=!1;window.addEventListener("blur",function(){a&&dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"facebook",eventVal:0,nonInteraction:0})});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseover",function(){window.focus();
a=!0});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseout",function(){a=!1})}try{window.twttr=function(b,a,d){var c,e=b.getElementsByTagName(a)[0];if(!b.getElementById(d))return b=b.createElement(a),b.id=d,b.src="//platform.twitter.com/widgets.js",e.parentNode.insertBefore(b,e),window.twttr||(c={_e:[],ready:function(a){c._e.push(a)}})}(document,"script","twitter-wjs"),twttr.ready(function(a){a.events.bind("tweet",trackTwitter)})}catch(b){}})();
null!==document.querySelector(".IN-widget")&&void 0!==document.querySelector(".IN-widget")&&document.querySelector(".IN-widget").addEventListener("click",function(){dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"linkedin",eventVal:0,nonInteraction:0})});
function trackTwitter(a){a&&(a.target&&"IFRAME"==a.target.nodeName&&(opt_target=extractParamFromUri(a.target.src,"url")),dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"twitter",eventVal:0,nonInteraction:0}))}function extractParamFromUri(a,b){if(a){var c=new RegExp("[\\?\x26#]"+b+"\x3d([^\x26#]*)");c=c.exec(a);if(null!=c)return unescape(c[1])}};</script></div><span><div id="KampyleAnimationContainer" style="z-index: 2147483000; border: 0px; position: fixed; display: block; width: 0px; height: 0px;"></div></span><iframe scrolling="no" frameborder="0" allowtransparency="true" src="./Chapter19_files/widget_iframe.097c1f5038f9e8a0d62a39a892838d66.html" title="Twitter settings iframe" style="display: none;"></iframe><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_AMS, sans-serif;"></div></div></body></html>