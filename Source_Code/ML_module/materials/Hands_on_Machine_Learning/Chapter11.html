<!DOCTYPE html>
<!-- saved from url=(0091)https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html -->
<html class=" js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/hands-on-machine-learning/9781492032632/part01.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="4626953" data-user-uuid="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79" data-username="17481074" data-account-type="B2B" data-activated-trial-date="" data-archive="9781492032632" data-publishers="O&#39;Reilly Media, Inc." data-htmlfile-name="part01.html" data-epub-title="Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781492032632"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><script type="text/javascript" async="" src="./Chapter11_files/cool-2.1.15.min.js.download"></script><script type="text/javascript" src="./Chapter11_files/510f1a6865"></script><script id="twitter-wjs" src="./Chapter11_files/widgets.js.download"></script><script src="./Chapter11_files/nr-1130.min.js.download"></script><script type="text/javascript" async="" src="./Chapter11_files/2508.js.download"></script><script async="" src="./Chapter11_files/fbevents.js.download"></script><script type="text/javascript" async="" src="./Chapter11_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter11_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter11_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter11_files/analytics.js.download"></script><script type="text/javascript" async="" src="./Chapter11_files/ec.js.download"></script><script type="text/javascript" async="" src="./Chapter11_files/bat.js.download"></script><script type="text/javascript" async="" src="./Chapter11_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter11_files/insight.min.js.download"></script><script type="text/javascript" async="" src="./Chapter11_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter11_files/linkid.js.download"></script><script async="" src="./Chapter11_files/gtm.js.download"></script><script async="" src="./Chapter11_files/analytics.js.download"></script><script type="text/javascript">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if("function"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e("handle"),a=e(3),u=e(4),f=e("ee").get("tracer"),c=e("loader"),s=NREUM;"undefined"==typeof window.newrelic&&(newrelic=s);var p=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],d="api-",l=d+"ixn-";a(p,function(e,n){s[n]=o(d+n,!0,"api")}),s.addPageAction=o(d+"addPageAction",!0),s.setCurrentRouteName=o(d+"routeName",!0),n.exports=newrelic,s.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(e,n){var t={},r=this,o="function"==typeof n;return i(l+"tracer",[c.now(),e,t],r),function(){if(f.emit((o?"":"no-")+"fn-start",[c.now(),r,o],t),o)try{return n.apply(this,arguments)}catch(e){throw f.emit("fn-err",[arguments,this,e],t),e}finally{f.emit("fn-end",[c.now()],t)}}}};a("actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(e,n){m[n]=o(l+n)}),newrelic.noticeError=function(e,n){"string"==typeof e&&(e=new Error(e)),i("err",[e,c.now(),!1,n])}},{}],2:[function(e,n,t){function r(e,n){if(!o)return!1;if(e!==o)return!1;if(!n)return!0;if(!i)return!1;for(var t=i.split("."),r=n.split("."),a=0;a<r.length;a++)if(r[a]!==t[a])return!1;return!0}var o=null,i=null,a=/Version\/(\S+)\s+Safari/;if(navigator.userAgent){var u=navigator.userAgent,f=u.match(a);f&&u.indexOf("Chrome")===-1&&u.indexOf("Chromium")===-1&&(o="Safari",i=f[1])}n.exports={agent:o,version:i,match:r}},{}],3:[function(e,n,t){function r(e,n){var t=[],r="",i=0;for(r in e)o.call(e,r)&&(t[i]=n(r,e[r]),i+=1);return t}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],4:[function(e,n,t){function r(e,n,t){n||(n=0),"undefined"==typeof t&&(t=e?e.length:0);for(var r=-1,o=t-n||0,i=Array(o<0?0:o);++r<o;)i[r]=e[n+r];return i}n.exports=r},{}],5:[function(e,n,t){n.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(e,n,t){function r(){}function o(e){function n(e){return e&&e instanceof r?e:e?f(e,u,i):i()}function t(t,r,o,i){if(!d.aborted||i){e&&e(t,r,o);for(var a=n(o),u=v(t),f=u.length,c=0;c<f;c++)u[c].apply(a,r);var p=s[y[t]];return p&&p.push([b,t,r,a]),a}}function l(e,n){h[e]=v(e).concat(n)}function m(e,n){var t=h[e];if(t)for(var r=0;r<t.length;r++)t[r]===n&&t.splice(r,1)}function v(e){return h[e]||[]}function g(e){return p[e]=p[e]||o(t)}function w(e,n){c(e,function(e,t){n=n||"feature",y[t]=n,n in s||(s[n]=[])})}var h={},y={},b={on:l,addEventListener:l,removeEventListener:m,emit:t,get:g,listeners:v,context:n,buffer:w,abort:a,aborted:!1};return b}function i(){return new r}function a(){(s.api||s.feature)&&(d.aborted=!0,s=d.backlog={})}var u="nr@context",f=e("gos"),c=e(3),s={},p={},d=n.exports=o();d.backlog=s},{}],gos:[function(e,n,t){function r(e,n,t){if(o.call(e,n))return e[n];var r=t();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,n,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return e[n]=r,r}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],handle:[function(e,n,t){function r(e,n,t,r){o.buffer([e],r),o.emit(e,n,t)}var o=e("ee").get("handle");n.exports=r,r.ee=o},{}],id:[function(e,n,t){function r(e){var n=typeof e;return!e||"object"!==n&&"function"!==n?-1:e===window?0:a(e,i,function(){return o++})}var o=1,i="nr@id",a=e("gos");n.exports=r},{}],loader:[function(e,n,t){function r(){if(!E++){var e=x.info=NREUM.info,n=l.getElementsByTagName("script")[0];if(setTimeout(s.abort,3e4),!(e&&e.licenseKey&&e.applicationID&&n))return s.abort();c(y,function(n,t){e[n]||(e[n]=t)}),f("mark",["onload",a()+x.offset],null,"api");var t=l.createElement("script");t.src="https://"+e.agent,n.parentNode.insertBefore(t,n)}}function o(){"complete"===l.readyState&&i()}function i(){f("mark",["domContent",a()+x.offset],null,"api")}function a(){return O.exists&&performance.now?Math.round(performance.now()):(u=Math.max((new Date).getTime(),u))-x.offset}var u=(new Date).getTime(),f=e("handle"),c=e(3),s=e("ee"),p=e(2),d=window,l=d.document,m="addEventListener",v="attachEvent",g=d.XMLHttpRequest,w=g&&g.prototype;NREUM.o={ST:setTimeout,SI:d.setImmediate,CT:clearTimeout,XHR:g,REQ:d.Request,EV:d.Event,PR:d.Promise,MO:d.MutationObserver};var h=""+location,y={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-1130.min.js"},b=g&&w&&w[m]&&!/CriOS/.test(navigator.userAgent),x=n.exports={offset:u,now:a,origin:h,features:{},xhrWrappable:b,userAgent:p};e(1),l[m]?(l[m]("DOMContentLoaded",i,!1),d[m]("load",r,!1)):(l[v]("onreadystatechange",o),d[v]("onload",r)),f("mark",["firstbyte",u],null,"api");var E=0,O=e(5)},{}]},{},["loader"]);</script><link rel="apple-touch-icon" href="https://learning.oreilly.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://learning.oreilly.com/favicon.ico" type="image/x-icon"><link href="./Chapter11_files/css" rel="stylesheet" type="text/css"><title>11. Training Deep Neural Networks - Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition</title><link rel="stylesheet" href="./Chapter11_files/output.68851547a55f.css" type="text/css"><link rel="stylesheet" type="text/css" href="./Chapter11_files/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="./Chapter11_files/font-awesome.min.css"><style type="text/css" title="ibis-book">@charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000 !important;padding-top:10px !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:2em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content p[data-type="footnote"]{font-size:90% !important;line-height:1.2em !important;margin-left:2.5em !important;text-indent:-2.3em !important}#sbo-rt-content p[data-type="footnote"] sup{display:inline-block !important;position:static !important;width:2em !important;text-align:right !important;font-size:100% !important;padding-right:.5em !important}#sbo-rt-content p[data-type="footnote"] a[href$="-marker"]{font-family:sans-serif !important;font-size:90% !important;color:#8e0012 !important}#sbo-rt-content p[data-type="footnote"] a[data-type="xref"]{margin:0 !important;padding:0 !important;text-indent:0 !important}#sbo-rt-content a[data-type="noteref"]{font-family:sans-serif !important;color:#8e0012;margin-left:0;padding-left:0}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dd{margin-left:1.5em !important;margin-bottom:.25em}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul>li,#sbo-rt-content ol ul,#sbo-rt-content ol ul>li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul>li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul>li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul>li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol>li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol>li,#sbo-rt-content ul ol,#sbo-rt-content ul ol>li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol>li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol>li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:middle;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content table.border tbody>tr:last-child>td{border-bottom:transparent}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:1em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content .width-10{width:10vw !important}#sbo-rt-content .width-20{width:20vw !important}#sbo-rt-content .width-30{width:30vw !important}#sbo-rt-content .width-40{width:40vw !important}#sbo-rt-content .width-50{width:50vw !important}#sbo-rt-content .width-60{width:60vw !important}#sbo-rt-content .width-70{width:70vw !important}#sbo-rt-content .width-80{width:80vw !important}#sbo-rt-content .width-90{width:90vw !important}#sbo-rt-content .width-full,#sbo-rt-content .width-100{width:100vw !important}#sbo-rt-content div[data-type="equation"].fifty-percent img{width:50%}</style><script> // <![CDATA[
    var g = {
      position_cache: {
        
          "chapter": "/api/v1/book/9781492032632/chapter/part01.html",
          "book_id": "9781492032632",
          "chapter_uri": "part01.html",
          "position": 100.0,
          "user_uuid": "d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79",
          "next_chapter_uri": "/library/view/hands-on-machine-learning/9781492032632/ch01.html"
        
      },
      title: "Hands\u002Don Machine Learning with Scikit\u002DLearn, Keras, and TensorFlow, 2nd Edition",
      author_list: "Aurélien Géron",
      format: "book",
      source: "application/epub+zip",
      is_system_book: true,
      is_public: false,
      loaded_from_server: true,
      allow_scripts: false,
      has_mathml: true
    };
    // ]]></script><script src="./Chapter11_files/modernizr.8e35451ddb64.js.download"></script><script>
    
      

      
        
          window.PUBLIC_ANNOTATIONS = true;
        
      

      window.MOBILE_PUBLIC_ANNOTATIONS = false;

    

    
      window.PRIVACY_CONTROL_OVERRIDE = false;
    

      window.PRIVACY_CONTROL_SWITCH = true;

      window.PUBLISHER_PAGES = true;

      window.SBO = {
        "constants": {
          "SITB_ENDPOINT": "/api/v2/sitb/",
          "SEARCH_SELECT_ENDPOINT": "https://learning.oreilly.com/api/v2/search/select/",
          "ENABLE_ONLINE_TRAINING": false
        }
      };
  </script><link rel="canonical" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta name="description" content=" Part I. The Fundamentals of Machine Learning "><meta property="og:title" content="I. The Fundamentals of Machine Learning"><meta itemprop="isPartOf" content="/library/view/hands-on-machine-learning/9781492032632/"><meta itemprop="name" content="I. The Fundamentals of Machine Learning"><meta property="og:url" itemprop="url" content="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://learning.oreilly.com/library/cover/9781492032632/"><meta property="og:description" itemprop="description" content=" Part I. The Fundamentals of Machine Learning "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O&#39;Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781492032649"><meta property="og:book:author" itemprop="author" content="Aurélien Géron"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: &lt;%= font_size %&gt; !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: &lt;%= font_family %&gt; !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: &lt;%= column_width %&gt;% !important; margin: 0 auto !important; }"></style><noscript><meta http-equiv="refresh" content="0; url=/library/no-js/" /></noscript><script type="text/javascript">
    (function(i,s,o,g,r,a,m) {
      i['GoogleAnalyticsObject']=r;
      i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();
      a=s.createElement(o),m=s.getElementsByTagName(o)[0];
      a.async=1;
      a.src=g;
      m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    var matches = document.cookie.match(/BrowserCookie\s*=\s*([a-f0-9\-]{36})/),
      user_uuid = null;

    if (matches && matches.length === 2) {
      user_uuid = matches[1];
    }

  
    ga('create', 'UA-39299553-7', {'userId': 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79' });
  

  
    
      ga('set', 'dimension1', 'B2B');
    
  

  ga('set', 'dimension6', user_uuid);

  
    ga('set', 'dimension2', 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79');
    
      ga('set', 'dimension7', '0012M0000229keZQAQ');
    
  

  

  

  //enable enhanced link tracking
  ga('require', 'linkid', 'linkid.js');

  // reading interface will track pageviews itself
  if (document.location.pathname.indexOf("/library/view") !== 0) {
    ga('send', 'pageview');
  }
  </script><script>
    var dataLayer = window.dataLayer || [];

    
      window.medalliaVsgUserIdentifier = 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79';
      dataLayer.push({userIdentifier: 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79'});
      dataLayer.push({loggedIn: 'yes'});

      
        window.medalliaVsgAccountIdentifier = '21bed0a7-6b7b-470c-8fa0-40a52db0b491';
        
        dataLayer.push({orgID: '21bed0a7-6b7b-470c-8fa0-40a52db0b491'});
        

        window.medalliaVsgIsIndividual = false;
        
          
          dataLayer.push({learningAccountType: 'enterprise'});
          
        

        
          dataLayer.push({learningPaidAccount: 'yes'});
        
      
    

    (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-5P4V6Z');
    (function () {
      var VERSION = 'V1.1';
      var AUTHOR = 'Awwad';
      if (!window.GtmHelper)
        window.GtmHelper = function () {
          var instance = this;
          var loc = document.location;
          this.version = VERSION;
          this.author = AUTHOR;
          this.readCookie = function (name) {
            var nameEQ = name + "=";
            var ca = document.cookie.split(';');
            for (var i = 0; i < ca.length; i++) {
              var c = ca[i];
              while (c.charAt(0) == ' ') c = c.substring(1, c.length);
              if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length, c.length);
            }
            return null;
          };
          this.createCookie = function (name, value, days, cookieDomain) {
            var domain = "";
            var expires = "";

            if (days) {
              var date = new Date();
              date.setTime(date.getTime() + Math.ceil(days * 24 * 60 * 60 * 1000));
              var expires = " expires=" + date.toGMTString() + ";";
            }

            if (typeof (cookieDomain) != 'undefined')
              domain = " domain=" + cookieDomain + "; ";

            document.cookie = name + "=" + value + ";" + expires + domain + "path=/";
          };

          this.isDuplicated = function (currentTransactionId) {
            // the previous transaction id:
            var previousTransIdValue = this.readCookie("previousTransId");

            if (currentTransactionId === previousTransIdValue) {
              return true; // Duplication
            } else {
              return false;
            }
          };
        }
    })()
  </script><script defer="" src="./Chapter11_files/vendor.a48a756c5182.js.download"></script><script defer="" src="./Chapter11_files/reader.f2a0c6bd2fee.js.download"></script><script src="./Chapter11_files/f(1).txt"></script><script src="./Chapter11_files/f(2).txt"></script><script src="./Chapter11_files/f(3).txt"></script><script src="./Chapter11_files/f(4).txt"></script><script async="" src="./Chapter11_files/MathJax.js.download"></script><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 2147483020;
}
.annotator-filter {
  z-index: 2147483010;
}</style><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
</style><script async="true" type="text/javascript" src="./Chapter11_files/roundtrip.js.download"></script><style type="text/css" id="kampyleStyle">.noOutline{outline: none !important;}.wcagOutline:focus{outline: 1px dashed #595959 !important;outline-offset: 2px !important;transition: none !important;}</style><script async="true" type="text/javascript" src="./Chapter11_files/roundtrip.js.download"></script><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Math-bold-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Script; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_AMS; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf?V=2.7.1') format('opentype')}
</style></head>


<body class="reading sidenav  scalefonts library nav-collapsed"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

    
  <noscript> 
    <iframe src="//www.googletagmanager.com/ns.html?id=GTM-5P4V6Z"
            height="0" width="0"
            style="display:none;visibility:hidden">
    </iframe>
  </noscript>



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#container" class="skip">Skip to content</a><header class="topbar t-topbar"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://learning.oreilly.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M11.738 14H9.254v-3.676a.617.617 0 0 0-.621-.613H7.39a.617.617 0 0 0-.62.613V14H4.284a.617.617 0 0 1-.622-.613V10.22c0-.327.132-.64.367-.87l3.547-3.493a.627.627 0 0 1 .875 0l3.54 3.499c.234.229.366.54.367.864v3.167a.617.617 0 0 1-.62.613zM7.57 2.181a.625.625 0 0 1 .882 0l5.77 5.692-.93.92-5.28-5.209-5.28 5.208-.932-.919 5.77-5.692z"></path></svg><span>Safari Home</span></a></li><li><a href="https://learning.oreilly.com/resource-centers/" class="t-resource-centers-nav l0 nav-icn"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="16px" height="16px" viewBox="0 0 16 16" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="Topic-Page-Design" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="Heron-Button" transform="translate(-20.000000, -78.000000)" fill="#4A3A30"><g id="Group-9" transform="translate(20.000000, 78.000000)"><rect id="Rectangle" x="9.6" y="0" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="9.6" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="0" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect></g></g></g></svg><span>Resource Centers</span></a></li><li><a href="https://learning.oreilly.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://learning.oreilly.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://learning.oreilly.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://learning.oreilly.com/learning-paths/" class="l1 nav-icn t-learningpaths-nav js-toggle-menu-item"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="32px" height="32px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 52.5 (67469) - http://www.bohemiancoding.com/sketch --><title>Mask</title><desc>Created with Sketch.</desc><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><path d="M0,16.0214227 C0,15.0387209 0.796453294,14.2411658 1.77779753,14.2411658 C2.75914177,14.2411658 3.55559506,15.0387209 3.55559506,16.0214227 C3.55559506,17.0041246 2.75914177,17.8016797 1.77779753,17.8016797 C0.796453294,17.8016797 0,17.0041246 0,16.0214227 Z M9.77788642,5.22914885 C8.9280992,5.72049977 7.84008711,5.42853763 7.34941499,4.57757479 C6.85874287,3.72661195 7.15030167,2.63709467 8.00008889,2.14574375 C8.84987611,1.65439282 9.9378882,1.94635496 10.4285603,2.7973178 C10.9192324,3.64828064 10.6276736,4.73779792 9.77788642,5.22914885 Z M4.57213969,7.35869225 C5.42192691,7.85004318 5.71348571,8.93956046 5.22281359,9.79052329 C4.73214147,10.6414861 3.64412938,10.9334483 2.79434216,10.4420974 C1.94455494,9.95074642 1.65299614,8.86122915 2.14366826,8.01026631 C2.63434038,7.15930347 3.72235247,6.86734132 4.57213969,7.35869225 Z M2.79434216,21.6007481 C3.64412938,21.1093972 4.73214147,21.4013594 5.22281359,22.2523222 C5.71348571,23.103285 5.42192691,24.1928023 4.57213969,24.6841532 C3.72235247,25.1755042 2.63434038,24.883542 2.14366826,24.0325792 C1.65299614,23.1816163 1.94455494,22.0920991 2.79434216,21.6007481 Z M7.34941499,27.4652707 C7.84008711,26.6143079 8.9280992,26.3223457 9.77788642,26.8136966 C10.6276736,27.3050476 10.9192324,28.3945649 10.4285603,29.2455277 C9.9378882,30.0964905 8.84987611,30.3884527 8.00008889,29.8971017 C7.15030167,29.4057508 6.85874287,28.3162335 7.34941499,27.4652707 Z M18.7118524,11.3165596 C21.3074367,12.8173162 22.1963355,16.1392758 20.6976522,18.738451 C19.1989689,21.3358459 15.8815987,22.2259744 13.2860143,20.726998 C10.6922077,19.2262414 9.80330893,15.9042818 11.3002144,13.3051066 C12.7988978,10.7059314 16.116268,9.81580294 18.7118524,11.3165596 Z M26.7821642,27.8093944 L30.1315348,31.1633985 C30.3982044,31.4304371 30.2097579,31.8844026 29.8346426,31.8844026 L21.5945511,31.8844026 C21.1287681,31.8844026 20.751875,31.5069881 20.751875,31.0405608 L20.751875,22.7890697 C20.751875,22.4134355 21.2052134,22.2247282 21.4701052,22.4899865 L24.2843587,25.3081333 C26.8337204,23.0240636 28.4444049,19.7092251 28.4444049,16.0223129 C28.4444049,9.15052091 22.8621207,3.56051397 15.9998222,3.56051397 L15.9998222,0 C24.8230314,0 32,7.18689745 32,16.0223129 C32,20.6919269 29.9750886,24.8790914 26.7821642,27.8093944 Z" id="Mask" fill="#8B889A"></path></g></svg><span>Learning Paths</span></a></li><li class="nav-highlights"><a href="https://learning.oreilly.com/u/d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" fill="#4A3C31"><path d="M13.325 18.071H8.036c0-6.736 4.324-10.925 14.464-12.477V0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35c5.142 0 9.175-3.515 9.175-8.816 0-4.628-2.367-7.293-6.253-8.113zm27.5 0h-5.26c0-6.736 4.295-10.925 14.435-12.477V0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35c5.113 0 9.146-3.515 9.146-8.816 0-4.628-2.338-7.293-6.253-8.113z" fill-rule="evenodd"></path></svg><span>Highlights</span></a></li><li><a href="https://learning.oreilly.com/u/preferences/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l1 no-icon">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://learning.oreilly.com/u/preferences/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l2">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9781492032632/chapter/part01.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"><div data-reactroot="" class="menu-dropdown-wrapper js-menu-dropdown-wrapper align-right"><img class="hidden" src="./Chapter11_files/ajax-transp.gif" alt="loading spinner"><div class="menu-control"><div class="control "><div class="js-playlists-menu"><button class="js-playlist-icon"><svg class="icon-add-to-playlist-sml" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill-rule="nonzero" fill="#000000"><g transform="translate(-1.000000, 0.000000)"><rect x="5" y="0" width="12" height="2"></rect><title>Playlists</title><path d="M4.5,14 C6.43299662,14 8,12.4329966 8,10.5 C8,8.56700338 6.43299662,7 4.5,7 C2.56700338,7 1,8.56700338 1,10.5 C1,12.4329966 2.56700338,14 4.5,14 Z M2.5,10 L4,10 L4,8.5 L5,8.5 L5,10 L6.5,10 L6.5,11 L5,11 L5,12.5 L4,12.5 L4,11 L2.5,11 L2.5,10 Z"></path><circle cx="2" cy="5" r="1"></circle><circle cx="1.94117647" cy="1" r="1"></circle><rect x="5" y="4" width="12" height="2"></rect><rect x="9" y="8" width="8" height="2"></rect><rect x="9" y="12" width="8" height="2"></rect></g></g></g></svg><div class="js-playlist-addto-label">Add&nbsp;To</div></button></div></div></div></div></div></div></li><li class="js-font-control-panel font-control-activator"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html&amp;text=Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%20I.%20The%20Fundamentals%20of%20Machine%20Learning&amp;body=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html%0D%0Afrom%20Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    
    <section role="document">
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">10. Introduction to Artificial Neural Networks with Keras</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch12.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">12. Custom Models and Training with TensorFlow</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content"><div class="annotator-wrapper"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 11. Training Deep Neural Networks"><div class="chapter" id="deep_chapter">
<h1><span class="label">Chapter 11. </span>Training Deep Neural Networks</h1>


<p>In <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#ann_chapter">Chapter&nbsp;10</a> we introduced artificial neural networks and trained our first deep neural networks. But they were shallow nets, with just a few hidden layers. What if you need to tackle a complex problem, such as detecting hundreds of types of objects in high-resolution images? You may need to train a much deeper DNN, perhaps with 10 layers or many more, each containing hundreds of neurons, linked by hundreds of thousands of connections. Training a deep DNN isn’t a walk in the park. Here are some of the problems you could run into:</p>

<ul>
<li>
<p>First, you would be faced with the tricky <em>vanishing gradients</em> problem or the related <em>exploding gradients</em> problem. This is when the gradients grow smaller and smaller, or larger and larger, when flowing backward through the DNN during training. Both of these problems make lower layers very hard to train.</p>
</li>
<li>
<p>Second, you might not have enough training data for such a large network, or it might be too costly to label.</p>
</li>
<li>
<p>Third, training may be extremely slow.</p>
</li>
<li>
<p>Fourth, a model with millions of parameters would severely risk overfitting the training set, especially if there are not enough training instances or if they are too noisy.</p>
</li>
</ul>

<p>In this chapter we will go through each of these problems and present techniques to solve them. We will start by explaining the vanishing gradients problem and exploring some of the most popular solutions to this problem. Next, we will look at transfer learning and unsupervised pretraining, which can help you tackle complex tasks even when you have little labeled data. Then we will discuss various optimizers that can speed up training large models tremendously. Finally, we will go through a few popular regularization techniques for large neural networks.</p>

<p>With these tools, you will be able to train very deep nets. Welcome to Deep Learning!</p>






<section data-type="sect1" data-pdf-bookmark="Vanishing/Exploding Gradients Problems"><div class="sect1" id="idm46263512869544">
<h1>Vanishing/Exploding Gradients Problems</h1>

<p>As we discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#ann_chapter">Chapter&nbsp;10</a>, the backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient along the way. Once the algorithm has computed the gradient of the cost function with regard to each parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step.</p>

<p>Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution, which is why we call this the <em>vanishing gradients</em> problem. In some cases, the opposite can happen: the gradients can grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges. This is the <em>exploding gradients</em> problem, which surfaces in recurrent neural networks (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#rnn_chapter">Chapter&nbsp;15</a>). More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.</p>

<p>This unfortunate behavior was empirically observed long ago, and it was one of the reasons deep neural networks were mostly abandoned in the early 2000s. It wasn’t clear what caused the gradients to be so unstable when training a DNN. But some light was shed in a paper titled <a href="https://homl.info/47">“Understanding the Difficulty of Training Deep Feedforward Neural Networks”</a> by Xavier Glorot and Yoshua Bengio.<sup><a data-type="noteref" id="idm46263512862504-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512862504" class="totri-footnote">1</a></sup> The authors found a few suspects, including the combination of the popular logistic sigmoid activation function and the weight initialization technique that was most popular at the time (i.e., a normal distribution with a mean of 0 and a standard deviation of 1). In short, they showed that with this activation function and this initialization scheme, the variance of the outputs of each layer is much greater than the variance of its inputs. Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers. This saturation is actually made worse by the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent function has a mean of 0 and behaves slightly better than the logistic function in deep networks).</p>

<p>Looking at the logistic activation function (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#sigmoid_saturation_plot">Figure&nbsp;11-1</a>), you can see that when inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close to 0. Thus when backpropagation kicks in, it has virtually no gradient to propagate back through the network, and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers.</p>

<figure class="smallerseventy"><div id="sigmoid_saturation_plot" class="figure">
<img src="./Chapter11_files/mls2_1101.png" alt="mls2 1101" width="1440" height="928" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1101.png">
<h6><span class="label">Figure 11-1. </span>Logistic activation function saturation</h6>
</div></figure>








<section data-type="sect2" data-pdf-bookmark="Glorot and He Initialization"><div class="sect2" id="idm46263512856776">
<h2>Glorot and He Initialization</h2>

<p>In their paper, Glorot and Bengio propose a way to significantly alleviate the unstable gradients problem. They point out that we need the signal to flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradients. We don’t want the signal to die out, nor do we want it to explode and saturate. For the signal to flow properly, the authors argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs,<sup><a data-type="noteref" id="idm46263512854264-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512854264" class="totri-footnote">2</a></sup> and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction (please check out the paper if you are interested in the mathematical details). It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons (these numbers are called the <em>fan-in</em> and <em>fan-out</em> of the layer), but Glorot and Bengio proposed a good compromise that has proven to work very well in practice: the connection weights of each layer must be initialized randomly as described in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#xavier_initialization_equation">Equation 11-1</a>, where <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-124-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;f a n Subscript avg Baseline equals left-parenthesis f a n Subscript in Baseline plus f a n Subscript out Baseline right-parenthesis slash 2&quot;&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mtext&gt;avg&lt;/mtext&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mtext&gt;in&lt;/mtext&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mtext&gt;out&lt;/mtext&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4938" aria-label="f a n Subscript avg Baseline equals left-parenthesis f a n Subscript in Baseline plus f a n Subscript out Baseline right-parenthesis slash 2" style="width: 12.702em; display: inline-block;"><span style="display: inline-block; position: relative; width: 12.342em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.185em, 1012.3em, 2.573em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-4939"><span class="mrow" id="MathJax-Span-4940"><span class="mi" id="MathJax-Span-4941" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mi" id="MathJax-Span-4942" style="font-family: MathJax_Math-italic;">a</span><span class="msub" id="MathJax-Span-4943"><span style="display: inline-block; position: relative; width: 1.751em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4944" style="font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mtext" id="MathJax-Span-4945" style="font-size: 70.7%; font-family: MathJax_Main;">avg</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4946" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mrow" id="MathJax-Span-4947" style="padding-left: 0.26em;"><span class="mo" id="MathJax-Span-4948" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4949" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mi" id="MathJax-Span-4950" style="font-family: MathJax_Math-italic;">a</span><span class="msub" id="MathJax-Span-4951"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4952" style="font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mtext" id="MathJax-Span-4953" style="font-size: 70.7%; font-family: MathJax_Main;">in</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4954" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-4955" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mi" id="MathJax-Span-4956" style="font-family: MathJax_Math-italic;">a</span><span class="msub" id="MathJax-Span-4957"><span style="display: inline-block; position: relative; width: 1.699em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4958" style="font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mtext" id="MathJax-Span-4959" style="font-size: 70.7%; font-family: MathJax_Main;">out</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4960" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4961" style="font-family: MathJax_Main; padding-left: 0.157em;">/</span><span class="mn" id="MathJax-Span-4962" style="font-family: MathJax_Main;">2</span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.368em; border-left: 0px solid; width: 0px; height: 1.168em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="f a n Subscript avg Baseline equals left-parenthesis f a n Subscript in Baseline plus f a n Subscript out Baseline right-parenthesis slash 2"><mrow><mi>f</mi><mi>a</mi><msub><mi>n</mi><mtext>avg</mtext></msub><mo>=</mo><mrow><mo>(</mo><mi>f</mi><mi>a</mi><msub><mi>n</mi><mtext>in</mtext></msub><mo>+</mo><mi>f</mi><mi>a</mi><msub><mi>n</mi><mtext>out</mtext></msub><mo>)</mo></mrow><mo>/</mo><mn>2</mn></mrow></math></span></span><script type="math/mml" id="MathJax-Element-124"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="f a n Subscript avg Baseline equals left-parenthesis f a n Subscript in Baseline plus f a n Subscript out Baseline right-parenthesis slash 2">
  <mrow>
    <mi>f</mi>
    <mi>a</mi>
    <msub><mi>n</mi> <mtext>avg</mtext> </msub>
    <mo>=</mo>
    <mrow>
      <mo>(</mo>
      <mi>f</mi>
      <mi>a</mi>
      <msub><mi>n</mi> <mtext>in</mtext> </msub>
      <mo>+</mo>
      <mi>f</mi>
      <mi>a</mi>
      <msub><mi>n</mi> <mtext>out</mtext> </msub>
      <mo>)</mo>
    </mrow>
    <mo>/</mo>
    <mn>2</mn>
  </mrow>
</math></script>. This initialization strategy is called <em>Xavier initialization</em> (after the author’s first name) or <em>Glorot initialization</em> (after his last name).</p>
<div data-type="equation" id="xavier_initialization_equation">
<h5><span class="label">Equation 11-1. </span>Glorot initialization (when using the logistic activation function)</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-125-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mtable columnalign=&quot;left&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mtext&gt;Normal distribution with mean 0 and variance&amp;#xA0;&lt;/mtext&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x3C3;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msub&gt;&lt;mi&gt;fan&lt;/mi&gt;&lt;mtext&gt;avg&lt;/mtext&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mtext&gt;Or a uniform distribution between&amp;#xA0;&lt;/mtext&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mtext&gt;&amp;#xA0;and&amp;#xA0;&lt;/mtext&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mtext&gt;, with&amp;#xA0;&lt;/mtext&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msqrt&gt;&lt;mfrac&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;msub&gt;&lt;mi&gt;fan&lt;/mi&gt;&lt;mtext&gt;avg&lt;/mtext&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;/msqrt&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4963" style="width: 29.257em; display: inline-block;"><span style="display: inline-block; position: relative; width: 28.383em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(-0.152em, 1028.24em, 3.859em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-4964"><span class="mtable" id="MathJax-Span-4965" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 28.075em; height: 0px;"><span style="position: absolute; clip: rect(2.47em, 1028.08em, 6.481em, -1000.01em); top: -4.728em; left: 0em;"><span style="display: inline-block; position: relative; width: 28.075em; height: 0px;"><span style="position: absolute; clip: rect(2.985em, 1024.64em, 4.784em, -1000.01em); top: -5.242em; left: 0em;"><span class="mtd" id="MathJax-Span-4966"><span class="mrow" id="MathJax-Span-4967"><span class="mtext" id="MathJax-Span-4968" style="font-family: MathJax_Main;">Normal distribution with mean 0 and variance&nbsp;</span><span class="msup" id="MathJax-Span-4969"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.447em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4970" style="font-family: MathJax_Math-italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.62em;"><span class="mn" id="MathJax-Span-4971" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4972" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mfrac" id="MathJax-Span-4973" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 1.905em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.396em, 1000.32em, 4.167em, -1000.01em); top: -4.419em; left: 50%; margin-left: -0.152em;"><span class="mn" id="MathJax-Span-4974" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.345em, 1001.76em, 4.373em, -1000.01em); top: -3.596em; left: 50%; margin-left: -0.871em;"><span class="msub" id="MathJax-Span-4975"><span style="display: inline-block; position: relative; width: 1.802em; height: 0px;"><span style="position: absolute; clip: rect(3.345em, 1000.94em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4976" style="font-size: 70.7%; font-family: MathJax_Main;">fan</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.905em; left: 0.979em;"><span class="mtext" id="MathJax-Span-4977" style="font-size: 50%; font-family: MathJax_Main;">avg</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1001.91em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.905em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.83em, 1028.08em, 4.938em, -1000.01em); top: -3.134em; left: 0em;"><span class="mtd" id="MathJax-Span-4978"><span class="mrow" id="MathJax-Span-4979"><span class="mtext" id="MathJax-Span-4980" style="font-family: MathJax_Main;">Or a uniform distribution between&nbsp;</span><span class="mo" id="MathJax-Span-4981" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mi" id="MathJax-Span-4982" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">r</span><span class="mtext" id="MathJax-Span-4983" style="font-family: MathJax_Main;">&nbsp;and&nbsp;</span><span class="mo" id="MathJax-Span-4984" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-4985" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">r</span><span class="mtext" id="MathJax-Span-4986" style="font-family: MathJax_Main;">, with&nbsp;</span><span class="mi" id="MathJax-Span-4987" style="font-family: MathJax_Math-italic;">r</span><span class="mo" id="MathJax-Span-4988" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="msqrt" id="MathJax-Span-4989" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 3.19em; height: 0px;"><span style="position: absolute; clip: rect(2.985em, 1002.12em, 4.784em, -1000.01em); top: -4.008em; left: 0.979em;"><span class="mrow" id="MathJax-Span-4990"><span class="mfrac" id="MathJax-Span-4991"><span style="display: inline-block; position: relative; width: 1.905em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.396em, 1000.32em, 4.167em, -1000.01em); top: -4.419em; left: 50%; margin-left: -0.152em;"><span class="mn" id="MathJax-Span-4992" style="font-size: 70.7%; font-family: MathJax_Main;">3</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.345em, 1001.76em, 4.373em, -1000.01em); top: -3.596em; left: 50%; margin-left: -0.871em;"><span class="msub" id="MathJax-Span-4993"><span style="display: inline-block; position: relative; width: 1.802em; height: 0px;"><span style="position: absolute; clip: rect(3.345em, 1000.94em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4994" style="font-size: 70.7%; font-family: MathJax_Main;">fan</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.905em; left: 0.979em;"><span class="mtext" id="MathJax-Span-4995" style="font-size: 50%; font-family: MathJax_Main;">avg</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1001.91em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.905em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.602em, 1002.17em, 3.961em, -1000.01em); top: -4.779em; left: 0.979em;"><span style="display: inline-block; position: relative; width: 2.162em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.1em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 1.494em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.414em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.928em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.728em, 1001.04em, 4.836em, -1000.01em); top: -3.905em; left: 0em;"><span style="font-family: MathJax_Size2;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.733em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.692em; border-left: 0px solid; width: 0px; height: 3.921em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>Normal distribution with mean 0 and variance&nbsp;</mtext><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mfrac><mn>1</mn><msub><mi>fan</mi><mtext>avg</mtext></msub></mfrac></mtd></mtr><mtr><mtd><mtext>Or a uniform distribution between&nbsp;</mtext><mo>-</mo><mi>r</mi><mtext>&nbsp;and&nbsp;</mtext><mo>+</mo><mi>r</mi><mtext>, with&nbsp;</mtext><mi>r</mi><mo>=</mo><msqrt><mfrac><mn>3</mn><msub><mi>fan</mi><mtext>avg</mtext></msub></mfrac></msqrt></mtd></mtr></mtable></math></span></span><script type="math/mml" id="MathJax-Element-125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>Normal distribution with mean 0 and variance </mtext><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mfrac><mn>1</mn><msub><mi>fan</mi><mtext>avg</mtext></msub></mfrac></mtd></mtr><mtr><mtd><mtext>Or a uniform distribution between </mtext><mo>-</mo><mi>r</mi><mtext> and </mtext><mo>+</mo><mi>r</mi><mtext>, with </mtext><mi>r</mi><mo>=</mo><msqrt><mfrac><mn>3</mn><msub><mi>fan</mi><mtext>avg</mtext></msub></mfrac></msqrt></mtd></mtr></mtable></math></script>
</div>

<p>If you replace <em>fan</em><sub>avg</sub> with <em>fan</em><sub>in</sub> in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#xavier_initialization_equation">Equation 11-1</a>, you get an initialization strategy that Yann LeCun proposed in the 1990s. He called it <em>LeCun initialization</em>. Genevieve Orr and Klaus-Robert Müller even recommended it in their 1998 book <em>Neural Networks: Tricks of the Trade</em> (Springer). LeCun initialization is equivalent to Glorot initialization when <em>fan</em><sub>in</sub> = <em>fan</em><sub>out</sub>. It took over a decade for researchers to realize how important this trick is. Using Glorot initialization can speed up training considerably, and it is one of the tricks that led to the success of Deep Learning.</p>

<p>Some papers<sup><a data-type="noteref" id="idm46263512823880-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512823880" class="totri-footnote">3</a></sup> have provided similar strategies for different activation functions. These strategies differ only by the scale of the variance and whether they use <em>fan</em><sub>avg</sub> or <em>fan</em><sub>in</sub>, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#initialization_table">Table&nbsp;11-1</a> (for the uniform distribution, just compute <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-126-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;r equals StartRoot 3 sigma squared EndRoot&quot;&gt;&lt;mrow&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x3C3;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4996" aria-label="r equals StartRoot 3 sigma squared EndRoot" style="width: 4.321em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.167em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.082em, 1004.18em, 2.368em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-4997"><span class="mrow" id="MathJax-Span-4998"><span class="mi" id="MathJax-Span-4999" style="font-family: MathJax_Math-italic;">r</span><span class="mo" id="MathJax-Span-5000" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="msqrt" id="MathJax-Span-5001" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 2.419em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1001.55em, 4.167em, -1000.01em); top: -4.008em; left: 0.825em;"><span class="mrow" id="MathJax-Span-5002"><span class="mrow" id="MathJax-Span-5003"><span class="mn" id="MathJax-Span-5004" style="font-family: MathJax_Main;">3</span><span class="msup" id="MathJax-Span-5005"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.447em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5006" style="font-family: MathJax_Math-italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.316em; left: 0.62em;"><span class="mn" id="MathJax-Span-5007" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.602em, 1001.55em, 3.961em, -1000.01em); top: -4.573em; left: 0.825em;"><span style="display: inline-block; position: relative; width: 1.596em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.1em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 0.877em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.414em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.036em, 1000.83em, 4.373em, -1000.01em); top: -4.111em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.156em; border-left: 0px solid; width: 0px; height: 1.115em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="r equals StartRoot 3 sigma squared EndRoot"><mrow><mi>r</mi><mo>=</mo><msqrt><mrow><mn>3</mn><msup><mi>σ</mi><mn>2</mn></msup></mrow></msqrt></mrow></math></span></span><script type="math/mml" id="MathJax-Element-126"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="r equals StartRoot 3 sigma squared EndRoot">
  <mrow>
    <mi>r</mi>
    <mo>=</mo>
    <msqrt>
      <mrow>
        <mn>3</mn>
        <msup><mi>σ</mi> <mn>2</mn> </msup>
      </mrow>
    </msqrt>
  </mrow>
</math></script>). <a href="https://homl.info/48">The initialization strategy</a> for the ReLU activation function (and its variants, including the ELU activation described shortly) is sometimes called <em>He initialization</em> (after the last name of its author). The SELU activation function will be explained later in this chapter. It should be used with LeCun initialization (preferably with a normal distribution, as we will see).</p>
<table id="initialization_table">
<caption><span class="label">Table 11-1. </span>Initialization parameters for each type of activation function</caption>
<thead>
<tr>
<th>Initialization</th>
<th>Activation functions</th>
<th><em>σ</em>² (Normal)</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Glorot</p></td>
<td><p>None, Tanh, Logistic, Softmax</p></td>
<td><p>1 / <em>fan</em><sub>avg</sub></p></td>
</tr>
<tr>
<td><p>He</p></td>
<td><p>ReLU &amp; variants</p></td>
<td><p>2 / <em>fan</em><sub>in</sub></p></td>
</tr>
<tr>
<td><p>LeCun</p></td>
<td><p>SELU</p></td>
<td><p>1 / <em>fan</em><sub>in</sub></p></td>
</tr>
</tbody>
</table>

<p>By default, Keras uses Glorot initialization with a uniform distribution. When creating a layer, you can change this to He initialization by setting <code>kernel_initializer="he_uniform"</code> or <code>kernel_initializer="he_normal"</code> like this:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">,</code> <code class="n">kernel_initializer</code><code class="o">=</code><code class="s2">"he_normal"</code><code class="p">)</code></pre>

<p>If you want He initialization with a uniform distribution but based on <em>fan</em><sub>avg</sub> rather than <em>fan</em><sub>in</sub>, you can use the <code>VarianceScaling</code> initializer like this:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">he_avg_init</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">initializers</code><code class="o">.</code><code class="n">VarianceScaling</code><code class="p">(</code><code class="n">scale</code><code class="o">=</code><code class="mf">2.</code><code class="p">,</code> <code class="n">mode</code><code class="o">=</code><code class="s1">'fan_avg'</code><code class="p">,</code>
                                                 <code class="n">distribution</code><code class="o">=</code><code class="s1">'uniform'</code><code class="p">)</code>
<code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">,</code> <code class="n">kernel_initializer</code><code class="o">=</code><code class="n">he_avg_init</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Nonsaturating Activation Functions"><div class="sect2" id="idm46263512856152">
<h2>Nonsaturating Activation Functions</h2>

<p>One of the insights in the 2010 paper by Glorot and Bengio was that the unstable gradients problems were in part due to a poor choice of activation function. Until then most people had assumed that if Mother Nature had chosen to use roughly sigmoid activation functions in biological neurons, they must be an excellent choice. But it turns out that other activation functions behave much better in deep neural networks, in particular the ReLU activation function, mostly because it does not saturate for positive values (and because it is fast to compute).</p>

<p>Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the <em>dying ReLUs</em>: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting&nbsp;0s, and gradient descent does not affect it anymore because the gradient of the ReLU function is&nbsp;0 when its input is negative.<sup><a data-type="noteref" id="idm46263512720088-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512720088" class="totri-footnote">4</a></sup></p>

<p>To solve this problem, you may want to use a variant of the ReLU function, such as the <em>leaky ReLU</em>. This function is defined as LeakyReLU<sub><em>α</em></sub>(<em>z</em>) = max(<em>αz</em>, <em>z</em>) (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#leaky_relu_plot">Figure&nbsp;11-2</a>). The hyperparameter <em>α</em> defines how much the function “leaks”: it is the slope of the function for <em>z</em> &lt; 0 and is typically set to 0.01. This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up. A <a href="https://homl.info/49">2015 paper</a><sup><a data-type="noteref" id="idm46263512713768-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512713768" class="totri-footnote">5</a></sup> compared several variants of the ReLU activation function, and one of its conclusions was that the leaky variants always outperformed the strict ReLU activation function. In fact, setting <em>α</em> = 0.2 (huge leak) seemed to result in better performance than <em>α</em> = 0.01 (small leak). The paper also evaluated the <em>randomized leaky ReLU</em> (RReLU), where <em>α</em> is picked randomly in a given range during training and is fixed to an average value during testing. RReLU also performed fairly well and seemed to act as a regularizer (reducing the risk of overfitting the training set). Finally, the paper evaluated the <em>parametric leaky ReLU</em> (PReLU), where <em>α</em> is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter). PReLU was reported to strongly outperform ReLU on large image datasets; but on smaller datasets, it runs the risk of overfitting the training set.</p>

<figure class="smallerfiftyfive"><div id="leaky_relu_plot" class="figure">
<img src="./Chapter11_files/mls2_1102.png" alt="mls2 1102" width="1439" height="924" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1102.png">
<h6><span class="label">Figure 11-2. </span>Leaky ReLU: like ReLU, but with a small slope for negative values</h6>
</div></figure>

<p>Last but not least, a <a href="https://homl.info/50">2015 paper</a> by Djork-Arné Clevert et al.<sup><a data-type="noteref" id="idm46263512706248-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512706248" class="totri-footnote">6</a></sup> proposed a new activation function called the <em>exponential linear unit</em> (ELU) that outperformed all the ReLU variants in their experiments: training time was reduced, and the neural network performed better on the test set. <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#elu_activation_plot">Figure&nbsp;11-3</a> graphs the function, and <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#elu_activation_equation">Equation 11-2</a> shows its definition.</p>
<div data-type="equation" id="elu_activation_equation">
<h5><span class="label">Equation 11-2. </span>ELU activation function</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-127-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mo form=&quot;prefix&quot;&gt;ELU&lt;/mo&gt;&lt;mi&gt;&amp;#x3B1;&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced separators=&quot;&quot; open=&quot;{&quot; close=&quot;&quot;&gt;&lt;mtable&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mi&gt;&amp;#x3B1;&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo form=&quot;prefix&quot;&gt;exp&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;if&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo&gt;&amp;lt;&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;if&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo&gt;&amp;#x2265;&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5008" style="width: 17.843em; display: inline-block;"><span style="display: inline-block; position: relative; width: 17.329em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.699em, 1017.13em, 4.476em, -1000.01em); top: -3.339em; left: 0em;"><span class="mrow" id="MathJax-Span-5009"><span class="mrow" id="MathJax-Span-5010"><span class="msub" id="MathJax-Span-5011" style="padding-left: 0.311em; padding-right: 0.311em;"><span style="display: inline-block; position: relative; width: 2.573em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1002.02em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-5012" style="font-family: MathJax_Main;">ELU</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 2.059em;"><span class="mi" id="MathJax-Span-5013" style="font-size: 70.7%; font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-5014"><span class="mo" id="MathJax-Span-5015" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5016" style="font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-5017" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-5018" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mfenced" id="MathJax-Span-5019" style="padding-left: 0.26em;"><span class="mo" id="MathJax-Span-5020" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">{</span></span><span class="mtable" id="MathJax-Span-5021" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 10.543em; height: 0px;"><span style="position: absolute; clip: rect(2.419em, 1006.44em, 4.99em, -1000.01em); top: -4.008em; left: 0em;"><span style="display: inline-block; position: relative; width: 6.532em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1006.44em, 4.424em, -1000.01em); top: -4.676em; left: 0em;"><span class="mtd" id="MathJax-Span-5022"><span class="mrow" id="MathJax-Span-5023"><span class="mrow" id="MathJax-Span-5024"><span class="mi" id="MathJax-Span-5025" style="font-family: MathJax_Math-italic;">α</span><span class="mo" id="MathJax-Span-5026" style="font-family: MathJax_Main;">(</span><span class="mo" id="MathJax-Span-5027" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">exp</span><span class="mo" id="MathJax-Span-5028" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5029" style="font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-5030" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-5031" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mn" id="MathJax-Span-5032" style="font-family: MathJax_Main; padding-left: 0.208em;">1</span><span class="mo" id="MathJax-Span-5033" style="font-family: MathJax_Main;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.167em, -1000.01em); top: -3.237em; left: 0em;"><span class="mtd" id="MathJax-Span-5042"><span class="mrow" id="MathJax-Span-5043"><span class="mi" id="MathJax-Span-5044" style="font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.47em, 1003.2em, 5.093em, -1000.01em); top: -4.008em; left: 7.303em;"><span style="display: inline-block; position: relative; width: 3.242em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1003.2em, 4.219em, -1000.01em); top: -4.676em; left: 0em;"><span class="mtd" id="MathJax-Span-5034"><span class="mrow" id="MathJax-Span-5035"><span class="mrow" id="MathJax-Span-5036"><span class="mtext" id="MathJax-Span-5037" style="font-family: MathJax_Main;">if</span><span class="mspace" id="MathJax-Span-5038" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-5039" style="font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-5040" style="font-family: MathJax_Main; padding-left: 0.26em;">&lt;</span><span class="mn" id="MathJax-Span-5041" style="font-family: MathJax_Main; padding-left: 0.26em;">0</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1003.2em, 4.321em, -1000.01em); top: -3.237em; left: 0em;"><span class="mtd" id="MathJax-Span-5045"><span class="mrow" id="MathJax-Span-5046"><span class="mrow" id="MathJax-Span-5047"><span class="mtext" id="MathJax-Span-5048" style="font-family: MathJax_Main;">if</span><span class="mspace" id="MathJax-Span-5049" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-5050" style="font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-5051" style="font-family: MathJax_Main; padding-left: 0.26em;">≥</span><span class="mn" id="MathJax-Span-5052" style="font-family: MathJax_Main; padding-left: 0.26em;">0</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.345em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.056em; border-left: 0px solid; width: 0px; height: 2.65em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mo form="prefix">ELU</mo><mi>α</mi></msub><mrow><mo>(</mo><mi>z</mi><mo>)</mo></mrow><mo>=</mo><mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mi>α</mi><mo>(</mo><mo form="prefix">exp</mo><mo>(</mo><mi>z</mi><mo>)</mo><mo>-</mo><mn>1</mn><mo>)</mo></mrow></mtd><mtd columnalign="left"><mrow><mtext>if</mtext><mspace width="4.pt"></mspace><mi>z</mi><mo>&lt;</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign="left"><mi>z</mi></mtd><mtd columnalign="left"><mrow><mtext>if</mtext><mspace width="4.pt"></mspace><mi>z</mi><mo>≥</mo><mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math></span></span></div><script type="math/mml" id="MathJax-Element-127"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <msub><mo form="prefix">ELU</mo> <mi>α</mi> </msub>
    <mrow>
      <mo>(</mo>
      <mi>z</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mfenced separators="" open="{" close="">
      <mtable>
        <mtr>
          <mtd columnalign="left">
            <mrow>
              <mi>α</mi>
              <mo>(</mo>
              <mo form="prefix">exp</mo>
              <mo>(</mo>
              <mi>z</mi>
              <mo>)</mo>
              <mo>-</mo>
              <mn>1</mn>
              <mo>)</mo>
            </mrow>
          </mtd>
          <mtd columnalign="left">
            <mrow>
              <mtext>if</mtext>
              <mspace width="4.pt"></mspace>
              <mi>z</mi>
              <mo>&lt;</mo>
              <mn>0</mn>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd columnalign="left">
            <mi>z</mi>
          </mtd>
          <mtd columnalign="left">
            <mrow>
              <mtext>if</mtext>
              <mspace width="4.pt"></mspace>
              <mi>z</mi>
              <mo>≥</mo>
              <mn>0</mn>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mfenced>
  </mrow>
</math></script>
</div>

<figure class="smallerfiftyfive"><div id="elu_activation_plot" class="figure">
<img src="./Chapter11_files/mls2_1103.png" alt="mls2 1103" width="1439" height="918" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1103.png">
<h6><span class="label">Figure 11-3. </span>ELU activation function</h6>
</div></figure>

<p>The ELU activation function looks a lot like the ReLU function, with a few major differences:</p>

<ul>
<li>
<p>First, it takes on negative values when <em>z</em> &lt; 0, which allows the unit to have an average output closer to 0 and helps alleviate the vanishing gradients problem. The hyperparameter <em>α</em> defines the value that the ELU function approaches when <em>z</em> is a large negative number. It is usually set to 1, but you can tweak it like any other hyperparameter.</p>
</li>
<li>
<p>Second, it has a nonzero gradient for <em>z</em> &lt; 0, which avoids the dead neurons problem.</p>
</li>
<li>
<p>Third, if <em>α</em> is equal to 1, then the function is smooth everywhere, including around <em>z</em> = 0, which helps speed up Gradient Descent, since it does not bounce as much left and right of <em>z</em> = 0.</p>
</li>
</ul>

<p>The main drawback of the ELU activation function is that it is slower to compute than the ReLU and its variants (due to the use of the exponential function). But its faster convergence rate during training compensates for that slow computation. Still, at test time an ELU network will be slower than a ReLU network.</p>

<p>Moreover, in a <a href="https://homl.info/selu">2017 paper</a><sup><a data-type="noteref" id="idm46263512642504-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512642504" class="totri-footnote">7</a></sup> by Günter Klambauer et al., called “Self-Normalizing Neural Networks”, the authors introduced the Scaled ELU (SELU) activation function: as its name suggests, it is a scaled variant of the ELU activation function. More importantly, they showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function, then the network will <em>self-normalize</em>: the output of each layer will tend to preserve mean&nbsp;0 and standard deviation&nbsp;1 during training, which solves the vanishing/exploding gradients problem. As a result, the SELU activation function often significantly outperforms other activation functions for such neural nets (especially deep ones). There are, however, a few conditions for self-normalization to happen (see the paper for the mathematical justification):</p>

<ul>
<li>
<p>The input features must be standardized (mean 0 and standard deviation 1).</p>
</li>
<li>
<p>Every hidden layer’s weights must be initialized with LeCun normal initialization. In Keras, this means setting <code>kernel_initializer="lecun_normal"</code>.</p>
</li>
<li>
<p>The network’s architecture must be sequential. Unfortunately, if you try to use SELU in nonsequential architectures, such as recurrent networks (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#rnn_chapter">Chapter&nbsp;15</a>) or networks with <em>skip connections</em> (i.e., connections that skip layers, such as in wide and deep nets), self-normalization will not be guaranteed, so SELU will not necessarily outperform other activation functions.</p>
</li>
<li>
<p>The paper only guarantees self-normalization if all layers are dense, but some researchers have noted that the SELU activation function can improve performance in convolutional neural nets as well (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cnn_chapter">Chapter&nbsp;14</a>).</p>
</li>
</ul>
<div data-type="tip"><h6>Tip</h6>
<p>So, which activation function should you use for the hidden layers of your deep neural networks? Although your mileage will vary, in general SELU &gt; ELU &gt; leaky ReLU (and its variants) &gt; ReLU &gt; tanh &gt; logistic. If the network’s architecture prevents it from self-normalizing, then ELU may perform better than SELU (since SELU is not smooth at <em>z</em>&nbsp;=&nbsp;0). If you care a lot about runtime latency, then you may prefer leaky ReLU. If you don’t want to tweak yet another hyperparameter, you may use the default <em>α</em> values used by Keras (e.g., 0.3 for the leaky ReLU). If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, in particular RReLU if your network is overfitting or PReLU if you have a huge training set. Because ReLU is the most used activation function (by far), many libraries and hardware accelerators provide ReLU-specific optimizations: so if speed is your priority, ReLU might still be the best choice.</p>
</div>

<p>To use the leaky ReLU activation function, create a <code>LeakyReLU</code> layer and add it to your model just after the layer you want to apply it to:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="p">[</code><code class="o">...</code><code class="p">]</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">kernel_initializer</code><code class="o">=</code><code class="s2">"he_normal"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">LeakyReLU</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="mf">0.2</code><code class="p">),</code>
    <code class="p">[</code><code class="o">...</code><code class="p">]</code>
<code class="p">])</code></pre>

<p>For PReLU, replace <code>LeakyRelu(alpha=0.2)</code> with <code>PReLU()</code>. There is currently no official implementation of RReLU in Keras, but you can fairly easily implement your own (to learn how to do that, see the exercises at the end of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch12.html#tensorflow_chapter">Chapter&nbsp;12</a>).</p>

<p>For SELU activation, set <code>activation="selu"</code> and <code>kernel_initializer="lecun_normal"</code> when creating a layer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">layer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">,</code>
                           <code class="n">kernel_initializer</code><code class="o">=</code><code class="s2">"lecun_normal"</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Batch Normalization"><div class="sect2" id="idm46263512521416">
<h2>Batch Normalization</h2>

<p>Although using He initialization along with ELU (or any variant of ReLU) can significantly reduce the vanishing/exploding gradients problems at the beginning of training, it doesn’t guarantee that they won’t come back during training.</p>

<p>In a <a href="https://homl.info/51">2015 paper</a>,<sup><a data-type="noteref" id="idm46263512518408-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512518408" class="totri-footnote">8</a></sup> Sergey Ioffe and Christian Szegedy proposed a technique called <em>Batch Normalization</em> (BN) that addresses the vanishing/exploding gradients problems. The technique consists of adding an operation in the model just before or after the activation function of each hidden layer. This operation simply zero-centers and normalizes each input, then it scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting. In other words, this operation lets the model learn the optimal scale and mean of each of the layer’s inputs. In many cases, if you add a BN layer as the very first layer of your neural network, you do not need to standardize your training set (e.g., using a <code>StandardScaler</code>), the BN layer will do it for you (well, approximately, since it only looks at one batch at a time, and it can also rescale and shift each input feature).</p>

<p>In order to zero-center and normalize the inputs, the algorithm needs to estimate each input’s mean and standard deviation. It does so by evaluating the mean and standard deviation of each input over the current mini-batch (hence the name “Batch Normalization”). The whole operation is summarized step by step in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#batch_normalization_algorithm">Equation 11-3</a>.</p>
<div class="fifty-percent" id="batch_normalization_algorithm" data-type="equation"><h5><span class="label">Equation 11-3. </span>Batch Normalization algorithm</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-128-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mtable displaystyle=&quot;true&quot;&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3BC;&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msub&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;munderover&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/msub&gt;&lt;/munderover&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3C3;&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msub&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;munderover&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/msub&gt;&lt;/munderover&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3BC;&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3BC;&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3C3;&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B5;&lt;/mi&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B3;&lt;/mi&gt;&lt;mo&gt;&amp;#x2297;&lt;/mo&gt;&lt;msup&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B2;&lt;/mi&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5053" style="width: 15.581em; display: inline-block;"><span style="display: inline-block; position: relative; width: 15.118em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(-3.699em, 1014.97em, 7.406em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-5054"><span class="mtable" id="MathJax-Span-5055" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 14.81em; height: 0px;"><span style="position: absolute; clip: rect(5.812em, 1001.76em, 15.787em, -1000.01em); top: -10.692em; left: 0em;"><span style="display: inline-block; position: relative; width: 1.802em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.167em, -1000.01em); top: -8.069em; right: 0em;"><span class="mtd" id="MathJax-Span-5056"><span class="mrow" id="MathJax-Span-5057"><span class="mrow" id="MathJax-Span-5058"><span class="mn" id="MathJax-Span-5059" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-5060" style="font-family: MathJax_Main;">.</span><span class="mspace" id="MathJax-Span-5061" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.167em, -1000.01em); top: -4.882em; right: 0em;"><span class="mtd" id="MathJax-Span-5091"><span class="mrow" id="MathJax-Span-5092"><span class="mrow" id="MathJax-Span-5093"><span class="mn" id="MathJax-Span-5094" style="font-family: MathJax_Main;">2</span><span class="mo" id="MathJax-Span-5095" style="font-family: MathJax_Main;">.</span><span class="mspace" id="MathJax-Span-5096" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.167em, -1000.01em); top: -1.694em; right: 0em;"><span class="mtd" id="MathJax-Span-5138"><span class="mrow" id="MathJax-Span-5139"><span class="mrow" id="MathJax-Span-5140"><span class="mn" id="MathJax-Span-5141" style="font-family: MathJax_Main;">3</span><span class="mo" id="MathJax-Span-5142" style="font-family: MathJax_Main;">.</span><span class="mspace" id="MathJax-Span-5143" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.167em, -1000.01em); top: 0.928em; right: 0em;"><span class="mtd" id="MathJax-Span-5181"><span class="mrow" id="MathJax-Span-5182"><span class="mrow" id="MathJax-Span-5183"><span class="mn" id="MathJax-Span-5184" style="font-family: MathJax_Main;">4</span><span class="mo" id="MathJax-Span-5185" style="font-family: MathJax_Main;">.</span><span class="mspace" id="MathJax-Span-5186" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 10.697em;"></span></span><span style="position: absolute; clip: rect(6.018em, 1012.25em, 17.124em, -1000.01em); top: -11.823em; left: 2.573em;"><span style="display: inline-block; position: relative; width: 12.239em; height: 0px;"><span style="position: absolute; clip: rect(2.265em, 1007.72em, 5.35em, -1000.01em); top: -8.069em; left: 0em;"><span class="mtd" id="MathJax-Span-5062"><span class="mrow" id="MathJax-Span-5063"><span class="mrow" id="MathJax-Span-5064"><span class="msub" id="MathJax-Span-5065"><span style="display: inline-block; position: relative; width: 1.339em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.68em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5066" style="font-family: MathJax_Math-bold-italic;">μ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.722em;"><span class="mi" id="MathJax-Span-5067" style="font-size: 70.7%; font-family: MathJax_Math-italic;">B</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5068" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mstyle" id="MathJax-Span-5069" style="padding-left: 0.26em;"><span class="mrow" id="MathJax-Span-5070"><span class="mfrac" id="MathJax-Span-5071"><span style="display: inline-block; position: relative; width: 1.596em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.19em, 1000.42em, 4.167em, -1000.01em); top: -4.676em; left: 50%; margin-left: -0.255em;"><span class="mn" id="MathJax-Span-5072" style="font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.5em, 4.321em, -1000.01em); top: -3.339em; left: 50%; margin-left: -0.717em;"><span class="msub" id="MathJax-Span-5073"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.88em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5074" style="font-family: MathJax_Math-italic;">m</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.877em;"><span class="mi" id="MathJax-Span-5075" style="font-size: 70.7%; font-family: MathJax_Math-italic;">B</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1001.61em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.596em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span><span class="munderover" id="MathJax-Span-5076" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1001.4em, 4.63em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-5077" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.09em, 4.27em, -1000.01em); top: -2.928em; left: 0.157em;"><span class="mrow" id="MathJax-Span-5078"><span class="mi" id="MathJax-Span-5079" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-5080" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="mn" id="MathJax-Span-5081" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.447em, 1001.04em, 4.27em, -1000.01em); top: -5.19em; left: 0.208em;"><span class="msub" id="MathJax-Span-5082"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.55em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5083" style="font-size: 70.7%; font-family: MathJax_Math-italic;">m</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.905em; left: 0.62em;"><span class="mi" id="MathJax-Span-5084" style="font-size: 50%; font-family: MathJax_Math-italic;">B</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msup" id="MathJax-Span-5085" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5086" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 0.62em;"><span class="mrow" id="MathJax-Span-5087"><span class="mo" id="MathJax-Span-5088" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5089" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-5090" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.265em, 1012.25em, 5.35em, -1000.01em); top: -4.882em; left: 0em;"><span class="mtd" id="MathJax-Span-5097"><span class="mrow" id="MathJax-Span-5098"><span class="mrow" id="MathJax-Span-5099"><span class="msup" id="MathJax-Span-5100"><span style="display: inline-block; position: relative; width: 1.751em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1001.3em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-5101"><span class="msub" id="MathJax-Span-5102"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.68em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5103" style="font-family: MathJax_Math-bold-italic;">σ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.671em;"><span class="mi" id="MathJax-Span-5104" style="font-size: 70.7%; font-family: MathJax_Math-italic;">B</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 1.288em;"><span class="mn" id="MathJax-Span-5105" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5106" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mstyle" id="MathJax-Span-5107" style="padding-left: 0.26em;"><span class="mrow" id="MathJax-Span-5108"><span class="mfrac" id="MathJax-Span-5109"><span style="display: inline-block; position: relative; width: 1.596em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.19em, 1000.42em, 4.167em, -1000.01em); top: -4.676em; left: 50%; margin-left: -0.255em;"><span class="mn" id="MathJax-Span-5110" style="font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.5em, 4.321em, -1000.01em); top: -3.339em; left: 50%; margin-left: -0.717em;"><span class="msub" id="MathJax-Span-5111"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.88em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5112" style="font-family: MathJax_Math-italic;">m</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.877em;"><span class="mi" id="MathJax-Span-5113" style="font-size: 70.7%; font-family: MathJax_Math-italic;">B</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1001.61em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.596em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span><span class="munderover" id="MathJax-Span-5114" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1001.4em, 4.63em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-5115" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.09em, 4.27em, -1000.01em); top: -2.928em; left: 0.157em;"><span class="mrow" id="MathJax-Span-5116"><span class="mi" id="MathJax-Span-5117" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-5118" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="mn" id="MathJax-Span-5119" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.447em, 1001.04em, 4.27em, -1000.01em); top: -5.19em; left: 0.208em;"><span class="msub" id="MathJax-Span-5120"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.55em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5121" style="font-size: 70.7%; font-family: MathJax_Math-italic;">m</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.905em; left: 0.62em;"><span class="mi" id="MathJax-Span-5122" style="font-size: 50%; font-family: MathJax_Math-italic;">B</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msup" id="MathJax-Span-5123" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 5.607em; height: 0px;"><span style="position: absolute; clip: rect(2.728em, 1005em, 4.836em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-5124"><span class="mo" id="MathJax-Span-5125" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">(</span></span><span class="msup" id="MathJax-Span-5126"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5127" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 0.62em;"><span class="mrow" id="MathJax-Span-5128"><span class="mo" id="MathJax-Span-5129" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5130" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-5131" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5132" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="msub" id="MathJax-Span-5133" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.339em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.68em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5134" style="font-family: MathJax_Math-bold-italic;">μ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.722em;"><span class="mi" id="MathJax-Span-5135" style="font-size: 70.7%; font-family: MathJax_Math-italic;">B</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5136" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">)</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.882em; left: 5.195em;"><span class="mn" id="MathJax-Span-5137" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.265em, 1007.57em, 5.35em, -1000.01em); top: -1.694em; left: 0em;"><span class="mtd" id="MathJax-Span-5144"><span class="mrow" id="MathJax-Span-5145"><span class="mrow" id="MathJax-Span-5146"><span class="msup" id="MathJax-Span-5147"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mover" id="MathJax-Span-5148"><span style="display: inline-block; position: relative; width: 0.62em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5149" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1000.58em, 3.602em, -1000.01em); top: -4.008em; left: 0.003em;"><span class="mo" id="MathJax-Span-5150" style=""><span style="font-family: MathJax_Size1;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.522em; left: 0.62em;"><span class="mrow" id="MathJax-Span-5151"><span class="mo" id="MathJax-Span-5152" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5153" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-5154" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5155" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mstyle" id="MathJax-Span-5156" style="padding-left: 0.26em;"><span class="mrow" id="MathJax-Span-5157"><span class="mfrac" id="MathJax-Span-5158"><span style="display: inline-block; position: relative; width: 4.527em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(2.985em, 1003.97em, 4.373em, -1000.01em); top: -4.676em; left: 50%; margin-left: -2.003em;"><span class="mrow" id="MathJax-Span-5159"><span class="msup" id="MathJax-Span-5160"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5161" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.62em;"><span class="mrow" id="MathJax-Span-5162"><span class="mo" id="MathJax-Span-5163" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5164" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-5165" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5166" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="msub" id="MathJax-Span-5167" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.339em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.68em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5168" style="font-family: MathJax_Math-bold-italic;">μ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.722em;"><span class="mi" id="MathJax-Span-5169" style="font-size: 70.7%; font-family: MathJax_Math-italic;">B</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.933em, 1004.43em, 4.476em, -1000.01em); top: -3.134em; left: 50%; margin-left: -2.208em;"><span class="msqrt" id="MathJax-Span-5170"><span style="display: inline-block; position: relative; width: 4.424em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1003.35em, 4.321em, -1000.01em); top: -4.008em; left: 0.979em;"><span class="mrow" id="MathJax-Span-5171"><span class="mrow" id="MathJax-Span-5172"><span class="msup" id="MathJax-Span-5173"><span style="display: inline-block; position: relative; width: 1.751em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1001.3em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-5174"><span class="msub" id="MathJax-Span-5175"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.68em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5176" style="font-family: MathJax_Math-bold-italic;">σ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.671em;"><span class="mi" id="MathJax-Span-5177" style="font-size: 70.7%; font-family: MathJax_Math-italic;">B</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.316em; left: 1.288em;"><span class="mn" id="MathJax-Span-5178" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5179" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-5180" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">ε</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.602em, 1003.4em, 3.961em, -1000.01em); top: -4.625em; left: 0.979em;"><span style="display: inline-block; position: relative; width: 3.396em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.1em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 2.728em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.465em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 1.031em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 1.596em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 2.162em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1001.04em, 4.527em, -1000.01em); top: -4.059em; left: 0em;"><span style="font-family: MathJax_Size1;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1004.54em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 4.527em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.83em, 1007.77em, 4.373em, -1000.01em); top: 0.928em; left: 0em;"><span class="mtd" id="MathJax-Span-5187"><span class="mrow" id="MathJax-Span-5188"><span class="mrow" id="MathJax-Span-5189"><span class="msup" id="MathJax-Span-5190"><span style="display: inline-block; position: relative; width: 1.391em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5191" style="font-family: MathJax_Main-bold;">z</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 0.517em;"><span class="mrow" id="MathJax-Span-5192"><span class="mo" id="MathJax-Span-5193" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5194" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-5195" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5196" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mi" id="MathJax-Span-5197" style="font-family: MathJax_Math-bold-italic; padding-left: 0.26em;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-5198" style="font-family: MathJax_Main; padding-left: 0.208em;">⊗</span><span class="msup" id="MathJax-Span-5199" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mover" id="MathJax-Span-5200"><span style="display: inline-block; position: relative; width: 0.62em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5201" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1000.58em, 3.602em, -1000.01em); top: -4.008em; left: 0.003em;"><span class="mo" id="MathJax-Span-5202" style=""><span style="font-family: MathJax_Size1;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.522em; left: 0.62em;"><span class="mrow" id="MathJax-Span-5203"><span class="mo" id="MathJax-Span-5204" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5205" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-5206" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5207" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-5208" style="font-family: MathJax_Math-bold-italic; padding-left: 0.208em;">β</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 11.828em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -5.346em; border-left: 0px solid; width: 0px; height: 11.229em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn><mo>.</mo><mspace width="1.em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><msub><mi mathvariant="bold">μ</mi><mi>B</mi></msub><mo>=</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn><msub><mi>m</mi><mi>B</mi></msub></mfrac></mstyle><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>m</mi><mi>B</mi></msub></munderover><msup><mi mathvariant="bold">x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr><mtr><mtd columnalign="right"><mrow><mn>2</mn><mo>.</mo><mspace width="1.em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><msup><mrow><msub><mi mathvariant="bold">σ</mi><mi>B</mi></msub></mrow><mn>2</mn></msup><mo>=</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mn>1</mn><msub><mi>m</mi><mi>B</mi></msub></mfrac></mstyle><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>m</mi><mi>B</mi></msub></munderover><msup><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>-</mo><msub><mi mathvariant="bold">μ</mi><mi>B</mi></msub><mo>)</mo></mrow><mn>2</mn></msup></mrow></mtd></mtr><mtr><mtd columnalign="right"><mrow><mn>3</mn><mo>.</mo><mspace width="1.em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><msup><mover accent="true"><mi mathvariant="bold">x</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>=</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><msup><mi mathvariant="bold">x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>-</mo><msub><mi mathvariant="bold">μ</mi><mi>B</mi></msub></mrow><msqrt><mrow><msup><mrow><msub><mi mathvariant="bold">σ</mi><mi>B</mi></msub></mrow><mn>2</mn></msup><mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac></mstyle></mrow></mtd></mtr><mtr><mtd columnalign="right"><mrow><mn>4</mn><mo>.</mo><mspace width="1.em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><msup><mi mathvariant="bold">z</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>=</mo><mi mathvariant="bold">γ</mi><mo>⊗</mo><msup><mover accent="true"><mi mathvariant="bold">x</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>+</mo><mi mathvariant="bold">β</mi></mrow></mtd></mtr></mtable></math></span></span></div><script type="math/mml" id="MathJax-Element-128"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>1</mn>
          <mo>.</mo>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <msub><mi mathvariant="bold">μ</mi> <mi>B</mi> </msub>
          <mo>=</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mn>1</mn> <msub><mi>m</mi> <mi>B</mi> </msub></mfrac>
          </mstyle>
          <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <msub><mi>m</mi> <mi>B</mi> </msub> </munderover>
          <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>2</mn>
          <mo>.</mo>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <msup><mrow><msub><mi mathvariant="bold">σ</mi> <mi>B</mi> </msub></mrow> <mn>2</mn> </msup>
          <mo>=</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mn>1</mn> <msub><mi>m</mi> <mi>B</mi> </msub></mfrac>
          </mstyle>
          <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <msub><mi>m</mi> <mi>B</mi> </msub> </munderover>
          <msup><mrow><mo>(</mo><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>-</mo><msub><mi mathvariant="bold">μ</mi> <mi>B</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>3</mn>
          <mo>.</mo>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <msup><mover accent="true"><mi mathvariant="bold">x</mi> <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <mo>=</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mrow><msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>-</mo><msub><mi mathvariant="bold">μ</mi> <mi>B</mi> </msub></mrow> <msqrt><mrow><msup><mrow><msub><mi mathvariant="bold">σ</mi> <mi>B</mi> </msub></mrow> <mn>2</mn> </msup><mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac>
          </mstyle>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>4</mn>
          <mo>.</mo>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <msup><mi mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <mo>=</mo>
          <mi mathvariant="bold">γ</mi>
          <mo>⊗</mo>
          <msup><mover accent="true"><mi mathvariant="bold">x</mi> <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <mo>+</mo>
          <mi mathvariant="bold">β</mi>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></script></div>

<p>In this algorithm:</p>

<ul>
<li>
<p><strong>μ</strong><sub><em>B</em></sub> is the vector of input means, evaluated over the whole mini-batch <em>B</em> (it contains one mean per input).</p>
</li>
<li>
<p><strong>σ</strong><sub><em>B</em></sub> is the vector of input standard deviations, also evaluated over the whole mini-batch (it contains one standard deviation per input).</p>
</li>
<li>
<p><em>m</em><sub><em>B</em></sub> is the number of instances in the mini-batch.</p>
</li>
<li>
<p><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-129-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5209" style="width: 0.671em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.62em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.185em, 1000.58em, 2.265em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-5210"><span class="mover" id="MathJax-Span-5211"><span style="display: inline-block; position: relative; width: 0.62em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5212" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1000.58em, 3.602em, -1000.01em); top: -4.008em; left: 0.003em;"><span class="mo" id="MathJax-Span-5213" style=""><span style="font-family: MathJax_Size1;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.05em; border-left: 0px solid; width: 0px; height: 0.903em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold">x</mi><mo>^</mo></mover></math></span></span><script type="math/mml" id="MathJax-Element-129"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <mover accent="true"><mi mathvariant="bold">x</mi> <mo>^</mo></mover>
</math></script><sup><em>(i)</em></sup> is the vector of zero-centered and normalized inputs for instance <em>i</em>.</p>
</li>
<li>
<p><strong>γ</strong> is the output scale parameter vector for the layer (it contains one scale parameter per input).</p>
</li>
<li>
<p>⊗ represents element-wise multiplication (each input is multiplied by its corresponding output scale parameter).</p>
</li>
<li>
<p><strong>β</strong> is the output shift (offset) parameter vector for the layer (it contains one offset parameter per input). Each input is offset by its corresponding shift parameter.</p>
</li>
<li>
<p><em>ε</em> is a tiny number that avoids division by zero (typically 10<sup>–5</sup>). This is called a <em>smoothing term</em>.</p>
</li>
<li>
<p><strong>z</strong><sup><em>(i)</em></sup> is the output of the BN operation. It is a rescaled and shifted version of the inputs.</p>
</li>
</ul>

<p>So during training, BN standardizes its inputs then rescales and offsets them. Good! What about at test time? Well, it is not that simple. Indeed, we may need to make predictions for individual instances rather than for batches of instances: in this case, we will have no way to compute each input’s mean and standard deviation. Moreover, even if we do have a batch of instances, it may be too small, or the instances may not be independent and identically distributed (IID), so computing statistics over the batch instances would be unreliable. One solution could be to wait until the end of training, then run the whole training set through the neural network, and compute the mean and standard deviation of each input of the BN layer. These “final” input means and standard deviations could then be used instead of the batch input means and standard deviations when making predictions. However, most implementations of Batch Normalization estimate these final statistics during training by using a moving average of the layer’s input means and standard deviations. This is what Keras does automatically when you use the <code>BatchNormalization</code> layer. To sum up, four parameter vectors are learned in each batch-normalized layer: <strong>γ</strong> (the output scale vector) and <strong>β</strong> (the output offset vector) are learned through regular backpropagation; <strong>μ</strong> (the final input mean vector) and <strong>σ</strong> (the final input standard deviation vector) are estimated using an exponential moving average. Note that <strong>μ</strong> and <strong>σ</strong> are estimated during training, but they are not used at all during training, only after training (to replace the batch input means and standard deviations in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#batch_normalization_algorithm">Equation 11-3</a>).</p>

<p>Ioffe and Szegedy demonstrated that Batch Normalization considerably improved all the deep neural networks they experimented with, leading to a huge improvement in the ImageNet classification task (ImageNet is a large database of images classified into many classes and commonly used to evaluate computer vision systems). The vanishing gradients problem was strongly reduced, to the point that they could use saturating activation functions such as the tanh and even the logistic activation function. The networks were also much less sensitive to the weight initialization. The authors were able to use much larger learning rates, significantly speeding up the learning process. Specifically, they note that:</p>
<blockquote>
<p>Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. […] Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.</p></blockquote>

<p>Finally, like a gift that keeps on giving, Batch Normalization acts like a regularizer, reducing the need for other regularization techniques (such as dropout, described later in this chapter).</p>

<p>Batch Normalization does, however, add some complexity to the model (although it can remove the need for normalizing the input data, as we discussed earlier). Moreover, there is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer. It’s possible to get rid of the BN layer after training by updating the previous layer’s weights and biases so that it directly produces outputs of the appropriate scale and offset. In other words, it is possible to fuse the BN layer with the previous layer. For example, if the previous layer computes <strong>XW</strong> + <strong>b</strong>, then the BN layer will compute <strong>γ</strong>⊗(<strong>XW</strong> + <strong>b</strong> - <strong>μ</strong>)/<strong>σ</strong> + <strong>β</strong> (ignoring the smoothing term <em>ε</em> in the denominator). If we define <strong>W</strong>′ = <strong>γ</strong>⊗<strong>W</strong>/<strong>σ</strong> and <strong>b</strong>′ = <strong>γ</strong>⊗(<strong>b</strong> - <strong>μ</strong>)/<strong>σ</strong> + <strong>β</strong>, the equation simplifies to <strong>XW</strong>′ + <strong>b</strong>′. So if we replace the previous layer’s weights and biases (<strong>W</strong> and <strong>b</strong>) with the updated weights and biases (<strong>W</strong>′ and <strong>b</strong>′), we can get rid of the BN layer (TFLite’s optimizer does this automatically; see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#deployment_chapter">Chapter&nbsp;19</a>).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You may find that training is rather slow, because each epoch takes much more time when you use Batch Normalization. This is usually counterbalanced by the fact that convergence is much faster with BN, so it will take fewer epochs to reach the same performance. All in all, <em>wall time</em> will usually be shorter (this is the time measured by the clock on your wall).</p>
</div>










<section data-type="sect3" data-pdf-bookmark="Implementing Batch Normalization with Keras"><div class="sect3" id="idm46263512408232">
<h3>Implementing Batch Normalization with Keras</h3>

<p>As with most things with Keras, implementing Batch Normalization is simple and intuitive. Just add a <code>BatchNormalization</code> layer before or after each hidden layer’s activation function, and optionally add a BN layer as well as the first layer in your model. For example, this model applies BN after every hidden layer and as the first layer in the model (after flattening the input images):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(</code><code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">BatchNormalization</code><code class="p">(),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">300</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"elu"</code><code class="p">,</code> <code class="n">kernel_initializer</code><code class="o">=</code><code class="s2">"he_normal"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">BatchNormalization</code><code class="p">(),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"elu"</code><code class="p">,</code> <code class="n">kernel_initializer</code><code class="o">=</code><code class="s2">"he_normal"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">BatchNormalization</code><code class="p">(),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"softmax"</code><code class="p">)</code>
<code class="p">])</code></pre>

<p>That’s all! In this tiny example with just two hidden layers, it’s unlikely that Batch Normalization will have a very positive impact; but for deeper networks it can make a tremendous difference.</p>

<p>Let’s display the model summary:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">model</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code>
<code class="go">Model: "sequential_3"</code>
<code class="go">_________________________________________________________________</code>
<code class="go">Layer (type)                 Output Shape              Param #</code>
<code class="go">=================================================================</code>
<code class="go">flatten_3 (Flatten)          (None, 784)               0</code>
<code class="go">_________________________________________________________________</code>
<code class="go">batch_normalization_v2 (Batc (None, 784)               3136</code>
<code class="go">_________________________________________________________________</code>
<code class="go">dense_50 (Dense)             (None, 300)               235500</code>
<code class="go">_________________________________________________________________</code>
<code class="go">batch_normalization_v2_1 (Ba (None, 300)               1200</code>
<code class="go">_________________________________________________________________</code>
<code class="go">dense_51 (Dense)             (None, 100)               30100</code>
<code class="go">_________________________________________________________________</code>
<code class="go">batch_normalization_v2_2 (Ba (None, 100)               400</code>
<code class="go">_________________________________________________________________</code>
<code class="go">dense_52 (Dense)             (None, 10)                1010</code>
<code class="go">=================================================================</code>
<code class="go">Total params: 271,346</code>
<code class="go">Trainable params: 268,978</code>
<code class="go">Non-trainable params: 2,368</code></pre>

<p>As you can see each BN layer adds four parameters per input: <strong>γ</strong>, <strong>β</strong>, <strong>μ</strong>, and <strong>σ</strong> (for example, the first BN layer adds 3,136 parameters, which is 4 × 784). The last two parameters, <strong>μ</strong> and <strong>σ</strong>, are the moving averages; they are not affected by backpropagation, so Keras calls them “Non-trainable”<sup><a data-type="noteref" id="idm46263512192504-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512192504" class="totri-footnote">9</a></sup> (if you count the total number of BN parameters, 3,136 + 1,200 + 400, and divide by 2, you get 2,368, which is the total number of non-trainable params in this model).</p>

<p>Let’s look at the parameters of the first BN layer. Two are trainable (by backprop), and two are not:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="p">[(</code><code class="n">var</code><code class="o">.</code><code class="n">name</code><code class="p">,</code> <code class="n">var</code><code class="o">.</code><code class="n">trainable</code><code class="p">)</code> <code class="k">for</code> <code class="n">var</code> <code class="ow">in</code> <code class="n">model</code><code class="o">.</code><code class="n">layers</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">variables</code><code class="p">]</code>
<code class="go">[('batch_normalization_v2/gamma:0', True),</code>
<code class="go"> ('batch_normalization_v2/beta:0', True),</code>
<code class="go"> ('batch_normalization_v2/moving_mean:0', False),</code>
<code class="go"> ('batch_normalization_v2/moving_variance:0', False)]</code></pre>

<p>Now when you create a BN layer in Keras, it also creates two operations that will be called by Keras at each iteration during training. These operations will update the moving averages. Since we are using the TensorFlow backend, these operations are TensorFlow operations (we will discuss TF operations in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch12.html#tensorflow_chapter">Chapter&nbsp;12</a>):</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">model</code><code class="o">.</code><code class="n">layers</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">updates</code>
<code class="go">[&lt;tf.Operation 'cond_2/Identity' type=Identity&gt;,</code>
<code class="go"> &lt;tf.Operation 'cond_3/Identity' type=Identity&gt;]</code></pre>

<p>The authors of the BN paper argued in favor of adding the BN layers before the activation functions, rather than after (as we just did). There is some debate about this, as it seems to depend on the task. So that’s one more thing you can experiment with to see which option works best on your dataset. To add the BN layers before the activation functions, we must remove the activation function from the hidden layers and add them as separate layers after the BN layers. Moreover, since a Batch Normalization layer includes one offset parameter per input, you can remove the bias term from the previous layer (just pass <code>use_bias=False</code> when creating it):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(</code><code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">BatchNormalization</code><code class="p">(),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">300</code><code class="p">,</code> <code class="n">kernel_initializer</code><code class="o">=</code><code class="s2">"he_normal"</code><code class="p">,</code> <code class="n">use_bias</code><code class="o">=</code><code class="bp">False</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">BatchNormalization</code><code class="p">(),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Activation</code><code class="p">(</code><code class="s2">"elu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">kernel_initializer</code><code class="o">=</code><code class="s2">"he_normal"</code><code class="p">,</code> <code class="n">use_bias</code><code class="o">=</code><code class="bp">False</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">BatchNormalization</code><code class="p">(),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Activation</code><code class="p">(</code><code class="s2">"elu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"softmax"</code><code class="p">)</code>
<code class="p">])</code></pre>

<p>The <code>BatchNormalization</code> class has quite a few hyperparameters you can tweak. The defaults will usually be fine, but you may occasionally need to tweak the <code>momentum</code>. This hyperparameter is used by the BatchNormalization layer when it updates the exponential moving averages: given a new value <strong>v</strong> (i.e., a new vector of input means or standard deviations computed over the current batch), the layer updates the running average <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-130-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;ModifyingAbove bold v With caret&quot;&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;&amp;#x1D42F;&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5214" aria-label="ModifyingAbove bold v With caret" style="width: 0.671em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.62em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.185em, 1000.58em, 2.265em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-5215"><span class="mover" id="MathJax-Span-5216"><span style="display: inline-block; position: relative; width: 0.62em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5217" style="font-family: MathJax_Main-bold;">v</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1000.58em, 3.602em, -1000.01em); top: -4.008em; left: 0.003em;"><span class="mo" id="MathJax-Span-5218" style=""><span style="font-family: MathJax_Size1;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.05em; border-left: 0px solid; width: 0px; height: 0.903em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="ModifyingAbove bold v With caret"><mover accent="true"><mi>𝐯</mi><mo>^</mo></mover></math></span></span><script type="math/mml" id="MathJax-Element-130"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="ModifyingAbove bold v With caret">
  <mover accent="true"><mi>𝐯</mi> <mo>^</mo></mover>
</math></script> using the following equation:</p>
<div data-type="equation">
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-131-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;&amp;#xD7;&lt;/mo&gt;&lt;mtext&gt;momentum&lt;/mtext&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;mo&gt;&amp;#xD7;&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mtext&gt;momentum&lt;/mtext&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5219" style="width: 19.9em; display: inline-block;"><span style="display: inline-block; position: relative; width: 19.283em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.237em, 1019.19em, 2.573em, -1000.01em); top: -2.157em; left: 0em;"><span class="mrow" id="MathJax-Span-5220"><span class="mrow" id="MathJax-Span-5221"><span class="mover" id="MathJax-Span-5222"><span style="display: inline-block; position: relative; width: 0.62em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5223" style="font-family: MathJax_Main-bold;">v</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1000.58em, 3.602em, -1000.01em); top: -4.008em; left: 0.003em;"><span class="mo" id="MathJax-Span-5224" style=""><span style="font-family: MathJax_Size1;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5225" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="mover" id="MathJax-Span-5226" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 0.62em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5227" style="font-family: MathJax_Main-bold;">v</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1000.58em, 3.602em, -1000.01em); top: -4.008em; left: 0.003em;"><span class="mo" id="MathJax-Span-5228" style=""><span style="font-family: MathJax_Size1;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5229" style="font-family: MathJax_Main; padding-left: 0.208em;">×</span><span class="mtext" id="MathJax-Span-5230" style="font-family: MathJax_Main; padding-left: 0.208em;">momentum</span><span class="mo" id="MathJax-Span-5231" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-5232" style="font-family: MathJax_Main-bold; padding-left: 0.208em;">v</span><span class="mo" id="MathJax-Span-5233" style="font-family: MathJax_Main; padding-left: 0.208em;">×</span><span class="mrow" id="MathJax-Span-5234" style="padding-left: 0.208em;"><span class="mo" id="MathJax-Span-5235" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-5236" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-5237" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mtext" id="MathJax-Span-5238" style="font-family: MathJax_Main; padding-left: 0.208em;">momentum</span><span class="mo" id="MathJax-Span-5239" style="font-family: MathJax_Main;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.162em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.315em; border-left: 0px solid; width: 0px; height: 1.168em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mover accent="true"><mi mathvariant="bold">v</mi><mo>^</mo></mover><mo>←</mo><mover accent="true"><mi mathvariant="bold">v</mi><mo>^</mo></mover><mo>×</mo><mtext>momentum</mtext><mo>+</mo><mi mathvariant="bold">v</mi><mo>×</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mtext>momentum</mtext><mo>)</mo></mrow></mrow></math></span></span></div><script type="math/mml" id="MathJax-Element-131"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <mover accent="true"><mi mathvariant="bold">v</mi> <mo>^</mo></mover>
    <mo>←</mo>
    <mover accent="true"><mi mathvariant="bold">v</mi> <mo>^</mo></mover>
    <mo>×</mo>
    <mtext>momentum</mtext>
    <mo>+</mo>
    <mi mathvariant="bold">v</mi>
    <mo>×</mo>
    <mrow>
      <mo>(</mo>
      <mn>1</mn>
      <mo>-</mo>
      <mtext>momentum</mtext>
      <mo>)</mo>
    </mrow>
  </mrow>
</math></script>
</div>

<p>A good momentum value is typically close to 1; for example, 0.9, 0.99, or 0.999 (you want more 9s for larger datasets and smaller mini-batches).</p>

<p>Another important hyperparameter is <code>axis</code>: it determines which axis should be normalized. It defaults to –1, meaning that by default it will normalize the last axis (using the means and standard deviations computed across the <em>other</em> axes). For example, when the input batch is 2D (i.e., the batch shape is [batch size, features]), this means that each input feature will be normalized based on the mean and standard deviation computed across all the instances in the batch. For example, the first BN layer in the previous code example will independently normalize (and rescale and shift) each of the 784 input features. If we move the first BN layer before the <code>Flatten</code> layer, then the input batches will be 3D, with shape [batch size, height, width], therefore the BN layer will compute 28 means and 28 standard deviations (one per column of pixels, computed across all instances in the batch and across all rows in the column), and it will normalize all pixels in a given column using the same mean and standard deviation. There will also be just 28 scale parameters and 28 shift parameters. If instead you still want to treat each of the 784 pixels independently, then you should set <code>axis=[1, 2]</code>.</p>

<p>Notice that the BN layer does not perform the same computation during training and after training: it uses batch statistics during training and the “final” statistics after training (i.e., the final value of the moving averages). Let’s take a peek at the source code of this class to see how this is handled:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">BatchNormalization</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Layer</code><code class="p">):</code>
    <code class="p">[</code><code class="o">...</code><code class="p">]</code>
    <code class="k">def</code> <code class="nf">call</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">,</code> <code class="n">training</code><code class="o">=</code><code class="bp">None</code><code class="p">):</code>
        <code class="p">[</code><code class="o">...</code><code class="p">]</code></pre>

<p>The <code>call()</code> method is the one that performs the computations; and as you can see, it has an extra <code>training</code> argument, which is set to <code>None</code> by default, but the <code>fit()</code> method sets to it to 1 during training. If you ever need to write a custom layer, and it must behave differently during training and testing, add a <code>training</code> argument to the <code>call()</code> method and use this argument in the method to decide what to compute<sup><a data-type="noteref" id="idm46263511970696-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511970696">10</a></sup> (we will discuss custom layers in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch12.html#tensorflow_chapter">Chapter&nbsp;12</a>).</p>

<p>Batch Normalization has become one of the most used layers in deep neural networks, to the point that it is often omitted in the diagrams, as it is assumed that BN is added after every layer. A recent <a href="https://homl.info/fixup">paper</a><sup><a data-type="noteref" id="idm46263511967752-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511967752">11</a></sup> by Hongyi Zhang et al. may change this assumption: the authors show that by using a novel fixed-update (fixup) weight initialization technique, they manage to train a very deep neural network (10,000 layers!) without BN, achieving state-of-the-art performance on complex image classification tasks. But as this is bleeding-edge research, you may want to wait for additional research to confirm this finding before you drop Batch Norm.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Gradient Clipping"><div class="sect2" id="idm46263512722696">
<h2>Gradient Clipping</h2>

<p>Another popular technique to lessen the exploding gradients problem is to clip the gradients during backpropagation so that they never exceed some threshold. This is called <a href="https://homl.info/52"><em>Gradient Clipping</em></a>.<sup><a data-type="noteref" id="idm46263511964024-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511964024">12</a></sup> This technique is most often used in recurrent neural networks, as Batch Normalization is tricky to use in RNNs, as we will see in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#rnn_chapter">Chapter&nbsp;15</a>. For other types of networks, BN is usually sufficient.</p>

<p>In Keras, implementing Gradient Clipping is just a matter of setting the <code>clipvalue</code> or <code>clipnorm</code> argument when creating an optimizer, like this:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">clipvalue</code><code class="o">=</code><code class="mf">1.0</code><code class="p">)</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"mse"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="n">optimizer</code><code class="p">)</code></pre>

<p>This optimizer will clip every component of the gradient vector to a value between –1.0 and 1.0. This means that all the partial derivatives of the loss (with regard to each and every trainable parameter) will be clipped between –1.0 and 1.0. The threshold is a hyperparameter you can tune. Note that it may change the orientation of the gradient vector. For instance, if the original gradient vector is [0.9, 100.0], it points mostly in the direction of the second axis; but once you clip it by value, you get [0.9, 1.0], which points roughly in the diagonal between the two axes. In practice, this approach works well. If you want to ensure that Gradient Clipping does not change the direction of the gradient vector, you should clip by norm by setting <code>clipnorm</code> instead of <code>clipvalue</code>. This will clip the whole gradient if its ℓ<sub>2</sub> norm is greater than the threshold you picked. For example, if you set <code>clipnorm=1.0</code>, then the vector [0.9, 100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation but almost eliminating the first component. If you observe that the gradients explode during training (you can track the size of the gradients using TensorBoard), you may want to try both clipping by value and clipping by norm, with different thresholds, and see which option performs best on the validation set.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Reusing Pretrained Layers"><div class="sect1" id="idm46263512868920">
<h1>Reusing Pretrained Layers</h1>

<p>It is generally not a good idea to train a very large DNN from scratch: instead, you should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle (we will discuss how to find them in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cnn_chapter">Chapter&nbsp;14</a>), then reuse the lower layers of this network. This technique is called <em>transfer learning</em>. It will not only speed up training considerably, but also require significantly less training data.</p>

<p>Suppose you have access to a DNN that was trained to classify pictures into 100 different categories, including animals, plants, vehicles, and everyday objects. You now want to train a DNN to classify specific types of vehicles. These tasks are very similar, even partly overlapping, so you should try to reuse parts of the first network (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#reuse_pretrained_diagram">Figure&nbsp;11-4</a>).</p>

<figure class="smallersixtyfive"><div id="reuse_pretrained_diagram" class="figure">
<img src="./Chapter11_files/mls2_1104.png" alt="mls2 1104" width="1440" height="1113" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1104.png">
<h6><span class="label">Figure 11-4. </span>Reusing pretrained layers</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>If the input pictures of your new task don’t have the same size as the ones used in the original task, you will usually have to add a preprocessing step to resize them to the size expected by the original model. More generally, transfer learning will work best when the inputs have similar low-level features.</p>
</div>

<p>The output layer of the original model should usually be replaced because it is most likely not useful at all for the new task, and it may not even have the right number of outputs for the new task.</p>

<p>Similarly, the upper hidden layers of the original model are less likely to be as useful as the lower layers, since the high-level features that are most useful for the new task may differ significantly from the ones that were most useful for the original task. You want to find the right number of layers to reuse.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The more similar the tasks are, the more layers you want to reuse (starting with the lower layers). For very similar tasks, try keeping all the hidden layers and just replace the output layer.</p>
</div>

<p>Try freezing all the reused layers first (i.e., make their weights non-trainable so that gradient descent won’t modify them), then train your model and see how it performs. Then try unfreezing one or two of the top hidden layers to let backpropagation tweak them and see if performance improves. The more training data you have, the more layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze reused layers: this will avoid wrecking their fine-tuned weights.</p>

<p>If you still cannot get good performance, and you have little training data, try dropping the top hidden layer(s) and freezing all the remaining hidden layers again. You can iterate until you find the right number of layers to reuse. If you have plenty of training data, you may try replacing the top hidden layers instead of dropping them, and even adding more hidden layers.</p>








<section data-type="sect2" data-pdf-bookmark="Transfer Learning With Keras"><div class="sect2" id="idm46263511859800">
<h2>Transfer Learning With Keras</h2>

<p>Let’s look at an example. Suppose the Fashion MNIST dataset only contained eight classes, for example, all classes except for sandals and shirts. Someone built and trained a Keras model on that set and got reasonably good performance (&gt;90% accuracy). Let’s call this model A. You now want to tackle a different task: you have images of sandals and shirts, and you want to train a binary classifier (positive=shirts, negative=sandals). Your dataset is quite small; you only have 200 labeled images. When you train a new model for this task (let’s call it model B) with the same architecture as model A, it performs reasonably well (97.2% accuracy). But since it’s a much easier task (there are just two classes), you were hoping for more. While drinking your morning coffee, you realize that your task is quite similar to task A, so perhaps transfer learning can help? Let’s find out!</p>

<p>First, you need to load model A and create a new model based on the model A’s layers. Let’s reuse all layers except for the output layer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model_A</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">load_model</code><code class="p">(</code><code class="s2">"my_model_A.h5"</code><code class="p">)</code>
<code class="n">model_B_on_A</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code><code class="n">model_A</code><code class="o">.</code><code class="n">layers</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">])</code>
<code class="n">model_B_on_A</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">))</code></pre>

<p>Note that <code>model_A</code> and <code>model_B_on_A</code> now share some layers. When you train <code>model_B_on_A</code>, it will also affect <code>model_A</code>. If you want to avoid that, you need to clone <code>model_A</code> before you reuse its layers. To do this, you must clone model A’s architecture, then copy its weights (since <code>clone_model()</code> does not clone the weights):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model_A_clone</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">clone_model</code><code class="p">(</code><code class="n">model_A</code><code class="p">)</code>
<code class="n">model_A_clone</code><code class="o">.</code><code class="n">set_weights</code><code class="p">(</code><code class="n">model_A</code><code class="o">.</code><code class="n">get_weights</code><code class="p">())</code></pre>

<p>Now we could train <code>model_B_on_A</code> for task B, but since the new output layer was initialized randomly, it will make large errors, at least during the first few epochs, so there will be large error gradients that may wreck the reused weights. To avoid this, one approach is to freeze the reused layers during the first few epochs, giving the new layer some time to learn reasonable weights. To do this, set every layer’s <code>trainable</code> attribute to <code>False</code> and compile the model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">layer</code> <code class="ow">in</code> <code class="n">model_B_on_A</code><code class="o">.</code><code class="n">layers</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">]:</code>
    <code class="n">layer</code><code class="o">.</code><code class="n">trainable</code> <code class="o">=</code> <code class="bp">False</code>

<code class="n">model_B_on_A</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"binary_crossentropy"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s2">"sgd"</code><code class="p">,</code>
                     <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s2">"accuracy"</code><code class="p">])</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You must always compile your model after you freeze or unfreeze layers.</p>
</div>

<p>Next, we can train the model for a few epochs, then unfreeze the reused layers (which requires compiling the model again) and continue training to fine-tune the reused layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce the learning rate, once again to avoid damaging the reused weights:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">history</code> <code class="o">=</code> <code class="n">model_B_on_A</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_B</code><code class="p">,</code> <code class="n">y_train_B</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code>
                           <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">X_valid_B</code><code class="p">,</code> <code class="n">y_valid_B</code><code class="p">))</code>

<code class="k">for</code> <code class="n">layer</code> <code class="ow">in</code> <code class="n">model_B_on_A</code><code class="o">.</code><code class="n">layers</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">]:</code>
    <code class="n">layer</code><code class="o">.</code><code class="n">trainable</code> <code class="o">=</code> <code class="bp">True</code>

<code class="n">optimizer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">1e-4</code><code class="p">)</code> <code class="c1"># the default lr is 1e-2</code>
<code class="n">model_B_on_A</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"binary_crossentropy"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="n">optimizer</code><code class="p">,</code>
                     <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s2">"accuracy"</code><code class="p">])</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model_B_on_A</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_B</code><code class="p">,</code> <code class="n">y_train_B</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">16</code><code class="p">,</code>
                           <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">X_valid_B</code><code class="p">,</code> <code class="n">y_valid_B</code><code class="p">))</code></pre>

<p>So, what’s the final verdict? Well, this model’s test accuracy is 99.25%, which means that transfer learning reduced the error rate from 2.8% down to almost 0.7%! That’s a factor of four!</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">model_B_on_A</code><code class="o">.</code><code class="n">evaluate</code><code class="p">(</code><code class="n">X_test_B</code><code class="p">,</code> <code class="n">y_test_B</code><code class="p">)</code>
<code class="go">[0.06887910133600235, 0.9925]</code></pre>

<p>Are you convinced? You shouldn’t be: I cheated! I tried many configurations until I found one that demonstrated a strong improvement. If you try to change the classes or the random seed, you will see that the improvement generally drops, or even vanishes or reverses. What I did is called “torturing the data until it confesses.” When a paper just looks too positive, you should be suspicious: perhaps the flashy new technique does not help much (in fact, it may even degrade performance), but the authors tried many variants and reported only the best results (which may be due to sheer luck), without mentioning how many failures they encountered on the way. Most of the time, this is not malicious at all, but it is part of the reason so many results in science can never be reproduced.</p>

<p>Why did I cheat? It turns out that transfer learning does not work very well with small dense networks, presumably because small networks learn few patterns, and dense networks learn very specific patterns, which are unlikely to be useful in other tasks. Transfer learning works best with deep convolutional neural networks, which tend to learn feature detectors that are much more general (especially in the lower layers). We will revisit transfer learning in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cnn_chapter">Chapter&nbsp;14</a>, using the techniques we just discussed (and this time there will be no cheating, I promise!).</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Unsupervised Pretraining"><div class="sect2" id="idm46263511859176">
<h2>Unsupervised Pretraining</h2>

<p>Suppose you want to tackle a complex task for which you don’t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. Don’t lose hope! First, you should try to gather more labeled training data, but if you can’t, you may still be able to perform <em>unsupervised pretraining</em> (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#unsupervised_pretraining_diagram">Figure&nbsp;11-5</a>). Indeed, it is often cheap to gather unlabeled training examples, but expensive to label them. If you can gather plenty of unlabeled training data, you can try to use it to train an unsupervised model, such as an autoencoder or a generative adversarial network (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#autoencoders_chapter">Chapter&nbsp;17</a>). Then, reuse the lower layers of the autoencoder or the lower layers of the GAN’s discriminator, add the output layer for your task on top, and fine-tune the final network using supervised learning (i.e., with the labeled training examples).</p>

<p>It is this technique that Geoffrey Hinton and his team used in 2006 and which led to the revival of neural networks and the success of Deep Learning. Until 2010, unsupervised pretraining (typically with RBMs) was the norm for deep nets, and only after the vanishing gradients problem was alleviated did it become much more common to train DNNs purely using supervised learning. Unsupervised pretraining (today typically using autoencoders or GANs rather than RBMs) is still a good option when you have a complex task to solve, no similar model you can reuse, and little labeled training data but plenty of unlabeled training data.</p>

<p>Note that in the early days of Deep Learning, it was difficult to train deep models, so people would use a technique called <em>greedy layer-wise pretraining</em> (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#unsupervised_pretraining_diagram">Figure&nbsp;11-5</a>). They would first train an unsupervised model with a single layer, typically a <em>Restricted Boltzmann Machine</em> (RBM; see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/app05.html#other_ann_appendix">Appendix&nbsp;E</a>), then they would freeze that layer and add another one on top of it, then train the model (effectively just training the new layer), then freeze the new layer and add another layer on top of it, train the model again, and so on. Nowadays, things are much simpler: people generally train the full unsupervised model in one shot (i.e., in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#unsupervised_pretraining_diagram">Figure&nbsp;11-5</a>, just start directly at step three) and use autoencoders or GANs rather than RBMs.</p>

<figure><div id="unsupervised_pretraining_diagram" class="figure">
<img src="./Chapter11_files/mls2_1105.png" alt="mls2 1105" width="1440" height="951" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1105.png">
<h6><span class="label">Figure 11-5. </span>In unsupervised training, a model is trained on the unlabeled data (or on all the data) using an unsupervised learning technique, then it is fine-tuned for the final task on the labeled data using a supervised learning technique; the unsupervised part may train one layer at a time as shown here, or it may train the full model directly</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Pretraining on an Auxiliary Task"><div class="sect2" id="idm46263511556440">
<h2>Pretraining on an Auxiliary Task</h2>

<p>If you do not have much labeled training data, one last option is to train a first neural network on an auxiliary task for which you can easily obtain or generate labeled training data, then reuse the lower layers of that network for your actual task. The first neural network’s lower layers will learn feature detectors that will likely be reusable by the second neural network.</p>

<p>For example, if you want to build a system to recognize faces, you may only have a few pictures of each individual—clearly not enough to train a good classifier. Gathering hundreds of pictures of each person would not be practical. You could, however, gather a lot of pictures of random people on the web and train a first neural network to detect whether or not two different pictures feature the same person. Such a network would learn good feature detectors for faces, so reusing its lower layers would allow you to train a good face classifier that uses little training data.</p>

<p>For <em>natural language processing</em> (NLP) applications, you can download millions of text documents and automatically generate labeled data from it. For example, you could randomly mask out some words and train a model to predict what the missing words are (e.g., it should predict that the missing word in the sentence “What ___ you saying?” is probably “are” or “were”). If you can train a model to reach good performance on this task, then it will already know quite a lot about language, and you can certainly reuse it for your actual task, and fine-tune it on your labeled data (we will discuss more pretraining tasks in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#rnn_chapter">Chapter&nbsp;15</a>).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><em>Self-supervised learning</em> is when you automatically generate the labels from the data itself, then you train a model on the resulting “labeled” dataset using supervised learning techniques. Since this approach requires no human labeling whatsoever, it is best classified as a form of unsupervised learning.</p>
</div>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Faster Optimizers"><div class="sect1" id="idm46263511872280">
<h1>Faster Optimizers</h1>

<p>Training a very large deep neural network can be painfully slow. So far we have seen four ways to speed up training (and reach a better solution): applying a good initialization strategy for the connection weights, using a good activation function, using Batch Normalization, and reusing parts of a pretrained network (possibly built on an auxiliary task or using unsupervised learning). Another huge speed boost comes from using a faster optimizer than the regular Gradient Descent optimizer. In this section we will present the most popular ones: momentum optimization, Nesterov Accelerated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization.</p>








<section data-type="sect2" data-pdf-bookmark="Momentum Optimization"><div class="sect2" id="idm46263511525960">
<h2>Momentum Optimization</h2>

<p>Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity (if there is some friction or air resistance). This is the very simple idea behind <em>momentum optimization</em>, <a href="https://homl.info/54">proposed by Boris Polyak in 1964</a>.<sup><a data-type="noteref" id="idm46263511522936-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511522936">13</a></sup> In contrast, regular Gradient Descent will simply take small, regular steps down the slope, so the algorithm will take much more time to reach the bottom.</p>

<p>Recall that Gradient Descent updates the weights <strong>θ</strong> by directly subtracting the gradient of the cost function <em>J</em>(<strong>θ</strong>) with regard to the weights (∇<sub><strong>θ</strong></sub><em>J</em>(<strong>θ</strong>)) multiplied by the learning rate  <em>η</em>. The equation is: <strong>θ</strong> ← <strong>θ</strong> – <em>η</em>∇<sub><strong>θ</strong></sub><em>J</em>(<strong>θ</strong>). It does not care about what the earlier gradients were. If the local gradient is tiny, it goes very slowly.</p>

<p>Momentum optimization cares a great deal about what previous gradients were: at each iteration, it subtracts the local gradient from the <em>momentum vector</em> <strong>m</strong> (multiplied by the learning rate <em>η</em>), and it updates the weights by adding this momentum vector (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#momentum_equation">Equation 11-4</a>). In other words, the gradient is used for acceleration, not for speed. To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter <em>β</em>, called the <em>momentum</em>, which must be set between 0 (high friction) and 1 (no friction). A typical momentum value is 0.9.</p>
<div class="fifty-percent" id="momentum_equation" data-type="equation"><h5><span class="label">Equation 11-4. </span>Momentum algorithm</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-132-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mtable displaystyle=&quot;true&quot;&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;m&lt;/mi&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mi&gt;&amp;#x3B2;&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;m&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mi&gt;&amp;#x3B7;&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2207;&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5240" style="width: 12.393em; display: inline-block;"><span style="display: inline-block; position: relative; width: 12.034em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(0.465em, 1011.78em, 3.242em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-5241"><span class="mtable" id="MathJax-Span-5242" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 11.725em; height: 0px;"><span style="position: absolute; clip: rect(2.522em, 1001.76em, 4.938em, -1000.01em); top: -4.008em; left: 0em;"><span style="display: inline-block; position: relative; width: 1.802em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.167em, -1000.01em); top: -4.676em; right: 0em;"><span class="mtd" id="MathJax-Span-5243"><span class="mrow" id="MathJax-Span-5244"><span class="mrow" id="MathJax-Span-5245"><span class="mn" id="MathJax-Span-5246" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-5247" style="font-family: MathJax_Main;">.</span><span class="mspace" id="MathJax-Span-5248" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.167em, -1000.01em); top: -3.237em; right: 0em;"><span class="mtd" id="MathJax-Span-5266"><span class="mrow" id="MathJax-Span-5267"><span class="mrow" id="MathJax-Span-5268"><span class="mn" id="MathJax-Span-5269" style="font-family: MathJax_Main;">2</span><span class="mo" id="MathJax-Span-5270" style="font-family: MathJax_Main;">.</span><span class="mspace" id="MathJax-Span-5271" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.419em, 1009.06em, 5.041em, -1000.01em); top: -4.008em; left: 2.573em;"><span style="display: inline-block; position: relative; width: 9.154em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1009.06em, 4.424em, -1000.01em); top: -4.676em; left: 0em;"><span class="mtd" id="MathJax-Span-5249"><span class="mrow" id="MathJax-Span-5250"><span class="mrow" id="MathJax-Span-5251"><span class="mi" id="MathJax-Span-5252" style="font-family: MathJax_Main-bold;">m</span><span class="mo" id="MathJax-Span-5253" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="mi" id="MathJax-Span-5254" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-5255" style="font-family: MathJax_Main-bold;">m</span><span class="mo" id="MathJax-Span-5256" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mi" id="MathJax-Span-5257" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">η<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="msub" id="MathJax-Span-5258"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.219em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5259" style="font-family: MathJax_Main;">∇</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.825em;"><span class="mi" id="MathJax-Span-5260" style="font-size: 70.7%; font-family: MathJax_Math-bold-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-5261" style="font-family: MathJax_Math-italic;">J<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mrow" id="MathJax-Span-5262" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-5263" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5264" style="font-family: MathJax_Math-bold-italic;">θ</span><span class="mo" id="MathJax-Span-5265" style="font-family: MathJax_Main;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1004.79em, 4.27em, -1000.01em); top: -3.237em; left: 0em;"><span class="mtd" id="MathJax-Span-5272"><span class="mrow" id="MathJax-Span-5273"><span class="mrow" id="MathJax-Span-5274"><span class="mi" id="MathJax-Span-5275" style="font-family: MathJax_Math-bold-italic;">θ</span><span class="mo" id="MathJax-Span-5276" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="mi" id="MathJax-Span-5277" style="font-family: MathJax_Math-bold-italic; padding-left: 0.26em;">θ</span><span class="mo" id="MathJax-Span-5278" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-5279" style="font-family: MathJax_Main-bold; padding-left: 0.208em;">m</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.056em; border-left: 0px solid; width: 0px; height: 2.65em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn><mo>.</mo><mspace width="1.em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><mi mathvariant="bold">m</mi><mo>←</mo><mi>β</mi><mi mathvariant="bold">m</mi><mo>-</mo><mi>η</mi><msub><mi>∇</mi><mi mathvariant="bold">θ</mi></msub><mi>J</mi><mrow><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign="right"><mrow><mn>2</mn><mo>.</mo><mspace width="1.em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><mi mathvariant="bold">θ</mi><mo>←</mo><mi mathvariant="bold">θ</mi><mo>+</mo><mi mathvariant="bold">m</mi></mrow></mtd></mtr></mtable></math></span></span></div><script type="math/mml" id="MathJax-Element-132"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>1</mn>
          <mo>.</mo>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">m</mi>
          <mo>←</mo>
          <mi>β</mi>
          <mi mathvariant="bold">m</mi>
          <mo>-</mo>
          <mi>η</mi>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>2</mn>
          <mo>.</mo>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">θ</mi>
          <mo>←</mo>
          <mi mathvariant="bold">θ</mi>
          <mo>+</mo>
          <mi mathvariant="bold">m</mi>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></script></div>

<p>You can easily verify that if the gradient remains constant, the terminal velocity (i.e., the maximum size of the weight updates) is equal to that gradient multiplied by the learning rate <em>η</em> multiplied by <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-133-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;StartFraction 1 Over 1 minus beta EndFraction&quot;&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mi&gt;&amp;#x3B2;&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5280" aria-label="StartFraction 1 Over 1 minus beta EndFraction" style="width: 1.699em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.648em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.082em, 1001.66em, 2.83em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-5281"><span class="mfrac" id="MathJax-Span-5282"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.396em, 1000.32em, 4.167em, -1000.01em); top: -4.419em; left: 50%; margin-left: -0.152em;"><span class="mn" id="MathJax-Span-5283" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.345em, 1001.3em, 4.321em, -1000.01em); top: -3.596em; left: 50%; margin-left: -0.666em;"><span class="mrow" id="MathJax-Span-5284"><span class="mn" id="MathJax-Span-5285" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-5286" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mi" id="MathJax-Span-5287" style="font-size: 70.7%; font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1001.45em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.442em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.633em; border-left: 0px solid; width: 0px; height: 1.591em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="StartFraction 1 Over 1 minus beta EndFraction"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>-</mo><mi>β</mi></mrow></mfrac></math></span></span><script type="math/mml" id="MathJax-Element-133"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="StartFraction 1 Over 1 minus beta EndFraction">
  <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>-</mo><mi>β</mi></mrow></mfrac>
</math></script> (ignoring the sign). For example, if <em>β</em> = 0.9, then the terminal velocity is equal to 10 times the gradient times the learning rate, so momentum optimization ends up going 10 times faster than Gradient Descent! This allows momentum optimization to escape from plateaus much faster than Gradient Descent. We saw in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a> that when the inputs have very different scales, the cost function will look like an elongated bowl (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#elongated_bowl_diagram">Figure&nbsp;4-7</a>). Gradient Descent goes down the steep slope quite fast, but then it takes a very long time to go down the valley. In contrast, momentum optimization will roll down the valley faster and faster until it reaches the bottom (the optimum). In deep neural networks that don’t use Batch Normalization, the upper layers will often end up having inputs with very different scales, so using momentum optimization helps a lot. It can also help roll past local optima.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Due to the momentum, the optimizer may overshoot a bit, then come back, overshoot again, and oscillate like this many times before stabilizing at the minimum. This is one of the reasons it’s good to have a bit of friction in the system: it gets rid of these oscillations and thus speeds up convergence.</p>
</div>

<p>Implementing momentum optimization in Keras is a no-brainer: just use the <code>SGD</code> optimizer and set its <code>momentum</code> hyperparameter, then lie back and profit!</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">0.001</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="mf">0.9</code><code class="p">)</code></pre>

<p>The one drawback of momentum optimization is that it adds yet another hyperparameter to tune. However, the momentum value of&nbsp;0.9 usually works well in practice and almost always goes faster than regular Gradient Descent.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Nesterov Accelerated Gradient"><div class="sect2" id="idm46263511525464">
<h2>Nesterov Accelerated Gradient</h2>

<p>One small variant to momentum optimization, proposed by <a href="https://homl.info/55">Yurii Nesterov in 1983</a>,<sup><a data-type="noteref" id="idm46263511415816-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511415816">14</a></sup> is almost always faster than vanilla momentum optimization. <em>Nesterov momentum optimization</em>, or <em>Nesterov Accelerated Gradient</em> (NAG), measures the gradient of the cost function not at the local position <strong>θ</strong> but slightly ahead in the direction of the momentum, at <strong>θ</strong> + <em>β</em><strong>m</strong> (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#nesterov_momentum_equation">Equation 11-5</a>).</p>
<div class="fifty-percent" id="nesterov_momentum_equation" data-type="equation"><h5><span class="label">Equation 11-5. </span>Nesterov Accelerated Gradient algorithm</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-134-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mtable displaystyle=&quot;true&quot;&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;m&lt;/mi&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mi&gt;&amp;#x3B2;&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;m&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mi&gt;&amp;#x3B7;&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2207;&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B2;&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;m&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5288" style="width: 15.221em; display: inline-block;"><span style="display: inline-block; position: relative; width: 14.758em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(0.465em, 1014.51em, 3.242em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-5289"><span class="mtable" id="MathJax-Span-5290" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 14.45em; height: 0px;"><span style="position: absolute; clip: rect(2.522em, 1001.76em, 4.938em, -1000.01em); top: -4.008em; left: 0em;"><span style="display: inline-block; position: relative; width: 1.802em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.167em, -1000.01em); top: -4.676em; right: 0em;"><span class="mtd" id="MathJax-Span-5291"><span class="mrow" id="MathJax-Span-5292"><span class="mrow" id="MathJax-Span-5293"><span class="mn" id="MathJax-Span-5294" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-5295" style="font-family: MathJax_Main;">.</span><span class="mspace" id="MathJax-Span-5296" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.167em, -1000.01em); top: -3.237em; right: 0em;"><span class="mtd" id="MathJax-Span-5317"><span class="mrow" id="MathJax-Span-5318"><span class="mrow" id="MathJax-Span-5319"><span class="mn" id="MathJax-Span-5320" style="font-family: MathJax_Main;">2</span><span class="mo" id="MathJax-Span-5321" style="font-family: MathJax_Main;">.</span><span class="mspace" id="MathJax-Span-5322" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.419em, 1011.78em, 5.041em, -1000.01em); top: -4.008em; left: 2.573em;"><span style="display: inline-block; position: relative; width: 11.879em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1011.78em, 4.424em, -1000.01em); top: -4.676em; left: 0em;"><span class="mtd" id="MathJax-Span-5297"><span class="mrow" id="MathJax-Span-5298"><span class="mrow" id="MathJax-Span-5299"><span class="mi" id="MathJax-Span-5300" style="font-family: MathJax_Main-bold;">m</span><span class="mo" id="MathJax-Span-5301" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="mi" id="MathJax-Span-5302" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-5303" style="font-family: MathJax_Main-bold;">m</span><span class="mo" id="MathJax-Span-5304" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mi" id="MathJax-Span-5305" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">η<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="msub" id="MathJax-Span-5306"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.219em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5307" style="font-family: MathJax_Main;">∇</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.825em;"><span class="mi" id="MathJax-Span-5308" style="font-size: 70.7%; font-family: MathJax_Math-bold-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-5309" style="font-family: MathJax_Math-italic;">J<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mrow" id="MathJax-Span-5310" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-5311" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5312" style="font-family: MathJax_Math-bold-italic;">θ</span><span class="mo" id="MathJax-Span-5313" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-5314" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-5315" style="font-family: MathJax_Main-bold;">m</span><span class="mo" id="MathJax-Span-5316" style="font-family: MathJax_Main;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1004.79em, 4.27em, -1000.01em); top: -3.237em; left: 0em;"><span class="mtd" id="MathJax-Span-5323"><span class="mrow" id="MathJax-Span-5324"><span class="mrow" id="MathJax-Span-5325"><span class="mi" id="MathJax-Span-5326" style="font-family: MathJax_Math-bold-italic;">θ</span><span class="mo" id="MathJax-Span-5327" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="mi" id="MathJax-Span-5328" style="font-family: MathJax_Math-bold-italic; padding-left: 0.26em;">θ</span><span class="mo" id="MathJax-Span-5329" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-5330" style="font-family: MathJax_Main-bold; padding-left: 0.208em;">m</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.056em; border-left: 0px solid; width: 0px; height: 2.65em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn><mo>.</mo><mspace width="1.em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><mi mathvariant="bold">m</mi><mo>←</mo><mi>β</mi><mi mathvariant="bold">m</mi><mo>-</mo><mi>η</mi><msub><mi>∇</mi><mi mathvariant="bold">θ</mi></msub><mi>J</mi><mrow><mo>(</mo><mi mathvariant="bold">θ</mi><mo>+</mo><mi>β</mi><mi mathvariant="bold">m</mi><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign="right"><mrow><mn>2</mn><mo>.</mo><mspace width="1.em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><mi mathvariant="bold">θ</mi><mo>←</mo><mi mathvariant="bold">θ</mi><mo>+</mo><mi mathvariant="bold">m</mi></mrow></mtd></mtr></mtable></math></span></span></div><script type="math/mml" id="MathJax-Element-134"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>1</mn>
          <mo>.</mo>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">m</mi>
          <mo>←</mo>
          <mi>β</mi>
          <mi mathvariant="bold">m</mi>
          <mo>-</mo>
          <mi>η</mi>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>+</mo>
            <mi>β</mi>
            <mi mathvariant="bold">m</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>2</mn>
          <mo>.</mo>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">θ</mi>
          <mo>←</mo>
          <mi mathvariant="bold">θ</mi>
          <mo>+</mo>
          <mi mathvariant="bold">m</mi>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></script></div>

<p>This small tweak works because in general the momentum vector will be pointing in the right direction (i.e., toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than using the gradient at the original position, as you can see in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#nesterov_momentum_diagram">Figure&nbsp;11-6</a> (where ∇<sub>1</sub> represents the gradient of the cost function measured at the starting point <strong>θ</strong>, and ∇<sub>2</sub> represents the gradient at the point located at <strong>θ</strong> + <em>β</em><strong>m</strong>). As you can see, the Nesterov update ends up slightly closer to the optimum. After a while, these small improvements add up and NAG ends up being significantly faster than regular momentum optimization. Moreover, note that when the momentum pushes the weights across a valley, ∇<sub>1</sub> continues to push farther across the valley, while ∇<sub>2</sub> pushes back toward the bottom of the valley. This helps reduce oscillations and thus NAG converges faster.</p>

<p>NAG is generally faster than regular momentum optimization. To use it, simply set <code>nesterov=True</code> when creating the <code>SGD</code> optimizer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">0.001</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="mf">0.9</code><code class="p">,</code> <code class="n">nesterov</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code></pre>

<figure class="smallerseventy"><div id="nesterov_momentum_diagram" class="figure">
<img src="./Chapter11_files/mls2_1106.png" alt="mls2 1106" width="1441" height="820" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1106.png">
<h6><span class="label">Figure 11-6. </span>Regular versus Nesterov momentum optimization: the former applies the gradients computed before the momentum step, while the latter applies the gradients computed after</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="AdaGrad"><div class="sect2" id="idm46263511352328">
<h2>AdaGrad</h2>

<p>Consider the elongated bowl problem again: Gradient Descent starts by quickly going down the steepest slope, which does not point straight toward the global optimum, then it very slowly goes down the bottom of the valley. It would be nice if the algorithm could correct its direction earlier to point a bit more toward the global optimum. The <a href="https://homl.info/56"><em>AdaGrad</em> algorithm</a><sup><a data-type="noteref" id="idm46263511332376-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511332376">15</a></sup> achieves this correction by scaling down the gradient vector along the steepest dimensions (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#adagrad_algorithm">Equation 11-6</a>).</p>
<div id="adagrad_algorithm" data-type="equation"><h5><span class="label">Equation 11-6. </span>AdaGrad algorithm</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-135-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mtable displaystyle=&quot;true&quot;&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;s&lt;/mi&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2207;&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x2297;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2207;&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mi&gt;&amp;#x3B7;&lt;/mi&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2207;&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x2298;&lt;/mo&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B5;&lt;/mi&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5331" style="width: 15.478em; display: inline-block;"><span style="display: inline-block; position: relative; width: 15.016em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(0.414em, 1014.87em, 3.293em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-5332"><span class="mtable" id="MathJax-Span-5333" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 14.707em; height: 0px;"><span style="position: absolute; clip: rect(2.47em, 1001.76em, 4.938em, -1000.01em); top: -4.008em; left: 0em;"><span style="display: inline-block; position: relative; width: 1.802em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.167em, -1000.01em); top: -4.728em; right: 0em;"><span class="mtd" id="MathJax-Span-5334"><span class="mrow" id="MathJax-Span-5335"><span class="mrow" id="MathJax-Span-5336"><span class="mn" id="MathJax-Span-5337" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-5338" style="font-family: MathJax_Main;">.</span><span class="mspace" id="MathJax-Span-5339" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.167em, -1000.01em); top: -3.237em; right: 0em;"><span class="mtd" id="MathJax-Span-5364"><span class="mrow" id="MathJax-Span-5365"><span class="mrow" id="MathJax-Span-5366"><span class="mn" id="MathJax-Span-5367" style="font-family: MathJax_Main;">2</span><span class="mo" id="MathJax-Span-5368" style="font-family: MathJax_Main;">.</span><span class="mspace" id="MathJax-Span-5369" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.368em, 1012.09em, 5.195em, -1000.01em); top: -4.008em; left: 2.573em;"><span style="display: inline-block; position: relative; width: 12.136em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1011.58em, 4.424em, -1000.01em); top: -4.728em; left: 0em;"><span class="mtd" id="MathJax-Span-5340"><span class="mrow" id="MathJax-Span-5341"><span class="mrow" id="MathJax-Span-5342"><span class="mi" id="MathJax-Span-5343" style="font-family: MathJax_Main-bold;">s</span><span class="mo" id="MathJax-Span-5344" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="mi" id="MathJax-Span-5345" style="font-family: MathJax_Main-bold; padding-left: 0.26em;">s</span><span class="mo" id="MathJax-Span-5346" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msub" id="MathJax-Span-5347" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.219em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5348" style="font-family: MathJax_Main;">∇</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.825em;"><span class="mi" id="MathJax-Span-5349" style="font-size: 70.7%; font-family: MathJax_Math-bold-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-5350" style="font-family: MathJax_Math-italic;">J<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mrow" id="MathJax-Span-5351" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-5352" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5353" style="font-family: MathJax_Math-bold-italic;">θ</span><span class="mo" id="MathJax-Span-5354" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-5355" style="font-family: MathJax_Main; padding-left: 0.208em;">⊗</span><span class="msub" id="MathJax-Span-5356" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.219em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5357" style="font-family: MathJax_Main;">∇</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.825em;"><span class="mi" id="MathJax-Span-5358" style="font-size: 70.7%; font-family: MathJax_Math-bold-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-5359" style="font-family: MathJax_Math-italic;">J<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mrow" id="MathJax-Span-5360" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-5361" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5362" style="font-family: MathJax_Math-bold-italic;">θ</span><span class="mo" id="MathJax-Span-5363" style="font-family: MathJax_Main;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1012.09em, 4.424em, -1000.01em); top: -3.237em; left: 0em;"><span class="mtd" id="MathJax-Span-5370"><span class="mrow" id="MathJax-Span-5371"><span class="mrow" id="MathJax-Span-5372"><span class="mi" id="MathJax-Span-5373" style="font-family: MathJax_Math-bold-italic;">θ</span><span class="mo" id="MathJax-Span-5374" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="mi" id="MathJax-Span-5375" style="font-family: MathJax_Math-bold-italic; padding-left: 0.26em;">θ</span><span class="mo" id="MathJax-Span-5376" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mi" id="MathJax-Span-5377" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">η<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mspace" id="MathJax-Span-5378" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="msub" id="MathJax-Span-5379"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.219em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5380" style="font-family: MathJax_Main;">∇</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.825em;"><span class="mi" id="MathJax-Span-5381" style="font-size: 70.7%; font-family: MathJax_Math-bold-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-5382" style="font-family: MathJax_Math-italic;">J<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mrow" id="MathJax-Span-5383" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-5384" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5385" style="font-family: MathJax_Math-bold-italic;">θ</span><span class="mo" id="MathJax-Span-5386" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-5387" style="font-family: MathJax_Main; padding-left: 0.208em;">⊘</span><span class="msqrt" id="MathJax-Span-5388" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 2.985em; height: 0px;"><span style="position: absolute; clip: rect(3.293em, 1002.07em, 4.27em, -1000.01em); top: -4.008em; left: 0.825em;"><span class="mrow" id="MathJax-Span-5389"><span class="mrow" id="MathJax-Span-5390"><span class="mi" id="MathJax-Span-5391" style="font-family: MathJax_Main-bold;">s</span><span class="mo" id="MathJax-Span-5392" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-5393" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">ε</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.602em, 1002.12em, 3.961em, -1000.01em); top: -4.47em; left: 0.825em;"><span style="display: inline-block; position: relative; width: 2.162em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.1em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 1.442em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.414em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.928em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.036em, 1000.83em, 4.373em, -1000.01em); top: -3.956em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.109em; border-left: 0px solid; width: 0px; height: 2.756em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn><mo>.</mo><mspace width="1.em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><mi mathvariant="bold">s</mi><mo>←</mo><mi mathvariant="bold">s</mi><mo>+</mo><msub><mi>∇</mi><mi mathvariant="bold">θ</mi></msub><mi>J</mi><mrow><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>⊗</mo><msub><mi>∇</mi><mi mathvariant="bold">θ</mi></msub><mi>J</mi><mrow><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign="right"><mrow><mn>2</mn><mo>.</mo><mspace width="1.em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><mi mathvariant="bold">θ</mi><mo>←</mo><mi mathvariant="bold">θ</mi><mo>-</mo><mi>η</mi><mspace width="0.166667em"></mspace><msub><mi>∇</mi><mi mathvariant="bold">θ</mi></msub><mi>J</mi><mrow><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>⊘</mo><msqrt><mrow><mi mathvariant="bold">s</mi><mo>+</mo><mi>ε</mi></mrow></msqrt></mrow></mtd></mtr></mtable></math></span></span></div><script type="math/mml" id="MathJax-Element-135"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>1</mn>
          <mo>.</mo>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">s</mi>
          <mo>←</mo>
          <mi mathvariant="bold">s</mi>
          <mo>+</mo>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
          <mo>⊗</mo>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>2</mn>
          <mo>.</mo>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">θ</mi>
          <mo>←</mo>
          <mi mathvariant="bold">θ</mi>
          <mo>-</mo>
          <mi>η</mi>
          <mspace width="0.166667em"></mspace>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
          <mo>⊘</mo>
          <msqrt>
            <mrow>
              <mi mathvariant="bold">s</mi>
              <mo>+</mo>
              <mi>ε</mi>
            </mrow>
          </msqrt>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></script></div>

<p>The first step accumulates the square of the gradients into the vector <strong>s</strong> (recall that the ⊗ symbol represents the element-wise multiplication). This vectorized form is equivalent to computing <em>s</em><sub><em>i</em></sub> ← <em>s</em><sub><em>i</em></sub> + (∂ <em>J</em>(<strong>θ</strong>) / ∂ <em>θ</em><sub><em>i</em></sub>)<sup>2</sup> for each element <em>s</em><sub><em>i</em></sub> of the vector <strong>s</strong>; in other words, each <em>s</em><sub><em>i</em></sub> accumulates the squares of the partial derivative of the cost function with regard to parameter <em>θ</em><sub><em>i</em></sub>. If the cost function is steep along the i<sup>th</sup> dimension, then <em>s</em><sub><em>i</em></sub> will get larger and larger at each iteration.</p>

<p>The second step is almost identical to Gradient Descent, but with one big difference: the gradient vector is scaled down by a factor of <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-136-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;StartRoot bold s plus epsilon EndRoot&quot;&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mi&gt;&amp;#x1D42C;&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3F5;&lt;/mi&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5394" aria-label="StartRoot bold s plus epsilon EndRoot" style="width: 3.036em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.933em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.185em, 1002.94em, 2.522em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-5395"><span class="msqrt" id="MathJax-Span-5396"><span style="display: inline-block; position: relative; width: 2.933em; height: 0px;"><span style="position: absolute; clip: rect(3.293em, 1002.02em, 4.27em, -1000.01em); top: -4.008em; left: 0.825em;"><span class="mrow" id="MathJax-Span-5397"><span class="mrow" id="MathJax-Span-5398"><span class="mi" id="MathJax-Span-5399" style="font-family: MathJax_Main-bold;">s</span><span class="mo" id="MathJax-Span-5400" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-5401" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">ϵ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.602em, 1002.07em, 3.961em, -1000.01em); top: -4.47em; left: 0.825em;"><span style="display: inline-block; position: relative; width: 2.059em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.1em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 1.391em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.414em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.877em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.036em, 1000.83em, 4.373em, -1000.01em); top: -3.956em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.315em; border-left: 0px solid; width: 0px; height: 1.115em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="StartRoot bold s plus epsilon EndRoot"><msqrt><mrow><mi>𝐬</mi><mo>+</mo><mi>ϵ</mi></mrow></msqrt></math></span></span><script type="math/mml" id="MathJax-Element-136"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="StartRoot bold s plus epsilon EndRoot">
  <msqrt>
    <mrow>
      <mi>𝐬</mi>
      <mo>+</mo>
      <mi>ϵ</mi>
    </mrow>
  </msqrt>
</math></script> (the ⊘ symbol represents the element-wise division, and ε is a smoothing term to avoid division by zero, typically set to 10<sup>–10</sup>). This vectorized form is equivalent to simultaneously computing <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-137-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mi&gt;&amp;#x3B7;&lt;/mi&gt;&lt;mo&gt;&amp;#x200A;&lt;/mo&gt;&lt;mo&gt;&amp;#x2202;&lt;/mo&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mo&gt;&amp;#x2202;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B5;&lt;/mi&gt;&lt;/msqrt&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5402" style="width: 13.525em; display: inline-block;"><span style="display: inline-block; position: relative; width: 13.113em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.185em, 1013.12em, 2.522em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-5403"><span class="msub" id="MathJax-Span-5404"><span style="display: inline-block; position: relative; width: 0.774em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.47em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5405" style="font-family: MathJax_Math-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.465em;"><span class="mi" id="MathJax-Span-5406" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5407" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="msub" id="MathJax-Span-5408" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 0.774em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.47em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5409" style="font-family: MathJax_Math-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.465em;"><span class="mi" id="MathJax-Span-5410" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5411" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mi" id="MathJax-Span-5412" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">η<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-5413" style="font-family: MathJax_Main;"> </span><span class="mo" id="MathJax-Span-5414" style="font-family: MathJax_Main; padding-left: 0.157em;">∂</span><span class="mi" id="MathJax-Span-5415" style="font-family: MathJax_Math-italic;">J<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mo" id="MathJax-Span-5416" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mi" id="MathJax-Span-5417" style="font-family: MathJax_Math-bold-italic;">θ</span><span class="mo" id="MathJax-Span-5418" style=""><span style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-5419" style="font-family: MathJax_Main;">/</span><span class="mo" id="MathJax-Span-5420" style="font-family: MathJax_Main;">∂</span><span class="msub" id="MathJax-Span-5421"><span style="display: inline-block; position: relative; width: 0.774em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.47em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5422" style="font-family: MathJax_Math-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.465em;"><span class="mi" id="MathJax-Span-5423" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5424" style="font-family: MathJax_Main;">/</span><span class="msqrt" id="MathJax-Span-5425"><span style="display: inline-block; position: relative; width: 3.293em; height: 0px;"><span style="position: absolute; clip: rect(3.293em, 1002.43em, 4.321em, -1000.01em); top: -4.008em; left: 0.825em;"><span class="mrow" id="MathJax-Span-5426"><span class="msub" id="MathJax-Span-5427"><span style="display: inline-block; position: relative; width: 0.774em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.42em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5428" style="font-family: MathJax_Math-italic;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.465em;"><span class="mi" id="MathJax-Span-5429" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5430" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-5431" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">ε</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.602em, 1002.48em, 3.961em, -1000.01em); top: -4.419em; left: 0.825em;"><span style="display: inline-block; position: relative; width: 2.47em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.1em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 1.802em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.362em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.825em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 1.288em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.036em, 1000.83em, 4.373em, -1000.01em); top: -3.956em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.315em; border-left: 0px solid; width: 0px; height: 1.168em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>θ</mi><mi>i</mi></msub><mo>←</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>-</mo><mi>η</mi><mo> </mo><mo>∂</mo><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo><mo>/</mo><mo>∂</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>/</mo><msqrt><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><mi>ε</mi></msqrt></math></span></span><script type="math/mml" id="MathJax-Element-137"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>θ</mi><mi>i</mi></msub><mo>←</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>-</mo><mi>η</mi><mo> </mo><mo>∂</mo><mi>J</mi><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo><mo>/</mo><mo>∂</mo><msub><mi>θ</mi><mi>i</mi></msub><mo>/</mo><msqrt><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><mi>ε</mi></msqrt></math></script> for all parameters <em>θ</em><sub><em>i</em></sub>.</p>

<p>In short, this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. This is called an <em>adaptive learning rate</em>. It helps point the resulting updates more directly toward the global optimum (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#adagrad_diagram">Figure&nbsp;11-7</a>). One additional benefit is that it requires much less tuning of the learning rate hyperparameter <em>η</em>.</p>

<figure class="smallerseventy"><div id="adagrad_diagram" class="figure">
<img src="./Chapter11_files/mls2_1107.png" alt="mls2 1107" width="1440" height="739" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1107.png">
<h6><span class="label">Figure 11-7. </span>AdaGrad versus Gradient Descent: the latter can correct its direction earlier to point to the optimum</h6>
</div></figure>

<p>AdaGrad often performs well for simple quadratic problems, but it often stops too early when training neural networks. The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum. So even though Keras has an <code>Adagrad</code> optimizer, you should not use it to train deep neural networks (it may be efficient for simpler tasks such as Linear Regression, though). Still, understanding Adagrad is helpful to grasp the other adaptive learning rate optimizers.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="RMSProp"><div class="sect2" id="idm46263511251768">
<h2>RMSProp</h2>

<p>Although AdaGrad slows down a bit too fast and ends up never converging to the global optimum, the <em>RMSProp</em> algorithm<sup><a data-type="noteref" id="idm46263511249672-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511249672">16</a></sup> fixes this by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#rmsprop_algorithm">Equation 11-7</a>).</p>
<div id="rmsprop_algorithm" data-type="equation"><h5><span class="label">Equation 11-7. </span>RMSProp algorithm</h5><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-138-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mtable displaystyle=&quot;true&quot;&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;s&lt;/mi&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mi&gt;&amp;#x3B2;&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mi&gt;&amp;#x3B2;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2207;&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x2297;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2207;&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mi&gt;&amp;#x3B7;&lt;/mi&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2207;&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x2298;&lt;/mo&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B5;&lt;/mi&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5432" style="width: 18.923em; display: inline-block;"><span style="display: inline-block; position: relative; width: 18.357em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(0.414em, 1018.11em, 3.293em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-5433"><span class="mtable" id="MathJax-Span-5434" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 18.049em; height: 0px;"><span style="position: absolute; clip: rect(2.47em, 1001.76em, 4.938em, -1000.01em); top: -4.008em; left: 0em;"><span style="display: inline-block; position: relative; width: 1.802em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.167em, -1000.01em); top: -4.728em; right: 0em;"><span class="mtd" id="MathJax-Span-5435"><span class="mrow" id="MathJax-Span-5436"><span class="mrow" id="MathJax-Span-5437"><span class="mn" id="MathJax-Span-5438" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-5439" style="font-family: MathJax_Main;">.</span><span class="mspace" id="MathJax-Span-5440" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.167em, -1000.01em); top: -3.237em; right: 0em;"><span class="mtd" id="MathJax-Span-5472"><span class="mrow" id="MathJax-Span-5473"><span class="mrow" id="MathJax-Span-5474"><span class="mn" id="MathJax-Span-5475" style="font-family: MathJax_Main;">2</span><span class="mo" id="MathJax-Span-5476" style="font-family: MathJax_Main;">.</span><span class="mspace" id="MathJax-Span-5477" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.368em, 1015.38em, 5.195em, -1000.01em); top: -4.008em; left: 2.573em;"><span style="display: inline-block; position: relative; width: 15.478em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1015.38em, 4.424em, -1000.01em); top: -4.728em; left: 0em;"><span class="mtd" id="MathJax-Span-5441"><span class="mrow" id="MathJax-Span-5442"><span class="mrow" id="MathJax-Span-5443"><span class="mi" id="MathJax-Span-5444" style="font-family: MathJax_Main-bold;">s</span><span class="mo" id="MathJax-Span-5445" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="mi" id="MathJax-Span-5446" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-5447" style="font-family: MathJax_Main-bold;">s</span><span class="mo" id="MathJax-Span-5448" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mrow" id="MathJax-Span-5449" style="padding-left: 0.208em;"><span class="mo" id="MathJax-Span-5450" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-5451" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-5452" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mi" id="MathJax-Span-5453" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-5454" style="font-family: MathJax_Main;">)</span></span><span class="msub" id="MathJax-Span-5455" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.219em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5456" style="font-family: MathJax_Main;">∇</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.825em;"><span class="mi" id="MathJax-Span-5457" style="font-size: 70.7%; font-family: MathJax_Math-bold-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-5458" style="font-family: MathJax_Math-italic;">J<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mrow" id="MathJax-Span-5459" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-5460" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5461" style="font-family: MathJax_Math-bold-italic;">θ</span><span class="mo" id="MathJax-Span-5462" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-5463" style="font-family: MathJax_Main; padding-left: 0.208em;">⊗</span><span class="msub" id="MathJax-Span-5464" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.219em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5465" style="font-family: MathJax_Main;">∇</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.825em;"><span class="mi" id="MathJax-Span-5466" style="font-size: 70.7%; font-family: MathJax_Math-bold-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-5467" style="font-family: MathJax_Math-italic;">J<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mrow" id="MathJax-Span-5468" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-5469" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5470" style="font-family: MathJax_Math-bold-italic;">θ</span><span class="mo" id="MathJax-Span-5471" style="font-family: MathJax_Main;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1012.09em, 4.424em, -1000.01em); top: -3.237em; left: 0em;"><span class="mtd" id="MathJax-Span-5478"><span class="mrow" id="MathJax-Span-5479"><span class="mrow" id="MathJax-Span-5480"><span class="mi" id="MathJax-Span-5481" style="font-family: MathJax_Math-bold-italic;">θ</span><span class="mo" id="MathJax-Span-5482" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="mi" id="MathJax-Span-5483" style="font-family: MathJax_Math-bold-italic; padding-left: 0.26em;">θ</span><span class="mo" id="MathJax-Span-5484" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mi" id="MathJax-Span-5485" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">η<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mspace" id="MathJax-Span-5486" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="msub" id="MathJax-Span-5487"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.219em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5488" style="font-family: MathJax_Main;">∇</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.825em;"><span class="mi" id="MathJax-Span-5489" style="font-size: 70.7%; font-family: MathJax_Math-bold-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-5490" style="font-family: MathJax_Math-italic;">J<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mrow" id="MathJax-Span-5491" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-5492" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5493" style="font-family: MathJax_Math-bold-italic;">θ</span><span class="mo" id="MathJax-Span-5494" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-5495" style="font-family: MathJax_Main; padding-left: 0.208em;">⊘</span><span class="msqrt" id="MathJax-Span-5496" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 2.985em; height: 0px;"><span style="position: absolute; clip: rect(3.293em, 1002.07em, 4.27em, -1000.01em); top: -4.008em; left: 0.825em;"><span class="mrow" id="MathJax-Span-5497"><span class="mrow" id="MathJax-Span-5498"><span class="mi" id="MathJax-Span-5499" style="font-family: MathJax_Main-bold;">s</span><span class="mo" id="MathJax-Span-5500" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-5501" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">ε</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.602em, 1002.12em, 3.961em, -1000.01em); top: -4.47em; left: 0.825em;"><span style="display: inline-block; position: relative; width: 2.162em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.1em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 1.442em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.414em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.928em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.036em, 1000.83em, 4.373em, -1000.01em); top: -3.956em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.109em; border-left: 0px solid; width: 0px; height: 2.756em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn><mo>.</mo><mspace width="1.em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><mi mathvariant="bold">s</mi><mo>←</mo><mi>β</mi><mi mathvariant="bold">s</mi><mo>+</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>β</mi><mo>)</mo></mrow><msub><mi>∇</mi><mi mathvariant="bold">θ</mi></msub><mi>J</mi><mrow><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>⊗</mo><msub><mi>∇</mi><mi mathvariant="bold">θ</mi></msub><mi>J</mi><mrow><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign="right"><mrow><mn>2</mn><mo>.</mo><mspace width="1.em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><mi mathvariant="bold">θ</mi><mo>←</mo><mi mathvariant="bold">θ</mi><mo>-</mo><mi>η</mi><mspace width="0.166667em"></mspace><msub><mi>∇</mi><mi mathvariant="bold">θ</mi></msub><mi>J</mi><mrow><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>⊘</mo><msqrt><mrow><mi mathvariant="bold">s</mi><mo>+</mo><mi>ε</mi></mrow></msqrt></mrow></mtd></mtr></mtable></math></span></span></div><script type="math/mml" id="MathJax-Element-138"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>1</mn>
          <mo>.</mo>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">s</mi>
          <mo>←</mo>
          <mi>β</mi>
          <mi mathvariant="bold">s</mi>
          <mo>+</mo>
          <mrow>
            <mo>(</mo>
            <mn>1</mn>
            <mo>-</mo>
            <mi>β</mi>
            <mo>)</mo>
          </mrow>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
          <mo>⊗</mo>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>2</mn>
          <mo>.</mo>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">θ</mi>
          <mo>←</mo>
          <mi mathvariant="bold">θ</mi>
          <mo>-</mo>
          <mi>η</mi>
          <mspace width="0.166667em"></mspace>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
          <mo>⊘</mo>
          <msqrt>
            <mrow>
              <mi mathvariant="bold">s</mi>
              <mo>+</mo>
              <mi>ε</mi>
            </mrow>
          </msqrt>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></script></div>

<p>The decay rate <em>β</em> is typically set to 0.9. Yes, it is once again a new hyperparameter, but this default value often works well, so you may not need to tune it at all.</p>

<p>As you might expect, Keras has an <code>RMSProp</code> optimizer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">RMSprop</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">0.001</code><code class="p">,</code> <code class="n">rho</code><code class="o">=</code><code class="mf">0.9</code><code class="p">)</code></pre>

<p>Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many researchers until Adam optimization came around.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Adam and Nadam Optimization"><div class="sect2" id="idm46263511196792">
<h2>Adam and Nadam Optimization</h2>

<p><a href="https://homl.info/59"><em>Adam</em></a>,<sup><a data-type="noteref" id="idm46263511185320-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511185320">17</a></sup> which stands for <em>adaptive moment estimation</em>, combines the ideas of momentum optimization and RMSProp: just like momentum optimization, it keeps track of an exponentially decaying average of past gradients; and just like RMSProp, it keeps track of an exponentially decaying average of past squared gradients (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#adam_algorithm">Equation 11-8</a>).<sup><a data-type="noteref" id="idm46263511182680-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511182680">18</a></sup></p>
<div class="pagebreak-before" id="adam_algorithm" data-type="equation"><h5><span class="label">Equation 11-8. </span>Adam algorithm</h5>

<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-139-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mtable displaystyle=&quot;true&quot;&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;m&lt;/mi&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B2;&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;m&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B2;&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2207;&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;s&lt;/mi&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B2;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B2;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2207;&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x2297;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2207;&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;m&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;m&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B2;&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;s&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;s&lt;/mi&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B2;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mn&gt;5&lt;/mn&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B7;&lt;/mi&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;m&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;&amp;#x2298;&lt;/mo&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;s&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B5;&lt;/mi&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5502" style="width: 19.746em; display: inline-block;"><span style="display: inline-block; position: relative; width: 19.18em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(-2.979em, 1018.93em, 6.686em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-5503"><span class="mtable" id="MathJax-Span-5504" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 18.872em; height: 0px;"><span style="position: absolute; clip: rect(4.99em, 1001.76em, 14.244em, -1000.01em); top: -9.92em; left: 0em;"><span style="display: inline-block; position: relative; width: 1.802em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.167em, -1000.01em); top: -8.121em; right: 0em;"><span class="mtd" id="MathJax-Span-5505"><span class="mrow" id="MathJax-Span-5506"><span class="mrow" id="MathJax-Span-5507"><span class="mn" id="MathJax-Span-5508" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-5509" style="font-family: MathJax_Main;">.</span><span class="mspace" id="MathJax-Span-5510" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.167em, -1000.01em); top: -6.63em; right: 0em;"><span class="mtd" id="MathJax-Span-5537"><span class="mrow" id="MathJax-Span-5538"><span class="mrow" id="MathJax-Span-5539"><span class="mn" id="MathJax-Span-5540" style="font-family: MathJax_Main;">2</span><span class="mo" id="MathJax-Span-5541" style="font-family: MathJax_Main;">.</span><span class="mspace" id="MathJax-Span-5542" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.167em, -1000.01em); top: -4.83em; right: 0em;"><span class="mtd" id="MathJax-Span-5578"><span class="mrow" id="MathJax-Span-5579"><span class="mrow" id="MathJax-Span-5580"><span class="mn" id="MathJax-Span-5581" style="font-family: MathJax_Main;">3</span><span class="mo" id="MathJax-Span-5582" style="font-family: MathJax_Main;">.</span><span class="mspace" id="MathJax-Span-5583" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.167em, -1000.01em); top: -2.26em; right: 0em;"><span class="mtd" id="MathJax-Span-5604"><span class="mrow" id="MathJax-Span-5605"><span class="mrow" id="MathJax-Span-5606"><span class="mn" id="MathJax-Span-5607" style="font-family: MathJax_Main;">4</span><span class="mo" id="MathJax-Span-5608" style="font-family: MathJax_Main;">.</span><span class="mspace" id="MathJax-Span-5609" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.167em, -1000.01em); top: 0.105em; right: 0em;"><span class="mtd" id="MathJax-Span-5630"><span class="mrow" id="MathJax-Span-5631"><span class="mrow" id="MathJax-Span-5632"><span class="mn" id="MathJax-Span-5633" style="font-family: MathJax_Main;">5</span><span class="mo" id="MathJax-Span-5634" style="font-family: MathJax_Main;">.</span><span class="mspace" id="MathJax-Span-5635" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 9.926em;"></span></span><span style="position: absolute; clip: rect(5.247em, 1016.21em, 14.861em, -1000.01em); top: -10.28em; left: 2.573em;"><span style="display: inline-block; position: relative; width: 16.301em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1012.61em, 4.424em, -1000.01em); top: -8.121em; left: 0em;"><span class="mtd" id="MathJax-Span-5511"><span class="mrow" id="MathJax-Span-5512"><span class="mrow" id="MathJax-Span-5513"><span class="mi" id="MathJax-Span-5514" style="font-family: MathJax_Main-bold;">m</span><span class="mo" id="MathJax-Span-5515" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="msub" id="MathJax-Span-5516" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5517" style="font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mn" id="MathJax-Span-5518" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-5519" style="font-family: MathJax_Main-bold;">m</span><span class="mo" id="MathJax-Span-5520" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mrow" id="MathJax-Span-5521" style="padding-left: 0.208em;"><span class="mo" id="MathJax-Span-5522" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-5523" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-5524" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="msub" id="MathJax-Span-5525" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5526" style="font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mn" id="MathJax-Span-5527" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5528" style="font-family: MathJax_Main;">)</span></span><span class="msub" id="MathJax-Span-5529" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.219em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5530" style="font-family: MathJax_Main;">∇</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.825em;"><span class="mi" id="MathJax-Span-5531" style="font-size: 70.7%; font-family: MathJax_Math-bold-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-5532" style="font-family: MathJax_Math-italic;">J<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mrow" id="MathJax-Span-5533" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-5534" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5535" style="font-family: MathJax_Math-bold-italic;">θ</span><span class="mo" id="MathJax-Span-5536" style="font-family: MathJax_Main;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1016.21em, 4.424em, -1000.01em); top: -6.63em; left: 0em;"><span class="mtd" id="MathJax-Span-5543"><span class="mrow" id="MathJax-Span-5544"><span class="mrow" id="MathJax-Span-5545"><span class="mi" id="MathJax-Span-5546" style="font-family: MathJax_Main-bold;">s</span><span class="mo" id="MathJax-Span-5547" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="msub" id="MathJax-Span-5548" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5549" style="font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mn" id="MathJax-Span-5550" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-5551" style="font-family: MathJax_Main-bold;">s</span><span class="mo" id="MathJax-Span-5552" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mrow" id="MathJax-Span-5553" style="padding-left: 0.208em;"><span class="mo" id="MathJax-Span-5554" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-5555" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-5556" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="msub" id="MathJax-Span-5557" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5558" style="font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mn" id="MathJax-Span-5559" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5560" style="font-family: MathJax_Main;">)</span></span><span class="msub" id="MathJax-Span-5561" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.219em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5562" style="font-family: MathJax_Main;">∇</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.825em;"><span class="mi" id="MathJax-Span-5563" style="font-size: 70.7%; font-family: MathJax_Math-bold-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-5564" style="font-family: MathJax_Math-italic;">J<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mrow" id="MathJax-Span-5565" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-5566" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5567" style="font-family: MathJax_Math-bold-italic;">θ</span><span class="mo" id="MathJax-Span-5568" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-5569" style="font-family: MathJax_Main; padding-left: 0.208em;">⊗</span><span class="msub" id="MathJax-Span-5570" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.219em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5571" style="font-family: MathJax_Main;">∇</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.825em;"><span class="mi" id="MathJax-Span-5572" style="font-size: 70.7%; font-family: MathJax_Math-bold-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-5573" style="font-family: MathJax_Math-italic;">J<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mrow" id="MathJax-Span-5574" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-5575" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5576" style="font-family: MathJax_Math-bold-italic;">θ</span><span class="mo" id="MathJax-Span-5577" style="font-family: MathJax_Main;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.728em, 1005.92em, 5.195em, -1000.01em); top: -4.83em; left: 0em;"><span class="mtd" id="MathJax-Span-5584"><span class="mrow" id="MathJax-Span-5585"><span class="mrow" id="MathJax-Span-5586"><span class="mover" id="MathJax-Span-5587"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.94em, 4.167em, -1000.01em); top: -4.008em; left: 0.003em;"><span class="mi" id="MathJax-Span-5588" style="font-family: MathJax_Main-bold;">m</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1000.99em, 3.602em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-5589" style=""><span style="font-family: MathJax_Size2;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5590" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="mstyle" id="MathJax-Span-5591" style="padding-left: 0.26em;"><span class="mrow" id="MathJax-Span-5592"><span class="mfrac" id="MathJax-Span-5593"><span style="display: inline-block; position: relative; width: 3.139em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.396em, 1000.94em, 4.167em, -1000.01em); top: -4.676em; left: 50%; margin-left: -0.46em;"><span class="mi" id="MathJax-Span-5594" style="font-family: MathJax_Main-bold;">m</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1003.04em, 4.373em, -1000.01em); top: -3.185em; left: 50%; margin-left: -1.488em;"><span class="mrow" id="MathJax-Span-5595"><span class="mn" id="MathJax-Span-5596" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-5597" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="msup" id="MathJax-Span-5598" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.339em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.99em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-5599"><span class="msub" id="MathJax-Span-5600"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5601" style="font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mn" id="MathJax-Span-5602" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 0.979em;"><span class="mi" id="MathJax-Span-5603" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1003.15em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.139em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.728em, 1005.41em, 5.195em, -1000.01em); top: -2.26em; left: 0em;"><span class="mtd" id="MathJax-Span-5610"><span class="mrow" id="MathJax-Span-5611"><span class="mrow" id="MathJax-Span-5612"><span class="mover" id="MathJax-Span-5613"><span style="display: inline-block; position: relative; width: 0.517em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.42em, 4.167em, -1000.01em); top: -4.008em; left: 0.003em;"><span class="mi" id="MathJax-Span-5614" style="font-family: MathJax_Main-bold;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1000.37em, 3.653em, -1000.01em); top: -4.059em; left: 0em;"><span class="mo" id="MathJax-Span-5615" style=""><span style="font-family: MathJax_Main;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5616" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="mstyle" id="MathJax-Span-5617" style="padding-left: 0.26em;"><span class="mrow" id="MathJax-Span-5618"><span class="mfrac" id="MathJax-Span-5619"><span style="display: inline-block; position: relative; width: 3.139em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.396em, 1000.42em, 4.167em, -1000.01em); top: -4.676em; left: 50%; margin-left: -0.203em;"><span class="mi" id="MathJax-Span-5620" style="font-family: MathJax_Main-bold;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1003.04em, 4.373em, -1000.01em); top: -3.185em; left: 50%; margin-left: -1.488em;"><span class="mrow" id="MathJax-Span-5621"><span class="mn" id="MathJax-Span-5622" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-5623" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="msup" id="MathJax-Span-5624" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.339em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.99em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-5625"><span class="msub" id="MathJax-Span-5626"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5627" style="font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mn" id="MathJax-Span-5628" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 0.979em;"><span class="mi" id="MathJax-Span-5629" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1003.15em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.139em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.933em, 1009.88em, 4.476em, -1000.01em); top: 0.105em; left: 0em;"><span class="mtd" id="MathJax-Span-5636"><span class="mrow" id="MathJax-Span-5637"><span class="mrow" id="MathJax-Span-5638"><span class="mi" id="MathJax-Span-5639" style="font-family: MathJax_Math-bold-italic;">θ</span><span class="mo" id="MathJax-Span-5640" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="mi" id="MathJax-Span-5641" style="font-family: MathJax_Math-bold-italic; padding-left: 0.26em;">θ</span><span class="mo" id="MathJax-Span-5642" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-5643" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">η<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mspace" id="MathJax-Span-5644" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mover" id="MathJax-Span-5645"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.94em, 4.167em, -1000.01em); top: -4.008em; left: 0.003em;"><span class="mi" id="MathJax-Span-5646" style="font-family: MathJax_Main-bold;">m</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1000.99em, 3.602em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-5647" style=""><span style="font-family: MathJax_Size2;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5648" style="font-family: MathJax_Main; padding-left: 0.208em;">⊘</span><span class="msqrt" id="MathJax-Span-5649" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 3.19em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1002.12em, 4.27em, -1000.01em); top: -4.008em; left: 0.979em;"><span class="mrow" id="MathJax-Span-5650"><span class="mrow" id="MathJax-Span-5651"><span class="mover" id="MathJax-Span-5652"><span style="display: inline-block; position: relative; width: 0.517em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.42em, 4.167em, -1000.01em); top: -4.008em; left: 0.003em;"><span class="mi" id="MathJax-Span-5653" style="font-family: MathJax_Main-bold;">s</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1000.37em, 3.653em, -1000.01em); top: -4.059em; left: 0em;"><span class="mo" id="MathJax-Span-5654" style=""><span style="font-family: MathJax_Main;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5655" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-5656" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">ε</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.602em, 1002.17em, 3.961em, -1000.01em); top: -4.625em; left: 0.979em;"><span style="display: inline-block; position: relative; width: 2.162em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.1em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 1.494em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.414em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.979em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1001.04em, 4.527em, -1000.01em); top: -4.059em; left: 0em;"><span style="font-family: MathJax_Size1;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 10.285em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -4.605em; border-left: 0px solid; width: 0px; height: 9.747em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mn>1</mn><mo>.</mo><mspace width="1.em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><mi mathvariant="bold">m</mi><mo>←</mo><msub><mi>β</mi><mn>1</mn></msub><mi mathvariant="bold">m</mi><mo>-</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><msub><mi>β</mi><mn>1</mn></msub><mo>)</mo></mrow><msub><mi>∇</mi><mi mathvariant="bold">θ</mi></msub><mi>J</mi><mrow><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign="right"><mrow><mn>2</mn><mo>.</mo><mspace width="1.em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><mi mathvariant="bold">s</mi><mo>←</mo><msub><mi>β</mi><mn>2</mn></msub><mi mathvariant="bold">s</mi><mo>+</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><msub><mi>β</mi><mn>2</mn></msub><mo>)</mo></mrow><msub><mi>∇</mi><mi mathvariant="bold">θ</mi></msub><mi>J</mi><mrow><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow><mo>⊗</mo><msub><mi>∇</mi><mi mathvariant="bold">θ</mi></msub><mi>J</mi><mrow><mo>(</mo><mi mathvariant="bold">θ</mi><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign="right"><mrow><mn>3</mn><mo>.</mo><mspace width="1.em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><mover accent="true"><mi mathvariant="bold">m</mi><mo>^</mo></mover><mo>←</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mi mathvariant="bold">m</mi><mrow><mn>1</mn><mo>-</mo><msup><mrow><msub><mi>β</mi><mn>1</mn></msub></mrow><mi>t</mi></msup></mrow></mfrac></mstyle></mrow></mtd></mtr><mtr><mtd columnalign="right"><mrow><mn>4</mn><mo>.</mo><mspace width="1.em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><mover accent="true"><mi mathvariant="bold">s</mi><mo>^</mo></mover><mo>←</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mi mathvariant="bold">s</mi><mrow><mn>1</mn><mo>-</mo><msup><mrow><msub><mi>β</mi><mn>2</mn></msub></mrow><mi>t</mi></msup></mrow></mfrac></mstyle></mrow></mtd></mtr><mtr><mtd columnalign="right"><mrow><mn>5</mn><mo>.</mo><mspace width="1.em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><mi mathvariant="bold">θ</mi><mo>←</mo><mi mathvariant="bold">θ</mi><mo>+</mo><mi>η</mi><mspace width="0.166667em"></mspace><mover accent="true"><mi mathvariant="bold">m</mi><mo>^</mo></mover><mo>⊘</mo><msqrt><mrow><mover accent="true"><mi mathvariant="bold">s</mi><mo>^</mo></mover><mo>+</mo><mi>ε</mi></mrow></msqrt></mrow></mtd></mtr></mtable></math></span></span></div><script type="math/mml" id="MathJax-Element-139"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>1</mn>
          <mo>.</mo>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">m</mi>
          <mo>←</mo>
          <msub><mi>β</mi> <mn>1</mn> </msub>
          <mi mathvariant="bold">m</mi>
          <mo>-</mo>
          <mrow>
            <mo>(</mo>
            <mn>1</mn>
            <mo>-</mo>
            <msub><mi>β</mi> <mn>1</mn> </msub>
            <mo>)</mo>
          </mrow>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>2</mn>
          <mo>.</mo>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">s</mi>
          <mo>←</mo>
          <msub><mi>β</mi> <mn>2</mn> </msub>
          <mi mathvariant="bold">s</mi>
          <mo>+</mo>
          <mrow>
            <mo>(</mo>
            <mn>1</mn>
            <mo>-</mo>
            <msub><mi>β</mi> <mn>2</mn> </msub>
            <mo>)</mo>
          </mrow>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
          <mo>⊗</mo>
          <msub><mi>∇</mi> <mi mathvariant="bold">θ</mi> </msub>
          <mi>J</mi>
          <mrow>
            <mo>(</mo>
            <mi mathvariant="bold">θ</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>3</mn>
          <mo>.</mo>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mover accent="true"><mi mathvariant="bold">m</mi><mo>^</mo></mover>
          <mo>←</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mi mathvariant="bold">m</mi> <mrow><mn>1</mn><mo>-</mo><msup><mrow><msub><mi>β</mi> <mn>1</mn> </msub></mrow> <mi>t</mi> </msup></mrow></mfrac>
          </mstyle>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>4</mn>
          <mo>.</mo>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mover accent="true"><mi mathvariant="bold">s</mi><mo>^</mo></mover>
          <mo>←</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mi mathvariant="bold">s</mi> <mrow><mn>1</mn><mo>-</mo><msup><mrow><msub><mi>β</mi> <mn>2</mn> </msub></mrow> <mi>t</mi> </msup></mrow></mfrac>
          </mstyle>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mn>5</mn>
          <mo>.</mo>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mi mathvariant="bold">θ</mi>
          <mo>←</mo>
          <mi mathvariant="bold">θ</mi>
          <mo>+</mo>
          <mi>η</mi>
          <mspace width="0.166667em"></mspace>
          <mover accent="true"><mi mathvariant="bold">m</mi><mo>^</mo></mover>
          <mo>⊘</mo>
          <msqrt>
            <mrow>
              <mover accent="true"><mi mathvariant="bold">s</mi><mo>^</mo></mover>
              <mo>+</mo>
              <mi>ε</mi>
            </mrow>
          </msqrt>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></script></div>

<p>In this equation, <em>t</em> represents the iteration number (starting at 1).</p>

<p>If you just look at steps 1, 2, and 5, you will notice Adam’s close similarity to both momentum optimization and RMSProp. The only difference is that step 1 computes an exponentially decaying average rather than an exponentially decaying sum, but these are actually equivalent except for a constant factor (the decaying average is just 1 – <em>β</em><sub>1</sub> times the decaying sum). Steps 3 and 4 are somewhat of a technical detail: since <strong>m</strong> and <strong>s</strong> are initialized at 0, they will be biased toward 0 at the beginning of training, so these two steps will help boost <strong>m</strong> and <strong>s</strong> at the beginning of training.</p>

<p>The momentum decay hyperparameter <em>β</em><sub>1</sub> is typically initialized to 0.9, while the scaling decay hyperparameter <em>β</em><sub>2</sub> is often initialized to 0.999. As earlier, the smoothing term <em>ε</em> is usually initialized to a tiny number such as 10<sup>–7</sup>. These are the default values for the <code>Adam</code> class (to be precise, <code>epsilon</code> defaults to <code>None</code>, which tells Keras to use <code>keras.backend.epsilon()</code>, which defaults to 10<sup>–7</sup>; you can change it using <code>keras.backend.set_epsilon()</code>). Here is how to create an Adam optimizer using Keras:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">0.001</code><code class="p">,</code> <code class="n">beta_1</code><code class="o">=</code><code class="mf">0.9</code><code class="p">,</code> <code class="n">beta_2</code><code class="o">=</code><code class="mf">0.999</code><code class="p">)</code></pre>

<p>Since Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it requires less tuning of the learning rate hyperparameter <em>η</em>. You can often use the default value <em>η</em> = 0.001, making Adam even easier to use than Gradient Descent.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If you are starting to feel overwhelmed by all these different techniques and are wondering how to choose the right ones for your task, don’t worry: some practical guidelines are provided at the end of this chapter.</p>
</div>

<p>Finally, two variants of Adam are worth mentioning:</p>
<dl>
<dt>Adamax, introduced in the same paper as Adam</dt>
<dd>
<p>Notice that in step 2 of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#adam_algorithm">Equation 11-8</a>, Adam accumulates the squares of the gradients in <strong>s</strong> (with a greater weight for more recent weights). In step 5, if we ignore <em>ε</em> and steps 3 and 4 (which are technical details anyway), Adam scales down the parameter updates by the square root of <strong>s</strong>. In short, Adam scales down the parameter updates by the ℓ<sub>2</sub> norm of the time-decayed gradients (recall that the ℓ<sub>2</sub> norm is the square root of the sum of squares). Adamax replaces the ℓ<sub>2</sub> norm with the ℓ<sub>∞</sub> norm (a fancy way of saying the max). Specifically, it replaces step 2 in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#adam_algorithm">Equation 11-8</a> with <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-140-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;bold s left-arrow max left-parenthesis beta 2 bold s comma normal nabla Subscript theta Baseline upper J left-parenthesis theta right-parenthesis right-parenthesis&quot;&gt;&lt;mrow&gt;&lt;mi&gt;&amp;#x1D42C;&lt;/mi&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;max&lt;/mo&gt;&lt;mfenced separators=&quot;&quot; open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B2;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mi&gt;&amp;#x1D42C;&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x2207;&lt;/mi&gt;&lt;mi&gt;&amp;#x3B8;&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;J&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5657" aria-label="bold s left-arrow max left-parenthesis beta 2 bold s comma normal nabla Subscript theta Baseline upper J left-parenthesis theta right-parenthesis right-parenthesis" style="width: 10.44em; display: inline-block;"><span style="display: inline-block; position: relative; width: 10.131em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.237em, 1010.04em, 2.573em, -1000.01em); top: -2.157em; left: 0em;"><span class="mrow" id="MathJax-Span-5658"><span class="mrow" id="MathJax-Span-5659"><span class="mi" id="MathJax-Span-5660" style="font-family: MathJax_Main-bold;">s</span><span class="mo" id="MathJax-Span-5661" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="mo" id="MathJax-Span-5662" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">max</span><span class="mfenced" id="MathJax-Span-5663"><span class="mo" id="MathJax-Span-5664" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="msub" id="MathJax-Span-5665"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5666" style="font-family: MathJax_Math-italic;">β<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mn" id="MathJax-Span-5667" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-5668" style="font-family: MathJax_Main-bold;">s</span><span class="mo" id="MathJax-Span-5669" style="font-family: MathJax_Main;">,</span><span class="msub" id="MathJax-Span-5670" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.237em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.219em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5671" style="font-family: MathJax_Main;">∇</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.825em;"><span class="mi" id="MathJax-Span-5672" style="font-size: 70.7%; font-family: MathJax_Math-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-5673" style="font-family: MathJax_Math-italic;">J<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mrow" id="MathJax-Span-5674" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-5675" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5676" style="font-family: MathJax_Math-italic;">θ</span><span class="mo" id="MathJax-Span-5677" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-5678" style=""><span style="font-family: MathJax_Main;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.162em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.315em; border-left: 0px solid; width: 0px; height: 1.115em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="bold s left-arrow max left-parenthesis beta 2 bold s comma normal nabla Subscript theta Baseline upper J left-parenthesis theta right-parenthesis right-parenthesis"><mrow><mi>𝐬</mi><mo>←</mo><mo movablelimits="true" form="prefix">max</mo><mfenced separators="" open="(" close=")"><msub><mi>β</mi><mn>2</mn></msub><mi>𝐬</mi><mo>,</mo><msub><mi>∇</mi><mi>θ</mi></msub><mi>J</mi><mrow><mo>(</mo><mi>θ</mi><mo>)</mo></mrow></mfenced></mrow></math></span></span><script type="math/mml" id="MathJax-Element-140"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="bold s left-arrow max left-parenthesis beta 2 bold s comma normal nabla Subscript theta Baseline upper J left-parenthesis theta right-parenthesis right-parenthesis">
  <mrow>
    <mi>𝐬</mi>
    <mo>←</mo>
    <mo movablelimits="true" form="prefix">max</mo>
    <mfenced separators="" open="(" close=")">
      <msub><mi>β</mi> <mn>2</mn> </msub>
      <mi>𝐬</mi>
      <mo>,</mo>
      <msub><mi>∇</mi> <mi>θ</mi> </msub>
      <mi>J</mi>
      <mrow>
        <mo>(</mo>
        <mi>θ</mi>
        <mo>)</mo>
      </mrow>
    </mfenced>
  </mrow>
</math></script>, it drops step 4, and in step 5 it scales down the gradient updates by a factor of <strong>s</strong>, which is just the max of the time-decayed gradients. In practice, this can make Adamax more stable than Adam, but it really depends on the dataset, and in general Adam performs better. So it’s just one more optimizer you can try if you experience problems with Adam on some task.</p>
</dd>
<dt><a href="https://homl.info/nadam">Nadam optimization</a><sup><a data-type="noteref" id="idm46263511034664-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511034664">19</a></sup> is more important</dt>
<dd>
<p>It is Adam optimization plus the Nesterov trick, so it will often converge slightly faster than Adam. In his report, the researcher Timothy Dozat compares many different optimizers on various tasks and finds that Nadam generally outperforms Adam but is sometimes outperformed by RMSProp.</p>
</dd>
</dl>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Adaptive optimization methods (including RMSProp, Adam, and Nadam optimization) are often great, converging fast to a good solution. However, a <a href="https://homl.info/60">2017 paper</a><sup><a data-type="noteref" id="idm46263511030792-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511030792">20</a></sup> by Ashia C. Wilson et al. showed that they can lead to solutions that generalize poorly on some datasets. So when you are disappointed by your model’s performance, try using plain Nesterov Accelerated Gradient instead: your dataset may just be allergic to adaptive gradients. Also check out the latest research, because it’s moving fast.</p>
</div>

<p>All the optimization techniques discussed so far only rely on the <em>first-order partial derivatives</em> (<em>Jacobians</em>). The optimization literature contains amazing algorithms based on the <em>second-order partial derivatives</em> (the <em>Hessians</em>, which are the partial derivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply to deep neural networks because there are <em>n</em><sup>2</sup> Hessians per output (where <em>n</em> is the number of parameters), as opposed to just <em>n</em> Jacobians per output. Since DNNs typically have tens of thousands of parameters, the second-order optimization algorithms often don’t even fit in memory, and even when they do, computing the Hessians is just too slow.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46263511006728">
<h5>Training Sparse Models</h5>
<p>All the optimization algorithms just presented produce dense models, meaning that most parameters will be nonzero. If you need a blazingly fast model at runtime, or if you need it to take up less memory, you may prefer to end up with a sparse model instead.</p>

<p>One easy way to achieve this is to train the model as usual, then get rid of the tiny weights (set them to 0). Note that this will typically not lead to a very sparse model, and it may degrade the model’s performance.</p>

<p>A better option is to apply strong ℓ<sub>1</sub> regularization during training (we will see how later in this chapter), as it pushes the optimizer to zero out as many weights as it can (as discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#lasso_regression">“Lasso Regression”</a> in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>).</p>

<p>If these techniques remain insufficient, check out the <a href="https://homl.info/tfmot">TensorFlow Model Optimization Toolkit</a> (TF-MOT), which provides a pruning API capable of iteratively removing connections during training based on their magnitude.</p>
</div></aside>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#optimizer_summary_table">Table&nbsp;11-2</a> compares all the optimizers we’ve discussed so far (* is bad, <strong> is average, and </strong>* is good).</p>
<table id="optimizer_summary_table">
<caption><span class="label">Table 11-2. </span>Optimizer comparison</caption>
<thead>
<tr>
<th>Class</th>
<th>Convergence speed</th>
<th>Convergence quality</th>
</tr>
</thead>
<tbody>
<tr>
<td><p><code>SGD</code></p></td>
<td><p>*</p></td>
<td><p>***</p></td>
</tr>
<tr>
<td><p><code>SGD(momentum=…)</code></p></td>
<td><p>**</p></td>
<td><p>***</p></td>
</tr>
<tr>
<td><p><code>SGD(momentum=…, nesterov=True)</code></p></td>
<td><p>**</p></td>
<td><p>***</p></td>
</tr>
<tr>
<td><p><code>Adagrad</code></p></td>
<td><p>***</p></td>
<td><p>* (stops too early)</p></td>
</tr>
<tr>
<td><p><code>RMSProp</code></p></td>
<td><p>***</p></td>
<td><p>** or ***</p></td>
</tr>
<tr>
<td><p><code>Adam</code></p></td>
<td><p>***</p></td>
<td><p>** or ***</p></td>
</tr>
<tr>
<td><p><code>Nadam</code></p></td>
<td><p>***</p></td>
<td><p>** or ***</p></td>
</tr>
<tr>
<td><p><code>Adamax</code></p></td>
<td><p>***</p></td>
<td><p>** or ***</p></td>
</tr>
</tbody>
</table>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Learning Rate Scheduling"><div class="sect2" id="idm46263511196536">
<h2>Learning Rate Scheduling</h2>

<p>Finding a good learning rate is very important. If you set it way too high, training may diverge (as we discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>). If you set it too low, training will eventually converge to the optimum, but it will take a very long time. If you set it slightly too high, it will make progress very quickly at first, but it will end up dancing around the optimum, never really settling down. If you have a limited computing budget, you may have to interrupt training before it has converged properly, yielding a suboptimal solution (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#learning_schedule_diagram">Figure&nbsp;11-8</a>).</p>

<figure class="smallerseventy"><div id="learning_schedule_diagram" class="figure">
<img src="./Chapter11_files/mls2_1108.png" alt="mls2 1108" width="1440" height="700" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1108.png">
<h6><span class="label">Figure 11-8. </span>Learning curves for various learning rates η</h6>
</div></figure>

<p>As we discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#ann_chapter">Chapter&nbsp;10</a>, you can find a good learning rate by training the model for a few hundred iterations, exponentially increasing the learning rate from a very small value to a very large value, and then looking at the learning curve and picking a learning rate slightly lower than the one at which the learning curve starts shooting back up. You can then reinitialize your model and train it with that learning rate.</p>

<p>But you can do better than a constant learning rate: if you start with a large learning rate and then reduce it once training stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many different strategies to reduce the learning rate during training. It can also be beneficial to start with a low learning rate, increase it, then drop it again. These strategies are called <em>learning schedules</em> (we briefly introduced this concept in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>). These are the most commonly used learning schedules:</p>
<dl>
<dt><em>Power scheduling</em></dt>
<dd>
<p>Set the learning rate to a function of the iteration number <em>t</em>: <em>η</em>(<em>t</em>) = <em>η</em><sub>0</sub> / (1 + <em>t</em>/<em>s</em>)<sup><em>c</em></sup>. The initial learning rate <em>η</em><sub>0</sub>, the power <em>c</em> (typically set to&nbsp;1), and the steps <em>s</em> are hyperparameters. The learning rate drops at each step. After <em>s</em> steps, it is down to <em>η</em><sub>0</sub> / 2. After <em>s</em> more steps, it is down to <em>η</em><sub>0</sub> / 3. Then down to <em>η</em><sub>0</sub> / 4, then <em>η</em><sub>0</sub> / 5, and so on. As you can see, this schedule first drops quickly, then more and more slowly. Of course, power scheduling requires tuning <em>η</em><sub>0</sub>, <em>s</em> (and possibly <em>c</em>).</p>
</dd>
<dt><em>Exponential scheduling</em></dt>
<dd>
<p>Set the learning rate to: <em>η</em>(<em>t</em>) = <em>η</em><sub>0</sub> 0.1<sup><em>t/s</em></sup>. The learning rate will gradually drop by a factor of&nbsp;10 every <em>s</em> steps. While power scheduling reduces the learning rate more and more slowly, exponential scheduling keeps slashing it by a factor of 10 every <em>s</em> steps.</p>
</dd>
<dt><em>Piecewise constant scheduling</em></dt>
<dd>
<p>Use a constant learning rate for a number of epochs (e.g., <em>η</em><sub>0</sub> = 0.1 for 5 epochs), then a smaller learning rate for another number of epochs (e.g., <em>η</em><sub>1</sub> = 0.001 for 50 epochs), and so on. Although this solution can work very well, it requires fiddling around to figure out the right sequence of learning rates and how long to use each of them.</p>
</dd>
<dt><em>Performance scheduling</em></dt>
<dd>
<p>Measure the validation error every <em>N</em> steps (just like for early stopping), and reduce the learning rate by a factor of <em>λ</em> when the error stops dropping.</p>
</dd>
<dt><em>1cycle scheduling</em></dt>
<dd>
<p>Contrary to the other approaches, <em>1cycle</em> (introduced in a <a href="https://homl.info/1cycle">2018 paper</a><sup><a data-type="noteref" id="idm46263510940008-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263510940008">21</a></sup> by Leslie Smith) starts by increasing the initial learning rate η<sub>0</sub>, growing linearly up to η<sub>1</sub> halfway through training. Then it decreases the learning rate linearly down to η<sub>0</sub> again during the second half of training, finishing the last few epochs by dropping the rate down by several orders of magnitude (still linearly). The maximum learning rate η<sub>1</sub> is chosen using the same approach we used to find the optimal learning rate, and the initial learning rate η<sub>0</sub> is chosen to be roughly 10 times lower. When using a momentum, we start with a high momentum first (e.g., 0.95), then drop it down to a lower momentum during the first half of training (e.g., down to 0.85, linearly), and then bring it back up to the maximum value (e.g., 0.95) during the second half of training, finishing the last few epochs with that maximum value. Smith did many experiments showing that this approach was often able to speed up training considerably and reach a better performance. For example, on the popular CIFAR10 image dataset, this approach reached 91.9% validation accuracy in just 100 epochs, instead of 90.3% accuracy in 800 epochs through a standard approach (with the same neural network architecture).</p>
</dd>
</dl>

<p>A <a href="https://homl.info/63">2013 paper</a><sup><a data-type="noteref" id="idm46263510934680-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263510934680">22</a></sup> by Andrew Senior et al. compared the performance of some of the most popular learning schedules when using momentum optimization to train deep neural networks for speech recognition. The authors concluded that, in this setting, both performance scheduling and exponential scheduling performed well. They favored exponential scheduling because it was easy to tune and it converged slightly faster to the optimal solution (they also mentioned that it was easier to implement than performance scheduling, but in Keras both options are easy). That said, the 1cycle approach seems to perform even better.</p>

<p>Implementing power scheduling in Keras is the easiest option: just set the <code>decay</code> hyperparameter when creating an optimizer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">0.01</code><code class="p">,</code> <code class="n">decay</code><code class="o">=</code><code class="mf">1e-4</code><code class="p">)</code></pre>

<p>The <code>decay</code> is the inverse of <em>s</em> (the number of steps it takes to divide the learning rate by one more unit), and Keras assumes that <em>c</em> is equal to 1.</p>

<p>Exponential scheduling and piecewise scheduling are quite simple too. You first need to define a function that takes the current epoch and returns the learning rate. For example, let’s implement exponential scheduling:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">exponential_decay_fn</code><code class="p">(</code><code class="n">epoch</code><code class="p">):</code>
    <code class="k">return</code> <code class="mf">0.01</code> <code class="o">*</code> <code class="mf">0.1</code><code class="o">**</code><code class="p">(</code><code class="n">epoch</code> <code class="o">/</code> <code class="mi">20</code><code class="p">)</code></pre>

<p>If you do not want to hard-code <em>η</em><sub>0</sub> and <em>s</em>, you can create a function that returns a configured function:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">exponential_decay</code><code class="p">(</code><code class="n">lr0</code><code class="p">,</code> <code class="n">s</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf">exponential_decay_fn</code><code class="p">(</code><code class="n">epoch</code><code class="p">):</code>
        <code class="k">return</code> <code class="n">lr0</code> <code class="o">*</code> <code class="mf">0.1</code><code class="o">**</code><code class="p">(</code><code class="n">epoch</code> <code class="o">/</code> <code class="n">s</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">exponential_decay_fn</code>

<code class="n">exponential_decay_fn</code> <code class="o">=</code> <code class="n">exponential_decay</code><code class="p">(</code><code class="n">lr0</code><code class="o">=</code><code class="mf">0.01</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">20</code><code class="p">)</code></pre>

<p>Next, create a <code>LearningRateScheduler</code> callback, giving it the schedule function, and pass this callback to the <code>fit()</code> method:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">lr_scheduler</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">callbacks</code><code class="o">.</code><code class="n">LearningRateScheduler</code><code class="p">(</code><code class="n">exponential_decay_fn</code><code class="p">)</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_scaled</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="p">[</code><code class="o">...</code><code class="p">],</code> <code class="n">callbacks</code><code class="o">=</code><code class="p">[</code><code class="n">lr_scheduler</code><code class="p">])</code></pre>

<p>The <code>LearningRateScheduler</code> will update the optimizer’s <code>learning_rate</code> attribute at the beginning of each epoch. Updating the learning rate once per epoch is usually enough, but if you want it to be updated more often, for example at every step, you can always write your own callback (see the notebook for an example). Updating the learning rate at every step makes sense if there are many steps per epoch. Alternatively, you can use the <code>keras.optimizers.schedules</code> approach, described shortly.</p>

<p>The schedule function can optionally take the current learning rate as a second argument. For example, the following schedule function multiplies the previous learning rate by 0.1<sup>1/20</sup>, which results in the same exponential decay (except the decay now starts at the beginning of epoch 0 instead of 1).</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">exponential_decay_fn</code><code class="p">(</code><code class="n">epoch</code><code class="p">,</code> <code class="n">lr</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">lr</code> <code class="o">*</code> <code class="mf">0.1</code><code class="o">**</code><code class="p">(</code><code class="mi">1</code> <code class="o">/</code> <code class="mi">20</code><code class="p">)</code></pre>

<p>This implementation relies on the optimizer’s initial learning rate (contrary to the previous implementation), so make sure to set it appropriately.</p>

<p>When you save a model, the optimizer and its learning rate get saved along with it. This means that with this new schedule function, you could just load a trained model and continue training where it left off, no problem. Things are not so simple if your schedule function uses the <code>epoch</code> argument: the epoch does not get saved, and it gets reset to 0 every time you call the <code>fit()</code> method. If you were to continue training a model where it left off, this could lead to a very large learning rate, which would likely damage your model’s weights. One solution is to manually set the <code>fit()</code> method’s <code>initial_epoch</code> argument so the <code>epoch</code> starts at the right value.</p>

<p>For piecewise constant scheduling, you can use a schedule function like the following one (as earlier, you can define a more general function if you want; see the notebook for an example), then create a <code>LearningRateScheduler</code> callback with this function and pass it to the <code>fit()</code> method, just like we did for exponential scheduling:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">piecewise_constant_fn</code><code class="p">(</code><code class="n">epoch</code><code class="p">):</code>
    <code class="k">if</code> <code class="n">epoch</code> <code class="o">&lt;</code> <code class="mi">5</code><code class="p">:</code>
        <code class="k">return</code> <code class="mf">0.01</code>
    <code class="k">elif</code> <code class="n">epoch</code> <code class="o">&lt;</code> <code class="mi">15</code><code class="p">:</code>
        <code class="k">return</code> <code class="mf">0.005</code>
    <code class="k">else</code><code class="p">:</code>
        <code class="k">return</code> <code class="mf">0.001</code></pre>

<p>For performance scheduling, use the <code>ReduceLROnPlateau</code> callback. For example, if you pass the following callback to the <code>fit()</code> method, it will multiply the learning rate by 0.5 whenever the best validation loss does not improve for 5 consecutive epochs (other options are available; please check the documentation for more details):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">lr_scheduler</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">callbacks</code><code class="o">.</code><code class="n">ReduceLROnPlateau</code><code class="p">(</code><code class="n">factor</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code> <code class="n">patience</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code></pre>

<p>Lastly, tf.keras offers an alternative way to implement learning rate scheduling: define the learning rate using one of the schedules available in <code>keras.optimizers.schedules</code>, then pass this learning rate to any optimizer. This approach updates the learning rate at each step rather than at each epoch. For example, here is how to implement the same exponential schedule as the <code>exponential_decay_fn()</code> function we defined earlier:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">s</code> <code class="o">=</code> <code class="mi">20</code> <code class="o">*</code> <code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code> <code class="o">//</code> <code class="mi">32</code> <code class="c1"># number of steps in 20 epochs (batch size = 32)</code>
<code class="n">learning_rate</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">schedules</code><code class="o">.</code><code class="n">ExponentialDecay</code><code class="p">(</code><code class="mf">0.01</code><code class="p">,</code> <code class="n">s</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">)</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">learning_rate</code><code class="p">)</code></pre>

<p>This is nice and simple, plus when you save the model, the learning rate and its schedule (including its state) get saved as well. This approach, however, is not part of the Keras API; it is specific to tf.keras.</p>

<p>As for the 1cycle approach, the implementation poses no particular difficulty: create a custom callback that modifies the learning rate at each iteration (you can update the optimizer’s learning rate by changing <code>self.model.optimizer.lr</code>). See the notebook for an example.</p>

<p>To sum up, exponential decay, performance scheduling, and 1cycle can considerably speed up convergence, so give them a try!</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Avoiding Overfitting Through Regularization"><div class="sect1" id="idm46263510975480">
<h1>Avoiding Overfitting Through Regularization</h1>
<blockquote>
<p>With four parameters I can fit an elephant and with five I can make him wiggle his trunk.</p>
<p data-type="attribution">John von Neumann, <cite>cited by Enrico Fermi in Nature 427</cite></p>
</blockquote>

<p>With thousands of parameters you can fit the whole zoo. Deep neural networks typically have tens of thousands of parameters, sometimes even millions. With so many parameters, the network has an incredible amount of freedom and can fit a huge variety of complex datasets. But this great flexibility also means that it is prone to overfitting the training set. We need regularization.</p>

<p>We already implemented one of the best regularization techniques in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#ann_chapter">Chapter&nbsp;10</a>: early stopping. Moreover, even though Batch Normalization was designed to solve the unstable gradients problems, is also acts like a pretty good regularizer. In this section we will present other popular regularization techniques for neural networks: ℓ<sub>1</sub> and ℓ<sub>2</sub> regularization, dropout, and max-norm regularization.</p>








<section data-type="sect2" data-pdf-bookmark="ℓ1 and ℓ2 Regularization"><div class="sect2" id="idm46263510542776">
<h2>ℓ<sub>1</sub> and ℓ<sub>2</sub> Regularization</h2>

<p>Just like you did in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a> for simple linear models, you can use ℓ<sub>2</sub> regularization to constrain a neural network’s connection weights, and/or ℓ<sub>1</sub> regularization if you want a sparse model (with many weights equal to 0). Here is how to apply ℓ<sub>2</sub> regularization to a Keras layer’s connection weights, using a regularization factor of 0.01:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">layer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"elu"</code><code class="p">,</code>
                           <code class="n">kernel_initializer</code><code class="o">=</code><code class="s2">"he_normal"</code><code class="p">,</code>
                           <code class="n">kernel_regularizer</code><code class="o">=</code><code class="n">keras</code><code class="o">.</code><code class="n">regularizers</code><code class="o">.</code><code class="n">l2</code><code class="p">(</code><code class="mf">0.01</code><code class="p">))</code></pre>

<p>The <code>l2()</code> function returns a regularizer that will be called at each step during training to compute the regularization loss. This regularization loss is then added to the final loss. As you might expect, you can just use <code>keras.regularizers.l1()</code> if you want ℓ<sub>1</sub> regularization; and if you want both ℓ<sub>1</sub> and ℓ<sub>2</sub> regularization, use <code>keras.regularizers.l1_l2()</code> (specifying both regularization factors).</p>

<p>Since you will typically want to apply the same regularizer to all layers in your network, as well as the same activation function and the same initialization strategy in all hidden layers, you may find yourself repeating the same arguments. This makes the code ugly and error-prone. To avoid this, you can try refactoring your code to use loops. Another option is to use Python’s <code>functools.partial()</code> function, which lets you create a thin wrapper for any callable, with some default argument values:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">functools</code> <code class="kn">import</code> <code class="n">partial</code>

<code class="n">RegularizedDense</code> <code class="o">=</code> <code class="n">partial</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">,</code>
                           <code class="n">activation</code><code class="o">=</code><code class="s2">"elu"</code><code class="p">,</code>
                           <code class="n">kernel_initializer</code><code class="o">=</code><code class="s2">"he_normal"</code><code class="p">,</code>
                           <code class="n">kernel_regularizer</code><code class="o">=</code><code class="n">keras</code><code class="o">.</code><code class="n">regularizers</code><code class="o">.</code><code class="n">l2</code><code class="p">(</code><code class="mf">0.01</code><code class="p">))</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(</code><code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">]),</code>
    <code class="n">RegularizedDense</code><code class="p">(</code><code class="mi">300</code><code class="p">),</code>
    <code class="n">RegularizedDense</code><code class="p">(</code><code class="mi">100</code><code class="p">),</code>
    <code class="n">RegularizedDense</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"softmax"</code><code class="p">,</code>
                     <code class="n">kernel_initializer</code><code class="o">=</code><code class="s2">"glorot_uniform"</code><code class="p">)</code>
<code class="p">])</code></pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Dropout"><div class="sect2" id="idm46263510478568">
<h2>Dropout</h2>

<p><em>Dropout</em> is one of the most popular regularization techniques for deep neural networks. It was <a href="https://homl.info/64">proposed</a><sup><a data-type="noteref" id="idm46263510359688-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263510359688">23</a></sup> by Geoffrey Hinton in 2012 and further detailed in a <a href="https://homl.info/65">paper</a><sup><a data-type="noteref" id="idm46263510358344-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263510358344">24</a></sup> by Nitish Srivastava et al., and it has proven to be highly successful: even the state-of-the-art neural networks got a 1–2% accuracy boost simply by adding dropout. This may not sound like a lot, but when a model already has 95% accuracy, getting a&nbsp;2% accuracy boost means dropping the error rate by almost 40% (going from 5% error to roughly 3%).</p>

<p>It is a fairly simple algorithm: at every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability <em>p</em> of being temporarily “dropped out,” meaning it will be entirely ignored during this training step, but it may be active during the next step (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#dropout_diagram">Figure&nbsp;11-9</a>). The hyperparameter <em>p</em> is called the <em>dropout rate</em>, and it is typically set between 10% and 50%: closer to 20-30% in recurrent neural nets (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#rnn_chapter">Chapter&nbsp;15</a>), and closer to 40-50% in convolutional neural networks (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cnn_chapter">Chapter&nbsp;14</a>). After training, neurons don’t get dropped anymore. And that’s all (except for a technical detail we will discuss momentarily).</p>

<figure class="smallerfiftyfive"><div id="dropout_diagram" class="figure">
<img src="./Chapter11_files/mls2_1109.png" alt="mls2 1109" width="1279" height="914" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1109.png">
<h6><span class="label">Figure 11-9. </span>With dropout regularization, at each training iteration, a random subset of all neurons in one or more layers—except in the output layer—are “dropped out”; these neurons output 0 at this iteration (represented by the dashed arrows)</h6>
</div></figure>

<p>It’s surprising at first that this brutal technique works at all. Would a company perform better if its employees were told to toss a coin every morning to decide whether or not to go to work? Well, who knows; perhaps it would! The company would be forced to adapt its organization; it could not rely on any single person to work the coffee machine or perform any other critical tasks, so this expertise would have to be spread across several people. Employees would have to learn to cooperate with many of their coworkers, not just a handful of them. The company would become much more resilient. If one person quit, it wouldn’t make much of a difference. It’s unclear whether this idea would actually work for companies, but it certainly does for neural networks. Neurons trained with dropout cannot co-adapt with their neighboring neurons; they have to be as useful as possible on their own. They also cannot rely excessively on just a few input neurons; they must pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end you get a more robust network that generalizes better.</p>

<p>Another way to understand the power of dropout is to realize that a unique neural network is generated at each training step. Since each neuron can be either present or absent, there is a total of 2<sup><em>N</em></sup> possible networks (where <em>N</em> is the total number of droppable neurons). This is such a huge number that it is virtually impossible for the same neural network to be sampled twice. Once you have run a 10,000 training steps, you have essentially trained 10,000 different neural networks (each with just one training instance). These neural networks are obviously not independent because they share many of their weights, but they are nevertheless all different. The resulting neural network can be seen as an averaging ensemble of all these smaller neural networks.</p>
<div data-type="tip"><h6>Tip</h6>
<p>In practice, you can usually apply dropout only to the neurons in the top one to three layers (excluding the output layers).</p>
</div>

<p>There is one small but important technical detail. Suppose <em>p</em> = 50%, in which case during testing a neuron would be connected to twice as many input neurons as it would be (on average) during training. To compensate for this fact, we need to multiply each neuron’s input connection weights by&nbsp;0.5 after training. If we don’t, each neuron will get a total input signal roughly twice as large as what the network was trained on and will be unlikely to perform well. More generally, we need to multiply each input connection weight by the <em>keep probability</em> (1 – <em>p</em>) after training. Alternatively, we can divide each neuron’s output by the keep probability during training (these alternatives are not perfectly equivalent, but they work equally well).</p>

<p>To implement dropout using Keras, you can use the <code>keras.layers.Dropout</code> layer. During training, it randomly drops some inputs (setting them to 0) and divides the remaining inputs by the keep probability. After training, it does nothing at all; it just passes the inputs to the next layer. The following code applies dropout regularization before every <code>Dense</code> layer, using a dropout rate of 0.2:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(</code><code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">rate</code><code class="o">=</code><code class="mf">0.2</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">300</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"elu"</code><code class="p">,</code> <code class="n">kernel_initializer</code><code class="o">=</code><code class="s2">"he_normal"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">rate</code><code class="o">=</code><code class="mf">0.2</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"elu"</code><code class="p">,</code> <code class="n">kernel_initializer</code><code class="o">=</code><code class="s2">"he_normal"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">rate</code><code class="o">=</code><code class="mf">0.2</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"softmax"</code><code class="p">)</code>
<code class="p">])</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Since dropout is only active during training, comparing the training loss and the validation loss can be misleading. In particular, a model may be overfitting the training set and yet have similar training and validation losses. So make sure to evaluate the training loss without dropout (e.g., after training).</p>
</div>

<p>If you observe that the model is overfitting, you can increase the dropout rate. Conversely, you should try decreasing the dropout rate if the model underfits the training set. It can also help to increase the dropout rate for large layers, and reduce it for small ones. Moreover, many state-of-the-art architectures only use dropout after the last hidden layer, so you may want to try this if full dropout is too strong.</p>

<p>Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. So, it is generally well worth the extra time and effort.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If you want to regularize a self-normalizing network based on the SELU activation function (as discussed earlier), you should use <code>AlphaDropout</code>: this is a variant of dropout that preserves the mean and standard deviation of its inputs (it was introduced in the same paper as SELU, as regular dropout would break self-normalization).</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Monte-Carlo (MC) Dropout"><div class="sect2" id="idm46263510477976">
<h2>Monte-Carlo (MC) Dropout</h2>

<p>In 2016, a <a href="https://homl.info/mcdropout">paper</a><sup><a data-type="noteref" id="idm46263510245016-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263510245016">25</a></sup> by Yarin Gal and Zoubin Ghahramani added more good reasons to use dropout:</p>

<ul>
<li>
<p>First, the paper established a profound connection between dropout networks (i.e., neural networks containing a dropout layer before every weight layer) and approximate Bayesian inference,<sup><a data-type="noteref" id="idm46263510242536-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263510242536">26</a></sup> giving dropout a solid mathematical justification.</p>
</li>
<li>
<p>Second, the authors introduced a powerful technique called <em>MC Dropout</em>, which can boost the performance of any trained dropout model, without having to retrain it or even modify it at all!</p>
</li>
<li>
<p>Third, the paper shows that MC Dropout provides a much better measure of the model’s uncertainty.</p>
</li>
<li>
<p>Finally, it is also amazingly simple to implement. If this all sounds like a “one weird trick” advertisement, then take a look at the following code. It is the full implementation of <em>MC Dropout</em>, boosting the dropout model we trained earlier, without retraining it:</p>
</li>
</ul>

<pre data-type="programlisting" data-code-language="python"><code class="n">y_probas</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">stack</code><code class="p">([</code><code class="n">model</code><code class="p">(</code><code class="n">X_test_scaled</code><code class="p">,</code> <code class="n">training</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
                     <code class="k">for</code> <code class="n">sample</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">100</code><code class="p">)])</code>
<code class="n">y_proba</code> <code class="o">=</code> <code class="n">y_probas</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code></pre>

<p>We just make 100 predictions over the test set, setting <code>training=True</code> to ensure that the <code>Dropout</code> layer is active, and we stack the predictions. Since dropout is active, all predictions will be different. Recall that <code>predict()</code> returns a matrix with one row per instance, and one column per class. Since there are 10,000 instances in the test set, and 10 classes, this is a matrix of shape [10000, 10]. We stack 100 such matrices, so <code>y_probas</code> is an array of shape [100, 10000, 10]. Once we average over the first dimension (<code>axis=0</code>), we get <code>y_proba</code>, an array of shape [10000, 10], like we would get with a single prediction. That’s all! Averaging over multiple predictions with dropout on gives us a Monte Carlo estimate that is generally more reliable than the result of a single prediction with dropout off. For example, let’s look at the model’s prediction for the first instance in the test set, with dropout off:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">np</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test_scaled</code><code class="p">[:</code><code class="mi">1</code><code class="p">]),</code> <code class="mi">2</code><code class="p">)</code>
<code class="go">array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],</code>
<code class="go">      dtype=float32)</code></pre>

<p>The model seems almost certain that this image belongs to class 9 (ankle boot). Should you trust it? Is there really so little room for doubt? Compare this with the predictions made when dropout is activated:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">np</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="n">y_probas</code><code class="p">[:,</code> <code class="p">:</code><code class="mi">1</code><code class="p">],</code> <code class="mi">2</code><code class="p">)</code>
<code class="go">array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.17, 0.  , 0.68]],</code>
<code class="go">       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.2 , 0.  , 0.64]],</code>
<code class="go">       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.01, 0.  , 0.97]],</code>
<code class="go">       [...]</code></pre>

<p>This tells a very different story: apparently, when we activate dropout, the model is not sure anymore. It still seems to prefer class 9, but sometimes it hesitates with classes 5 (sandal) and 7 (sneaker), which makes sense, given they’re all footwear. Once we average over the first dimension, we get the following MC dropout predictions:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">np</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="n">y_proba</code><code class="p">[:</code><code class="mi">1</code><code class="p">],</code> <code class="mi">2</code><code class="p">)</code>
<code class="go">array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.22, 0.  , 0.16, 0.  , 0.62]],</code>
<code class="go">      dtype=float32)</code></pre>

<p>The model still thinks this image belongs to class 9, but only with a 62% confidence, which seems much more reasonable than 99%. Plus it’s useful to know exactly which other classes it thinks are likely. And you can also take a look at the <a href="https://xkcd.com/2110">standard deviation of the probability estimates</a>:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_std</code> <code class="o">=</code> <code class="n">y_probas</code><code class="o">.</code><code class="n">std</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">np</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="n">y_std</code><code class="p">[:</code><code class="mi">1</code><code class="p">],</code> <code class="mi">2</code><code class="p">)</code>
<code class="go">array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.21, 0.02, 0.32]],</code>
<code class="go">      dtype=float32)</code></pre>

<p>Apparently there’s quite a lot of variance in the probability estimates: if you were building a risk-sensitive system (e.g., a medical or financial system), you should probably treat such an uncertain prediction with extreme caution. You definitely would not treat it like a 99% confident prediction. Moreover, the model’s accuracy got a small boost from 86.8 to 86.9:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">accuracy</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">y_pred</code> <code class="o">==</code> <code class="n">y_test</code><code class="p">)</code> <code class="o">/</code> <code class="nb">len</code><code class="p">(</code><code class="n">y_test</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">accuracy</code>
<code class="go">0.8694</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The number of Monte Carlo samples you use (100 in this example) is a hyperparameter you can tweak. The higher it is, the more accurate the predictions and their uncertainty estimates will be. However, if you double it, inference time will also be doubled. Moreover, above a certain number of samples, you will notice little improvement. So your job is to find the right trade-off between latency and accuracy, depending on your application.</p>
</div>

<p>If your model contains other layers that behave in a special way during training (such as Batch Normalization layers), then you should not force training mode like we just did. Instead, you should replace the <code>Dropout</code> layers with the following <code>MCDropout</code> class:<sup><a data-type="noteref" id="idm46263509917848-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263509917848">27</a></sup></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">MCDropout</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dropout</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf">call</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">):</code>
        <code class="k">return</code> <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="n">call</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">training</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code></pre>

<p>We just subclass the <code>Dropout</code> layer and override the <code>call()</code> method to force its <code>training</code> argument to <code>True</code> (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch12.html#tensorflow_chapter">Chapter&nbsp;12</a>). Similarly, you could define an <code>MCAlphaDropout</code> class by subclassing <code>AlphaDropout</code> instead. If you are creating a model from scratch, it’s just a matter of using <code>MCDropout</code> rather than <code>Dropout</code>. But if you have a model that was already trained using <code>Dropout</code>, you need to create a new model that’s identical to the existing model except that it replaces the <code>Dropout</code> layers with <code>MCDropout</code>, then copy the existing model’s weights to your new model.</p>

<p>In short, MC Dropout is a fantastic technique that boosts dropout models and provides better uncertainty estimates. And of course, since it is just regular dropout during training, it also acts like a regularizer.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Max-Norm Regularization"><div class="sect2" id="idm46263510246568">
<h2>Max-Norm Regularization</h2>

<p>Another regularization technique that is popular for neural networks is called <em>max-norm regularization</em>: for each neuron, it constrains the weights <strong>w</strong> of the incoming connections such that ∥&nbsp;*w*&nbsp;∥<sub>2</sub> ≤ <em>r</em>, where <em>r</em> is the max-norm hyperparameter and <span class="keep-together">∥ · ∥<sub>2</sub></span> is the ℓ<sub>2</sub> norm.</p>

<p>Max-norm regularization does not add a regularization loss term to the overall loss function. Instead, it is typically implemented by computing ∥<strong>w</strong>∥<sub>2</sub> after each training step and rescaling <strong>w</strong> if needed (<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-141-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;mfrac&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;msub&gt;&lt;mfenced open=&quot;&amp;#x2225;&quot; close=&quot;&amp;#x2225;&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;w&lt;/mi&gt;&lt;/mfenced&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5679" style="width: 5.247em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.093em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.288em, 1005.1em, 2.933em, -1000.01em); top: -2.157em; left: 0em;"><span class="mrow" id="MathJax-Span-5680"><span class="mrow" id="MathJax-Span-5681"><span class="mi" id="MathJax-Span-5682" style="font-family: MathJax_Main-bold;">w</span><span class="mo" id="MathJax-Span-5683" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="mi" id="MathJax-Span-5684" style="font-family: MathJax_Main-bold; padding-left: 0.26em;">w</span><span class="mfrac" id="MathJax-Span-5685"><span style="display: inline-block; position: relative; width: 1.699em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.55em, 1000.32em, 4.167em, -1000.01em); top: -4.419em; left: 50%; margin-left: -0.152em;"><span class="mi" id="MathJax-Span-5686" style="font-size: 70.7%; font-family: MathJax_Math-italic;">r</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.345em, 1001.61em, 4.373em, -1000.01em); top: -3.596em; left: 50%; margin-left: -0.82em;"><span class="msub" id="MathJax-Span-5687"><span style="display: inline-block; position: relative; width: 1.596em; height: 0px;"><span style="position: absolute; clip: rect(3.345em, 1001.19em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mfenced" id="MathJax-Span-5688"><span class="mo" id="MathJax-Span-5689" style=""><span><span style="font-size: 70.7%; font-family: MathJax_Main;">∥</span></span></span><span class="mi" id="MathJax-Span-5690" style="font-size: 70.7%; font-family: MathJax_Main-bold;">w</span><span class="mo" id="MathJax-Span-5691" style=""><span><span style="font-size: 70.7%; font-family: MathJax_Main;">∥</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 1.288em;"><span class="mn" id="MathJax-Span-5692" style="font-size: 50%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1001.71em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.699em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.162em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.686em; border-left: 0px solid; width: 0px; height: 1.538em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">w</mi><mo>←</mo><mi mathvariant="bold">w</mi><mfrac><mi>r</mi><msub><mfenced open="∥" close="∥"><mi mathvariant="bold">w</mi></mfenced><mn>2</mn></msub></mfrac></mrow></math></span></span><script type="math/mml" id="MathJax-Element-141"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow>
    <mi mathvariant="bold">w</mi>
    <mo>←</mo>
    <mi mathvariant="bold">w</mi>
    <mfrac><mi>r</mi> <msub><mfenced open="∥" close="∥"><mi mathvariant="bold">w</mi></mfenced> <mn>2</mn> </msub></mfrac>
  </mrow>
</math></script>).</p>

<p>Reducing <em>r</em> increases the amount of regularization and helps reduce overfitting. Max-norm regularization can also help alleviate the unstable gradients problems (if you are not using Batch Normalization).</p>

<p>To implement max-norm regularization in Keras, set every hidden layer’s <code>kernel_constraint</code> argument to a <code>max_norm()</code> constraint, with the appropriate max value like this:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"elu"</code><code class="p">,</code> <code class="n">kernel_initializer</code><code class="o">=</code><code class="s2">"he_normal"</code><code class="p">,</code>
                   <code class="n">kernel_constraint</code><code class="o">=</code><code class="n">keras</code><code class="o">.</code><code class="n">constraints</code><code class="o">.</code><code class="n">max_norm</code><code class="p">(</code><code class="mf">1.</code><code class="p">))</code></pre>

<p>After each training iteration, the model’s <code>fit()</code> method will call the object returned by <code>max_norm()</code>, passing it the layer’s weights and getting rescaled weights in return, which then replace the layer’s weights. As we will see in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch12.html#tensorflow_chapter">Chapter&nbsp;12</a>, you can define your own custom constraint function if you ever need to and use it as the <code>kernel_constraint</code>. You can also constrain the bias terms by setting the <code>bias_constraint</code> argument.</p>

<p>The <code>max_norm()</code> function has an <code>axis</code> argument that defaults to 0. A <code>Dense</code> layer usually has weights of shape [number of inputs, number of neurons], so using <code>axis=0</code> means that the max norm constraint will apply independently to each neuron’s weight vector. If you want to use max-norm with convolutional layers (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cnn_chapter">Chapter&nbsp;14</a>), make sure to set the <code>max_norm()</code> constraint’s <code>axis</code> argument appropriately (usually <code>axis=[0, 1, 2]</code>).</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary and Practical Guidelines"><div class="sect1" id="idm46263509792968">
<h1>Summary and Practical Guidelines</h1>

<p>In this chapter we have covered a wide range of techniques, and you may be wondering which ones you should use. This depends on the task, and there is no clear consensus yet, but I have found the configuration in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#default_deep_neural_network_config">Table&nbsp;11-3</a> to work fine in most cases, without requiring much hyperparameter tuning. That said, please do not consider these defaults as hard rules!</p>
<table id="default_deep_neural_network_config">
<caption><span class="label">Table 11-3. </span>Default DNN configuration</caption>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Default value</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Kernel initializer</p></td>
<td><p>He initialization</p></td>
</tr>
<tr>
<td><p>Activation function</p></td>
<td><p>ELU</p></td>
</tr>
<tr>
<td><p>Normalization</p></td>
<td><p>None if shallow; Batch Norm if deep</p></td>
</tr>
<tr>
<td><p>Regularization</p></td>
<td><p>Early stopping (+ℓ<sub>2</sub> reg. if needed)</p></td>
</tr>
<tr>
<td><p>Optimizer</p></td>
<td><p>Momentum optimization (or RMSProp or Nadam)</p></td>
</tr>
<tr>
<td><p>Learning rate schedule</p></td>
<td><p>1cycle</p></td>
</tr>
</tbody>
</table>

<p>If the network is a simple stack of dense layers, then it can self-normalize, and you should use the configuration in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#self_norm_deep_neural_network_config">Table&nbsp;11-4</a> instead.</p>
<table id="self_norm_deep_neural_network_config">
<caption><span class="label">Table 11-4. </span>DNN configuration for a self-normalizing net</caption>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Default value</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Kernel initializer</p></td>
<td><p>LeCun initialization</p></td>
</tr>
<tr>
<td><p>Activation function</p></td>
<td><p>SELU</p></td>
</tr>
<tr>
<td><p>Normalization</p></td>
<td><p>None (self-normalization)</p></td>
</tr>
<tr>
<td><p>Regularization</p></td>
<td><p>Alpha-Dropout if needed</p></td>
</tr>
<tr>
<td><p>Optimizer</p></td>
<td><p>Momentum optimization (or RMSProp or Nadam)</p></td>
</tr>
<tr>
<td><p>Learning rate schedule</p></td>
<td><p>1cycle</p></td>
</tr>
</tbody>
</table>

<p>Don’t forget to normalize the input features! You should also try to reuse parts of a pretrained neural network if you can find one that solves a similar problem, or use unsupervised pretraining if you have a lot of unlabeled data, or use pretraining on an auxiliary task if you have a lot of labeled data for a similar task.</p>

<p>While the previous guidelines should cover most cases, here are some exceptions:</p>

<ul>
<li>
<p>If you need a sparse model, you can use ℓ<sub>1</sub> regularization (and optionally zero out the tiny weights after training). If you need an even sparser model, you can use the TensorFlow Model Optimization Toolkit. This will break self-normalization, so you should use the default configuration in this case.</p>
</li>
<li>
<p>If you need a low-latency model (one that performs lightning-fast predictions), you may need to use fewer layers, fold the Batch Normalization layers into the previous layers, and possibly use faster activation functions such as leaky ReLU or just ReLU. Having a sparse model will also help. You may also want to reduce the float precision from 32-bits to 16-bits (or even 8-bits) (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#deployment_chapter">Chapter&nbsp;19</a>). Check out the optimization toolkit.</p>
</li>
<li>
<p>If you are building a risk-sensitive application, or inference latency is not very important in your application, you can use MC Dropout to boost performance and get more reliable probability estimates, along with uncertainty estimates.</p>
</li>
</ul>

<p>With these guidelines, you are now ready to train very deep nets! I hope you are now convinced that you can go a very long way using just Keras. There may come a time, however, when you need to have even more control, for example, to write a custom loss function or to tweak the training algorithm. For such cases, you will need to use TensorFlow’s lower-level API, as we will see in the next chapter.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="idm46263509727720">
<h1>Exercises</h1>
<ol>
<li>
<p>Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?</p>
</li>
<li>
<p>Is it OK to initialize the bias terms to 0?</p>
</li>
<li>
<p>Name three advantages of the SELU activation function over ReLU.</p>
</li>
<li>
<p>In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?</p>
</li>
<li>
<p>What may happen if you set the <code>momentum</code> hyperparameter too close to 1 (e.g., 0.99999) when using an <code>SGD</code> optimizer?</p>
</li>
<li>
<p>Name three ways you can produce a sparse model.</p>
</li>
<li>
<p>Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC dropout?</p>
</li>
<li>
<p>Let’s train a deep neural network on CIFAR10:</p>
<ol>
<li>
<p>Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.</p>
</li>
<li>
<p>Using Nadam optimization and early stopping, try training it on the CIFAR10 image dataset. You can load it using <code>keras.datasets.cifar10.load_data()</code>. It is composed of 60,000 32 × 32 color images (50,000 for training, 10,000 for testing) with 10 classes, so you will need a softmax output layer with 10 neurons. Remember to search for the right learning rate every time you change the model’s architecture or hyperparameters.</p>
</li>
<li>
<p>Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model? How does it affect training speed?</p>
</li>
<li>
<p>Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).</p>
</li>
<li>
<p>Try regularizing the model with alpha dropout. Then without retraining your model, see if you can achieve better accuracy using MC Dropout.</p>
</li>
<li>
<p>Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy.</p>
</li>

</ol>
</li>

</ol>

<p>Solutions to these exercises are available in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/app01.html#solutions_appendix">Appendix&nbsp;A</a>.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46263512862504"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512862504-marker" class="totri-footnote">1</a></sup> Xavier Glorot and Yoshua Bengio, “Understanding the Difficulty of Training Deep Feedforward Neural Networks” in proceedings of Artificial Intelligence and Statistics 2010: 249–256.</p><p data-type="footnote" id="idm46263512854264"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512854264-marker" class="totri-footnote">2</a></sup> Here’s an analogy: if you set a microphone amplifier’s knob too close to zero, people won’t hear your voice, but if you set it too close to the max, your voice will be saturated and people won’t understand what you are saying. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come out loud and clear at the end of the chain. Your voice has to come out of each amplifier at the same amplitude as it came in.</p><p data-type="footnote" id="idm46263512823880"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512823880-marker" class="totri-footnote">3</a></sup> “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,” K. He et al. (2015).</p><p data-type="footnote" id="idm46263512720088"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512720088-marker" class="totri-footnote">4</a></sup> Unless it is part of the first hidden layer, a dead neuron may sometimes come back to life: gradient descent may indeed tweak neurons in the layers below in such a way that the weighted sum of the dead neuron’s inputs is positive again.</p><p data-type="footnote" id="idm46263512713768"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512713768-marker" class="totri-footnote">5</a></sup> Bing Xu et al., “Empirical Evaluation of Rectified Activations in Convolutional Network” (2015).</p><p data-type="footnote" id="idm46263512706248"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512706248-marker" class="totri-footnote">6</a></sup> Djork-Arné Clevert et al., “Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)” in <em>ICLR</em> (2016).</p><p data-type="footnote" id="idm46263512642504"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512642504-marker" class="totri-footnote">7</a></sup> Günter Klambauer et al., “Self-Normalizing Neural Networks,” in <em>Advances in Neural Information Processing Systems (NIPS)</em> (2017).</p><p data-type="footnote" id="idm46263512518408"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512518408-marker" class="totri-footnote">8</a></sup> Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift” in <em>Proceedings of the 25th International Conference on Machine Learning</em> (2015): 448–456.</p><p data-type="footnote" id="idm46263512192504"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263512192504-marker" class="totri-footnote">9</a></sup> However, they are estimated during training, based on the training data, so arguably they <em>are</em> trainable. In Keras, “Non-trainable” really means “untouched by backpropagation.”</p><p data-type="footnote" id="idm46263511970696"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511970696-marker">10</a></sup> The Keras API also specifies a <code>keras.backend.learning_phase()</code> function that should return 1 during training and 0 otherwise.</p><p data-type="footnote" id="idm46263511967752"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511967752-marker">11</a></sup> Hongyi Zhang et al., “Fixup Initialization: Residual Learning Without Normalization,” <em>ICLR</em> (2019).</p><p data-type="footnote" id="idm46263511964024"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511964024-marker">12</a></sup> Razvan Pascanu et al., “On the Difficulty of Training Recurrent Neural Networks” in <em>Proceedings of the 25th International Conference on Machine Learning</em> (2013): 1310–1318.</p><p data-type="footnote" id="idm46263511522936"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511522936-marker">13</a></sup> Boris T. Polyak, “Some Methods of Speeding Up the Convergence of Iteration Methods,” <em>USSR Computational Mathematics and Mathematical Physics</em> 4, no. 5 (December 1964): 1–7.</p><p data-type="footnote" id="idm46263511415816"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511415816-marker">14</a></sup> “A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence O(1/k<sup>2</sup>),” Yurii Nesterov (1983).</p><p data-type="footnote" id="idm46263511332376"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511332376-marker">15</a></sup> John Duchi et al., “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,” <em>JMLR</em> 12 (July 2011): 2121–2159.</p><p data-type="footnote" id="idm46263511249672"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511249672-marker">16</a></sup> This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012 and presented by Geoffrey Hinton in his Coursera class on neural networks (slides: <a href="https://homl.info/57"><em class="hyperlink">https://homl.info/57</em></a>; video: <a href="https://homl.info/58"><em class="hyperlink">https://homl.info/58</em></a>). Amusingly, since the authors did not write a paper to describe the algorithm, researchers often cite “slide 29 in lecture 6” in their papers.</p><p data-type="footnote" id="idm46263511185320"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511185320-marker">17</a></sup> Diederik P. Kingma and Jimmy Ba, “Adam: A Method for Stochastic Optimization,” <em>ICLR</em> (2015).</p><p data-type="footnote" id="idm46263511182680"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511182680-marker">18</a></sup> These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the <em>first moment</em>, while the variance is often called the <em>second moment</em>, hence the name of the algorithm.</p><p data-type="footnote" id="idm46263511034664"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511034664-marker">19</a></sup> Timothy Dozat, “Incorporating Nesterov Momentum into Adam” (presented as a workshop at International Conference on Machine Learning (2016).</p><p data-type="footnote" id="idm46263511030792"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263511030792-marker">20</a></sup> Ashia C. Wilson et al., “The Marginal Value of Adaptive Gradient Methods in Machine Learning” (presentation, 31st Conference on Neural Information Processing Systems, Long Beach, CA, 2017).</p><p data-type="footnote" id="idm46263510940008"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263510940008-marker">21</a></sup> Smith, <em>A Disciplined Approach to Neural Network Hyper-Parameters: Part 1 — Learning Rate, Batch Size, Momentum, and Weight Decay</em>.</p><p data-type="footnote" id="idm46263510934680"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263510934680-marker">22</a></sup> Andrew Senior et al., <em>An Empirical Study of Learning Rates in Deep Neural Networks for Speech Recognition</em> (New York: Google Inc., 2013).</p><p data-type="footnote" id="idm46263510359688"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263510359688-marker">23</a></sup> Geoffrey E. Hinton et al., “Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors” (2012).</p><p data-type="footnote" id="idm46263510358344"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263510358344-marker">24</a></sup> Nitish Srivastava et al., “Dropout: A Simple Way to Prevent Neural Networks from Overfitting,” <em>JMLR</em> 15 (June 2014): 1929–1958.</p><p data-type="footnote" id="idm46263510245016"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263510245016-marker">25</a></sup> Yarin Gal and Zoubin Ghahramani, “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning,” <em>PMLR</em> 48 (2016): 1050–1059.</p><p data-type="footnote" id="idm46263510242536"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263510242536-marker">26</a></sup> Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian inference in a specific type of probabilistic model called a <em>Deep Gaussian Process</em>.</p><p data-type="footnote" id="idm46263509917848"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#idm46263509917848-marker">27</a></sup> This <code>MCDropout</code> class will work with all Keras APIs, including the Sequential API. If you only care about the Functional API or the Subclassing API, you do not have to create an <code>MCDropout</code> class; you can create a regular <code>Dropout</code> layer, and call it with <code>training=True</code>.</p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-12" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders">
		
		<li class="copy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#">Add Highlight</a></li>
		<li class="add-note"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#">
			Add Note
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">10. Introduction to Artificial Neural Networks with Keras</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch12.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">12. Custom Models and Training with TensorFlow</div>
        </a>
    
  
  </div>

</section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    



        
      </div>
      



  <footer class="pagefoot">
    <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#" class="icon-up" style="display: none;"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      <li class="full-support"><a href="https://www.oreilly.com/online-learning/support/">Support</a></li>
      <li><a href="https://www.oreilly.com/online-learning/apps.html">Get the App</a></li>
      
        <li><a href="https://learning.oreilly.com/accounts/logout/">Sign Out</a></li>
      
    </ul>
    <span class="copyright">© 2019 <a href="https://learning.oreilly.com/" target="_blank">Safari</a>.</span>
    <a href="https://learning.oreilly.com/terms/">Terms of Service</a> /
    <a href="https://learning.oreilly.com/membership-agreement/">Membership Agreement</a> /
    <a href="https://www.oreilly.com/privacy.html">Privacy Policy</a>
  </footer>

<script type="text/javascript">window.NREUM||(NREUM={});NREUM.info={"applicationID":"172641827,79672898,93931619","errorBeacon":"bam.nr-data.net","agent":"","applicationTime":451,"licenseKey":"510f1a6865","transactionName":"YgdaZ0NSW0cEB0RdWltNfkZfUEFdCgofXFBHDVYdR1pQQxZeRl1QQj1aWkU=","beacon":"bam.nr-data.net","queueTime":4}</script>


    
    <script src="./Chapter11_files/saved_resource" charset="utf-8"></script><script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","1732687426968531");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=1732687426968531&amp;ev=PageView&amp;noscript=1"></noscript><script type="text/javascript" id="">(function(){window.medalliaUserIdentifier=document.documentElement.dataset.userUuid;window.medalliaUserName=document.documentElement.dataset.username})();</script>
<script type="text/javascript" id="" src="./Chapter11_files/embed.js.download"></script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script>
<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("set","agent","tmgoogletagmanager","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>

<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>
<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.09556792590964802"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.5229133033930915" width="0" height="0" alt="" src="./Chapter11_files/0"></div>
    <script src="./Chapter11_files/saved_resource(1)" charset="utf-8"></script>
  

<script src="./Chapter11_files/saved_resource(2)" type="text/javascript"></script><script type="text/javascript" id="">window._pp=window._pp||[];if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/register/")_pp.targetUrl="/confirm/trial";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/subscribe/")_pp.targetUrl="/confirm/paid";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/signup/")_pp.targetUrl="/confirm/paid";_pp.siteId="2508";
_pp.siteUId="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79";_pp.orderValue="undefined";_pp.orderId="undefined";(function(){var ppjs=document.createElement("script");ppjs.type="text/javascript";ppjs.async=true;ppjs.src=("https:"==document.location.protocol?"https:":"http:")+"//cdn.pbbl.co/r/"+_pp.siteId+".js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(ppjs,s)})();</script><div class="annotator-notice"></div><div class="font-flyout" style="top: 201px; left: 1194px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#">Reset</a>
</div>
</div><script type="text/javascript" async="" src="./Chapter11_files/generic1566415868241.js.download" charset="UTF-8"></script><div style="display: none; visibility: hidden;"><script>(function(){if(null!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')&&void 0!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')){var a=!1;window.addEventListener("blur",function(){a&&dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"facebook",eventVal:0,nonInteraction:0})});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseover",function(){window.focus();
a=!0});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseout",function(){a=!1})}try{window.twttr=function(b,a,d){var c,e=b.getElementsByTagName(a)[0];if(!b.getElementById(d))return b=b.createElement(a),b.id=d,b.src="//platform.twitter.com/widgets.js",e.parentNode.insertBefore(b,e),window.twttr||(c={_e:[],ready:function(a){c._e.push(a)}})}(document,"script","twitter-wjs"),twttr.ready(function(a){a.events.bind("tweet",trackTwitter)})}catch(b){}})();
null!==document.querySelector(".IN-widget")&&void 0!==document.querySelector(".IN-widget")&&document.querySelector(".IN-widget").addEventListener("click",function(){dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"linkedin",eventVal:0,nonInteraction:0})});
function trackTwitter(a){a&&(a.target&&"IFRAME"==a.target.nodeName&&(opt_target=extractParamFromUri(a.target.src,"url")),dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"twitter",eventVal:0,nonInteraction:0}))}function extractParamFromUri(a,b){if(a){var c=new RegExp("[\\?\x26#]"+b+"\x3d([^\x26#]*)");c=c.exec(a);if(null!=c)return unescape(c[1])}};</script></div><span><div id="KampyleAnimationContainer" style="z-index: 2147483000; border: 0px; position: fixed; display: block; width: 0px; height: 0px;"></div></span><iframe scrolling="no" frameborder="0" allowtransparency="true" src="./Chapter11_files/widget_iframe.097c1f5038f9e8a0d62a39a892838d66.html" title="Twitter settings iframe" style="display: none;"></iframe><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_AMS, sans-serif;"></div></div></body></html>