<!DOCTYPE html>
<!-- saved from url=(0091)https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html -->
<html class=" js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/hands-on-machine-learning/9781492032632/part01.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="4626953" data-user-uuid="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79" data-username="17481074" data-account-type="B2B" data-activated-trial-date="" data-archive="9781492032632" data-publishers="O&#39;Reilly Media, Inc." data-htmlfile-name="part01.html" data-epub-title="Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781492032632"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><script type="text/javascript" async="" src="./Chapter15_files/cool-2.1.15.min.js.download"></script><script type="text/javascript" src="./Chapter15_files/510f1a6865"></script><script id="twitter-wjs" src="./Chapter15_files/widgets.js.download"></script><script src="./Chapter15_files/nr-1130.min.js.download"></script><script type="text/javascript" async="" src="./Chapter15_files/2508.js.download"></script><script async="" src="./Chapter15_files/fbevents.js.download"></script><script type="text/javascript" async="" src="./Chapter15_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter15_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter15_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter15_files/analytics.js.download"></script><script type="text/javascript" async="" src="./Chapter15_files/ec.js.download"></script><script type="text/javascript" async="" src="./Chapter15_files/bat.js.download"></script><script type="text/javascript" async="" src="./Chapter15_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter15_files/insight.min.js.download"></script><script type="text/javascript" async="" src="./Chapter15_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter15_files/linkid.js.download"></script><script async="" src="./Chapter15_files/gtm.js.download"></script><script async="" src="./Chapter15_files/analytics.js.download"></script><script type="text/javascript">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if("function"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e("handle"),a=e(3),u=e(4),f=e("ee").get("tracer"),c=e("loader"),s=NREUM;"undefined"==typeof window.newrelic&&(newrelic=s);var p=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],d="api-",l=d+"ixn-";a(p,function(e,n){s[n]=o(d+n,!0,"api")}),s.addPageAction=o(d+"addPageAction",!0),s.setCurrentRouteName=o(d+"routeName",!0),n.exports=newrelic,s.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(e,n){var t={},r=this,o="function"==typeof n;return i(l+"tracer",[c.now(),e,t],r),function(){if(f.emit((o?"":"no-")+"fn-start",[c.now(),r,o],t),o)try{return n.apply(this,arguments)}catch(e){throw f.emit("fn-err",[arguments,this,e],t),e}finally{f.emit("fn-end",[c.now()],t)}}}};a("actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(e,n){m[n]=o(l+n)}),newrelic.noticeError=function(e,n){"string"==typeof e&&(e=new Error(e)),i("err",[e,c.now(),!1,n])}},{}],2:[function(e,n,t){function r(e,n){if(!o)return!1;if(e!==o)return!1;if(!n)return!0;if(!i)return!1;for(var t=i.split("."),r=n.split("."),a=0;a<r.length;a++)if(r[a]!==t[a])return!1;return!0}var o=null,i=null,a=/Version\/(\S+)\s+Safari/;if(navigator.userAgent){var u=navigator.userAgent,f=u.match(a);f&&u.indexOf("Chrome")===-1&&u.indexOf("Chromium")===-1&&(o="Safari",i=f[1])}n.exports={agent:o,version:i,match:r}},{}],3:[function(e,n,t){function r(e,n){var t=[],r="",i=0;for(r in e)o.call(e,r)&&(t[i]=n(r,e[r]),i+=1);return t}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],4:[function(e,n,t){function r(e,n,t){n||(n=0),"undefined"==typeof t&&(t=e?e.length:0);for(var r=-1,o=t-n||0,i=Array(o<0?0:o);++r<o;)i[r]=e[n+r];return i}n.exports=r},{}],5:[function(e,n,t){n.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(e,n,t){function r(){}function o(e){function n(e){return e&&e instanceof r?e:e?f(e,u,i):i()}function t(t,r,o,i){if(!d.aborted||i){e&&e(t,r,o);for(var a=n(o),u=v(t),f=u.length,c=0;c<f;c++)u[c].apply(a,r);var p=s[y[t]];return p&&p.push([b,t,r,a]),a}}function l(e,n){h[e]=v(e).concat(n)}function m(e,n){var t=h[e];if(t)for(var r=0;r<t.length;r++)t[r]===n&&t.splice(r,1)}function v(e){return h[e]||[]}function g(e){return p[e]=p[e]||o(t)}function w(e,n){c(e,function(e,t){n=n||"feature",y[t]=n,n in s||(s[n]=[])})}var h={},y={},b={on:l,addEventListener:l,removeEventListener:m,emit:t,get:g,listeners:v,context:n,buffer:w,abort:a,aborted:!1};return b}function i(){return new r}function a(){(s.api||s.feature)&&(d.aborted=!0,s=d.backlog={})}var u="nr@context",f=e("gos"),c=e(3),s={},p={},d=n.exports=o();d.backlog=s},{}],gos:[function(e,n,t){function r(e,n,t){if(o.call(e,n))return e[n];var r=t();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,n,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return e[n]=r,r}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],handle:[function(e,n,t){function r(e,n,t,r){o.buffer([e],r),o.emit(e,n,t)}var o=e("ee").get("handle");n.exports=r,r.ee=o},{}],id:[function(e,n,t){function r(e){var n=typeof e;return!e||"object"!==n&&"function"!==n?-1:e===window?0:a(e,i,function(){return o++})}var o=1,i="nr@id",a=e("gos");n.exports=r},{}],loader:[function(e,n,t){function r(){if(!E++){var e=x.info=NREUM.info,n=l.getElementsByTagName("script")[0];if(setTimeout(s.abort,3e4),!(e&&e.licenseKey&&e.applicationID&&n))return s.abort();c(y,function(n,t){e[n]||(e[n]=t)}),f("mark",["onload",a()+x.offset],null,"api");var t=l.createElement("script");t.src="https://"+e.agent,n.parentNode.insertBefore(t,n)}}function o(){"complete"===l.readyState&&i()}function i(){f("mark",["domContent",a()+x.offset],null,"api")}function a(){return O.exists&&performance.now?Math.round(performance.now()):(u=Math.max((new Date).getTime(),u))-x.offset}var u=(new Date).getTime(),f=e("handle"),c=e(3),s=e("ee"),p=e(2),d=window,l=d.document,m="addEventListener",v="attachEvent",g=d.XMLHttpRequest,w=g&&g.prototype;NREUM.o={ST:setTimeout,SI:d.setImmediate,CT:clearTimeout,XHR:g,REQ:d.Request,EV:d.Event,PR:d.Promise,MO:d.MutationObserver};var h=""+location,y={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-1130.min.js"},b=g&&w&&w[m]&&!/CriOS/.test(navigator.userAgent),x=n.exports={offset:u,now:a,origin:h,features:{},xhrWrappable:b,userAgent:p};e(1),l[m]?(l[m]("DOMContentLoaded",i,!1),d[m]("load",r,!1)):(l[v]("onreadystatechange",o),d[v]("onload",r)),f("mark",["firstbyte",u],null,"api");var E=0,O=e(5)},{}]},{},["loader"]);</script><link rel="apple-touch-icon" href="https://learning.oreilly.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://learning.oreilly.com/favicon.ico" type="image/x-icon"><link href="./Chapter15_files/css" rel="stylesheet" type="text/css"><title>15. Processing Sequences Using RNNs and CNNs - Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition</title><link rel="stylesheet" href="./Chapter15_files/output.68851547a55f.css" type="text/css"><link rel="stylesheet" type="text/css" href="./Chapter15_files/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="./Chapter15_files/font-awesome.min.css"><style type="text/css" title="ibis-book">@charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000 !important;padding-top:10px !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:2em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content p[data-type="footnote"]{font-size:90% !important;line-height:1.2em !important;margin-left:2.5em !important;text-indent:-2.3em !important}#sbo-rt-content p[data-type="footnote"] sup{display:inline-block !important;position:static !important;width:2em !important;text-align:right !important;font-size:100% !important;padding-right:.5em !important}#sbo-rt-content p[data-type="footnote"] a[href$="-marker"]{font-family:sans-serif !important;font-size:90% !important;color:#8e0012 !important}#sbo-rt-content p[data-type="footnote"] a[data-type="xref"]{margin:0 !important;padding:0 !important;text-indent:0 !important}#sbo-rt-content a[data-type="noteref"]{font-family:sans-serif !important;color:#8e0012;margin-left:0;padding-left:0}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dd{margin-left:1.5em !important;margin-bottom:.25em}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul>li,#sbo-rt-content ol ul,#sbo-rt-content ol ul>li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul>li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul>li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul>li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol>li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol>li,#sbo-rt-content ul ol,#sbo-rt-content ul ol>li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol>li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol>li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:middle;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content table.border tbody>tr:last-child>td{border-bottom:transparent}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:1em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content .width-10{width:10vw !important}#sbo-rt-content .width-20{width:20vw !important}#sbo-rt-content .width-30{width:30vw !important}#sbo-rt-content .width-40{width:40vw !important}#sbo-rt-content .width-50{width:50vw !important}#sbo-rt-content .width-60{width:60vw !important}#sbo-rt-content .width-70{width:70vw !important}#sbo-rt-content .width-80{width:80vw !important}#sbo-rt-content .width-90{width:90vw !important}#sbo-rt-content .width-full,#sbo-rt-content .width-100{width:100vw !important}#sbo-rt-content div[data-type="equation"].fifty-percent img{width:50%}</style><script> // <![CDATA[
    var g = {
      position_cache: {
        
          "chapter": "/api/v1/book/9781492032632/chapter/part01.html",
          "book_id": "9781492032632",
          "chapter_uri": "part01.html",
          "position": 100.0,
          "user_uuid": "d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79",
          "next_chapter_uri": "/library/view/hands-on-machine-learning/9781492032632/ch01.html"
        
      },
      title: "Hands\u002Don Machine Learning with Scikit\u002DLearn, Keras, and TensorFlow, 2nd Edition",
      author_list: "Aurélien Géron",
      format: "book",
      source: "application/epub+zip",
      is_system_book: true,
      is_public: false,
      loaded_from_server: true,
      allow_scripts: false,
      has_mathml: true
    };
    // ]]></script><script src="./Chapter15_files/modernizr.8e35451ddb64.js.download"></script><script>
    
      

      
        
          window.PUBLIC_ANNOTATIONS = true;
        
      

      window.MOBILE_PUBLIC_ANNOTATIONS = false;

    

    
      window.PRIVACY_CONTROL_OVERRIDE = false;
    

      window.PRIVACY_CONTROL_SWITCH = true;

      window.PUBLISHER_PAGES = true;

      window.SBO = {
        "constants": {
          "SITB_ENDPOINT": "/api/v2/sitb/",
          "SEARCH_SELECT_ENDPOINT": "https://learning.oreilly.com/api/v2/search/select/",
          "ENABLE_ONLINE_TRAINING": false
        }
      };
  </script><link rel="canonical" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta name="description" content=" Part I. The Fundamentals of Machine Learning "><meta property="og:title" content="I. The Fundamentals of Machine Learning"><meta itemprop="isPartOf" content="/library/view/hands-on-machine-learning/9781492032632/"><meta itemprop="name" content="I. The Fundamentals of Machine Learning"><meta property="og:url" itemprop="url" content="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://learning.oreilly.com/library/cover/9781492032632/"><meta property="og:description" itemprop="description" content=" Part I. The Fundamentals of Machine Learning "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O&#39;Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781492032649"><meta property="og:book:author" itemprop="author" content="Aurélien Géron"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: &lt;%= font_size %&gt; !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: &lt;%= font_family %&gt; !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: &lt;%= column_width %&gt;% !important; margin: 0 auto !important; }"></style><noscript><meta http-equiv="refresh" content="0; url=/library/no-js/" /></noscript><script type="text/javascript">
    (function(i,s,o,g,r,a,m) {
      i['GoogleAnalyticsObject']=r;
      i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();
      a=s.createElement(o),m=s.getElementsByTagName(o)[0];
      a.async=1;
      a.src=g;
      m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    var matches = document.cookie.match(/BrowserCookie\s*=\s*([a-f0-9\-]{36})/),
      user_uuid = null;

    if (matches && matches.length === 2) {
      user_uuid = matches[1];
    }

  
    ga('create', 'UA-39299553-7', {'userId': 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79' });
  

  
    
      ga('set', 'dimension1', 'B2B');
    
  

  ga('set', 'dimension6', user_uuid);

  
    ga('set', 'dimension2', 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79');
    
      ga('set', 'dimension7', '0012M0000229keZQAQ');
    
  

  

  

  //enable enhanced link tracking
  ga('require', 'linkid', 'linkid.js');

  // reading interface will track pageviews itself
  if (document.location.pathname.indexOf("/library/view") !== 0) {
    ga('send', 'pageview');
  }
  </script><script>
    var dataLayer = window.dataLayer || [];

    
      window.medalliaVsgUserIdentifier = 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79';
      dataLayer.push({userIdentifier: 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79'});
      dataLayer.push({loggedIn: 'yes'});

      
        window.medalliaVsgAccountIdentifier = '21bed0a7-6b7b-470c-8fa0-40a52db0b491';
        
        dataLayer.push({orgID: '21bed0a7-6b7b-470c-8fa0-40a52db0b491'});
        

        window.medalliaVsgIsIndividual = false;
        
          
          dataLayer.push({learningAccountType: 'enterprise'});
          
        

        
          dataLayer.push({learningPaidAccount: 'yes'});
        
      
    

    (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-5P4V6Z');
    (function () {
      var VERSION = 'V1.1';
      var AUTHOR = 'Awwad';
      if (!window.GtmHelper)
        window.GtmHelper = function () {
          var instance = this;
          var loc = document.location;
          this.version = VERSION;
          this.author = AUTHOR;
          this.readCookie = function (name) {
            var nameEQ = name + "=";
            var ca = document.cookie.split(';');
            for (var i = 0; i < ca.length; i++) {
              var c = ca[i];
              while (c.charAt(0) == ' ') c = c.substring(1, c.length);
              if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length, c.length);
            }
            return null;
          };
          this.createCookie = function (name, value, days, cookieDomain) {
            var domain = "";
            var expires = "";

            if (days) {
              var date = new Date();
              date.setTime(date.getTime() + Math.ceil(days * 24 * 60 * 60 * 1000));
              var expires = " expires=" + date.toGMTString() + ";";
            }

            if (typeof (cookieDomain) != 'undefined')
              domain = " domain=" + cookieDomain + "; ";

            document.cookie = name + "=" + value + ";" + expires + domain + "path=/";
          };

          this.isDuplicated = function (currentTransactionId) {
            // the previous transaction id:
            var previousTransIdValue = this.readCookie("previousTransId");

            if (currentTransactionId === previousTransIdValue) {
              return true; // Duplication
            } else {
              return false;
            }
          };
        }
    })()
  </script><script defer="" src="./Chapter15_files/vendor.a48a756c5182.js.download"></script><script defer="" src="./Chapter15_files/reader.f2a0c6bd2fee.js.download"></script><script src="./Chapter15_files/f(1).txt"></script><script src="./Chapter15_files/f(2).txt"></script><script src="./Chapter15_files/f(3).txt"></script><script src="./Chapter15_files/f(4).txt"></script><script async="" src="./Chapter15_files/MathJax.js.download"></script><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 2147483020;
}
.annotator-filter {
  z-index: 2147483010;
}</style><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
</style><script async="true" type="text/javascript" src="./Chapter15_files/roundtrip.js.download"></script><style type="text/css" id="kampyleStyle">.noOutline{outline: none !important;}.wcagOutline:focus{outline: 1px dashed #595959 !important;outline-offset: 2px !important;transition: none !important;}</style><script async="true" type="text/javascript" src="./Chapter15_files/roundtrip.js.download"></script><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Math-bold-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Script; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_AMS; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf?V=2.7.1') format('opentype')}
</style></head>


<body class="reading sidenav  scalefonts library nav-collapsed"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

    
  <noscript> 
    <iframe src="//www.googletagmanager.com/ns.html?id=GTM-5P4V6Z"
            height="0" width="0"
            style="display:none;visibility:hidden">
    </iframe>
  </noscript>



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#container" class="skip">Skip to content</a><header class="topbar t-topbar"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://learning.oreilly.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M11.738 14H9.254v-3.676a.617.617 0 0 0-.621-.613H7.39a.617.617 0 0 0-.62.613V14H4.284a.617.617 0 0 1-.622-.613V10.22c0-.327.132-.64.367-.87l3.547-3.493a.627.627 0 0 1 .875 0l3.54 3.499c.234.229.366.54.367.864v3.167a.617.617 0 0 1-.62.613zM7.57 2.181a.625.625 0 0 1 .882 0l5.77 5.692-.93.92-5.28-5.209-5.28 5.208-.932-.919 5.77-5.692z"></path></svg><span>Safari Home</span></a></li><li><a href="https://learning.oreilly.com/resource-centers/" class="t-resource-centers-nav l0 nav-icn"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="16px" height="16px" viewBox="0 0 16 16" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="Topic-Page-Design" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="Heron-Button" transform="translate(-20.000000, -78.000000)" fill="#4A3A30"><g id="Group-9" transform="translate(20.000000, 78.000000)"><rect id="Rectangle" x="9.6" y="0" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="9.6" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="0" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect></g></g></g></svg><span>Resource Centers</span></a></li><li><a href="https://learning.oreilly.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://learning.oreilly.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://learning.oreilly.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://learning.oreilly.com/learning-paths/" class="l1 nav-icn t-learningpaths-nav js-toggle-menu-item"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="32px" height="32px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 52.5 (67469) - http://www.bohemiancoding.com/sketch --><title>Mask</title><desc>Created with Sketch.</desc><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><path d="M0,16.0214227 C0,15.0387209 0.796453294,14.2411658 1.77779753,14.2411658 C2.75914177,14.2411658 3.55559506,15.0387209 3.55559506,16.0214227 C3.55559506,17.0041246 2.75914177,17.8016797 1.77779753,17.8016797 C0.796453294,17.8016797 0,17.0041246 0,16.0214227 Z M9.77788642,5.22914885 C8.9280992,5.72049977 7.84008711,5.42853763 7.34941499,4.57757479 C6.85874287,3.72661195 7.15030167,2.63709467 8.00008889,2.14574375 C8.84987611,1.65439282 9.9378882,1.94635496 10.4285603,2.7973178 C10.9192324,3.64828064 10.6276736,4.73779792 9.77788642,5.22914885 Z M4.57213969,7.35869225 C5.42192691,7.85004318 5.71348571,8.93956046 5.22281359,9.79052329 C4.73214147,10.6414861 3.64412938,10.9334483 2.79434216,10.4420974 C1.94455494,9.95074642 1.65299614,8.86122915 2.14366826,8.01026631 C2.63434038,7.15930347 3.72235247,6.86734132 4.57213969,7.35869225 Z M2.79434216,21.6007481 C3.64412938,21.1093972 4.73214147,21.4013594 5.22281359,22.2523222 C5.71348571,23.103285 5.42192691,24.1928023 4.57213969,24.6841532 C3.72235247,25.1755042 2.63434038,24.883542 2.14366826,24.0325792 C1.65299614,23.1816163 1.94455494,22.0920991 2.79434216,21.6007481 Z M7.34941499,27.4652707 C7.84008711,26.6143079 8.9280992,26.3223457 9.77788642,26.8136966 C10.6276736,27.3050476 10.9192324,28.3945649 10.4285603,29.2455277 C9.9378882,30.0964905 8.84987611,30.3884527 8.00008889,29.8971017 C7.15030167,29.4057508 6.85874287,28.3162335 7.34941499,27.4652707 Z M18.7118524,11.3165596 C21.3074367,12.8173162 22.1963355,16.1392758 20.6976522,18.738451 C19.1989689,21.3358459 15.8815987,22.2259744 13.2860143,20.726998 C10.6922077,19.2262414 9.80330893,15.9042818 11.3002144,13.3051066 C12.7988978,10.7059314 16.116268,9.81580294 18.7118524,11.3165596 Z M26.7821642,27.8093944 L30.1315348,31.1633985 C30.3982044,31.4304371 30.2097579,31.8844026 29.8346426,31.8844026 L21.5945511,31.8844026 C21.1287681,31.8844026 20.751875,31.5069881 20.751875,31.0405608 L20.751875,22.7890697 C20.751875,22.4134355 21.2052134,22.2247282 21.4701052,22.4899865 L24.2843587,25.3081333 C26.8337204,23.0240636 28.4444049,19.7092251 28.4444049,16.0223129 C28.4444049,9.15052091 22.8621207,3.56051397 15.9998222,3.56051397 L15.9998222,0 C24.8230314,0 32,7.18689745 32,16.0223129 C32,20.6919269 29.9750886,24.8790914 26.7821642,27.8093944 Z" id="Mask" fill="#8B889A"></path></g></svg><span>Learning Paths</span></a></li><li class="nav-highlights"><a href="https://learning.oreilly.com/u/d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" fill="#4A3C31"><path d="M13.325 18.071H8.036c0-6.736 4.324-10.925 14.464-12.477V0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35c5.142 0 9.175-3.515 9.175-8.816 0-4.628-2.367-7.293-6.253-8.113zm27.5 0h-5.26c0-6.736 4.295-10.925 14.435-12.477V0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35c5.113 0 9.146-3.515 9.146-8.816 0-4.628-2.338-7.293-6.253-8.113z" fill-rule="evenodd"></path></svg><span>Highlights</span></a></li><li><a href="https://learning.oreilly.com/u/preferences/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l1 no-icon">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://learning.oreilly.com/u/preferences/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l2">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9781492032632/chapter/part01.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"><div data-reactroot="" class="menu-dropdown-wrapper js-menu-dropdown-wrapper align-right"><img class="hidden" src="./Chapter15_files/ajax-transp.gif" alt="loading spinner"><div class="menu-control"><div class="control "><div class="js-playlists-menu"><button class="js-playlist-icon"><svg class="icon-add-to-playlist-sml" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill-rule="nonzero" fill="#000000"><g transform="translate(-1.000000, 0.000000)"><rect x="5" y="0" width="12" height="2"></rect><title>Playlists</title><path d="M4.5,14 C6.43299662,14 8,12.4329966 8,10.5 C8,8.56700338 6.43299662,7 4.5,7 C2.56700338,7 1,8.56700338 1,10.5 C1,12.4329966 2.56700338,14 4.5,14 Z M2.5,10 L4,10 L4,8.5 L5,8.5 L5,10 L6.5,10 L6.5,11 L5,11 L5,12.5 L4,12.5 L4,11 L2.5,11 L2.5,10 Z"></path><circle cx="2" cy="5" r="1"></circle><circle cx="1.94117647" cy="1" r="1"></circle><rect x="5" y="4" width="12" height="2"></rect><rect x="9" y="8" width="8" height="2"></rect><rect x="9" y="12" width="8" height="2"></rect></g></g></g></svg><div class="js-playlist-addto-label">Add&nbsp;To</div></button></div></div></div></div></div></div></li><li class="js-font-control-panel font-control-activator"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html&amp;text=Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%20I.%20The%20Fundamentals%20of%20Machine%20Learning&amp;body=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html%0D%0Afrom%20Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    
    <section role="document">
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">14. Deep Computer Vision Using Convolutional Neural Networks</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">16. Natural Language Processing with RNNs and Attention</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content"><div class="annotator-wrapper"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 15. Processing Sequences Using RNNs and CNNs"><div class="chapter" id="rnn_chapter">
<h1><span class="label">Chapter 15. </span>Processing Sequences Using RNNs and CNNs</h1>


<p>The batter hits the ball. The outfielder immediately starts running, anticipating the ball’s trajectory. He tracks it, adapts his movements, and finally catches it (under a thunder of applause). Predicting the future is something you do all the time, whether you are finishing a friend’s sentence or anticipating the smell of coffee at breakfast. In this chapter we will discuss <em>recurrent neural networks</em> (RNNs), a class of nets that can predict the future (well, up to a point, of course). They can analyze <em>time series</em> data such as stock prices, and tell you when to buy or sell. In autonomous driving systems, they can anticipate car trajectories and help avoid accidents. More generally, they can work on <em>sequences</em> of arbitrary lengths, rather than on fixed-sized inputs like all the nets we have considered so far. For example, they can take sentences, documents, or audio samples as input, making them extremely useful for <em>natural language processing</em> (NLP) systems such as automatic translation or speech-to-text.</p>

<p>In this chapter we will first look at the fundamental concepts underlying RNNs and how to train them using backpropagation through time, then we will use them to forecast a time series. After that we’ll explore the two main difficulties that RNNs face:</p>

<ul>
<li>
<p>Unstable gradients (discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>), which can be alleviated using various techniques, including recurrent dropout and recurrent layer normalization.</p>
</li>
<li>
<p>A (very) limited short-term memory, which can be extended using LSTM and GRU cells.</p>
</li>
</ul>

<p>RNNs are not the only types of neural networks capable of handling sequential data: for small sequences, a regular dense network can do the trick; and for very long sequences, such as audio samples or text, convolutional neural networks can actually work quite well too. We will discuss both of these possibilities, and we will finish this chapter by implementing a <em>WaveNet</em>: this is a CNN architecture capable of handling sequences of tens of thousands of time steps. In <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#nlp_chapter">Chapter&nbsp;16</a>, we will continue to explore RNNs and see how to use them for natural language processing, along with more recent architectures based on attention mechanisms. Let’s get started!</p>






<section data-type="sect1" data-pdf-bookmark="Recurrent Neurons and Layers"><div class="sect1" id="idm46263497551144">
<h1>Recurrent Neurons and Layers</h1>

<p>Up to now we have focused on feedforward neural networks, where the activations flow only in one direction, from the input layer to the output layer (with a few exceptions in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/app05.html#other_ann_appendix">Appendix&nbsp;E</a>). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let’s look at the simplest possible RNN, composed of one neuron receiving inputs, producing an output, and sending that output back to itself, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#simple_rnn_diagram">Figure&nbsp;15-1</a> (left). At each <em>time step</em> <em>t</em> (also called a <em>frame</em>), this <em>recurrent neuron</em> receives the inputs <strong>x</strong><sub>(<em>t</em>)</sub> as well as its own output from the previous time step, <em>y</em><sub>(<em>t</em>–1)</sub>. Since there is no previous output at the first time step, it is generally set to 0. We can represent this tiny network against the time axis, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#simple_rnn_diagram">Figure&nbsp;15-1</a> (right). This is called <em>unrolling the network through time</em> (it’s the same recurrent neuron represented once per time step).</p>

<figure class="smallerseventy"><div id="simple_rnn_diagram" class="figure">
<img src="./Chapter15_files/mls2_1501.png" alt="mls2 1501" width="1363" height="594" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1501.png">
<h6><span class="label">Figure 15-1. </span>A recurrent neuron (left) unrolled through time (right)</h6>
</div></figure>

<p>You can easily create a layer of recurrent neurons. At each time step <em>t</em>, every neuron receives both the input vector <strong>x</strong><sub>(<em>t</em>)</sub> and the output vector from the previous time step <strong>y</strong><sub>(<em>t</em>–1)</sub>, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#rnn_layer_diagram">Figure&nbsp;15-2</a>. Note that both the inputs and outputs are vectors now (when there was just a single neuron, the output was a scalar).</p>

<figure class="smallereighty"><div id="rnn_layer_diagram" class="figure">
<img src="./Chapter15_files/mls2_1502.png" alt="mls2 1502" width="1441" height="549" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1502.png">
<h6><span class="label">Figure 15-2. </span>A layer of recurrent neurons (left) unrolled through time (right)</h6>
</div></figure>

<p>Each recurrent neuron has two sets of weights: one for the inputs <strong>x</strong><sub>(<em>t</em>)</sub> and the other for the outputs of the previous time step, <strong>y</strong><sub>(<em>t</em>–1)</sub>. Let’s call these weight vectors <strong>w</strong><sub><em>x</em></sub> and <strong>w</strong><sub><em>y</em></sub>. If we consider the whole recurrent layer instead of just one recurrent neuron, we can place all the weight vectors in two weight matrices, <strong>W</strong><sub><em>x</em></sub> and <strong>W</strong><sub><em>y</em></sub>. The output vector of the whole recurrent layer can then be computed pretty much as you might expect, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#rnn_output_equation">Equation 15-1</a> (<strong>b</strong> is the bias vector and ϕ(·) is the activation function, e.g., ReLUfootnote:[Note that many researchers prefer to use the hyperbolic tangent (tanh) activation function in RNNs rather than the ReLU activation function. For example, take a look at by Vu Pham et al.’s paper <a href="https://homl.info/91">“Dropout Improves Recurrent Neural Networks for Handwriting Recognition.”</a> ReLU-based RNNs are also possible, as shown in Quoc V. Le et al.’s paper <a href="https://homl.info/92">“A Simple Way to Initialize Recurrent Networks of Rectified Linear Units.”</a>).</p>
<div id="rnn_output_equation" data-type="equation"><h5><span class="label">Equation 15-1. </span>Output of a recurrent layer for a single instance</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-144-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;&amp;#x3D5;&lt;/mi&gt;&lt;mfenced&gt;&lt;mrow&gt;&lt;msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x200A;&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;&amp;#x200A;&lt;/mo&gt;&lt;msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x200A;&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;&amp;#x200A;&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5900" style="width: 16.507em; display: inline-block;"><span style="display: inline-block; position: relative; width: 15.992em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.237em, 1015.85em, 2.779em, -1000.01em); top: -2.26em; left: 0em;"><span class="mrow" id="MathJax-Span-5901"><span class="msub" id="MathJax-Span-5902"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5903" style="font-family: MathJax_Main-bold;">y</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-5904"><span class="mo" id="MathJax-Span-5905" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5906" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-5907" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5908" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mi" id="MathJax-Span-5909" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">ϕ</span><span class="mfenced" id="MathJax-Span-5910" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-5911" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="mrow" id="MathJax-Span-5912"><span class="msup" id="MathJax-Span-5913"><span style="display: inline-block; position: relative; width: 2.265em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.66em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="msub" id="MathJax-Span-5914"><span style="display: inline-block; position: relative; width: 1.648em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5915" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mi" id="MathJax-Span-5916" style="font-size: 70.7%; font-family: MathJax_Math-italic;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 1.648em;"><span class="mi" id="MathJax-Span-5917" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-5918"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5919" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-5920"><span class="mo" id="MathJax-Span-5921" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5922" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-5923" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5924" style="font-family: MathJax_Main;"> </span><span class="mo" id="MathJax-Span-5925" style="font-family: MathJax_Main; padding-left: 0.157em;">+</span><span class="mo" id="MathJax-Span-5926" style="font-family: MathJax_Main;"> </span><span class="msup" id="MathJax-Span-5927" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 2.213em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.61em, 4.476em, -1000.01em); top: -4.008em; left: 0em;"><span class="msub" id="MathJax-Span-5928"><span style="display: inline-block; position: relative; width: 1.596em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5929" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mi" id="MathJax-Span-5930" style="font-size: 70.7%; font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 1.596em;"><span class="mi" id="MathJax-Span-5931" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-5932"><span style="display: inline-block; position: relative; width: 2.419em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5933" style="font-family: MathJax_Main-bold;">y</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-5934"><span class="mo" id="MathJax-Span-5935" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5936" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-5937" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-5938" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-5939" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5940" style="font-family: MathJax_Main;"> </span><span class="mo" id="MathJax-Span-5941" style="font-family: MathJax_Main; padding-left: 0.157em;">+</span><span class="mo" id="MathJax-Span-5942" style="font-family: MathJax_Main;"> </span><span class="mi" id="MathJax-Span-5943" style="font-family: MathJax_Main-bold; padding-left: 0.157em;">b</span></span><span class="mo" id="MathJax-Span-5944" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.265em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.421em; border-left: 0px solid; width: 0px; height: 1.38em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="bold">y</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><mi>ϕ</mi><mfenced><mrow><msup><msub><mi mathvariant="bold">W</mi><mi>x</mi></msub><mi>T</mi></msup><msub><mi mathvariant="bold">x</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo> </mo><mo>+</mo><mo> </mo><msup><msub><mi mathvariant="bold">W</mi><mi>y</mi></msub><mi>T</mi></msup><msub><mi mathvariant="bold">y</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub><mo> </mo><mo>+</mo><mo> </mo><mi mathvariant="bold">b</mi></mrow></mfenced></math></span></span><script type="math/mml" id="MathJax-Element-144"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi mathvariant="bold">y</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><mi>ϕ</mi><mfenced><mrow><msup><msub><mi mathvariant="bold">W</mi><mi>x</mi></msub><mi>T</mi></msup><msub><mi mathvariant="bold">x</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo> </mo><mo>+</mo><mo> </mo><msup><msub><mi mathvariant="bold">W</mi><mi>y</mi></msub><mi>T</mi></msup><msub><mi mathvariant="bold">y</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub><mo> </mo><mo>+</mo><mo> </mo><mi mathvariant="bold">b</mi></mrow></mfenced></math></script></div>

<p>Just as with feedforward neural networks, we can compute a recurrent layer’s output in one shot for a whole mini-batch by placing all the inputs at time step <em>t</em> in an input matrix <strong>X</strong><sub>(<em>t</em>)</sub> (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#rnn_output_vectorized_equation">Equation 15-2</a>).</p>
<div id="rnn_output_vectorized_equation" data-type="equation"><h5><span class="label">Equation 15-2. </span>Outputs of a layer of recurrent neurons for all instances in a mini-batch</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-145-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mtable displaystyle=&quot;true&quot;&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;Y&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;&amp;#x3D5;&lt;/mi&gt;&lt;mfenced separators=&quot;&quot; open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;Y&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;b&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd /&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;&amp;#x3D5;&lt;/mi&gt;&lt;mfenced separators=&quot;&quot; open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mfenced separators=&quot;&quot; open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;Y&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;b&lt;/mi&gt;&lt;/mfenced&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;with&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced separators=&quot;&quot; open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mtable&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-5945" style="width: 23.807em; display: inline-block;"><span style="display: inline-block; position: relative; width: 23.088em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(-0.357em, 1022.68em, 4.116em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-5946"><span class="mtable" id="MathJax-Span-5947" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 22.779em; height: 0px;"><span style="position: absolute; clip: rect(1.699em, 1001.76em, 4.99em, -1000.01em); top: -4.008em; left: 0em;"><span style="display: inline-block; position: relative; width: 1.751em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.76em, 4.527em, -1000.01em); top: -5.499em; right: 0em;"><span class="mtd" id="MathJax-Span-5948"><span class="mrow" id="MathJax-Span-5949"><span class="msub" id="MathJax-Span-5950"><span style="display: inline-block; position: relative; width: 1.751em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.83em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5951" style="font-family: MathJax_Main-bold;">Y</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.877em;"><span class="mrow" id="MathJax-Span-5952"><span class="mo" id="MathJax-Span-5953" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5954" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-5955" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.859em, 1000.01em, 4.167em, -1000.01em); top: -3.185em; left: 50%; margin-left: 0em;"><span class="mtd" id="MathJax-Span-5987"><span class="mrow" id="MathJax-Span-5988"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.728em, 1020.01em, 7.201em, -1000.01em); top: -5.19em; left: 2.573em;"><span style="display: inline-block; position: relative; width: 20.26em; height: 0px;"><span style="position: absolute; clip: rect(2.985em, 1013.28em, 4.527em, -1000.01em); top: -5.499em; left: 0em;"><span class="mtd" id="MathJax-Span-5956"><span class="mrow" id="MathJax-Span-5957"><span class="mrow" id="MathJax-Span-5958"><span class="mo" id="MathJax-Span-5959" style="font-family: MathJax_Main;">=</span><span class="mi" id="MathJax-Span-5960" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">ϕ</span><span class="mfenced" id="MathJax-Span-5961" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-5962" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="msub" id="MathJax-Span-5963"><span style="display: inline-block; position: relative; width: 1.751em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.83em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5964" style="font-family: MathJax_Main-bold;">X</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.877em;"><span class="mrow" id="MathJax-Span-5965"><span class="mo" id="MathJax-Span-5966" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5967" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-5968" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-5969"><span style="display: inline-block; position: relative; width: 1.648em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5970" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mi" id="MathJax-Span-5971" style="font-size: 70.7%; font-family: MathJax_Math-italic;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5972" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msub" id="MathJax-Span-5973" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 2.676em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.83em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5974" style="font-family: MathJax_Main-bold;">Y</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.877em;"><span class="mrow" id="MathJax-Span-5975"><span class="mo" id="MathJax-Span-5976" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5977" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-5978" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-5979" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-5980" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-5981"><span style="display: inline-block; position: relative; width: 1.596em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5982" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mi" id="MathJax-Span-5983" style="font-size: 70.7%; font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-5984" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-5985" style="font-family: MathJax_Main-bold; padding-left: 0.208em;">b</span><span class="mo" id="MathJax-Span-5986" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.368em, 1020.01em, 5.195em, -1000.01em); top: -3.185em; left: 0em;"><span class="mtd" id="MathJax-Span-5989"><span class="mrow" id="MathJax-Span-5990"><span class="mrow" id="MathJax-Span-5991"><span class="mo" id="MathJax-Span-5992" style="font-family: MathJax_Main;">=</span><span class="mi" id="MathJax-Span-5993" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">ϕ</span><span class="mfenced" id="MathJax-Span-5994" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-5995" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="mfenced" id="MathJax-Span-5996"><span class="mo" id="MathJax-Span-5997" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">[</span></span><span class="msub" id="MathJax-Span-5998"><span style="display: inline-block; position: relative; width: 1.751em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.83em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-5999" style="font-family: MathJax_Main-bold;">X</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.877em;"><span class="mrow" id="MathJax-Span-6000"><span class="mo" id="MathJax-Span-6001" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6002" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6003" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mspace" id="MathJax-Span-6004" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span><span class="msub" id="MathJax-Span-6005"><span style="display: inline-block; position: relative; width: 2.676em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.83em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6006" style="font-family: MathJax_Main-bold;">Y</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.877em;"><span class="mrow" id="MathJax-Span-6007"><span class="mo" id="MathJax-Span-6008" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6009" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6010" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-6011" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-6012" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6013" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">]</span></span></span><span class="mi" id="MathJax-Span-6014" style="font-family: MathJax_Main-bold;">W</span><span class="mo" id="MathJax-Span-6015" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-6016" style="font-family: MathJax_Main-bold; padding-left: 0.208em;">b</span><span class="mo" id="MathJax-Span-6017" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span><span class="mspace" id="MathJax-Span-6018" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-6019" style="font-family: MathJax_Main;">with</span><span class="mspace" id="MathJax-Span-6020" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-6021" style="font-family: MathJax_Main-bold;">W</span><span class="mo" id="MathJax-Span-6022" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mfenced" id="MathJax-Span-6023" style="padding-left: 0.26em;"><span class="mo" id="MathJax-Span-6024" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">[</span></span><span class="mtable" id="MathJax-Span-6025" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.648em; height: 0px;"><span style="position: absolute; clip: rect(2.47em, 1001.66em, 5.195em, -1000.01em); top: -4.008em; left: 0em;"><span style="display: inline-block; position: relative; width: 1.648em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.66em, 4.321em, -1000.01em); top: -4.728em; left: 50%; margin-left: -0.82em;"><span class="mtd" id="MathJax-Span-6026"><span class="mrow" id="MathJax-Span-6027"><span class="msub" id="MathJax-Span-6028"><span style="display: inline-block; position: relative; width: 1.648em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6029" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mi" id="MathJax-Span-6030" style="font-size: 70.7%; font-family: MathJax_Math-italic;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1001.61em, 4.476em, -1000.01em); top: -3.288em; left: 50%; margin-left: -0.82em;"><span class="mtd" id="MathJax-Span-6031"><span class="mrow" id="MathJax-Span-6032"><span class="msub" id="MathJax-Span-6033"><span style="display: inline-block; position: relative; width: 1.596em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6034" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mi" id="MathJax-Span-6035" style="font-size: 70.7%; font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6036" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">]</span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 5.195em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.957em; border-left: 0px solid; width: 0px; height: 4.398em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi mathvariant="bold">Y</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd><mtd columnalign="left"><mrow><mo>=</mo><mi>ϕ</mi><mfenced separators="" open="(" close=")"><msub><mi mathvariant="bold">X</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><msub><mi mathvariant="bold">W</mi><mi>x</mi></msub><mo>+</mo><msub><mi mathvariant="bold">Y</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub><msub><mi mathvariant="bold">W</mi><mi>y</mi></msub><mo>+</mo><mi mathvariant="bold">b</mi></mfenced></mrow></mtd></mtr><mtr><mtd></mtd><mtd columnalign="left"><mrow><mo>=</mo><mi>ϕ</mi><mfenced separators="" open="(" close=")"><mfenced separators="" open="[" close="]"><msub><mi mathvariant="bold">X</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mspace width="1.em"></mspace><msub><mi mathvariant="bold">Y</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub></mfenced><mi mathvariant="bold">W</mi><mo>+</mo><mi mathvariant="bold">b</mi></mfenced><mspace width="4.pt"></mspace><mtext>with</mtext><mspace width="4.pt"></mspace><mi mathvariant="bold">W</mi><mo>=</mo><mfenced separators="" open="[" close="]"><mtable><mtr><mtd><msub><mi mathvariant="bold">W</mi><mi>x</mi></msub></mtd></mtr><mtr><mtd><msub><mi mathvariant="bold">W</mi><mi>y</mi></msub></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math></span></span></div><script type="math/mml" id="MathJax-Element-145"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">Y</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mi>ϕ</mi>
          <mfenced separators="" open="(" close=")">
            <msub><mi mathvariant="bold">X</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
            <msub><mi mathvariant="bold">W</mi> <mi>x</mi> </msub>
            <mo>+</mo>
            <msub><mi mathvariant="bold">Y</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
            <msub><mi mathvariant="bold">W</mi> <mi>y</mi> </msub>
            <mo>+</mo>
            <mi mathvariant="bold">b</mi>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd></mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mi>ϕ</mi>
          <mfenced separators="" open="(" close=")">
            <mfenced separators="" open="[" close="]">
              <msub><mi mathvariant="bold">X</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
              <mspace width="1.em"></mspace>
              <msub><mi mathvariant="bold">Y</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
            </mfenced>
            <mi mathvariant="bold">W</mi>
            <mo>+</mo>
            <mi mathvariant="bold">b</mi>
          </mfenced>
          <mspace width="4.pt"></mspace>
          <mtext>with</mtext>
          <mspace width="4.pt"></mspace>
          <mi mathvariant="bold">W</mi>
          <mo>=</mo>
          <mfenced separators="" open="[" close="]">
            <mtable>
              <mtr>
                <mtd>
                  <msub><mi mathvariant="bold">W</mi> <mi>x</mi> </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub><mi mathvariant="bold">W</mi> <mi>y</mi> </msub>
                </mtd>
              </mtr>
            </mtable>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></script></div>

<p>In this equation:</p>

<ul>
<li>
<p><strong>Y</strong><sub>(<em>t</em>)</sub> is an <em>m</em> × <em>n</em><sub>neurons</sub> matrix containing the layer’s outputs at time step <em>t</em> for each instance in the mini-batch (<em>m</em> is the number of instances in the mini-batch and <em>n</em><sub>neurons</sub> is the number of neurons).</p>
</li>
<li>
<p><strong>X</strong><sub>(<em>t</em>)</sub> is an <em>m</em> × <em>n</em><sub>inputs</sub> matrix containing the inputs for all instances (<em>n</em><sub>inputs</sub> is the number of input features).</p>
</li>
<li>
<p><strong>W</strong><sub><em>x</em></sub> is an <em>n</em><sub>inputs</sub> × <em>n</em><sub>neurons</sub> matrix containing the connection weights for the inputs of the current time step.</p>
</li>
<li>
<p><strong>W</strong><sub><em>y</em></sub> is an <em>n</em><sub>neurons</sub> × <em>n</em><sub>neurons</sub> matrix containing the connection weights for the outputs of the previous time step.</p>
</li>
<li>
<p><strong>b</strong> is a vector of size <em>n</em><sub>neurons</sub> containing each neuron’s bias term.</p>
</li>
<li>
<p>The weight matrices <strong>W</strong><sub><em>x</em></sub> and <strong>W</strong><sub><em>y</em></sub> are often concatenated vertically into a single weight matrix <strong>W</strong> of shape (<em>n</em><sub>inputs</sub> + <em>n</em><sub>neurons</sub>) × <em>n</em><sub>neurons</sub> (see the second line of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#rnn_output_vectorized_equation">Equation 15-2</a>).</p>
</li>
<li>
<p>The notation [<strong>X</strong><sub>(<em>t</em>)</sub> <strong>Y</strong><sub>(<em>t</em>–1)</sub>] represents the horizontal concatenation of the matrices <strong>X</strong><sub>(<em>t</em>)</sub> and <strong>Y</strong><sub>(<em>t</em>–1)</sub>.</p>
</li>
</ul>

<p>Notice that <strong>Y</strong><sub>(<em>t</em>)</sub> is a function of <strong>X</strong><sub>(<em>t</em>)</sub> and <strong>Y</strong><sub>(<em>t</em>–1)</sub>, which is a function of <strong>X</strong><sub>(<em>t</em>–1)</sub> and <strong>Y</strong><sub>(<em>t</em>–2)</sub>, which is a function of <strong>X</strong><sub>(<em>t</em>–2)</sub> and <strong>Y</strong><sub>(<em>t</em>–3)</sub>, and so on. This makes <strong>Y</strong><sub>(<em>t</em>)</sub> a function of all the inputs since time <em>t</em> = 0 (that is, <strong>X</strong><sub>(0)</sub>, <strong>X</strong><sub>(1)</sub>, …, <strong>X</strong><sub>(<em>t</em>)</sub>). At the first time step, <em>t</em> = 0, there are no previous outputs, so they are typically assumed to be all zeros.</p>








<section data-type="sect2" data-pdf-bookmark="Memory Cells"><div class="sect2" id="idm46263497435288">
<h2>Memory Cells</h2>

<p>Since the output of a recurrent neuron at time step <em>t</em> is a function of all the inputs from previous time steps, you could say it has a form of <em>memory</em>. A part of a neural network that preserves some state across time steps is called a <em>memory cell</em> (or simply a <em>cell</em>). A single recurrent neuron, or a layer of recurrent neurons, is a very <em>basic cell</em>, capable of learning only short patterns (about 10 steps long, but this varies, depending on the task). Later in this chapter, we will look at some more complex and powerful types of cells capable of learning longer patterns (roughly 10 times longer, but again, this depends on the task).</p>

<p>In general a cell’s state at time step <em>t</em>, denoted <strong>h</strong><sub>(<em>t</em>)</sub> (the “h” stands for “hidden”), is a function of some inputs at that time step and its state at the previous time step: <strong>h</strong><sub>(<em>t</em>)</sub> = <em>f</em>(<strong>h</strong><sub>(<em>t</em>–1)</sub>, <strong>x</strong><sub>(<em>t</em>)</sub>). Its output at time step <em>t</em>, denoted <strong>y</strong><sub>(<em>t</em>)</sub>, is also a function of the previous state and the current inputs. In the case of the basic cells we have discussed so far, the output is simply equal to the state, but in more complex cells this is not always the case, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#hidden_state_diagram">Figure&nbsp;15-3</a>.</p>

<figure class="smallerseventy"><div id="hidden_state_diagram" class="figure">
<img src="./Chapter15_files/mls2_1503.png" alt="mls2 1503" width="1095" height="513" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1503.png">
<h6><span class="label">Figure 15-3. </span>A cell’s hidden state and its output may be different</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Input and Output Sequences"><div class="sect2" id="idm46263497406824">
<h2>Input and Output Sequences</h2>

<p>An RNN can simultaneously take a sequence of inputs and produce a sequence of outputs (see the top-left network in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#seq_to_seq_diagram">Figure&nbsp;15-4</a>). This type of network is useful for predicting time series such as stock prices: you feed it the prices over the last <em>N</em> days, and it must output the prices shifted by one day into the future (i.e., from <em>N</em> – 1 days ago to tomorrow).</p>

<p>Alternatively, you could feed the network a sequence of inputs and ignore all outputs except for the last one (see the top-right network in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#seq_to_seq_diagram">Figure&nbsp;15-4</a>). In other words, this is a sequence-to-vector network. For example, you could feed the network a sequence of words corresponding to a movie review, and the network would output a sentiment score (e.g., from –1 [hate] to +1 [love]).</p>

<p>Conversely, you could feed the network the same input vector over and over again at each time step and let it output a sequence (see the bottom-left network of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#seq_to_seq_diagram">Figure&nbsp;15-4</a>). This is a vector-to-sequence network. For example, the input could be an image (or the output of a CNN), and the output could be a caption for that image.</p>

<p>Lastly, you could have a sequence-to-vector network, called an <em>encoder</em>, followed by a vector-to-sequence network, called a <em>decoder</em> (see the bottom-right network of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#seq_to_seq_diagram">Figure&nbsp;15-4</a>). For example, this can be used for translating a sentence from one language to another. You would feed the network a sentence in one language, the encoder would convert this sentence into a single vector representation, and then the decoder would decode this vector into a sentence in another language. This two-step model, called an Encoder–Decoder, works much better than trying to translate on the fly with a single sequence-to-sequence RNN (like the one represented on the top left), since the last words of a sentence can affect the first words of the translation, so you need to wait until you have seen the whole sentence before translating it. We will see how to implement an Encoder–Decoder in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#nlp_chapter">Chapter&nbsp;16</a> (as we will see, it is a bit more complex than in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#seq_to_seq_diagram">Figure&nbsp;15-4</a>).</p>

<figure><div id="seq_to_seq_diagram" class="figure">
<img src="./Chapter15_files/mls2_1504.png" alt="mls2 1504" width="1440" height="965" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1504.png">
<h6><span class="label">Figure 15-4. </span>Seq to seq (top left), seq to vector (top right), vector to seq (bottom left), encoder/decoder (bottom right)</h6>
</div></figure>

<p>Sounds promising, but how do you train a recurrent neural network?</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Training RNNs"><div class="sect1" id="idm46263497550552">
<h1>Training RNNs</h1>

<p>To train an RNN, the trick is to unroll it through time (like we just did) and then simply use regular backpropagation (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#bptt_diagram">Figure&nbsp;15-5</a>). This strategy is called <em>backpropagation through time</em> (BPTT).</p>

<figure class="smallerseventy"><div id="bptt_diagram" class="figure">
<img src="./Chapter15_files/mls2_1505.png" alt="mls2 1505" width="1091" height="662" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1505.png">
<h6><span class="label">Figure 15-5. </span>Backpropagation through time</h6>
</div></figure>

<p>Just like in regular backpropagation, there is a first forward pass through the unrolled network (represented by the dashed arrows). Then the output sequence is evaluated using a cost function <em>C</em>(<strong>Y</strong><sub>(0)</sub>, <strong>Y</strong><sub>(1)</sub>, …<strong>Y</strong><sub>(<em>T</em>)</sub>) (where <em>T</em> is the max time step). Note that this cost function may ignore some outputs, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#bptt_diagram">Figure&nbsp;15-5</a> (for example, in a sequence-to-vector RNN, all outputs are ignored, except for the very last one). The gradients of that cost function are then propagated backward through the unrolled network (represented by the solid arrows). Finally the model parameters are updated using the gradients computed during BPTT. Note that the gradients flow backward through all the outputs used by the cost function, not just through the final output (for example, in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#bptt_diagram">Figure&nbsp;15-5</a> the cost function is computed using the last three outputs of the network, <strong>Y</strong><sub>(2)</sub>, <strong>Y</strong><sub>(3)</sub>, and <strong>Y</strong><sub>(4)</sub>, so gradients flow through these three outputs, but not through <strong>Y</strong><sub>(0)</sub> and <strong>Y</strong><sub>(1)</sub>). Moreover, since the same parameters <strong>W</strong> and <strong>b</strong> are used at each time step, backpropagation will do the right thing and sum over all time steps.</p>

<p>Fortunately, tf.keras takes care of all of this complexity for you, so let’s start coding!</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Forecasting a Time Series"><div class="sect1" id="idm46263497376216">
<h1>Forecasting a Time Series</h1>

<p>Suppose you are studying the number of active users on your website, hour per hour, or the daily temperature in your city, or your company’s financial health, measured quarterly using multiple metrics. In all these cases, the data will be a sequence of one or more values per time step. This is called a <em>time series</em>. In the first two examples, there is a single value per time step, so it is a <em>univariate time series</em>, while in the financial example, there are multiple values per time step (e.g., the company’s revenue, debt, and so on), so it is a <em>multivariate time series</em>. A typical task is to predict future values, which is called <em>forecasting</em>. Another common task is to fill in the blanks: to predict (or rather “postdict”) missing values from the past. This is called <em>imputation</em>. For example, <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#time_series_plot">Figure&nbsp;15-6</a> shows 3 univariate time series, each of them 50 time steps long, and the goal here is to forecast the value at the next time step (represented by the X) for each of them.</p>

<figure class="smallerseventy"><div id="time_series_plot" class="figure">
<img src="./Chapter15_files/mls2_1506.png" alt="mls2 1506" width="1439" height="447" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1506.png">
<h6><span class="label">Figure 15-6. </span>Time series forecasting</h6>
</div></figure>

<p>For simplicity, we are using a time series generated by the following <code>generate_time_series()</code> function. This function creates as many time series as requested (via the <code>batch_size</code> argument), each of length <code>n_steps</code>, and there is just one value per time step in each series (i.e., all series are univariate). This function returns a NumPy array of shape <code>[batch_size, n_steps, 1]</code>, where each series is the sum of two sine waves of fixed amplitudes but random frequencies and phases, plus a bit of noise:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">generate_time_series</code><code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="n">n_steps</code><code class="p">):</code>
    <code class="n">freq1</code><code class="p">,</code> <code class="n">freq2</code><code class="p">,</code> <code class="n">offsets1</code><code class="p">,</code> <code class="n">offsets2</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">rand</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
    <code class="n">time</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">n_steps</code><code class="p">)</code>
    <code class="n">series</code> <code class="o">=</code> <code class="mf">0.5</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">sin</code><code class="p">((</code><code class="n">time</code> <code class="o">-</code> <code class="n">offsets1</code><code class="p">)</code> <code class="o">*</code> <code class="p">(</code><code class="n">freq1</code> <code class="o">*</code> <code class="mi">10</code> <code class="o">+</code> <code class="mi">10</code><code class="p">))</code>  <code class="c1">#   wave 1</code>
    <code class="n">series</code> <code class="o">+=</code> <code class="mf">0.2</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">sin</code><code class="p">((</code><code class="n">time</code> <code class="o">-</code> <code class="n">offsets2</code><code class="p">)</code> <code class="o">*</code> <code class="p">(</code><code class="n">freq2</code> <code class="o">*</code> <code class="mi">20</code> <code class="o">+</code> <code class="mi">20</code><code class="p">))</code> <code class="c1"># + wave 2</code>
    <code class="n">series</code> <code class="o">+=</code> <code class="mf">0.1</code> <code class="o">*</code> <code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">rand</code><code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="n">n_steps</code><code class="p">)</code> <code class="o">-</code> <code class="mf">0.5</code><code class="p">)</code>   <code class="c1"># + noise</code>
    <code class="k">return</code> <code class="n">series</code><code class="p">[</code><code class="o">...</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">]</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>When dealing with time series (and other types of sequences such as sentences), the input features are generally represented as 3D arrays of shape <code>[batch_size, n_steps, dimensionality]</code>, where <code>dimensionality</code> is 1 for univariate time series and more for multivariate time series.</p>
</div>

<p>Now let’s create a training set, a validation set, and a test set using this function:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">n_steps</code> <code class="o">=</code> <code class="mi">50</code>
<code class="n">series</code> <code class="o">=</code> <code class="n">generate_time_series</code><code class="p">(</code><code class="mi">10000</code><code class="p">,</code> <code class="n">n_steps</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code> <code class="o">=</code> <code class="n">series</code><code class="p">[:</code><code class="mi">7000</code><code class="p">,</code> <code class="p">:</code><code class="n">n_steps</code><code class="p">],</code> <code class="n">series</code><code class="p">[:</code><code class="mi">7000</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">]</code>
<code class="n">X_valid</code><code class="p">,</code> <code class="n">y_valid</code> <code class="o">=</code> <code class="n">series</code><code class="p">[</code><code class="mi">7000</code><code class="p">:</code><code class="mi">9000</code><code class="p">,</code> <code class="p">:</code><code class="n">n_steps</code><code class="p">],</code> <code class="n">series</code><code class="p">[</code><code class="mi">7000</code><code class="p">:</code><code class="mi">9000</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">]</code>
<code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">series</code><code class="p">[</code><code class="mi">9000</code><code class="p">:,</code> <code class="p">:</code><code class="n">n_steps</code><code class="p">],</code> <code class="n">series</code><code class="p">[</code><code class="mi">9000</code><code class="p">:,</code> <code class="o">-</code><code class="mi">1</code><code class="p">]</code></pre>

<p><code>X_train</code>  contains 7,000 time series (i.e., its shape is <code>[7000, 50, 1]</code>), while <code>X_valid</code> contains 2,000 (from the 7,000th time series to the 8,999th), and <code>X_test</code> contains 1,000 (from the 9,000<sup>th</sup> to the 9,999<sup>th</sup>). Since we want to forecast a single value for each series, the targets are column vectors (e.g., <code>y_train</code> has a shape of <code>[7000, 1]</code>).</p>








<section data-type="sect2" data-pdf-bookmark="Baseline Metrics"><div class="sect2" id="idm46263497104936">
<h2>Baseline Metrics</h2>

<p>Before we start using RNNs, it is often a good idea to have a few baseline metrics, or else we may end up thinking our model works great when in fact it is doing worse than basic models. For example, the simplest approach is to predict the last value in each series. This is called <em>naive forecasting</em>, and it is sometimes surprisingly difficult to outperform. In this case, it gives us a mean squared error of about 0.020:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code> <code class="o">=</code> <code class="n">X_valid</code><code class="p">[:,</code> <code class="o">-</code><code class="mi">1</code><code class="p">]</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">losses</code><code class="o">.</code><code class="n">mean_squared_error</code><code class="p">(</code><code class="n">y_valid</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">))</code>
<code class="go">0.020211367</code></pre>

<p>Another simple approach is to use a fully connected network. Since it expects a flat list of features for each input, we need to add a <code>Flatten</code> layer. Let’s just use a simple Linear Regression model so that each prediction will just be a linear combination of the values in the time series:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(</code><code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">50</code><code class="p">,</code> <code class="mi">1</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="p">])</code></pre>

<p>If we compile this model using the MSE loss and the default Adam optimizer, then fit it on the training set for 20 epochs and evaluate it on the validation set, we get an MSE of about 0.004. That’s much better than the naive approach!</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Implementing a Simple RNN"><div class="sect2" id="idm46263496998280">
<h2>Implementing a Simple RNN</h2>

<p>Let’s see if we can beat that with a simple RNN:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
  <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">SimpleRNN</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">,</code> <code class="mi">1</code><code class="p">])</code>
<code class="p">])</code></pre>

<p>That’s really the simplest RNN you can build. It just contains a single layer, with a single neuron, as we saw in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#simple_rnn_diagram">Figure&nbsp;15-1</a>. We do not need to specify the length of the input sequences (as opposed to the previous model), since a recurrent neural network can process any number of time steps (this is why we set the first input dimension to <code>None</code>). By default, the <code>SimpleRNN</code> layer uses the hyperbolic tangent activation function. It works exactly as we saw earlier: the initial state <em>h</em><sub>(init)</sub> is set to 0, and it is passed to a single recurrent neuron, along with the value of the first time step <em>x</em><sub>(0)</sub>. The neuron computes a weighted sum of these values and applies the hyperbolic tangent activation function to the result, and this gives the first output <em>y</em><sub>0</sub>. In a simple RNN, this output is also the new state <em>h</em><sub>0</sub>. This new state is passed to the same recurrent neuron, along with the next input value <em>x</em><sub>(1)</sub>, and the process is repeated until the last time step. Then the layer just outputs the last value <em>y</em><sub>49</sub>. All of this is performed simultaneously for every time series.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>By default, recurrent layers in Keras only return the final output. To make them return one output per time step, you must set <code>return_sequences=True</code>, as we will see.</p>
</div>

<p>If you compile, fit, and evaluate this model (just like earlier, we train for 20 epochs using Adam), you will find that it reaches only 0.014, so it is better than the naive approach, but it does not beat a simple linear model. Note that for each neuron, a linear model has one parameter per input and per time step, plus a bias term (in the simple linear model we used, that’s a total of 51 parameters). In contrast, for each recurrent neuron in a simple RNN, there is just one parameter per input and per hidden state dimension (in a simple RNN, that’s just the number of recurrent neurons in the layer), plus a bias term. In this simple RNN, that’s just a total of three parameters.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46263496925976">
<h5>Trend and Seasonality</h5>
<p>There are many other models to forecast time series, such as <em>weighted moving average</em> models or <em>autoregressive integrated moving average</em> (ARIMA) models. Some of them require you to first remove the trend and seasonality. For example, if you are studying the number of active users on your website, and it is growing by 10% every month, you would have to remove this trend from the time series. Once the model is trained and starts making predictions, you would have to add the trend back to get the final predictions. Similarly, if you are trying to predict the amount of sunscreen lotion sold every month, you will probably observe strong seasonality: since it sells well every summer, a similar pattern will be repeated every year. You would have to remove this seasonality from the time series, for example by computing the difference between the value at each time step and the value one year earlier (this technique is called <em>differencing</em>). Again, after the model is trained and makes predictions, you would have to add the seasonal pattern back to get the final predictions.</p>

<p>When using RNNs, it is generally not necessary to do all this, but it may improve performance in some cases, since the model will not have to learn the trend or the seasonality.</p>
</div></aside>

<p>Apparently our simple RNN was too simple to get good performance. So let’s try to add more recurrent layers!</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Deep RNNs"><div class="sect2" id="idm46263496920952">
<h2>Deep RNNs</h2>

<p>It is quite common to stack multiple layers of cells, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#deep_rnn_diagram">Figure&nbsp;15-7</a>. This gives you a <em>deep RNN</em>.</p>

<figure class="smallerseventy"><div id="deep_rnn_diagram" class="figure">
<img src="./Chapter15_files/mls2_1507.png" alt="mls2 1507" width="1439" height="812" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1507.png">
<h6><span class="label">Figure 15-7. </span>Deep RNN (left) unrolled through time (right)</h6>
</div></figure>

<p>Implementing a deep RNN with tf.keras is quite simple: just stack recurrent layers. In this example, we use three <code>SimpleRNN</code> layers (but we could add any other type of recurrent layer, such as an LSTM layer or a GRU layer, which we will discuss shortly):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">SimpleRNN</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">,</code> <code class="mi">1</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">SimpleRNN</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">SimpleRNN</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="p">])</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Make sure to set <code>return_sequences=True</code> for all recurrent layers (except the last one, if you only care about the last output). If you don’t, the layer will output a 2D array (containing only the output of the last time step) instead of a 3D array (containing outputs for all time steps), and the next recurrent layer will complain that you are not feeding it sequences in the expected 3D format.</p>
</div>

<p>If you compile, fit, and evaluate this model, you will find that it reaches an MSE of 0.003. We finally managed to beat the linear model!</p>

<p>Note that the last layer is not ideal: it must have a single unit because we want to forecast a univariate time series, and this means we must have a single output value per time step. However, having a single unit means that the hidden state is just a single number. That’s really not much, and it’s probably not that useful; presumably, the RNN will mostly use the hidden states of the other recurrent layers to carry all the information it needs over from time step to time step, and it will not use the final layer’s hidden state very much. Moreover, since a <code>SimpleRNN</code> layer uses the tanh activation function by default, the predicted values must lie within the range -1 to 1. But what if you want to use another activation function? For both these reasons, it might be preferable to replace the output layer with a <code>Dense</code> layer: it would run slightly faster, the accuracy would be roughly the same, and it would allow us to choose any output activation function we want. To make this change, replace the output layer with a <code>Dense</code> layer, but also make sure to remove <code>return_sequences=True</code> from the second (now last) recurrent layer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">SimpleRNN</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">,</code> <code class="mi">1</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">SimpleRNN</code><code class="p">(</code><code class="mi">20</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="p">])</code></pre>

<p>If you train this model, you will see that it converges faster and performs just as well. Plus, we could change the output activation function if we wanted.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Forecasting Several Time Steps Ahead"><div class="sect2" id="idm46263496920328">
<h2>Forecasting Several Time Steps Ahead</h2>

<p>So far we have only predicted the value at the next time step, but we could just as easily have predicted the value several steps ahead by changing the targets appropriately (e.g., to predict 10 steps ahead, just change the targets to be the value 10 steps ahead instead of 1 step ahead). But what if we want to predict the next 10 values?</p>

<p>The first option is to use the model we already trained, make it predict the next value, then add that value to the inputs (acting as if this predicted value had actually occurred), and use the model again to predict the following value, and so on, as in the following code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">series</code> <code class="o">=</code> <code class="n">generate_time_series</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">n_steps</code> <code class="o">+</code> <code class="mi">10</code><code class="p">)</code>
<code class="n">X_new</code><code class="p">,</code> <code class="n">Y_new</code> <code class="o">=</code> <code class="n">series</code><code class="p">[:,</code> <code class="p">:</code><code class="n">n_steps</code><code class="p">],</code> <code class="n">series</code><code class="p">[:,</code> <code class="n">n_steps</code><code class="p">:]</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">X_new</code>
<code class="k">for</code> <code class="n">step_ahead</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">10</code><code class="p">):</code>
    <code class="n">y_pred_one</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="n">step_ahead</code><code class="p">:])[:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">,</code> <code class="p">:]</code>
    <code class="n">X</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">concatenate</code><code class="p">([</code><code class="n">X</code><code class="p">,</code> <code class="n">y_pred_one</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="n">Y_pred</code> <code class="o">=</code> <code class="n">X</code><code class="p">[:,</code> <code class="n">n_steps</code><code class="p">:]</code></pre>

<p>As you might expect, the prediction for the next step will usually be more accurate than the predictions for later time steps, since the errors might accumulate (as you can see in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#forecast_ahead_plot">Figure&nbsp;15-8</a>). If you evaluate this approach on the validation set, you will find an MSE of about 0.029. This is much higher than the previous models, but it’s also a much harder task, so the comparison doesn’t mean much. It’s much more meaningful to compare this performance with naive predictions (just forecasting that the time series will remain constant for 10 time steps) or with a simple linear model. The naive approach is terrible (it gives an MSE of about 0.223), but the linear model gives an MSE of about 0.0188: it’s much better than using our RNN to forecast the future one step at a time, and also much faster to train and run. Still, if you only want to forecast a few time steps ahead, on more complex tasks, then this approach may work well.</p>

<figure class="smallerseventy"><div id="forecast_ahead_plot" class="figure">
<img src="./Chapter15_files/mls2_1508.png" alt="mls2 1508" width="1441" height="919" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1508.png">
<h6><span class="label">Figure 15-8. </span>Forecasting 10 steps ahead, one step at a time</h6>
</div></figure>

<p>The second option is to train an RNN to predict all 10 next values at once. We can still use a sequence-to-vector model, but it will output 10 values instead of 1. However, we first need to change the targets to be vectors containing the next 10 values:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">series</code> <code class="o">=</code> <code class="n">generate_time_series</code><code class="p">(</code><code class="mi">10000</code><code class="p">,</code> <code class="n">n_steps</code> <code class="o">+</code> <code class="mi">10</code><code class="p">)</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code> <code class="o">=</code> <code class="n">series</code><code class="p">[:</code><code class="mi">7000</code><code class="p">,</code> <code class="p">:</code><code class="n">n_steps</code><code class="p">],</code> <code class="n">series</code><code class="p">[:</code><code class="mi">7000</code><code class="p">,</code> <code class="o">-</code><code class="mi">10</code><code class="p">:,</code> <code class="mi">0</code><code class="p">]</code>
<code class="n">X_valid</code><code class="p">,</code> <code class="n">Y_valid</code> <code class="o">=</code> <code class="n">series</code><code class="p">[</code><code class="mi">7000</code><code class="p">:</code><code class="mi">9000</code><code class="p">,</code> <code class="p">:</code><code class="n">n_steps</code><code class="p">],</code> <code class="n">series</code><code class="p">[</code><code class="mi">7000</code><code class="p">:</code><code class="mi">9000</code><code class="p">,</code> <code class="o">-</code><code class="mi">10</code><code class="p">:,</code> <code class="mi">0</code><code class="p">]</code>
<code class="n">X_test</code><code class="p">,</code> <code class="n">Y_test</code> <code class="o">=</code> <code class="n">series</code><code class="p">[</code><code class="mi">9000</code><code class="p">:,</code> <code class="p">:</code><code class="n">n_steps</code><code class="p">],</code> <code class="n">series</code><code class="p">[</code><code class="mi">9000</code><code class="p">:,</code> <code class="o">-</code><code class="mi">10</code><code class="p">:,</code> <code class="mi">0</code><code class="p">]</code></pre>

<p>Next, we just need the output layer to have 10 units instead of 1:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">SimpleRNN</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">,</code> <code class="mi">1</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">SimpleRNN</code><code class="p">(</code><code class="mi">20</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code>
<code class="p">])</code></pre>

<p>After training this model, you can predict the next 10 values at once very easily:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Y_pred</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code></pre>

<p>This model works nicely: the MSE for the next 10 time steps is about 0.008. That’s much better than the linear model. But we can still do better: indeed, instead of training the model to forecast the next 10 values only at the very last time step, we can train it to forecast the next 10 values at each and every time step. In other words, we can turn this sequence-to-vector RNN into a sequence-to-sequence RNN. The advantage of this technique is that the loss will contain a term for the output of the RNN at each and every time step, not just the output at the last time step. This means there will be many more error gradients flowing through the model, and they won’t have to flow only through time; they will also flow from the output of each time step: this will both stabilize and speed up training.</p>

<p>To be clear, at time step 0 the model will output a vector containing the forecasts for time steps 1 to 10, then at time step 1 the model will forecast time steps 2 to 11, and so on. So each target must be a sequence of the same length as the input sequence, containing a 10-dimensional vector at each step. Let’s prepare these target sequences:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Y</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">empty</code><code class="p">((</code><code class="mi">10000</code><code class="p">,</code> <code class="n">n_steps</code><code class="p">,</code> <code class="mi">10</code><code class="p">))</code> <code class="c1"># each target is a sequence of 10-D vectors</code>
<code class="k">for</code> <code class="n">step_ahead</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">10</code> <code class="o">+</code> <code class="mi">1</code><code class="p">):</code>
    <code class="n">Y</code><code class="p">[:,</code> <code class="p">:,</code> <code class="n">step_ahead</code> <code class="o">-</code> <code class="mi">1</code><code class="p">]</code> <code class="o">=</code> <code class="n">series</code><code class="p">[:,</code> <code class="n">step_ahead</code><code class="p">:</code><code class="n">step_ahead</code> <code class="o">+</code> <code class="n">n_steps</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code>
<code class="n">Y_train</code> <code class="o">=</code> <code class="n">Y</code><code class="p">[:</code><code class="mi">7000</code><code class="p">]</code>
<code class="n">Y_valid</code> <code class="o">=</code> <code class="n">Y</code><code class="p">[</code><code class="mi">7000</code><code class="p">:</code><code class="mi">9000</code><code class="p">]</code>
<code class="n">Y_test</code> <code class="o">=</code> <code class="n">Y</code><code class="p">[</code><code class="mi">9000</code><code class="p">:]</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>It may be surprising that the targets will contain values that appear in the inputs (there is a lot of overlap between <code>X_train</code> and <code>Y_train</code>). Isn’t that cheating? Fortunately, not at all: at each time step, the model only knows about past time steps, so it cannot look ahead. It is said to be a <em>causal</em> model.</p>
</div>

<p>To turn the model into a sequence-to-sequence model, we must set <code>return_sequences=True</code> in all recurrent layers (even the last one), and we must apply the output <code>Dense</code> layer at every time step. Keras offers a <code>TimeDistributed</code> layer for this very purpose: it wraps any layer (e.g., a <code>Dense</code> layer) and applies it at every time step of its input sequence. It does this efficiently, by reshaping the inputs so that each time step is treated as a separate instance (i.e., it reshapes the inputs from <code>[batch_size, n_steps, in_dims]</code> to <code>[batch_size × n_steps, in_dims]</code>; in this example, <code>in_dims</code> is 20 because the previous <code>SimpleRNN</code> layer has 20 units), then it runs the <code>Dense</code> layer, and finally it reshapes the outputs back to sequences (i.e., it reshapes the outputs from <code>[batch_size × n_steps, out_dims]</code> to <code>[batch_size, n_steps, out_dims]</code>; in this example <code>out_dims</code> is 10, since the <code>Dense</code> layer has 10 units).<sup><a data-type="noteref" id="idm46263496319896-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263496319896" class="totri-footnote">1</a></sup> So here is the updated model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">SimpleRNN</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">,</code> <code class="mi">1</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">SimpleRNN</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">TimeDistributed</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">))</code>
<code class="p">])</code></pre>

<p>The <code>Dense</code> layer actually supports sequences as inputs (and even higher-dimensional inputs): it handles them just like <code>TimeDistributed(Dense(…))</code>, meaning it is applied to the last input dimension only (independently across all time steps). So we could replace the last layer with just <code>Dense(10)</code>. For the sake of clarity we will keep using <code>TimeDistributed(Dense(10))</code> because it makes it clear that the <code>Dense</code> layer is applied independently at each time step and that the model will output a sequence, not just a single vector.</p>

<p>All outputs are needed during training, but only the output at the last time step is useful for predictions and for evaluation. So although we will rely on the MSE over all the outputs for training, we will use a custom metric for evaluation, to only compute the MSE over the output at the last time step:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">last_time_step_mse</code><code class="p">(</code><code class="n">Y_true</code><code class="p">,</code> <code class="n">Y_pred</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">keras</code><code class="o">.</code><code class="n">metrics</code><code class="o">.</code><code class="n">mean_squared_error</code><code class="p">(</code><code class="n">Y_true</code><code class="p">[:,</code> <code class="o">-</code><code class="mi">1</code><code class="p">],</code> <code class="n">Y_pred</code><code class="p">[:,</code> <code class="o">-</code><code class="mi">1</code><code class="p">])</code>

<code class="n">optimizer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">0.01</code><code class="p">)</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"mse"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="n">optimizer</code><code class="p">,</code> <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="n">last_time_step_mse</code><code class="p">])</code></pre>

<p>We get a validation MSE of about 0.006, which is 25% better than the previous model. You can combine this approach with the first one: just predict the next 10 values using this RNN, then concatenate these values to the input time series and use the model again to predict the next 10 values, and repeat the process as many times as needed. With this approach, you can generate arbitrarily long sequences. It may not be very accurate for long-term predictions, but it may be just fine if your goal is to generate original music or text, as we will see in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#nlp_chapter">Chapter&nbsp;16</a>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>When forecasting time series, it is often useful to have some error bars along with your predictions. For this, an efficient technique is MC Dropout, introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>: add an MC Dropout layer within each memory cell, dropping part of the inputs and hidden states. After training, to forecast a new time series, use the model many times and compute the mean and standard deviation of the predictions at each time step.</p>
</div>

<p>Simple RNNs can be quite good at forecasting time series or handling other kinds of sequences, but they do not perform as well on long time series or sequences. Let’s discuss why and see what we can do about it.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Handling Long Sequences"><div class="sect1" id="idm46263496230584">
<h1>Handling Long Sequences</h1>

<p>To train an RNN on long sequences, we must run it over many time steps, making the unrolled RNN a very deep network. Just like any deep neural network it may suffer from the unstable gradients problem, discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>: it may take forever to train, or training may be unstable. Moreover, when an RNN processes a long sequence, it will gradually forget the first inputs in the sequence. Let’s look at both these problems, starting with the unstable gradients problem.</p>








<section data-type="sect2" data-pdf-bookmark="Fighting the Unstable Gradients Problem"><div class="sect2" id="idm46263496153784">
<h2>Fighting the Unstable Gradients Problem</h2>

<p>Many of the tricks we used in deep nets to alleviate the unstable gradients problem can also be used for RNNs: good parameter initialization, faster optimizers, dropout, and so on. However, nonsaturating activation functions (e.g., ReLU) may not help as much here; in fact, they may actually lead the RNN to be even more unstable during training. Why? Well, suppose Gradient Descent updates the weights in a way that increases the outputs slightly at the first time step. Because the same weights are used at every time step, the outputs at the second time step may also be slightly increased, and the third, and so on until the outputs explode, and a nonsaturating activation function does not prevent that. You can reduce this risk by using a smaller learning rate, but you can also simply use a saturating activation function like the hyperbolic tangent (this explains why it is the default). In much the same way, the gradients themselves can explode. If you notice that training is unstable, you may want to monitor the size of the gradients (e.g., using TensorBoard) and perhaps use Gradient Clipping.</p>

<p>Moreover, Batch Normalization cannot be used as efficiently with RNNs as with deep feedforward nets. In fact, you cannot use it between time steps, only between recurrent layers. To be more precise, it is technically possible to add a BN layer to a memory cell (as we will see shortly) so that will be applied at each time step (both on the inputs for that time step and the hidden state from the previous step). However, the same BN layer will be used at each time step, with the same parameters, regardless of the actual scale and offset of the inputs and hidden state. In practice, this does not yield good results, as was demonstrated by César Laurent et al. in a <a href="https://homl.info/rnnbn">2015 paper</a>:<sup><a data-type="noteref" id="idm46263496149336-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263496149336" class="totri-footnote">2</a></sup> the authors found that BN was only slightly beneficial when it was applied to the inputs, not to the hidden states. In other words, it was slightly better than nothing when applied between recurrent layers (i.e., vertically in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#deep_rnn_diagram">Figure&nbsp;15-7</a>), but not within recurrent layers (i.e., horizontally). In Keras, this can be done simply by adding a <code>BatchNormalization</code> layer before each recurrent layer. But don’t expect too much from it.</p>

<p>Another form of normalization often works better with RNNs: <em>Layer Normalization</em>. This idea was introduced by Jimmy Lei Ba et al. in a <a href="https://homl.info/layernorm">2016 paper</a>:<sup><a data-type="noteref" id="idm46263496145416-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263496145416" class="totri-footnote">3</a></sup> it is very similar to Batch Normalization, but instead of normalizing across the batch dimension, it normalizes across the features dimension. One advantage is that it can compute the required statistics on the fly, at each time step, independently for each instance. This also means that it behaves the same way during training and testing (as opposed to BN), and it does not need to use exponential moving averages to estimate the feature statistics across all instances in the training set. Like BN, Layer Normalization learns a scale and an offset parameter for each input. In an RNN, it is typically used right after the linear combination of the inputs and the hidden states.</p>

<p>Let’s use tf.keras to implement Layer Norm within a simple memory cell. For this, we need to define a custom memory cell. It is just like a regular layer, except its <code>call()</code> method takes two arguments: the <code>inputs</code> at the current time step and the hidden <code>states</code> from the previous time step. Note that the <code>states</code> argument is a list containing one or more tensors. In the case of a simple RNN cell, it contains a single tensor, equal to the outputs of the previous time step, but other cells may have multiple state tensors (e.g., an <code>LSTMCell</code> has a long-term state and a short-term state, as we will see shortly).
A cell must also have a <code>state_size</code> attribute and an <code>output_size</code> attribute. In a simple RNN, both are simply equal to the number of units. The following code implements a custom memory cell which will behave like a SimpleRNNCell, except it will also apply Layer Normalization at each time step:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">LNSimpleRNNCell</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Layer</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">units</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"tanh"</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="nf-Magic">__init__</code><code class="p">(</code><code class="o">**</code><code class="n">kwargs</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">state_size</code> <code class="o">=</code> <code class="n">units</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">output_size</code> <code class="o">=</code> <code class="n">units</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">simple_rnn_cell</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">SimpleRNNCell</code><code class="p">(</code><code class="n">units</code><code class="p">,</code>
                                                          <code class="n">activation</code><code class="o">=</code><code class="bp">None</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">layer_norm</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">LayerNormalization</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">activation</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">activations</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">activation</code><code class="p">)</code>
    <code class="k">def</code> <code class="nf">call</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">,</code> <code class="n">states</code><code class="p">):</code>
        <code class="n">outputs</code><code class="p">,</code> <code class="n">new_states</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">simple_rnn_cell</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">states</code><code class="p">)</code>
        <code class="n">norm_outputs</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">activation</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">layer_norm</code><code class="p">(</code><code class="n">outputs</code><code class="p">))</code>
        <code class="k">return</code> <code class="n">norm_outputs</code><code class="p">,</code> <code class="p">[</code><code class="n">norm_outputs</code><code class="p">]</code></pre>

<p>The code is quite straightforward:<sup><a data-type="noteref" id="idm46263496136808-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263496136808" class="totri-footnote">4</a></sup> our <code>LNSimpleRNNCell</code> class inherits from the <code>keras.layers.Layer</code> class, just like any custom layer. The constructor takes the number of units and the desired activation function, it sets the <code>state_size</code> and <code>output_size</code> attributes, then it creates a <code>SimpleRNNCell</code> with no activation function (because we want to perform layer normalization after the linear operation but before the activation function). Then the constructor creates the <code>LayerNormalization</code> layer, and finally it fetches the desired activation function. The <code>call()</code> method starts by applying the simple RNN cell, which computes a linear combination of the current inputs and the previous hidden states, and it returns the result twice (indeed, in a <code>SimpleRNNCell</code>, the outputs are just equal to the hidden states: in other words, <code>new_states[0]</code> is equal to <code>outputs</code>, so we can safely ignore <code>new_states</code> in the rest of the <code>call()</code> method). Next, the <code>call()</code> method applies layer normalization, followed by the activation function. Finally, it returns the outputs twice (once as the outputs, and once as the new hidden states). To use this custom cell, all we need to do is to create a <code>keras.layers.RNN</code> layer, passing it a cell instance:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">RNN</code><code class="p">(</code><code class="n">LNSimpleRNNCell</code><code class="p">(</code><code class="mi">20</code><code class="p">),</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
                     <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">,</code> <code class="mi">1</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">RNN</code><code class="p">(</code><code class="n">LNSimpleRNNCell</code><code class="p">(</code><code class="mi">20</code><code class="p">),</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">TimeDistributed</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">))</code>
<code class="p">])</code></pre>

<p>Similarly, you could create a custom cell to apply dropout between each time step. But there’s a simpler way: all recurrent layers (except for <code>keras.layers.RNN</code>) and all cells provided by Keras have a <code>dropout</code> hyperparameter and a <code>recurrent_dropout</code> hyperparameter: the former defines the dropout rate to apply to the inputs (at each time step), and the latter defines the dropout rate for the hidden states (also at each time step). No need to create a custom cell to apply dropout at each time step in an RNN.</p>

<p>With these techniques, you can alleviate the unstable gradients problem and train an RNN much more efficiently. Now let’s look at how to deal with the short-memory problem.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Tackling the Short-Term Memory Problem"><div class="sect2" id="idm46263496153192">
<h2>Tackling the Short-Term Memory Problem</h2>

<p>Due to the transformations that the data goes through when traversing an RNN, some information is lost at each time step. After a while, the RNN’s state contains virtually no trace of the first inputs. This can be a showstopper: imagine Dory the fish<sup><a data-type="noteref" id="idm46263495818440-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495818440" class="totri-footnote">5</a></sup> trying to translate a long sentence. By the time she’s finished reading it, she has no clue how it started. To tackle this problem, various types of cells with long-term memory have been introduced. They have proven so successful that the basic cells are not used much anymore. Let’s first look at the most popular of these long-term memory cells: the LSTM cell.</p>










<section data-type="sect3" data-pdf-bookmark="LSTM cell"><div class="sect3" id="idm46263495917672">
<h3>LSTM cell</h3>

<p>The <em>Long Short-Term Memory</em> (LSTM) cell was <a href="https://homl.info/93">proposed in 1997</a><sup><a data-type="noteref" id="idm46263495914712-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495914712" class="totri-footnote">6</a></sup> by Sepp Hochreiter and Jürgen Schmidhuber, and it was gradually improved over the years by several researchers, such as Alex Graves, <a href="https://homl.info/94">Haşim Sak</a>,<sup><a data-type="noteref" id="idm46263495912808-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495912808" class="totri-footnote">7</a></sup> and <a href="https://homl.info/95">Wojciech Zaremba</a>.<sup><a data-type="noteref" id="idm46263495910856-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495910856" class="totri-footnote">8</a></sup> If you consider the LSTM cell as a black box, it can be used very much like a basic cell, except it will perform much better; training will converge faster, and it will detect long-term dependencies in the data. In Keras, you can simply use the <code>LSTM</code> layer instead of the <code>SimpleRNN</code> layer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">LSTM</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">,</code> <code class="mi">1</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">LSTM</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">TimeDistributed</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">))</code>
<code class="p">])</code></pre>

<p>Alternatively, you could use the general-purpose <code>keras.layers.RNN</code> layer, giving it an <code>LSTMCell</code> as an argument:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">RNN</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">LSTMCell</code><code class="p">(</code><code class="mi">20</code><code class="p">),</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
                     <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">,</code> <code class="mi">1</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">RNN</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">LSTMCell</code><code class="p">(</code><code class="mi">20</code><code class="p">),</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">TimeDistributed</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">))</code>
<code class="p">])</code></pre>

<p>However, the <code>LSTM</code> layer uses an optimized implementation when running on a GPU (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#deployment_chapter">Chapter&nbsp;19</a>), so in general it is preferable to use it (the <code>RNN</code> layer is mostly useful when you define custom cells, as we did earlier).</p>

<p>So how does an LSTM cell work? Its architecture is shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#lstm_cell_diagram">Figure&nbsp;15-9</a>.</p>

<figure><div id="lstm_cell_diagram" class="figure">
<img src="./Chapter15_files/mls2_1509.png" alt="mls2 1509" width="1438" height="859" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1509.png">
<h6><span class="label">Figure 15-9. </span>LSTM cell</h6>
</div></figure>

<p>If you don’t look at what’s inside the box, the LSTM cell looks exactly like a regular cell, except that its state is split in two vectors: <strong>h</strong><sub>(<em>t</em>)</sub> and <strong>c</strong><sub>(<em>t</em>)</sub> (“c” stands for “cell”). You can think of <strong>h</strong><sub>(<em>t</em>)</sub> as the short-term state and <strong>c</strong><sub>(<em>t</em>)</sub> as the long-term state.</p>

<p>Now let’s open the box! The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it. As the long-term state <strong>c</strong><sub>(<em>t</em>–1)</sub> traverses the network from left to right, you can see that it first goes through a <em>forget gate</em>, dropping some memories, and then it adds some new memories via the addition operation (which adds the memories that were selected by an <em>input gate</em>). The result <strong>c</strong><sub>(<em>t</em>)</sub> is sent straight out, without any further transformation. So, at each time step, some memories are dropped and some memories are added. Moreover, after the addition operation, the long-term state is copied and passed through the tanh function, and then the result is filtered by the <em>output gate</em>. This produces the short-term state <strong>h</strong><sub>(<em>t</em>)</sub> (which is equal to the cell’s output for this time step <strong>y</strong><sub>(<em>t</em>)</sub>). Now let’s look at where new memories come from and how the gates work.</p>

<p>First, the current input vector <strong>x</strong><sub>(<em>t</em>)</sub> and the previous short-term state <strong>h</strong><sub>(<em>t</em>–1)</sub> are fed to four different fully connected layers. They all serve a different purpose:</p>

<ul>
<li>
<p>The main layer is the one that outputs <strong>g</strong><sub>(<em>t</em>)</sub>. It has the usual role of analyzing the current inputs <strong>x</strong><sub>(<em>t</em>)</sub> and the previous (short-term) state <strong>h</strong><sub>(<em>t</em>–1)</sub>. In a basic cell, there is nothing else than this layer, and its output goes straight out to <strong>y</strong><sub>(<em>t</em>)</sub> and <strong>h</strong><sub>(<em>t</em>)</sub>. In contrast, in an LSTM cell this layer’s output does not go straight out, but instead its most important parts are stored in the long-term state (and the rest is dropped).</p>
</li>
<li>
<p>The three other layers are <em>gate controllers</em>. Since they use the logistic activation function, their outputs range from 0 to 1. As you can see, their outputs are fed to element-wise multiplication operations, so if they output 0s, they close the gate, and if they output 1s, they open it. Specifically:</p>

<ul>
<li>
<p>The <em>forget gate</em> (controlled by <strong>f</strong><sub>(<em>t</em>)</sub>) controls which parts of the long-term state should be erased.</p>
</li>
<li>
<p>The <em>input gate</em> (controlled by <strong>i</strong><sub>(<em>t</em>)</sub>) controls which parts of <strong>g</strong><sub>(<em>t</em>)</sub> should be added to the long-term state.</p>
</li>
<li>
<p>Finally, the <em>output gate</em> (controlled by <strong>o</strong><sub>(<em>t</em>)</sub>) controls which parts of the long-term state should be read and output at this time step, both to <strong>h</strong><sub>(<em>t</em>)</sub> and <strong>y</strong><sub>(<em>t</em>)</sub>.</p>
</li>
</ul>
</li>
</ul>

<p>In short, an LSTM cell can learn to recognize an important input (that’s the role of the input gate), store it in the long-term state, learn to preserve it for as long as it is needed (that’s the role of the forget gate), and learn to extract it whenever it is needed. This explains why they have been amazingly successful at capturing long-term patterns in time series, long texts, audio recordings, and more.</p>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#lstm_equation">Equation 15-3</a> summarizes how to compute the cell’s long-term state, its short-term state, and its output at each time step for a single instance (the equations for a whole mini-batch are very similar).</p>
<div id="lstm_equation" data-type="equation"><h5><span class="label">Equation 15-3. </span>LSTM computations</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-146-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mtable displaystyle=&quot;true&quot;&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;i&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;&amp;#x3C3;&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;h&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;b&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;&amp;#x3C3;&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;h&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;b&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;o&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;&amp;#x3C3;&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;h&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;b&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;g&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo form=&quot;prefix&quot;&gt;tanh&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;h&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;b&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;c&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;f&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2297;&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;c&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;i&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2297;&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;g&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;h&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;o&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2297;&lt;/mo&gt;&lt;mo form=&quot;prefix&quot;&gt;tanh&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;c&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-6037" style="width: 20.054em; display: inline-block;"><span style="display: inline-block; position: relative; width: 19.437em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(-3.082em, 1019.14em, 6.789em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-6038"><span class="mtable" id="MathJax-Span-6039" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 19.129em; height: 0px;"><span style="position: absolute; clip: rect(5.35em, 1001.5em, 15.016em, -1000.01em); top: -10.332em; left: 0em;"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1001.19em, 4.527em, -1000.01em); top: -8.121em; right: 0em;"><span class="mtd" id="MathJax-Span-6040"><span class="mrow" id="MathJax-Span-6041"><span class="msub" id="MathJax-Span-6042"><span style="display: inline-block; position: relative; width: 1.185em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.32em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6043" style="font-family: MathJax_Main-bold;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.311em;"><span class="mrow" id="MathJax-Span-6044"><span class="mo" id="MathJax-Span-6045" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6046" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6047" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1001.25em, 4.527em, -1000.01em); top: -6.476em; right: 0em;"><span class="mtd" id="MathJax-Span-6090"><span class="mrow" id="MathJax-Span-6091"><span class="msub" id="MathJax-Span-6092"><span style="display: inline-block; position: relative; width: 1.237em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.47em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6093" style="font-family: MathJax_Main-bold;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.362em;"><span class="mrow" id="MathJax-Span-6094"><span class="mo" id="MathJax-Span-6095" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6096" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6097" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.45em, 4.527em, -1000.01em); top: -4.779em; right: 0em;"><span class="mtd" id="MathJax-Span-6140"><span class="mrow" id="MathJax-Span-6141"><span class="msub" id="MathJax-Span-6142"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.52em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6143" style="font-family: MathJax_Main-bold;">o</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.568em;"><span class="mrow" id="MathJax-Span-6144"><span class="mo" id="MathJax-Span-6145" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6146" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6147" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.45em, 4.527em, -1000.01em); top: -3.082em; right: 0em;"><span class="mtd" id="MathJax-Span-6190"><span class="mrow" id="MathJax-Span-6191"><span class="msub" id="MathJax-Span-6192"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6193" style="font-family: MathJax_Main-bold;">g</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.568em;"><span class="mrow" id="MathJax-Span-6194"><span class="mo" id="MathJax-Span-6195" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6196" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6197" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.4em, 4.527em, -1000.01em); top: -1.488em; right: 0em;"><span class="mtd" id="MathJax-Span-6240"><span class="mrow" id="MathJax-Span-6241"><span class="msub" id="MathJax-Span-6242"><span style="display: inline-block; position: relative; width: 1.391em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6243" style="font-family: MathJax_Main-bold;">c</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.517em;"><span class="mrow" id="MathJax-Span-6244"><span class="mo" id="MathJax-Span-6245" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6246" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6247" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.5em, 4.527em, -1000.01em); top: 0.157em; right: 0em;"><span class="mtd" id="MathJax-Span-6283"><span class="mrow" id="MathJax-Span-6284"><span class="msub" id="MathJax-Span-6285"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6286" style="font-family: MathJax_Main-bold;">y</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6287"><span class="mo" id="MathJax-Span-6288" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6289" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6290" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 10.337em;"></span></span><span style="position: absolute; clip: rect(5.35em, 1016.67em, 15.221em, -1000.01em); top: -10.537em; left: 2.265em;"><span style="display: inline-block; position: relative; width: 16.815em; height: 0px;"><span style="position: absolute; clip: rect(2.985em, 1014.61em, 4.527em, -1000.01em); top: -8.121em; left: 0em;"><span class="mtd" id="MathJax-Span-6048"><span class="mrow" id="MathJax-Span-6049"><span class="mrow" id="MathJax-Span-6050"><span class="mo" id="MathJax-Span-6051" style="font-family: MathJax_Main;">=</span><span class="mi" id="MathJax-Span-6052" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-6053" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="msup" id="MathJax-Span-6054"><span style="display: inline-block; position: relative; width: 2.47em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.91em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6055"><span class="msub" id="MathJax-Span-6056"><span style="display: inline-block; position: relative; width: 1.905em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6057" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mrow" id="MathJax-Span-6058"><span class="mi" id="MathJax-Span-6059" style="font-size: 70.7%; font-family: MathJax_Math-italic;">x</span><span class="mi" id="MathJax-Span-6060" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 1.905em;"><span class="mi" id="MathJax-Span-6061" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-6062"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6063" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6064"><span class="mo" id="MathJax-Span-6065" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6066" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6067" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6068" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msup" id="MathJax-Span-6069" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 2.47em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.91em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6070"><span class="msub" id="MathJax-Span-6071"><span style="display: inline-block; position: relative; width: 1.905em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6072" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mrow" id="MathJax-Span-6073"><span class="mi" id="MathJax-Span-6074" style="font-size: 70.7%; font-family: MathJax_Math-italic;">h</span><span class="mi" id="MathJax-Span-6075" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 1.905em;"><span class="mi" id="MathJax-Span-6076" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-6077"><span style="display: inline-block; position: relative; width: 2.419em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6078" style="font-family: MathJax_Main-bold;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6079"><span class="mo" id="MathJax-Span-6080" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6081" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6082" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-6083" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-6084" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6085" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msub" id="MathJax-Span-6086" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6087" style="font-family: MathJax_Main-bold;">b</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mi" id="MathJax-Span-6088" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6089" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1015.02em, 4.527em, -1000.01em); top: -6.476em; left: 0em;"><span class="mtd" id="MathJax-Span-6098"><span class="mrow" id="MathJax-Span-6099"><span class="mrow" id="MathJax-Span-6100"><span class="mo" id="MathJax-Span-6101" style="font-family: MathJax_Main;">=</span><span class="mi" id="MathJax-Span-6102" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-6103" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="msup" id="MathJax-Span-6104"><span style="display: inline-block; position: relative; width: 2.625em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1002.07em, 4.476em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6105"><span class="msub" id="MathJax-Span-6106"><span style="display: inline-block; position: relative; width: 2.059em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6107" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mrow" id="MathJax-Span-6108"><span class="mi" id="MathJax-Span-6109" style="font-size: 70.7%; font-family: MathJax_Math-italic;">x</span><span class="mi" id="MathJax-Span-6110" style="font-size: 70.7%; font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 2.059em;"><span class="mi" id="MathJax-Span-6111" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-6112"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6113" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6114"><span class="mo" id="MathJax-Span-6115" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6116" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6117" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6118" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msup" id="MathJax-Span-6119" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 2.625em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1002.07em, 4.476em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6120"><span class="msub" id="MathJax-Span-6121"><span style="display: inline-block; position: relative; width: 2.059em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6122" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mrow" id="MathJax-Span-6123"><span class="mi" id="MathJax-Span-6124" style="font-size: 70.7%; font-family: MathJax_Math-italic;">h</span><span class="mi" id="MathJax-Span-6125" style="font-size: 70.7%; font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 2.059em;"><span class="mi" id="MathJax-Span-6126" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-6127"><span style="display: inline-block; position: relative; width: 2.419em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6128" style="font-family: MathJax_Main-bold;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6129"><span class="mo" id="MathJax-Span-6130" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6131" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6132" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-6133" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-6134" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6135" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msub" id="MathJax-Span-6136" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.082em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6137" style="font-family: MathJax_Main-bold;">b</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mi" id="MathJax-Span-6138" style="font-size: 70.7%; font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6139" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1014.92em, 4.527em, -1000.01em); top: -4.779em; left: 0em;"><span class="mtd" id="MathJax-Span-6148"><span class="mrow" id="MathJax-Span-6149"><span class="mrow" id="MathJax-Span-6150"><span class="mo" id="MathJax-Span-6151" style="font-family: MathJax_Main;">=</span><span class="mi" id="MathJax-Span-6152" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-6153" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="msup" id="MathJax-Span-6154"><span style="display: inline-block; position: relative; width: 2.573em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1002.02em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6155"><span class="msub" id="MathJax-Span-6156"><span style="display: inline-block; position: relative; width: 2.008em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6157" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mrow" id="MathJax-Span-6158"><span class="mi" id="MathJax-Span-6159" style="font-size: 70.7%; font-family: MathJax_Math-italic;">x</span><span class="mi" id="MathJax-Span-6160" style="font-size: 70.7%; font-family: MathJax_Math-italic;">o</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 2.008em;"><span class="mi" id="MathJax-Span-6161" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-6162"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6163" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6164"><span class="mo" id="MathJax-Span-6165" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6166" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6167" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6168" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msup" id="MathJax-Span-6169" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 2.573em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1002.02em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6170"><span class="msub" id="MathJax-Span-6171"><span style="display: inline-block; position: relative; width: 2.008em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6172" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mrow" id="MathJax-Span-6173"><span class="mi" id="MathJax-Span-6174" style="font-size: 70.7%; font-family: MathJax_Math-italic;">h</span><span class="mi" id="MathJax-Span-6175" style="font-size: 70.7%; font-family: MathJax_Math-italic;">o</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 2.008em;"><span class="mi" id="MathJax-Span-6176" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-6177"><span style="display: inline-block; position: relative; width: 2.419em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6178" style="font-family: MathJax_Main-bold;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6179"><span class="mo" id="MathJax-Span-6180" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6181" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6182" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-6183" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-6184" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6185" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msub" id="MathJax-Span-6186" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.082em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6187" style="font-family: MathJax_Main-bold;">b</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mi" id="MathJax-Span-6188" style="font-size: 70.7%; font-family: MathJax_Math-italic;">o</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6189" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1016.67em, 4.527em, -1000.01em); top: -3.082em; left: 0em;"><span class="mtd" id="MathJax-Span-6198"><span class="mrow" id="MathJax-Span-6199"><span class="mrow" id="MathJax-Span-6200"><span class="mo" id="MathJax-Span-6201" style="font-family: MathJax_Main;">=</span><span class="mo" id="MathJax-Span-6202" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">tanh</span><span class="mo" id="MathJax-Span-6203" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="msup" id="MathJax-Span-6204"><span style="display: inline-block; position: relative; width: 2.573em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1002.02em, 4.476em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6205"><span class="msub" id="MathJax-Span-6206"><span style="display: inline-block; position: relative; width: 2.008em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6207" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mrow" id="MathJax-Span-6208"><span class="mi" id="MathJax-Span-6209" style="font-size: 70.7%; font-family: MathJax_Math-italic;">x</span><span class="mi" id="MathJax-Span-6210" style="font-size: 70.7%; font-family: MathJax_Math-italic;">g<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 2.008em;"><span class="mi" id="MathJax-Span-6211" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-6212"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6213" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6214"><span class="mo" id="MathJax-Span-6215" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6216" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6217" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6218" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msup" id="MathJax-Span-6219" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 2.573em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1002.02em, 4.476em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6220"><span class="msub" id="MathJax-Span-6221"><span style="display: inline-block; position: relative; width: 2.008em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6222" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mrow" id="MathJax-Span-6223"><span class="mi" id="MathJax-Span-6224" style="font-size: 70.7%; font-family: MathJax_Math-italic;">h</span><span class="mi" id="MathJax-Span-6225" style="font-size: 70.7%; font-family: MathJax_Math-italic;">g<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 2.008em;"><span class="mi" id="MathJax-Span-6226" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-6227"><span style="display: inline-block; position: relative; width: 2.419em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6228" style="font-family: MathJax_Main-bold;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6229"><span class="mo" id="MathJax-Span-6230" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6231" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6232" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-6233" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-6234" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6235" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msub" id="MathJax-Span-6236" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6237" style="font-family: MathJax_Main-bold;">b</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mi" id="MathJax-Span-6238" style="font-size: 70.7%; font-family: MathJax_Math-italic;">g<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6239" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1011.12em, 4.527em, -1000.01em); top: -1.488em; left: 0em;"><span class="mtd" id="MathJax-Span-6248"><span class="mrow" id="MathJax-Span-6249"><span class="mrow" id="MathJax-Span-6250"><span class="mo" id="MathJax-Span-6251" style="font-family: MathJax_Main;">=</span><span class="msub" id="MathJax-Span-6252" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 1.237em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.47em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6253" style="font-family: MathJax_Main-bold;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.362em;"><span class="mrow" id="MathJax-Span-6254"><span class="mo" id="MathJax-Span-6255" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6256" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6257" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6258" style="font-family: MathJax_Main; padding-left: 0.208em;">⊗</span><span class="msub" id="MathJax-Span-6259" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 2.316em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6260" style="font-family: MathJax_Main-bold;">c</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.517em;"><span class="mrow" id="MathJax-Span-6261"><span class="mo" id="MathJax-Span-6262" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6263" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6264" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-6265" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-6266" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mspace" id="MathJax-Span-6267" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-6268" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mspace" id="MathJax-Span-6269" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="msub" id="MathJax-Span-6270" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.185em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.32em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6271" style="font-family: MathJax_Main-bold;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.311em;"><span class="mrow" id="MathJax-Span-6272"><span class="mo" id="MathJax-Span-6273" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6274" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6275" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6276" style="font-family: MathJax_Main; padding-left: 0.208em;">⊗</span><span class="msub" id="MathJax-Span-6277" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6278" style="font-family: MathJax_Main-bold;">g</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.568em;"><span class="mrow" id="MathJax-Span-6279"><span class="mo" id="MathJax-Span-6280" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6281" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6282" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1011.07em, 4.527em, -1000.01em); top: 0.157em; left: 0em;"><span class="mtd" id="MathJax-Span-6291"><span class="mrow" id="MathJax-Span-6292"><span class="mrow" id="MathJax-Span-6293"><span class="mo" id="MathJax-Span-6294" style="font-family: MathJax_Main;">=</span><span class="msub" id="MathJax-Span-6295" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 1.545em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6296" style="font-family: MathJax_Main-bold;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6297"><span class="mo" id="MathJax-Span-6298" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6299" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6300" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6301" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="msub" id="MathJax-Span-6302" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.52em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6303" style="font-family: MathJax_Main-bold;">o</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.568em;"><span class="mrow" id="MathJax-Span-6304"><span class="mo" id="MathJax-Span-6305" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6306" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6307" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6308" style="font-family: MathJax_Main; padding-left: 0.208em;">⊗</span><span class="mo" id="MathJax-Span-6309" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">tanh</span><span class="mrow" id="MathJax-Span-6310"><span class="mo" id="MathJax-Span-6311" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="msub" id="MathJax-Span-6312"><span style="display: inline-block; position: relative; width: 1.391em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6313" style="font-family: MathJax_Main-bold;">c</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.517em;"><span class="mrow" id="MathJax-Span-6314"><span class="mo" id="MathJax-Span-6315" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6316" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6317" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6318" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 10.543em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -4.711em; border-left: 0px solid; width: 0px; height: 9.959em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi mathvariant="bold">i</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd><mtd columnalign="left"><mrow><mo>=</mo><mi>σ</mi><mo>(</mo><msup><mrow><msub><mi mathvariant="bold">W</mi><mrow><mi>x</mi><mi>i</mi></mrow></msub></mrow><mi>T</mi></msup><msub><mi mathvariant="bold">x</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>+</mo><msup><mrow><msub><mi mathvariant="bold">W</mi><mrow><mi>h</mi><mi>i</mi></mrow></msub></mrow><mi>T</mi></msup><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub><mo>+</mo><msub><mi mathvariant="bold">b</mi><mi>i</mi></msub><mo>)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right"><msub><mi mathvariant="bold">f</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd><mtd columnalign="left"><mrow><mo>=</mo><mi>σ</mi><mo>(</mo><msup><mrow><msub><mi mathvariant="bold">W</mi><mrow><mi>x</mi><mi>f</mi></mrow></msub></mrow><mi>T</mi></msup><msub><mi mathvariant="bold">x</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>+</mo><msup><mrow><msub><mi mathvariant="bold">W</mi><mrow><mi>h</mi><mi>f</mi></mrow></msub></mrow><mi>T</mi></msup><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub><mo>+</mo><msub><mi mathvariant="bold">b</mi><mi>f</mi></msub><mo>)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right"><msub><mi mathvariant="bold">o</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd><mtd columnalign="left"><mrow><mo>=</mo><mi>σ</mi><mo>(</mo><msup><mrow><msub><mi mathvariant="bold">W</mi><mrow><mi>x</mi><mi>o</mi></mrow></msub></mrow><mi>T</mi></msup><msub><mi mathvariant="bold">x</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>+</mo><msup><mrow><msub><mi mathvariant="bold">W</mi><mrow><mi>h</mi><mi>o</mi></mrow></msub></mrow><mi>T</mi></msup><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub><mo>+</mo><msub><mi mathvariant="bold">b</mi><mi>o</mi></msub><mo>)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right"><msub><mi mathvariant="bold">g</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd><mtd columnalign="left"><mrow><mo>=</mo><mo form="prefix">tanh</mo><mo>(</mo><msup><mrow><msub><mi mathvariant="bold">W</mi><mrow><mi>x</mi><mi>g</mi></mrow></msub></mrow><mi>T</mi></msup><msub><mi mathvariant="bold">x</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>+</mo><msup><mrow><msub><mi mathvariant="bold">W</mi><mrow><mi>h</mi><mi>g</mi></mrow></msub></mrow><mi>T</mi></msup><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub><mo>+</mo><msub><mi mathvariant="bold">b</mi><mi>g</mi></msub><mo>)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right"><msub><mi mathvariant="bold">c</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd><mtd columnalign="left"><mrow><mo>=</mo><msub><mi mathvariant="bold">f</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊗</mo><msub><mi mathvariant="bold">c</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub><mspace width="0.166667em"></mspace><mo>+</mo><mspace width="0.166667em"></mspace><msub><mi mathvariant="bold">i</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊗</mo><msub><mi mathvariant="bold">g</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mrow></mtd></mtr><mtr><mtd columnalign="right"><msub><mi mathvariant="bold">y</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd><mtd columnalign="left"><mrow><mo>=</mo><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><msub><mi mathvariant="bold">o</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊗</mo><mo form="prefix">tanh</mo><mrow><mo>(</mo><msub><mi mathvariant="bold">c</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>)</mo></mrow></mrow></mtd></mtr></mtable></math></span></span></div><script type="math/mml" id="MathJax-Element-146"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mi>σ</mi>
          <mo>(</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>i</mi></mrow> </msub></mrow> <mi>T</mi> </msup>
          <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>i</mi></mrow> </msub></mrow> <mi>T</mi> </msup>
          <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msub><mi mathvariant="bold">b</mi> <mi>i</mi> </msub>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">f</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mi>σ</mi>
          <mo>(</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>f</mi></mrow> </msub></mrow> <mi>T</mi> </msup>
          <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>f</mi></mrow> </msub></mrow> <mi>T</mi> </msup>
          <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msub><mi mathvariant="bold">b</mi> <mi>f</mi> </msub>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">o</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mi>σ</mi>
          <mo>(</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>o</mi></mrow> </msub></mrow> <mi>T</mi> </msup>
          <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>o</mi></mrow> </msub></mrow> <mi>T</mi> </msup>
          <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msub><mi mathvariant="bold">b</mi> <mi>o</mi> </msub>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">g</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mo form="prefix">tanh</mo>
          <mo>(</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>g</mi></mrow> </msub></mrow> <mi>T</mi> </msup>
          <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>g</mi></mrow> </msub></mrow> <mi>T</mi> </msup>
          <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msub><mi mathvariant="bold">b</mi> <mi>g</mi> </msub>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">c</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <msub><mi mathvariant="bold">f</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>⊗</mo>
          <msub><mi mathvariant="bold">c</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
          <mspace width="0.166667em"></mspace>
          <mo>+</mo>
          <mspace width="0.166667em"></mspace>
          <msub><mi mathvariant="bold">i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>⊗</mo>
          <msub><mi mathvariant="bold">g</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">y</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>=</mo>
          <msub><mi mathvariant="bold">o</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>⊗</mo>
          <mo form="prefix">tanh</mo>
          <mrow>
            <mo>(</mo>
            <msub><mi mathvariant="bold">c</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></script></div>

<p>In this equation:</p>

<ul>
<li>
<p><strong>W</strong><sub><em>xi</em></sub>, <strong>W</strong><sub><em>xf</em></sub>, <strong>W</strong><sub><em>xo</em></sub>, <strong>W</strong><sub><em>xg</em></sub> are the weight matrices of each of the four layers for their connection to the input vector <strong>x</strong><sub>(<em>t</em>)</sub>.</p>
</li>
<li>
<p><strong>W</strong><sub><em>hi</em></sub>, <strong>W</strong><sub><em>hf</em></sub>, <strong>W</strong><sub><em>ho</em></sub>, and <strong>W</strong><sub><em>hg</em></sub> are the weight matrices of each of the four layers for their connection to the previous short-term state <strong>h</strong><sub>(<em>t</em>–1)</sub>.</p>
</li>
<li>
<p><strong>b</strong><sub><em>i</em></sub>, <strong>b</strong><sub><em>f</em></sub>, <strong>b</strong><sub><em>o</em></sub>, and <strong>b</strong><sub><em>g</em></sub> are the bias terms for each of the four layers. Note that TensorFlow initializes <strong>b</strong><sub><em>f</em></sub> to a vector full of 1s instead of 0s. This prevents forgetting everything at the beginning of training.</p>
</li>
</ul>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Peephole connections"><div class="sect3" id="idm46263495917080">
<h3>Peephole connections</h3>

<p>In a regular LSTM cell, the gate controllers can look only at the input <strong>x</strong><sub>(<em>t</em>)</sub> and the previous short-term state <strong>h</strong><sub>(<em>t</em>–1)</sub>. It may be a good idea to give them a bit more context by letting them peek at the long-term state as well. This idea was <a href="https://homl.info/96">proposed by Felix Gers and Jürgen Schmidhuber in 2000</a>.<sup><a data-type="noteref" id="idm46263495465816-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495465816" class="totri-footnote">9</a></sup> They proposed an LSTM variant with extra connections called <em>peephole connections</em>: the previous long-term state <strong>c</strong><sub>(<em>t</em>–1)</sub> is added as an input to the controllers of the forget gate and the input gate, and the current long-term state <strong>c</strong><sub>(<em>t</em>)</sub> is added as input to the controller of the output gate. This often improves performance, but not always, and there is no clear pattern on which tasks are better off with or without them: you will have to try it on your task and see if it helps.</p>

<p>In Keras, the <code>LSTM</code> layer is based on the <code>keras.layers.LSTMCell</code> cell, which does not support peepholes. However, the experimental <code>tf.keras.experimental.PeepholeLSTMCell</code> does, so you can create a <code>keras.layers.RNN</code> layer, passing a <code>PeepholeLSTMCell</code> to its constructor.</p>

<p>There are many other variants of the LSTM cell. One particularly popular variant is the GRU cell, which we will look at now.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="GRU cell"><div class="sect3" id="idm46263495458488">
<h3>GRU cell</h3>

<p>The <em>Gated Recurrent Unit</em> (GRU) cell (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#gru_cell_diagram">Figure&nbsp;15-10</a>) was proposed by Kyunghyun Cho et al. in a <a href="https://homl.info/97">2014 paper</a><sup><a data-type="noteref" id="idm46263495455048-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495455048">10</a></sup> that also introduced the Encoder–Decoder network we discussed earlier.</p>

<figure><div id="gru_cell_diagram" class="figure">
<img src="./Chapter15_files/mls2_1510.png" alt="mls2 1510" width="1440" height="979" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1510.png">
<h6><span class="label">Figure 15-10. </span>GRU cell</h6>
</div></figure>

<p>The GRU cell is a simplified version of the LSTM cell, and it seems to perform just as well<sup><a data-type="noteref" id="idm46263495451944-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495451944">11</a></sup> (which explains its growing popularity). These are the main simplifications:</p>

<ul>
<li>
<p>Both state vectors are merged into a single vector <strong>h</strong><sub>(<em>t</em>)</sub>.</p>
</li>
<li>
<p>A single gate controller <strong>z</strong><sub>(<em>t</em>)</sub> controls both the forget gate and the input gate. If the gate controller outputs a 1, the forget gate is open (=&nbsp;1) and the input gate is closed (1&nbsp;–&nbsp;1&nbsp;=&nbsp;0). If it outputs a 0, the opposite happens. In other words, whenever a memory must be stored, the location where it will be stored is erased first. This is actually a frequent variant to the LSTM cell in and of itself.</p>
</li>
<li>
<p>There is no output gate; the full state vector is output at every time step. However, there is a new gate controller <strong>r</strong><sub>(<em>t</em>)</sub> that controls which part of the previous state will be shown to the main layer (<strong>g</strong><sub>(<em>t</em>)</sub>).</p>
</li>
</ul>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#gru_equation">Equation 15-4</a> summarizes how to compute the cell’s state at each time step for a single instance.</p>
<div id="gru_equation" data-type="equation"><h5><span class="label">Equation 15-4. </span>GRU computations</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-147-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mtable displaystyle=&quot;true&quot;&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;&amp;#x3C3;&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;h&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;b&lt;/mi&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;r&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;&amp;#x3C3;&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;h&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;b&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;g&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo form=&quot;prefix&quot;&gt;tanh&lt;/mo&gt;&lt;mfenced separators=&quot;&quot; open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;r&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2297;&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;h&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;b&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;h&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;&amp;#x2297;&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;h&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x2297;&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;g&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-6319" style="width: 23.807em; display: inline-block;"><span style="display: inline-block; position: relative; width: 23.088em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(-1.437em, 1022.79em, 5.144em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-6320"><span class="mtable" id="MathJax-Span-6321" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 22.779em; height: 0px;"><span style="position: absolute; clip: rect(3.756em, 1001.5em, 9.874em, -1000.01em); top: -6.836em; left: 0em;"><span style="display: inline-block; position: relative; width: 1.545em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1001.4em, 4.527em, -1000.01em); top: -6.476em; right: 0em;"><span class="mtd" id="MathJax-Span-6322"><span class="mrow" id="MathJax-Span-6323"><span class="msub" id="MathJax-Span-6324"><span style="display: inline-block; position: relative; width: 1.391em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6325" style="font-family: MathJax_Main-bold;">z</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.517em;"><span class="mrow" id="MathJax-Span-6326"><span class="mo" id="MathJax-Span-6327" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6328" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6329" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.35em, 4.527em, -1000.01em); top: -4.83em; right: 0em;"><span class="mtd" id="MathJax-Span-6372"><span class="mrow" id="MathJax-Span-6373"><span class="msub" id="MathJax-Span-6374"><span style="display: inline-block; position: relative; width: 1.339em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.42em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6375" style="font-family: MathJax_Main-bold;">r</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.465em;"><span class="mrow" id="MathJax-Span-6376"><span class="mo" id="MathJax-Span-6377" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6378" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6379" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.45em, 4.527em, -1000.01em); top: -3.134em; right: 0em;"><span class="mtd" id="MathJax-Span-6422"><span class="mrow" id="MathJax-Span-6423"><span class="msub" id="MathJax-Span-6424"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6425" style="font-family: MathJax_Main-bold;">g</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.568em;"><span class="mrow" id="MathJax-Span-6426"><span class="mo" id="MathJax-Span-6427" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6428" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6429" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1001.5em, 4.527em, -1000.01em); top: -1.488em; right: 0em;"><span class="mtd" id="MathJax-Span-6483"><span class="mrow" id="MathJax-Span-6484"><span class="msub" id="MathJax-Span-6485"><span style="display: inline-block; position: relative; width: 1.545em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6486" style="font-family: MathJax_Main-bold;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6487"><span class="mo" id="MathJax-Span-6488" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6489" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6490" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 6.841em;"></span></span><span style="position: absolute; clip: rect(3.704em, 1020.32em, 10.285em, -1000.01em); top: -7.247em; left: 2.316em;"><span style="display: inline-block; position: relative; width: 20.465em; height: 0px;"><span style="position: absolute; clip: rect(2.985em, 1014.87em, 4.527em, -1000.01em); top: -6.476em; left: 0em;"><span class="mtd" id="MathJax-Span-6330"><span class="mrow" id="MathJax-Span-6331"><span class="mrow" id="MathJax-Span-6332"><span class="mo" id="MathJax-Span-6333" style="font-family: MathJax_Main;">=</span><span class="mi" id="MathJax-Span-6334" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-6335" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="msup" id="MathJax-Span-6336"><span style="display: inline-block; position: relative; width: 2.573em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1002.02em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6337"><span class="msub" id="MathJax-Span-6338"><span style="display: inline-block; position: relative; width: 2.008em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6339" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mrow" id="MathJax-Span-6340"><span class="mi" id="MathJax-Span-6341" style="font-size: 70.7%; font-family: MathJax_Math-italic;">x</span><span class="mi" id="MathJax-Span-6342" style="font-size: 70.7%; font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 2.008em;"><span class="mi" id="MathJax-Span-6343" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-6344"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6345" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6346"><span class="mo" id="MathJax-Span-6347" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6348" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6349" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6350" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msup" id="MathJax-Span-6351" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 2.573em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1002.02em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6352"><span class="msub" id="MathJax-Span-6353"><span style="display: inline-block; position: relative; width: 2.008em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6354" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mrow" id="MathJax-Span-6355"><span class="mi" id="MathJax-Span-6356" style="font-size: 70.7%; font-family: MathJax_Math-italic;">h</span><span class="mi" id="MathJax-Span-6357" style="font-size: 70.7%; font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 2.008em;"><span class="mi" id="MathJax-Span-6358" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-6359"><span style="display: inline-block; position: relative; width: 2.419em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6360" style="font-family: MathJax_Main-bold;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6361"><span class="mo" id="MathJax-Span-6362" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6363" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6364" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-6365" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-6366" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6367" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msub" id="MathJax-Span-6368" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6369" style="font-family: MathJax_Main-bold;">b</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mi" id="MathJax-Span-6370" style="font-size: 70.7%; font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6371" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1014.82em, 4.527em, -1000.01em); top: -4.83em; left: 0em;"><span class="mtd" id="MathJax-Span-6380"><span class="mrow" id="MathJax-Span-6381"><span class="mrow" id="MathJax-Span-6382"><span class="mo" id="MathJax-Span-6383" style="font-family: MathJax_Main;">=</span><span class="mi" id="MathJax-Span-6384" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-6385" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="msup" id="MathJax-Span-6386"><span style="display: inline-block; position: relative; width: 2.573em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.97em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6387"><span class="msub" id="MathJax-Span-6388"><span style="display: inline-block; position: relative; width: 2.008em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6389" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mrow" id="MathJax-Span-6390"><span class="mi" id="MathJax-Span-6391" style="font-size: 70.7%; font-family: MathJax_Math-italic;">x</span><span class="mi" id="MathJax-Span-6392" style="font-size: 70.7%; font-family: MathJax_Math-italic;">r</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 2.008em;"><span class="mi" id="MathJax-Span-6393" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-6394"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6395" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6396"><span class="mo" id="MathJax-Span-6397" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6398" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6399" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6400" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msup" id="MathJax-Span-6401" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 2.573em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1002.02em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6402"><span class="msub" id="MathJax-Span-6403"><span style="display: inline-block; position: relative; width: 2.008em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6404" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mrow" id="MathJax-Span-6405"><span class="mi" id="MathJax-Span-6406" style="font-size: 70.7%; font-family: MathJax_Math-italic;">h</span><span class="mi" id="MathJax-Span-6407" style="font-size: 70.7%; font-family: MathJax_Math-italic;">r</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 2.008em;"><span class="mi" id="MathJax-Span-6408" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-6409"><span style="display: inline-block; position: relative; width: 2.419em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6410" style="font-family: MathJax_Main-bold;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6411"><span class="mo" id="MathJax-Span-6412" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6413" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6414" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-6415" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-6416" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6417" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msub" id="MathJax-Span-6418" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6419" style="font-family: MathJax_Main-bold;">b</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mi" id="MathJax-Span-6420" style="font-size: 70.7%; font-family: MathJax_Math-italic;">r</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6421" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1020.32em, 4.527em, -1000.01em); top: -3.134em; left: 0em;"><span class="mtd" id="MathJax-Span-6430"><span class="mrow" id="MathJax-Span-6431"><span class="mrow" id="MathJax-Span-6432"><span class="mo" id="MathJax-Span-6433" style="font-family: MathJax_Main;">=</span><span class="mo" id="MathJax-Span-6434" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">tanh</span><span class="mfenced" id="MathJax-Span-6435"><span class="mo" id="MathJax-Span-6436" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="msup" id="MathJax-Span-6437"><span style="display: inline-block; position: relative; width: 2.573em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1002.02em, 4.476em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6438"><span class="msub" id="MathJax-Span-6439"><span style="display: inline-block; position: relative; width: 2.008em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6440" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mrow" id="MathJax-Span-6441"><span class="mi" id="MathJax-Span-6442" style="font-size: 70.7%; font-family: MathJax_Math-italic;">x</span><span class="mi" id="MathJax-Span-6443" style="font-size: 70.7%; font-family: MathJax_Math-italic;">g<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 2.008em;"><span class="mi" id="MathJax-Span-6444" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-6445"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6446" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6447"><span class="mo" id="MathJax-Span-6448" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6449" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6450" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6451" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msup" id="MathJax-Span-6452" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 2.573em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1002.02em, 4.476em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6453"><span class="msub" id="MathJax-Span-6454"><span style="display: inline-block; position: relative; width: 2.008em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.14em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6455" style="font-family: MathJax_Main-bold;">W</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 1.185em;"><span class="mrow" id="MathJax-Span-6456"><span class="mi" id="MathJax-Span-6457" style="font-size: 70.7%; font-family: MathJax_Math-italic;">h</span><span class="mi" id="MathJax-Span-6458" style="font-size: 70.7%; font-family: MathJax_Math-italic;">g<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 2.008em;"><span class="mi" id="MathJax-Span-6459" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-6460" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-6461" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="msub" id="MathJax-Span-6462"><span style="display: inline-block; position: relative; width: 1.339em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.42em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6463" style="font-family: MathJax_Main-bold;">r</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.465em;"><span class="mrow" id="MathJax-Span-6464"><span class="mo" id="MathJax-Span-6465" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6466" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6467" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6468" style="font-family: MathJax_Main; padding-left: 0.208em;">⊗</span><span class="msub" id="MathJax-Span-6469" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 2.419em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6470" style="font-family: MathJax_Main-bold;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6471"><span class="mo" id="MathJax-Span-6472" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6473" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6474" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-6475" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-6476" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6477" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span><span class="mo" id="MathJax-Span-6478" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msub" id="MathJax-Span-6479" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6480" style="font-family: MathJax_Main-bold;">b</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mi" id="MathJax-Span-6481" style="font-size: 70.7%; font-family: MathJax_Math-italic;">g<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6482" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1013.89em, 4.527em, -1000.01em); top: -1.488em; left: 0em;"><span class="mtd" id="MathJax-Span-6491"><span class="mrow" id="MathJax-Span-6492"><span class="mrow" id="MathJax-Span-6493"><span class="mo" id="MathJax-Span-6494" style="font-family: MathJax_Main;">=</span><span class="msub" id="MathJax-Span-6495" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 1.391em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6496" style="font-family: MathJax_Main-bold;">z</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.517em;"><span class="mrow" id="MathJax-Span-6497"><span class="mo" id="MathJax-Span-6498" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6499" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6500" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6501" style="font-family: MathJax_Main; padding-left: 0.208em;">⊗</span><span class="msub" id="MathJax-Span-6502" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 2.419em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6503" style="font-family: MathJax_Main-bold;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.62em;"><span class="mrow" id="MathJax-Span-6504"><span class="mo" id="MathJax-Span-6505" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6506" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6507" style="font-size: 70.7%; font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-6508" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-6509" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6510" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mrow" id="MathJax-Span-6511" style="padding-left: 0.208em;"><span class="mo" id="MathJax-Span-6512" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="mn" id="MathJax-Span-6513" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-6514" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="msub" id="MathJax-Span-6515" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.391em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6516" style="font-family: MathJax_Main-bold;">z</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.517em;"><span class="mrow" id="MathJax-Span-6517"><span class="mo" id="MathJax-Span-6518" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6519" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6520" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6521" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span><span class="mo" id="MathJax-Span-6522" style="font-family: MathJax_Main; padding-left: 0.208em;">⊗</span><span class="msub" id="MathJax-Span-6523" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6524" style="font-family: MathJax_Main-bold;">g</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.802em; left: 0.568em;"><span class="mrow" id="MathJax-Span-6525"><span class="mo" id="MathJax-Span-6526" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6527" style="font-size: 70.7%; font-family: MathJax_Math-italic;">t</span><span class="mo" id="MathJax-Span-6528" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 7.252em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -3.016em; border-left: 0px solid; width: 0px; height: 6.569em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi mathvariant="bold">z</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd><mtd columnalign="left"><mrow><mo>=</mo><mi>σ</mi><mo>(</mo><msup><mrow><msub><mi mathvariant="bold">W</mi><mrow><mi>x</mi><mi>z</mi></mrow></msub></mrow><mi>T</mi></msup><msub><mi mathvariant="bold">x</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>+</mo><msup><mrow><msub><mi mathvariant="bold">W</mi><mrow><mi>h</mi><mi>z</mi></mrow></msub></mrow><mi>T</mi></msup><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub><mo>+</mo><msub><mi mathvariant="bold">b</mi><mi>z</mi></msub><mo>)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right"><msub><mi mathvariant="bold">r</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd><mtd columnalign="left"><mrow><mo>=</mo><mi>σ</mi><mo>(</mo><msup><mrow><msub><mi mathvariant="bold">W</mi><mrow><mi>x</mi><mi>r</mi></mrow></msub></mrow><mi>T</mi></msup><msub><mi mathvariant="bold">x</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>+</mo><msup><mrow><msub><mi mathvariant="bold">W</mi><mrow><mi>h</mi><mi>r</mi></mrow></msub></mrow><mi>T</mi></msup><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub><mo>+</mo><msub><mi mathvariant="bold">b</mi><mi>r</mi></msub><mo>)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right"><msub><mi mathvariant="bold">g</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd><mtd columnalign="left"><mrow><mo>=</mo><mo form="prefix">tanh</mo><mfenced separators="" open="(" close=")"><msup><mrow><msub><mi mathvariant="bold">W</mi><mrow><mi>x</mi><mi>g</mi></mrow></msub></mrow><mi>T</mi></msup><msub><mi mathvariant="bold">x</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>+</mo><msup><mrow><msub><mi mathvariant="bold">W</mi><mrow><mi>h</mi><mi>g</mi></mrow></msub></mrow><mi>T</mi></msup><mrow><mo>(</mo><msub><mi mathvariant="bold">r</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊗</mo><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub><mo>)</mo></mrow><mo>+</mo><msub><mi mathvariant="bold">b</mi><mi>g</mi></msub></mfenced></mrow></mtd></mtr><mtr><mtd columnalign="right"><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd><mtd columnalign="left"><mrow><mo>=</mo><msub><mi mathvariant="bold">z</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>⊗</mo><msub><mi mathvariant="bold">h</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub><mo>+</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><msub><mi mathvariant="bold">z</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>)</mo></mrow><mo>⊗</mo><msub><mi mathvariant="bold">g</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mrow></mtd></mtr></mtable></math></span></span></div><script type="math/mml" id="MathJax-Element-147"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mi>σ</mi>
          <mo>(</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>z</mi></mrow> </msub></mrow> <mi>T</mi> </msup>
          <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>z</mi></mrow> </msub></mrow> <mi>T</mi> </msup>
          <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msub><mi mathvariant="bold">b</mi><mi>z</mi></msub>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">r</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mi>σ</mi>
          <mo>(</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>r</mi></mrow> </msub></mrow> <mi>T</mi> </msup>
          <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>r</mi></mrow> </msub></mrow> <mi>T</mi> </msup>
          <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <msub><mi mathvariant="bold">b</mi><mi>r</mi></msub>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">g</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <mo form="prefix">tanh</mo>
          <mfenced separators="" open="(" close=")">
            <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>g</mi></mrow> </msub></mrow> <mi>T</mi> </msup>
            <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
            <mo>+</mo>
            <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>g</mi></mrow> </msub></mrow> <mi>T</mi> </msup>
            <mrow>
              <mo>(</mo>
              <msub><mi mathvariant="bold">r</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
              <mo>⊗</mo>
              <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
              <mo>)</mo>
            </mrow>
            <mo>+</mo>
            <msub><mi mathvariant="bold">b</mi><mi>g</mi></msub>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>=</mo>
          <msub><mi mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
          <mo>⊗</mo>
          <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow> </msub>
          <mo>+</mo>
          <mrow>
            <mo>(</mo>
            <mn>1</mn>
            <mo>-</mo>
            <msub><mi mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
            <mo>)</mo>
          </mrow>
          <mo>⊗</mo>
          <msub><mi mathvariant="bold">g</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow> </msub>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></script></div>

<p>Keras provides a <code>keras.layers.GRU</code> layer (which is based on the <code>keras.layers.GRUCell</code> memory cell), so using it is just a matter of replacing <code>SimpleRNN</code> or <code>LSTM</code> with <code>GRU</code>.</p>

<p>LSTM or GRU cells are one of the main reasons behind the success of RNNs. Although they can tackle much longer sequences than simple RNNs, they still have a fairly limited short-term memory, and they have a hard time learning long term patterns in sequences of 100 time steps or more, such as audio samples, long time series, or long sentences. One way to solve this is to shorten the input sequences, for example using 1D convolutional layers.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Using 1D convolutional layers to process sequences"><div class="sect3" id="idm46263495354904">
<h3>Using 1D convolutional layers to process sequences</h3>

<p>In <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cnn_chapter">Chapter&nbsp;14</a>, we saw that a 2D convolutional layer works by sliding several fairly small kernels (or filters) across an image, producing multiple 2D feature maps (one per kernel). Similarly, a 1D convolutional layer slides several kernels across a sequence, producing a 1D feature map per kernel. Each kernel will learn to detect a single very short sequential pattern (no longer than the kernel size). If you use 10 kernels, then the layer’s output will be composed of 10 one-dimensional sequences (all of the same length), or equivalently you can view this output as a single 10-dimensional sequence. This means that you can build a neural network composed of a mix of recurrent layers and 1D convolutional layers (or even 1D pooling layers). If you use a 1D convolutional layer with a stride of 1 and <code>"same"</code> padding, then the output sequence will have the same length as the input sequence. But if you use <code>"valid"</code> padding or a stride greater than 1, then the output sequence will be shorter than the input sequence, so make sure you adjust the targets accordingly. For example, the following model is the same as earlier, except it starts with a 1D convolutional layer that downsamples the input sequence by a factor of 2, using a stride of 2. The kernel size is larger than the stride, so all inputs will be used to compute the layer’s output, and therefore the model can learn to preserve the useful information, dropping only the unimportant details. By shortening the sequences, the convolutional layer may help the GRU layers detect longer patterns. Note that we must also crop off the first 3 time steps in the targets (since the kernel’s size is 4, the first output of the convolutional layer will be based on the input time steps 0 to 3), and downsample the targets by a factor of 2:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv1D</code><code class="p">(</code><code class="n">filters</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"valid"</code><code class="p">,</code>
                        <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">,</code> <code class="mi">1</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">GRU</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">TimeDistributed</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">))</code>
<code class="p">])</code>

<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"mse"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s2">"adam"</code><code class="p">,</code> <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="n">last_time_step_mse</code><code class="p">])</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">[:,</code> <code class="mi">3</code><code class="p">::</code><code class="mi">2</code><code class="p">],</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code>
                    <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">X_valid</code><code class="p">,</code> <code class="n">Y_valid</code><code class="p">[:,</code> <code class="mi">3</code><code class="p">::</code><code class="mi">2</code><code class="p">]))</code></pre>

<p>If you train and evaluate this model, you will find that it is the best model so far. The convolutional layer really helps. In fact, it is actually possible to use only 1D convolutional layers and drop the recurrent layers entirely!</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="WaveNet"><div class="sect3" id="idm46263495346712">
<h3>WaveNet</h3>

<p>In a <a href="https://homl.info/wavenet">2016 paper</a><sup><a data-type="noteref" id="idm46263495258408-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495258408">12</a></sup> by Aaron van den Oord and other DeepMind researchers, the authors introduced an architecture called <em>WaveNet</em>. They stacked 1D convolutional layers, doubling the <em>dilation rate</em> at every layer (that’s how spread apart each neuron’s inputs are): the lowest layer gets a glimpse of just two time steps at a time, while the next one sees four time steps (its receptive field is four time steps long), the next one sees eight time steps, and so on (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#wavenet_diagram">Figure&nbsp;15-11</a>). This way, the lower layers will learn short-term patterns, while the higher layers will learn long-term patterns. Thanks to the doubling dilation rate, the network can process extremely large sequences very efficiently.</p>

<figure><div id="wavenet_diagram" class="figure">
<img src="./Chapter15_files/mls2_1511.png" alt="mls2 1511" width="1441" height="555" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1511.png">
<h6><span class="label">Figure 15-11. </span>WaveNet architecture</h6>
</div></figure>

<p>In the WaveNet paper, the authors actually stacked 10 convolutional layers with dilation rates of 1, 2, 4, 8, …, 256, 512, then they stacked another group of 10 identical layers (also with dilation rates 1, 2, 4, 8, …, 256, 512), then again another identical group of 10 layers. They justified this architecture by pointing out that a single stack of 10 convolutional layers with these dilation rates will act like a super efficient convolutional layer with a kernel of size 1,024 (except way faster, more powerful, and using significantly fewer parameters), which is why they stacked 3 such blocks. They also left-padded the input sequences with a number of zeros equal to the dilation rate before every layer, to preserve the same sequence length throughout the network. Here is how to implement a simplified WaveNet to tackle the same sequences as earlier:<sup><a data-type="noteref" id="idm46263495251608-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495251608">13</a></sup></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">InputLayer</code><code class="p">(</code><code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">,</code> <code class="mi">1</code><code class="p">]))</code>
<code class="k">for</code> <code class="n">rate</code> <code class="ow">in</code> <code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">8</code><code class="p">)</code> <code class="o">*</code> <code class="mi">2</code><code class="p">:</code>
    <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv1D</code><code class="p">(</code><code class="n">filters</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"causal"</code><code class="p">,</code>
                                  <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">,</code> <code class="n">dilation_rate</code><code class="o">=</code><code class="n">rate</code><code class="p">))</code>
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv1D</code><code class="p">(</code><code class="n">filters</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">1</code><code class="p">))</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"mse"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s2">"adam"</code><code class="p">,</code> <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="n">last_time_step_mse</code><code class="p">])</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code>
                    <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">X_valid</code><code class="p">,</code> <code class="n">Y_valid</code><code class="p">))</code></pre>

<p>This sequential model starts with an explicit input layer (this is simpler than trying to set <code>input_shape</code> only on the first layer), then the model continues with a 1D convolutional layer, using <code>"causal"</code> padding: this ensures that the convolutional layer does not peek into the future when making predictions (it is equivalent to padding the inputs with the right amount of 0s on the left and using <code>"valid"</code> padding). We then add similar pairs of layers using growing dilation rates: 1, 2, 4, 8, and again 1, 2, 4, 8. Finally, we add the output layer: a convolutional layer with a 10 filters of size 1 and without any activation function. Thanks to the padding layers, every convolutional layer outputs a sequence of the same length as the inputs sequences, so the targets we use during training can be the full sequences: no need to crop them or downsample them.</p>

<p>The last two models offer the best performance so far in forecasting our time series! In the WaveNet paper, the authors achieved state-of-the-art performance on various audio tasks (hence the name of the architecture), including text-to-speech tasks, producing incredibly realistic voices across several languages. They also used the model to generate music, one audio sample at a time. This feat is all the more impressive when you realize that a single second of audio can contain tens of thousands of time steps! Even LSTMs and GRUs cannot handle such long sequences.</p>

<p>In <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#nlp_chapter">Chapter&nbsp;16</a>, we will continue to explore RNNs, and we will see how they can tackle various NLP tasks.</p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="idm46263495047416">
<h1>Exercises</h1>
<ol>
<li>
<p>Can you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN? And a vector-to-sequence RNN?</p>
</li>
<li>
<p>How many dimensions must the inputs of an RNN layer have? What does each dimension represent? What about its outputs?</p>
</li>
<li>
<p>If you want to build a deep sequence-to-sequence RNN, which RNN layers should have <code>return_sequences=True</code>? What about a sequence-to-vector RNN?</p>
</li>
<li>
<p>Suppose you have a daily univariate time series, and you want to forecast the next seven days. Which RNN architecture should you use?</p>
</li>
<li>
<p>What are the main difficulties when training RNNs? How can you handle them?</p>
</li>
<li>
<p>Can you sketch the LSTM cell’s architecture?</p>
</li>
<li>
<p>Why would you want to use 1D convolutional layers in an RNN?</p>
</li>
<li>
<p>Which neural network architecture could you use to classify videos?</p>
</li>
<li>
<p>Train a classification model for the SketchRNN dataset, available in TensorFlow Datasets.</p>
</li>
<li>
<p>Download the <a href="https://homl.info/bach">Bach chorales</a> dataset and unzip it. It is composed of 382 chorales composed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long, and each time step contains 4 integers, where each integer corresponds to a note’s index on a piano, except for the value 0 which means that no note is played. Train a model (recurrent, convolutional, or both) that can predict the next time step (four notes), given a sequence of time steps from a chorale. Then use this model to generate Bach-like music, one note at a time: you can do this by giving the model the start of a chorale and asking it to predict the next time step, then appending these time steps to the input sequence and asking the model for the next note, and so on. Also make sure to check out <a href="https://homl.info/coconet">Google’s Coconet model</a>, which was used for a nice Google doodle about Bach.</p>
</li>

</ol>

<p>Solutions to these exercises are available in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/app01.html#solutions_appendix">Appendix&nbsp;A</a>.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46263496319896"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263496319896-marker" class="totri-footnote">1</a></sup> Note that a <code>TimeDistributed(Dense(n))</code> layer is equivalent to a <code>Conv1D(n, filter_size=1)</code> layer.</p><p data-type="footnote" id="idm46263496149336"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263496149336-marker" class="totri-footnote">2</a></sup> César Laurent et al., “Batch Normalized Recurrent Neural Networks” in IEEE International Conference on Acoustics, Speech and Signal Processing 2016: 2657–2661.</p><p data-type="footnote" id="idm46263496145416"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263496145416-marker" class="totri-footnote">3</a></sup> Jimmy Lei Ba et al., “Layer Normalization” (2016).</p><p data-type="footnote" id="idm46263496136808"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263496136808-marker" class="totri-footnote">4</a></sup> It would have been simpler to inherit from <code>SimpleRNNCell</code> instead so that we wouldn’t have to create an internal <code>SimpleRNNCell</code> or handle the <code>state_size</code> and <code>output_size</code> attributes, but the goal here was to show how to create a custom cell from scratch.</p><p data-type="footnote" id="idm46263495818440"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495818440-marker" class="totri-footnote">5</a></sup> A character from the animated movies <em>Finding Nemo</em> and <em>Finding Dory</em> who has a short term memory.</p><p data-type="footnote" id="idm46263495914712"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495914712-marker" class="totri-footnote">6</a></sup> Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory,” <em>Neural Computation</em> 9, no. 8 (November 1997): 1735–1780.</p><p data-type="footnote" id="idm46263495912808"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495912808-marker" class="totri-footnote">7</a></sup> Haşim Sak et al., <em>Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition</em> (Google Inc., February 2014).</p><p data-type="footnote" id="idm46263495910856"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495910856-marker" class="totri-footnote">8</a></sup> Wojciech Zaremba et al., “Recurrent Neural Network Regularization” (2015).</p><p data-type="footnote" id="idm46263495465816"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495465816-marker" class="totri-footnote">9</a></sup> F. A. Gers and J. Schmidhuber, “Recurrent Nets That Time and Count” (IEEE-INNS-ENNS International Joint Conference on Neural Networks, Como, Italy, July 27, 2000).</p><p data-type="footnote" id="idm46263495455048"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495455048-marker">10</a></sup> Kyunghyun Cho et al., “Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation” in proceedings of Empirical Methods in Natural Language Processing (2014): 1724–1734.</p><p data-type="footnote" id="idm46263495451944"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495451944-marker">11</a></sup> A 2015 paper by Klaus Greff et al., <a href="https://homl.info/98">“LSTM: A Search Space Odyssey,”</a> seems to show that all LSTM variants perform roughly the same.</p><p data-type="footnote" id="idm46263495258408"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495258408-marker">12</a></sup> Aaron van den Oord et al., <em>WaveNet: A Generative Model for Raw Audio</em> (London: Google DeepMind, September 2016).</p><p data-type="footnote" id="idm46263495251608"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#idm46263495251608-marker">13</a></sup> The complete WaveNet uses a few more tricks, such as skip connections like in a ResNet, and <em>Gated Activation Units</em> similar to those found in a GRU cell. Please see the notebook for more details.</p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-16" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders">
		
		<li class="copy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#">Add Highlight</a></li>
		<li class="add-note"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#">
			Add Note
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">14. Deep Computer Vision Using Convolutional Neural Networks</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">16. Natural Language Processing with RNNs and Attention</div>
        </a>
    
  
  </div>

</section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    



        
      </div>
      



  <footer class="pagefoot">
    <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#" class="icon-up" style="display: none;"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      <li class="full-support"><a href="https://www.oreilly.com/online-learning/support/">Support</a></li>
      <li><a href="https://www.oreilly.com/online-learning/apps.html">Get the App</a></li>
      
        <li><a href="https://learning.oreilly.com/accounts/logout/">Sign Out</a></li>
      
    </ul>
    <span class="copyright">© 2019 <a href="https://learning.oreilly.com/" target="_blank">Safari</a>.</span>
    <a href="https://learning.oreilly.com/terms/">Terms of Service</a> /
    <a href="https://learning.oreilly.com/membership-agreement/">Membership Agreement</a> /
    <a href="https://www.oreilly.com/privacy.html">Privacy Policy</a>
  </footer>

<script type="text/javascript">window.NREUM||(NREUM={});NREUM.info={"applicationID":"172641827,79672898,93931619","errorBeacon":"bam.nr-data.net","agent":"","applicationTime":451,"licenseKey":"510f1a6865","transactionName":"YgdaZ0NSW0cEB0RdWltNfkZfUEFdCgofXFBHDVYdR1pQQxZeRl1QQj1aWkU=","beacon":"bam.nr-data.net","queueTime":4}</script>


    
    <script src="./Chapter15_files/saved_resource" charset="utf-8"></script><script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","1732687426968531");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=1732687426968531&amp;ev=PageView&amp;noscript=1"></noscript><script type="text/javascript" id="">(function(){window.medalliaUserIdentifier=document.documentElement.dataset.userUuid;window.medalliaUserName=document.documentElement.dataset.username})();</script>
<script type="text/javascript" id="" src="./Chapter15_files/embed.js.download"></script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script>
<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("set","agent","tmgoogletagmanager","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>

<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>
<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.09556792590964802"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.5229133033930915" width="0" height="0" alt="" src="./Chapter15_files/0"></div>
    <script src="./Chapter15_files/saved_resource(1)" charset="utf-8"></script>
  

<script src="./Chapter15_files/saved_resource(2)" type="text/javascript"></script><script type="text/javascript" id="">window._pp=window._pp||[];if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/register/")_pp.targetUrl="/confirm/trial";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/subscribe/")_pp.targetUrl="/confirm/paid";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/signup/")_pp.targetUrl="/confirm/paid";_pp.siteId="2508";
_pp.siteUId="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79";_pp.orderValue="undefined";_pp.orderId="undefined";(function(){var ppjs=document.createElement("script");ppjs.type="text/javascript";ppjs.async=true;ppjs.src=("https:"==document.location.protocol?"https:":"http:")+"//cdn.pbbl.co/r/"+_pp.siteId+".js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(ppjs,s)})();</script><div class="annotator-notice"></div><div class="font-flyout" style="top: 201px; left: 1194px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#">Reset</a>
</div>
</div><script type="text/javascript" async="" src="./Chapter15_files/generic1566415868241.js.download" charset="UTF-8"></script><div style="display: none; visibility: hidden;"><script>(function(){if(null!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')&&void 0!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')){var a=!1;window.addEventListener("blur",function(){a&&dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"facebook",eventVal:0,nonInteraction:0})});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseover",function(){window.focus();
a=!0});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseout",function(){a=!1})}try{window.twttr=function(b,a,d){var c,e=b.getElementsByTagName(a)[0];if(!b.getElementById(d))return b=b.createElement(a),b.id=d,b.src="//platform.twitter.com/widgets.js",e.parentNode.insertBefore(b,e),window.twttr||(c={_e:[],ready:function(a){c._e.push(a)}})}(document,"script","twitter-wjs"),twttr.ready(function(a){a.events.bind("tweet",trackTwitter)})}catch(b){}})();
null!==document.querySelector(".IN-widget")&&void 0!==document.querySelector(".IN-widget")&&document.querySelector(".IN-widget").addEventListener("click",function(){dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"linkedin",eventVal:0,nonInteraction:0})});
function trackTwitter(a){a&&(a.target&&"IFRAME"==a.target.nodeName&&(opt_target=extractParamFromUri(a.target.src,"url")),dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"twitter",eventVal:0,nonInteraction:0}))}function extractParamFromUri(a,b){if(a){var c=new RegExp("[\\?\x26#]"+b+"\x3d([^\x26#]*)");c=c.exec(a);if(null!=c)return unescape(c[1])}};</script></div><span><div id="KampyleAnimationContainer" style="z-index: 2147483000; border: 0px; position: fixed; display: block; width: 0px; height: 0px;"></div></span><iframe scrolling="no" frameborder="0" allowtransparency="true" src="./Chapter15_files/widget_iframe.097c1f5038f9e8a0d62a39a892838d66.html" title="Twitter settings iframe" style="display: none;"></iframe><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_AMS, sans-serif;"></div></div></body></html>