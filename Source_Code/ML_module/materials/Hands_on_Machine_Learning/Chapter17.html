<!DOCTYPE html>
<!-- saved from url=(0091)https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html -->
<html class=" js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/hands-on-machine-learning/9781492032632/part01.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="4626953" data-user-uuid="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79" data-username="17481074" data-account-type="B2B" data-activated-trial-date="" data-archive="9781492032632" data-publishers="O&#39;Reilly Media, Inc." data-htmlfile-name="part01.html" data-epub-title="Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781492032632"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><script type="text/javascript" async="" src="./Chapter17_files/cool-2.1.15.min.js.download"></script><script type="text/javascript" src="./Chapter17_files/510f1a6865"></script><script id="twitter-wjs" src="./Chapter17_files/widgets.js.download"></script><script src="./Chapter17_files/nr-1130.min.js.download"></script><script type="text/javascript" async="" src="./Chapter17_files/2508.js.download"></script><script async="" src="./Chapter17_files/fbevents.js.download"></script><script type="text/javascript" async="" src="./Chapter17_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter17_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter17_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter17_files/analytics.js.download"></script><script type="text/javascript" async="" src="./Chapter17_files/ec.js.download"></script><script type="text/javascript" async="" src="./Chapter17_files/bat.js.download"></script><script type="text/javascript" async="" src="./Chapter17_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter17_files/insight.min.js.download"></script><script type="text/javascript" async="" src="./Chapter17_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter17_files/linkid.js.download"></script><script async="" src="./Chapter17_files/gtm.js.download"></script><script async="" src="./Chapter17_files/analytics.js.download"></script><script type="text/javascript">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if("function"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e("handle"),a=e(3),u=e(4),f=e("ee").get("tracer"),c=e("loader"),s=NREUM;"undefined"==typeof window.newrelic&&(newrelic=s);var p=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],d="api-",l=d+"ixn-";a(p,function(e,n){s[n]=o(d+n,!0,"api")}),s.addPageAction=o(d+"addPageAction",!0),s.setCurrentRouteName=o(d+"routeName",!0),n.exports=newrelic,s.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(e,n){var t={},r=this,o="function"==typeof n;return i(l+"tracer",[c.now(),e,t],r),function(){if(f.emit((o?"":"no-")+"fn-start",[c.now(),r,o],t),o)try{return n.apply(this,arguments)}catch(e){throw f.emit("fn-err",[arguments,this,e],t),e}finally{f.emit("fn-end",[c.now()],t)}}}};a("actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(e,n){m[n]=o(l+n)}),newrelic.noticeError=function(e,n){"string"==typeof e&&(e=new Error(e)),i("err",[e,c.now(),!1,n])}},{}],2:[function(e,n,t){function r(e,n){if(!o)return!1;if(e!==o)return!1;if(!n)return!0;if(!i)return!1;for(var t=i.split("."),r=n.split("."),a=0;a<r.length;a++)if(r[a]!==t[a])return!1;return!0}var o=null,i=null,a=/Version\/(\S+)\s+Safari/;if(navigator.userAgent){var u=navigator.userAgent,f=u.match(a);f&&u.indexOf("Chrome")===-1&&u.indexOf("Chromium")===-1&&(o="Safari",i=f[1])}n.exports={agent:o,version:i,match:r}},{}],3:[function(e,n,t){function r(e,n){var t=[],r="",i=0;for(r in e)o.call(e,r)&&(t[i]=n(r,e[r]),i+=1);return t}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],4:[function(e,n,t){function r(e,n,t){n||(n=0),"undefined"==typeof t&&(t=e?e.length:0);for(var r=-1,o=t-n||0,i=Array(o<0?0:o);++r<o;)i[r]=e[n+r];return i}n.exports=r},{}],5:[function(e,n,t){n.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(e,n,t){function r(){}function o(e){function n(e){return e&&e instanceof r?e:e?f(e,u,i):i()}function t(t,r,o,i){if(!d.aborted||i){e&&e(t,r,o);for(var a=n(o),u=v(t),f=u.length,c=0;c<f;c++)u[c].apply(a,r);var p=s[y[t]];return p&&p.push([b,t,r,a]),a}}function l(e,n){h[e]=v(e).concat(n)}function m(e,n){var t=h[e];if(t)for(var r=0;r<t.length;r++)t[r]===n&&t.splice(r,1)}function v(e){return h[e]||[]}function g(e){return p[e]=p[e]||o(t)}function w(e,n){c(e,function(e,t){n=n||"feature",y[t]=n,n in s||(s[n]=[])})}var h={},y={},b={on:l,addEventListener:l,removeEventListener:m,emit:t,get:g,listeners:v,context:n,buffer:w,abort:a,aborted:!1};return b}function i(){return new r}function a(){(s.api||s.feature)&&(d.aborted=!0,s=d.backlog={})}var u="nr@context",f=e("gos"),c=e(3),s={},p={},d=n.exports=o();d.backlog=s},{}],gos:[function(e,n,t){function r(e,n,t){if(o.call(e,n))return e[n];var r=t();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,n,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return e[n]=r,r}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],handle:[function(e,n,t){function r(e,n,t,r){o.buffer([e],r),o.emit(e,n,t)}var o=e("ee").get("handle");n.exports=r,r.ee=o},{}],id:[function(e,n,t){function r(e){var n=typeof e;return!e||"object"!==n&&"function"!==n?-1:e===window?0:a(e,i,function(){return o++})}var o=1,i="nr@id",a=e("gos");n.exports=r},{}],loader:[function(e,n,t){function r(){if(!E++){var e=x.info=NREUM.info,n=l.getElementsByTagName("script")[0];if(setTimeout(s.abort,3e4),!(e&&e.licenseKey&&e.applicationID&&n))return s.abort();c(y,function(n,t){e[n]||(e[n]=t)}),f("mark",["onload",a()+x.offset],null,"api");var t=l.createElement("script");t.src="https://"+e.agent,n.parentNode.insertBefore(t,n)}}function o(){"complete"===l.readyState&&i()}function i(){f("mark",["domContent",a()+x.offset],null,"api")}function a(){return O.exists&&performance.now?Math.round(performance.now()):(u=Math.max((new Date).getTime(),u))-x.offset}var u=(new Date).getTime(),f=e("handle"),c=e(3),s=e("ee"),p=e(2),d=window,l=d.document,m="addEventListener",v="attachEvent",g=d.XMLHttpRequest,w=g&&g.prototype;NREUM.o={ST:setTimeout,SI:d.setImmediate,CT:clearTimeout,XHR:g,REQ:d.Request,EV:d.Event,PR:d.Promise,MO:d.MutationObserver};var h=""+location,y={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-1130.min.js"},b=g&&w&&w[m]&&!/CriOS/.test(navigator.userAgent),x=n.exports={offset:u,now:a,origin:h,features:{},xhrWrappable:b,userAgent:p};e(1),l[m]?(l[m]("DOMContentLoaded",i,!1),d[m]("load",r,!1)):(l[v]("onreadystatechange",o),d[v]("onload",r)),f("mark",["firstbyte",u],null,"api");var E=0,O=e(5)},{}]},{},["loader"]);</script><link rel="apple-touch-icon" href="https://learning.oreilly.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://learning.oreilly.com/favicon.ico" type="image/x-icon"><link href="./Chapter17_files/css" rel="stylesheet" type="text/css"><title>17. Representation Learning and Generative Learning Using Autoencoders and GANs - Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition</title><link rel="stylesheet" href="./Chapter17_files/output.68851547a55f.css" type="text/css"><link rel="stylesheet" type="text/css" href="./Chapter17_files/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="./Chapter17_files/font-awesome.min.css"><style type="text/css" title="ibis-book">@charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000 !important;padding-top:10px !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:2em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content p[data-type="footnote"]{font-size:90% !important;line-height:1.2em !important;margin-left:2.5em !important;text-indent:-2.3em !important}#sbo-rt-content p[data-type="footnote"] sup{display:inline-block !important;position:static !important;width:2em !important;text-align:right !important;font-size:100% !important;padding-right:.5em !important}#sbo-rt-content p[data-type="footnote"] a[href$="-marker"]{font-family:sans-serif !important;font-size:90% !important;color:#8e0012 !important}#sbo-rt-content p[data-type="footnote"] a[data-type="xref"]{margin:0 !important;padding:0 !important;text-indent:0 !important}#sbo-rt-content a[data-type="noteref"]{font-family:sans-serif !important;color:#8e0012;margin-left:0;padding-left:0}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dd{margin-left:1.5em !important;margin-bottom:.25em}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul>li,#sbo-rt-content ol ul,#sbo-rt-content ol ul>li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul>li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul>li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul>li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol>li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol>li,#sbo-rt-content ul ol,#sbo-rt-content ul ol>li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol>li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol>li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:middle;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content table.border tbody>tr:last-child>td{border-bottom:transparent}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:1em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content .width-10{width:10vw !important}#sbo-rt-content .width-20{width:20vw !important}#sbo-rt-content .width-30{width:30vw !important}#sbo-rt-content .width-40{width:40vw !important}#sbo-rt-content .width-50{width:50vw !important}#sbo-rt-content .width-60{width:60vw !important}#sbo-rt-content .width-70{width:70vw !important}#sbo-rt-content .width-80{width:80vw !important}#sbo-rt-content .width-90{width:90vw !important}#sbo-rt-content .width-full,#sbo-rt-content .width-100{width:100vw !important}#sbo-rt-content div[data-type="equation"].fifty-percent img{width:50%}</style><script> // <![CDATA[
    var g = {
      position_cache: {
        
          "chapter": "/api/v1/book/9781492032632/chapter/part01.html",
          "book_id": "9781492032632",
          "chapter_uri": "part01.html",
          "position": 100.0,
          "user_uuid": "d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79",
          "next_chapter_uri": "/library/view/hands-on-machine-learning/9781492032632/ch01.html"
        
      },
      title: "Hands\u002Don Machine Learning with Scikit\u002DLearn, Keras, and TensorFlow, 2nd Edition",
      author_list: "Aurélien Géron",
      format: "book",
      source: "application/epub+zip",
      is_system_book: true,
      is_public: false,
      loaded_from_server: true,
      allow_scripts: false,
      has_mathml: true
    };
    // ]]></script><script src="./Chapter17_files/modernizr.8e35451ddb64.js.download"></script><script>
    
      

      
        
          window.PUBLIC_ANNOTATIONS = true;
        
      

      window.MOBILE_PUBLIC_ANNOTATIONS = false;

    

    
      window.PRIVACY_CONTROL_OVERRIDE = false;
    

      window.PRIVACY_CONTROL_SWITCH = true;

      window.PUBLISHER_PAGES = true;

      window.SBO = {
        "constants": {
          "SITB_ENDPOINT": "/api/v2/sitb/",
          "SEARCH_SELECT_ENDPOINT": "https://learning.oreilly.com/api/v2/search/select/",
          "ENABLE_ONLINE_TRAINING": false
        }
      };
  </script><link rel="canonical" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta name="description" content=" Part I. The Fundamentals of Machine Learning "><meta property="og:title" content="I. The Fundamentals of Machine Learning"><meta itemprop="isPartOf" content="/library/view/hands-on-machine-learning/9781492032632/"><meta itemprop="name" content="I. The Fundamentals of Machine Learning"><meta property="og:url" itemprop="url" content="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://learning.oreilly.com/library/cover/9781492032632/"><meta property="og:description" itemprop="description" content=" Part I. The Fundamentals of Machine Learning "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O&#39;Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781492032649"><meta property="og:book:author" itemprop="author" content="Aurélien Géron"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: &lt;%= font_size %&gt; !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: &lt;%= font_family %&gt; !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: &lt;%= column_width %&gt;% !important; margin: 0 auto !important; }"></style><noscript><meta http-equiv="refresh" content="0; url=/library/no-js/" /></noscript><script type="text/javascript">
    (function(i,s,o,g,r,a,m) {
      i['GoogleAnalyticsObject']=r;
      i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();
      a=s.createElement(o),m=s.getElementsByTagName(o)[0];
      a.async=1;
      a.src=g;
      m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    var matches = document.cookie.match(/BrowserCookie\s*=\s*([a-f0-9\-]{36})/),
      user_uuid = null;

    if (matches && matches.length === 2) {
      user_uuid = matches[1];
    }

  
    ga('create', 'UA-39299553-7', {'userId': 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79' });
  

  
    
      ga('set', 'dimension1', 'B2B');
    
  

  ga('set', 'dimension6', user_uuid);

  
    ga('set', 'dimension2', 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79');
    
      ga('set', 'dimension7', '0012M0000229keZQAQ');
    
  

  

  

  //enable enhanced link tracking
  ga('require', 'linkid', 'linkid.js');

  // reading interface will track pageviews itself
  if (document.location.pathname.indexOf("/library/view") !== 0) {
    ga('send', 'pageview');
  }
  </script><script>
    var dataLayer = window.dataLayer || [];

    
      window.medalliaVsgUserIdentifier = 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79';
      dataLayer.push({userIdentifier: 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79'});
      dataLayer.push({loggedIn: 'yes'});

      
        window.medalliaVsgAccountIdentifier = '21bed0a7-6b7b-470c-8fa0-40a52db0b491';
        
        dataLayer.push({orgID: '21bed0a7-6b7b-470c-8fa0-40a52db0b491'});
        

        window.medalliaVsgIsIndividual = false;
        
          
          dataLayer.push({learningAccountType: 'enterprise'});
          
        

        
          dataLayer.push({learningPaidAccount: 'yes'});
        
      
    

    (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-5P4V6Z');
    (function () {
      var VERSION = 'V1.1';
      var AUTHOR = 'Awwad';
      if (!window.GtmHelper)
        window.GtmHelper = function () {
          var instance = this;
          var loc = document.location;
          this.version = VERSION;
          this.author = AUTHOR;
          this.readCookie = function (name) {
            var nameEQ = name + "=";
            var ca = document.cookie.split(';');
            for (var i = 0; i < ca.length; i++) {
              var c = ca[i];
              while (c.charAt(0) == ' ') c = c.substring(1, c.length);
              if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length, c.length);
            }
            return null;
          };
          this.createCookie = function (name, value, days, cookieDomain) {
            var domain = "";
            var expires = "";

            if (days) {
              var date = new Date();
              date.setTime(date.getTime() + Math.ceil(days * 24 * 60 * 60 * 1000));
              var expires = " expires=" + date.toGMTString() + ";";
            }

            if (typeof (cookieDomain) != 'undefined')
              domain = " domain=" + cookieDomain + "; ";

            document.cookie = name + "=" + value + ";" + expires + domain + "path=/";
          };

          this.isDuplicated = function (currentTransactionId) {
            // the previous transaction id:
            var previousTransIdValue = this.readCookie("previousTransId");

            if (currentTransactionId === previousTransIdValue) {
              return true; // Duplication
            } else {
              return false;
            }
          };
        }
    })()
  </script><script defer="" src="./Chapter17_files/vendor.a48a756c5182.js.download"></script><script defer="" src="./Chapter17_files/reader.f2a0c6bd2fee.js.download"></script><script src="./Chapter17_files/f(1).txt"></script><script src="./Chapter17_files/f(2).txt"></script><script src="./Chapter17_files/f(3).txt"></script><script src="./Chapter17_files/f(4).txt"></script><script async="" src="./Chapter17_files/MathJax.js.download"></script><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 2147483020;
}
.annotator-filter {
  z-index: 2147483010;
}</style><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
</style><script async="true" type="text/javascript" src="./Chapter17_files/roundtrip.js.download"></script><style type="text/css" id="kampyleStyle">.noOutline{outline: none !important;}.wcagOutline:focus{outline: 1px dashed #595959 !important;outline-offset: 2px !important;transition: none !important;}</style><script async="true" type="text/javascript" src="./Chapter17_files/roundtrip.js.download"></script><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Math-bold-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Script; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_AMS; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf?V=2.7.1') format('opentype')}
</style></head>


<body class="reading sidenav  scalefonts library nav-collapsed"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

    
  <noscript> 
    <iframe src="//www.googletagmanager.com/ns.html?id=GTM-5P4V6Z"
            height="0" width="0"
            style="display:none;visibility:hidden">
    </iframe>
  </noscript>



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#container" class="skip">Skip to content</a><header class="topbar t-topbar"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://learning.oreilly.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M11.738 14H9.254v-3.676a.617.617 0 0 0-.621-.613H7.39a.617.617 0 0 0-.62.613V14H4.284a.617.617 0 0 1-.622-.613V10.22c0-.327.132-.64.367-.87l3.547-3.493a.627.627 0 0 1 .875 0l3.54 3.499c.234.229.366.54.367.864v3.167a.617.617 0 0 1-.62.613zM7.57 2.181a.625.625 0 0 1 .882 0l5.77 5.692-.93.92-5.28-5.209-5.28 5.208-.932-.919 5.77-5.692z"></path></svg><span>Safari Home</span></a></li><li><a href="https://learning.oreilly.com/resource-centers/" class="t-resource-centers-nav l0 nav-icn"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="16px" height="16px" viewBox="0 0 16 16" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="Topic-Page-Design" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="Heron-Button" transform="translate(-20.000000, -78.000000)" fill="#4A3A30"><g id="Group-9" transform="translate(20.000000, 78.000000)"><rect id="Rectangle" x="9.6" y="0" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="9.6" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="0" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect></g></g></g></svg><span>Resource Centers</span></a></li><li><a href="https://learning.oreilly.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://learning.oreilly.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://learning.oreilly.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://learning.oreilly.com/learning-paths/" class="l1 nav-icn t-learningpaths-nav js-toggle-menu-item"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="32px" height="32px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 52.5 (67469) - http://www.bohemiancoding.com/sketch --><title>Mask</title><desc>Created with Sketch.</desc><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><path d="M0,16.0214227 C0,15.0387209 0.796453294,14.2411658 1.77779753,14.2411658 C2.75914177,14.2411658 3.55559506,15.0387209 3.55559506,16.0214227 C3.55559506,17.0041246 2.75914177,17.8016797 1.77779753,17.8016797 C0.796453294,17.8016797 0,17.0041246 0,16.0214227 Z M9.77788642,5.22914885 C8.9280992,5.72049977 7.84008711,5.42853763 7.34941499,4.57757479 C6.85874287,3.72661195 7.15030167,2.63709467 8.00008889,2.14574375 C8.84987611,1.65439282 9.9378882,1.94635496 10.4285603,2.7973178 C10.9192324,3.64828064 10.6276736,4.73779792 9.77788642,5.22914885 Z M4.57213969,7.35869225 C5.42192691,7.85004318 5.71348571,8.93956046 5.22281359,9.79052329 C4.73214147,10.6414861 3.64412938,10.9334483 2.79434216,10.4420974 C1.94455494,9.95074642 1.65299614,8.86122915 2.14366826,8.01026631 C2.63434038,7.15930347 3.72235247,6.86734132 4.57213969,7.35869225 Z M2.79434216,21.6007481 C3.64412938,21.1093972 4.73214147,21.4013594 5.22281359,22.2523222 C5.71348571,23.103285 5.42192691,24.1928023 4.57213969,24.6841532 C3.72235247,25.1755042 2.63434038,24.883542 2.14366826,24.0325792 C1.65299614,23.1816163 1.94455494,22.0920991 2.79434216,21.6007481 Z M7.34941499,27.4652707 C7.84008711,26.6143079 8.9280992,26.3223457 9.77788642,26.8136966 C10.6276736,27.3050476 10.9192324,28.3945649 10.4285603,29.2455277 C9.9378882,30.0964905 8.84987611,30.3884527 8.00008889,29.8971017 C7.15030167,29.4057508 6.85874287,28.3162335 7.34941499,27.4652707 Z M18.7118524,11.3165596 C21.3074367,12.8173162 22.1963355,16.1392758 20.6976522,18.738451 C19.1989689,21.3358459 15.8815987,22.2259744 13.2860143,20.726998 C10.6922077,19.2262414 9.80330893,15.9042818 11.3002144,13.3051066 C12.7988978,10.7059314 16.116268,9.81580294 18.7118524,11.3165596 Z M26.7821642,27.8093944 L30.1315348,31.1633985 C30.3982044,31.4304371 30.2097579,31.8844026 29.8346426,31.8844026 L21.5945511,31.8844026 C21.1287681,31.8844026 20.751875,31.5069881 20.751875,31.0405608 L20.751875,22.7890697 C20.751875,22.4134355 21.2052134,22.2247282 21.4701052,22.4899865 L24.2843587,25.3081333 C26.8337204,23.0240636 28.4444049,19.7092251 28.4444049,16.0223129 C28.4444049,9.15052091 22.8621207,3.56051397 15.9998222,3.56051397 L15.9998222,0 C24.8230314,0 32,7.18689745 32,16.0223129 C32,20.6919269 29.9750886,24.8790914 26.7821642,27.8093944 Z" id="Mask" fill="#8B889A"></path></g></svg><span>Learning Paths</span></a></li><li class="nav-highlights"><a href="https://learning.oreilly.com/u/d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" fill="#4A3C31"><path d="M13.325 18.071H8.036c0-6.736 4.324-10.925 14.464-12.477V0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35c5.142 0 9.175-3.515 9.175-8.816 0-4.628-2.367-7.293-6.253-8.113zm27.5 0h-5.26c0-6.736 4.295-10.925 14.435-12.477V0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35c5.113 0 9.146-3.515 9.146-8.816 0-4.628-2.338-7.293-6.253-8.113z" fill-rule="evenodd"></path></svg><span>Highlights</span></a></li><li><a href="https://learning.oreilly.com/u/preferences/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l1 no-icon">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://learning.oreilly.com/u/preferences/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l2">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9781492032632/chapter/part01.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"><div data-reactroot="" class="menu-dropdown-wrapper js-menu-dropdown-wrapper align-right"><img class="hidden" src="./Chapter17_files/ajax-transp.gif" alt="loading spinner"><div class="menu-control"><div class="control "><div class="js-playlists-menu"><button class="js-playlist-icon"><svg class="icon-add-to-playlist-sml" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill-rule="nonzero" fill="#000000"><g transform="translate(-1.000000, 0.000000)"><rect x="5" y="0" width="12" height="2"></rect><title>Playlists</title><path d="M4.5,14 C6.43299662,14 8,12.4329966 8,10.5 C8,8.56700338 6.43299662,7 4.5,7 C2.56700338,7 1,8.56700338 1,10.5 C1,12.4329966 2.56700338,14 4.5,14 Z M2.5,10 L4,10 L4,8.5 L5,8.5 L5,10 L6.5,10 L6.5,11 L5,11 L5,12.5 L4,12.5 L4,11 L2.5,11 L2.5,10 Z"></path><circle cx="2" cy="5" r="1"></circle><circle cx="1.94117647" cy="1" r="1"></circle><rect x="5" y="4" width="12" height="2"></rect><rect x="9" y="8" width="8" height="2"></rect><rect x="9" y="12" width="8" height="2"></rect></g></g></g></svg><div class="js-playlist-addto-label">Add&nbsp;To</div></button></div></div></div></div></div></div></li><li class="js-font-control-panel font-control-activator"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html&amp;text=Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%20I.%20The%20Fundamentals%20of%20Machine%20Learning&amp;body=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html%0D%0Afrom%20Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    
    <section role="document">
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">16. Natural Language Processing with RNNs and Attention</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">18. Reinforcement Learning</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content"><div class="annotator-wrapper"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 17. Representation Learning and Generative Learning Using Autoencoders and GANs"><div class="chapter" id="autoencoders_chapter">
<h1><span class="label">Chapter 17. </span>Representation Learning and Generative Learning Using Autoencoders and GANs</h1>


<p>Autoencoders are artificial neural networks capable of learning dense representations of the input data, called <em>latent representations</em>, or <em>codings</em>, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch08.html#dimensionality_chapter">Chapter&nbsp;8</a>), especially for visualization purposes. Autoencoders also act as feature detectors, and they can be used for unsupervised pretraining of deep neural networks (as we discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>). Lastly, some autoencoders are <em>generative models</em>: they are capable of randomly generating new data that looks very similar to the training data. For example, you could train an autoencoder on pictures of faces, and it would then be able to generate new faces. However, the generated images are usually fuzzy and not entirely realistic.</p>

<p>In contrast, faces generated by <em>Generative Adversarial Networks</em> (GANs) are now so convincing that it is hard to believe that the people they represent do not exist. You can judge so for yourself by visiting <a href="https://thispersondoesnotexist.com/"><em class="hyperlink">https://thispersondoesnotexist.com/</em></a>, a website that shows faces generated by a recent GAN architecture called <em>StyleGAN</em> (you can also check out <a href="https://thisrentaldoesnotexist.com/"><em class="hyperlink">https://thisrentaldoesnotexist.com/</em></a> to see some generated Airbnb bedrooms). GANs are now widely used for super resolution (increasing the resolution of an image), <a href="https://github.com/jantic/DeOldify">colorization</a>, powerful image editing (e.g., replacing photo bombers with realistic background), turning a simple sketch into a photorealistic image, predicting the next frames in a video, augmenting a dataset (to train other models), generating other types of data (such as text, audio, and time series), identifying the weaknesses in other models and strengthening them, and more.</p>

<p>Autoencoders and GANs are both unsupervised, they both learn dense representations, they can both be used as generative models, and they have many similar applications. However, they work very differently:</p>

<ul>
<li>
<p>Autoencoders simply learn to copy their inputs to their outputs. This may sound like a trivial task, but we will see that constraining the network in various ways can make it rather difficult. For example, you can limit the size of the latent representations, or you can add noise to the inputs and train the network to recover the original inputs. These constraints prevent the autoencoder from trivially copying the inputs directly to the outputs, which forces it to learn efficient ways of representing the data. In short, the codings are byproducts of the autoencoder learning the identity function under some constraints.</p>
</li>
<li>
<p>GANs are composed of two neural networks: a <em>generator</em> that tries to generate data that looks similar to the training data, and a <em>discriminator</em> that tries to tell real data from fake data. This architecture is very original in Deep Learning in that the generator and the discriminator compete against each other during training: the generator is often compared to a criminal trying to make realistic counterfeit money, while the discriminator is like the police investigator trying to tell real money from fake one. <em>Adversarial training</em> (training competing neural networks) is widely considered as one of the most important ideas in recent years. In 2016, Yann Lecun even said that it was “the most interesting idea in the last 10 years in Machine Learning.”</p>
</li>
</ul>

<p>In this chapter we will start by explaining in more depth how autoencoders work and how to use them for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. This will naturally lead us to GANs. We will start by building a simple GAN to generate fake images, but we will see that training is often quite difficult. We will discuss the main difficulties you will encounter with adversarial training, as well as some of the main techniques to work around these difficulties. Let’s start with autoencoders!</p>






<section data-type="sect1" class="pagebreak-before" data-pdf-bookmark="Efficient Data Representations"><div class="sect1" id="idm46263490901480">
<h1>Efficient Data Representations</h1>

<p>Which of the following number sequences do you find the easiest to memorize?</p>

<ul>
<li>
<p>40, 27, 25, 36, 81, 57, 10, 73, 19, 68</p>
</li>
<li>
<p>50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14</p>
</li>
</ul>

<p>At first glance, it would seem that the first sequence should be easier, since it is much shorter. However, if you look carefully at the second sequence, you will notice that it is just the list of even numbers from 50 down to 14. Once you notice this pattern, the second sequence becomes much easier to memorize than the first because you only need to remember the pattern (i.e., decreasing even numbers) and the starting and ending numbers (i.e., 50 and 14). Note that if you could quickly and easily memorize very long sequences, you would not care much about the existence of a pattern in the second sequence. You would just learn every number by heart, and that would be that. The fact that it is hard to memorize long sequences is what makes it useful to recognize patterns, and hopefully this clarifies why constraining an autoencoder during training pushes it to discover and exploit patterns in the data.</p>

<p>The relationship between memory, perception, and pattern matching was <a href="https://homl.info/111">famously studied by William Chase and Herbert Simon in the early 1970s</a>.<sup><a data-type="noteref" id="idm46263490894888-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263490894888" class="totri-footnote">1</a></sup> They observed that expert chess players were able to memorize the positions of all the pieces in a game by looking at the board for just five seconds, a task that most people would find impossible. However, this was only the case when the pieces were placed in realistic positions (from actual games), not when the pieces were placed randomly. Chess experts don’t have a much better memory than you and I; they just see chess patterns more easily, thanks to their experience with the game. Noticing patterns helps them store information efficiently.</p>

<p>Just like the chess players in this memory experiment, an autoencoder looks at the inputs, converts them to an efficient latent representation, and then spits out something that (hopefully) looks very close to the inputs. An autoencoder is always composed of two parts: an <em>encoder</em> (or <em>recognition network</em>) that converts the inputs to a latent representation, followed by a <em>decoder</em> (or <em>generative network</em>) that converts the internal representation to the outputs (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#encoder_decoder_diagram">Figure&nbsp;17-1</a>).</p>

<figure class="smallerseventy"><div id="encoder_decoder_diagram" class="figure">
<img src="./Chapter17_files/mls2_1701.png" alt="mls2 1701" width="1440" height="869" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1701.png">
<h6><span class="label">Figure 17-1. </span>The chess memory experiment (left) and a simple autoencoder (right)</h6>
</div></figure>

<p>As you can see, an autoencoder typically has the same architecture as a Multi-Layer Perceptron (MLP; see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#ann_chapter">Chapter&nbsp;10</a>), except that the number of neurons in the output layer must be equal to the number of inputs. In this example, there is just one hidden layer composed of two neurons (the encoder), and one output layer composed of three neurons (the decoder). The outputs are often called the <em>reconstructions</em> because the autoencoder tries to reconstruct the inputs, and the cost function contains a <em>reconstruction loss</em> that penalizes the model when the reconstructions are different from the inputs.</p>

<p>Because the internal representation has a lower dimensionality than the input data (it is 2D instead of 3D), the autoencoder is said to be <em>undercomplete</em>. An undercomplete autoencoder cannot trivially copy its inputs to the codings, yet it must find a way to output a copy of its inputs. It is forced to learn the most important features in the input data (and drop the unimportant ones).</p>

<p>Let’s see how to implement a very simple undercomplete autoencoder for dimensionality reduction.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Performing PCA with an Undercomplete Linear Autoencoder"><div class="sect1" id="idm46263490900856">
<h1>Performing PCA with an Undercomplete Linear Autoencoder</h1>

<p>If the autoencoder uses only linear activations and the cost function is the mean squared error (MSE), then it ends up performing Principal Component Analysis (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch08.html#dimensionality_chapter">Chapter&nbsp;8</a>).</p>

<p>The following code builds a simple linear autoencoder to perform PCA on a 3D dataset, projecting it to 2D:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tensorflow</code> <code class="kn">import</code> <code class="n">keras</code>

<code class="n">encoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">3</code><code class="p">])])</code>
<code class="n">decoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">2</code><code class="p">])])</code>
<code class="n">autoencoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="n">encoder</code><code class="p">,</code> <code class="n">decoder</code><code class="p">])</code>

<code class="n">autoencoder</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"mse"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">0.1</code><code class="p">))</code></pre>

<p>This code is really not very different from all the MLPs we built in past chapters, but there are a few things to note:</p>

<ul>
<li>
<p>We organized the autoencoder into two subcomponents: the encoder and the decoder. Both are regular sequential models with a single dense layer each, and the autoencoder is a sequential model containing the encoder followed by the decoder (remember that a model can be used as a layer in another model).</p>
</li>
<li>
<p>The autoencoder’s number of outputs is equal to the number of inputs (i.e., 3).</p>
</li>
<li>
<p>To perform simple PCA, we do not use any activation function (i.e., all neurons are linear), and the cost function is the MSE. We will see more complex autoencoders shortly.</p>
</li>
</ul>

<p>Now let’s train the model on a simple generated 3D dataset and use it to encode that same set (i.e., project it to 2D):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">history</code> <code class="o">=</code> <code class="n">autoencoder</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">20</code><code class="p">)</code>
<code class="n">codings</code> <code class="o">=</code> <code class="n">encoder</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code></pre>

<p>Note that the same dataset <code>X_train</code> is used as both the inputs and the targets. <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#linear_autoencoder_pca_diagram">Figure&nbsp;17-2</a> shows the original 3D dataset (on the left) and the output of the autoencoder’s hidden layer (i.e., the coding layer, on the right). As you can see, the autoencoder found the best 2D plane to project the data onto, preserving as much variance in the data as it could (just like PCA).</p>

<figure><div id="linear_autoencoder_pca_diagram" class="figure">
<img src="./Chapter17_files/mls2_1702.png" alt="mls2 1702" width="1440" height="532" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1702.png">
<h6><span class="label">Figure 17-2. </span>PCA performed by an undercomplete linear autoencoder</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You can think of autoencoders as a form of self-supervised learning (i.e., using a supervised learning technique with automatically generated labels, in this case simply equal to the inputs).</p>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Stacked Autoencoders"><div class="sect1" id="idm46263490864072">
<h1>Stacked Autoencoders</h1>

<p>Just like other neural networks we have discussed, autoencoders can have multiple hidden layers. In this case they are called <em>stacked autoencoders</em> (or <em>deep autoencoders</em>). Adding more layers helps the autoencoder learn more complex codings. That said, one must be careful not to make the autoencoder too powerful. Imagine an encoder so powerful that it just learns to map each input to a single arbitrary number (and the decoder learns the reverse mapping). Obviously such an autoencoder will reconstruct the training data perfectly, but it will not have learned any useful data representation in the process (and it is unlikely to generalize well to new instances).</p>

<p>The architecture of a stacked autoencoder is typically symmetrical with regard to the central hidden layer (the coding layer). To put it simply, it looks like a sandwich. For example, an autoencoder for MNIST (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch03.html#classification_chapter">Chapter&nbsp;3</a>) may have 784 inputs, followed by a hidden layer with 100 neurons, then a central hidden layer of 30 neurons, then another hidden layer with 100 neurons, and an output layer with 784 neurons. This stacked autoencoder is represented in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#stacked_autoencoder_diagram">Figure&nbsp;17-3</a>.</p>

<figure class="smallerseventy"><div id="stacked_autoencoder_diagram" class="figure">
<img src="./Chapter17_files/mls2_1703.png" alt="mls2 1703" width="1289" height="798" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1703.png">
<h6><span class="label">Figure 17-3. </span>Stacked autoencoder</h6>
</div></figure>








<section data-type="sect2" data-pdf-bookmark="Implementing a Stacked Autoencoder Using Keras"><div class="sect2" id="idm46263490745736">
<h2>Implementing a Stacked Autoencoder Using Keras</h2>

<p>You can implement a stacked autoencoder very much like a regular deep MLP. In particular, the same techniques we used in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a> for training deep nets can be applied. For example, the following code builds a stacked autoencoder for Fashion MNIST (loaded and normalized as in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#ann_chapter">Chapter&nbsp;10</a>), using the SELU activation function:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">stacked_encoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(</code><code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">30</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">),</code>
<code class="p">])</code>
<code class="n">stacked_decoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">30</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Reshape</code><code class="p">([</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">])</code>
<code class="p">])</code>
<code class="n">stacked_ae</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="n">stacked_encoder</code><code class="p">,</code> <code class="n">stacked_decoder</code><code class="p">])</code>
<code class="n">stacked_ae</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"binary_crossentropy"</code><code class="p">,</code>
                   <code class="n">optimizer</code><code class="o">=</code><code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">1.5</code><code class="p">))</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">stacked_ae</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
                         <code class="n">validation_data</code><code class="o">=</code><code class="p">[</code><code class="n">X_valid</code><code class="p">,</code> <code class="n">X_valid</code><code class="p">])</code></pre>

<p>Let’s go through this code:</p>

<ul>
<li>
<p>Just like earlier, we split the autoencoder model into two submodels: the encoder and the decoder.</p>
</li>
<li>
<p>The encoder takes 28 × 28 grayscale images, flattens them so that each image is represented as a vector of size 784, then processes these vectors through two dense layers of diminishing sizes (100 units then 30 units), both using the SELU activation function (you may want to add LeCun normal initialization as well, but the network is not very deep so it won’t make a big difference). For each input image, the encoder outputs a vector of size 30.</p>
</li>
<li>
<p>The decoder takes codings of size 30 (output by the encoder) and processes them through two dense layers of increasing sizes (100 units then 784 units), and it reshapes the finals vectors into 28 × 28 arrays so the decoder’s outputs have the same shape as the encoder’s inputs.</p>
</li>
<li>
<p>When compiling the stacked autoencoder, we use the binary cross-entropy loss instead of the mean squared error: we are treating the reconstruction task as a multilabel binary classification problem: each pixel intensity represents the probability that the pixel should be black. Framing it this way (rather than as a regression problem) tends to make the model converge faster.<sup><a data-type="noteref" id="idm46263490532776-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263490532776" class="totri-footnote">2</a></sup></p>
</li>
<li>
<p>Finally, we train the model using <code>X_train</code> both as the inputs and the targets (and similarly, we use <code>X_valid</code> as both the validation inputs and targets).</p>
</li>
</ul>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Visualizing the Reconstructions"><div class="sect2" id="idm46263490667576">
<h2>Visualizing the Reconstructions</h2>

<p>One way to ensure that an autoencoder is properly trained is to compare the inputs and the outputs: the differences should not be too significant. Let’s plot a few images from the validation set, as well as their reconstructions:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">plot_image</code><code class="p">(</code><code class="n">image</code><code class="p">):</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"binary"</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">show_reconstructions</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">n_images</code><code class="o">=</code><code class="mi">5</code><code class="p">):</code>
    <code class="n">reconstructions</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_valid</code><code class="p">[:</code><code class="n">n_images</code><code class="p">])</code>
    <code class="n">fig</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="n">n_images</code> <code class="o">*</code> <code class="mf">1.5</code><code class="p">,</code> <code class="mi">3</code><code class="p">))</code>
    <code class="k">for</code> <code class="n">image_index</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_images</code><code class="p">):</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">subplot</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="n">n_images</code><code class="p">,</code> <code class="mi">1</code> <code class="o">+</code> <code class="n">image_index</code><code class="p">)</code>
        <code class="n">plot_image</code><code class="p">(</code><code class="n">X_valid</code><code class="p">[</code><code class="n">image_index</code><code class="p">])</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">subplot</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="n">n_images</code><code class="p">,</code> <code class="mi">1</code> <code class="o">+</code> <code class="n">n_images</code> <code class="o">+</code> <code class="n">image_index</code><code class="p">)</code>
        <code class="n">plot_image</code><code class="p">(</code><code class="n">reconstructions</code><code class="p">[</code><code class="n">image_index</code><code class="p">])</code>

<code class="n">show_reconstructions</code><code class="p">(</code><code class="n">stacked_ae</code><code class="p">)</code></pre>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#reconstruction_plot">Figure&nbsp;17-4</a> shows the resulting images.</p>

<figure class="smallerthirty"><div id="reconstruction_plot" class="figure">
<img src="./Chapter17_files/mls2_1704.png" alt="mls2 1704" width="1439" height="514" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1704.png">
<h6><span class="label">Figure 17-4. </span>Original images (top) and their reconstructions (bottom)</h6>
</div></figure>

<p>The reconstructions are recognizable, but a bit too lossy. We may need to train the model for longer, or make the encoder and decoder deeper, or make the codings larger. But if we make the network too powerful, it will manage to make perfect reconstructions without having learned any useful patterns in the data. For now, let’s go with this model.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Visualizing the Fashion MNIST Dataset"><div class="sect2" id="idm46263490330328">
<h2>Visualizing the Fashion MNIST Dataset</h2>

<p>Now that we have trained a stacked autoencoder, we can use it to reduce the dataset’s dimensionality. For visualization, this does not give great results compared to other dimensionality reduction algorithms (such as those we discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch08.html#dimensionality_chapter">Chapter&nbsp;8</a>), but one big advantage of autoencoders is that they can handle large datasets, with many instances and many features. So one strategy is to use an autoencoder to reduce the dimensionality down to a reasonable level, then use another dimensionality reduction algorithm for visualization. Let’s use this strategy to visualize Fashion MNIST. First, we use the encoder from our stacked autoencoder to reduce the dimensionality down to 30, then we use Scikit-Learn’s implementation of the t-SNE algorithm to reduce the dimensionality down to 2 for visualization:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="kn">import</code> <code class="n">TSNE</code>

<code class="n">X_valid_compressed</code> <code class="o">=</code> <code class="n">stacked_encoder</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_valid</code><code class="p">)</code>
<code class="n">tsne</code> <code class="o">=</code> <code class="n">TSNE</code><code class="p">()</code>
<code class="n">X_valid_2D</code> <code class="o">=</code> <code class="n">tsne</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_valid_compressed</code><code class="p">)</code></pre>

<p>Now we can plot the dataset:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X_valid_2D</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_valid_2D</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">y_valid</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"tab10"</code><code class="p">)</code></pre>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#fashion_mnist_visualization_plot">Figure&nbsp;17-5</a> shows the resulting scatterplot (beautified a bit by displaying some of the images). The t-SNE algorithm identified several clusters which match the classes reasonably well (each class is represented with a different color).</p>

<figure><div id="fashion_mnist_visualization_plot" class="figure">
<img src="./Chapter17_files/mls2_1705.png" alt="mls2 1705" width="1441" height="1144" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1705.png">
<h6><span class="label">Figure 17-5. </span>Fashion MNIST visualization using an autoencoder followed by t-SNE</h6>
</div></figure>

<p>So, autoencoders can be used for dimensionality reduction. Another application is for unsupervised pretraining.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Unsupervised Pretraining Using Stacked Autoencoders"><div class="sect1" id="idm46263490219624">
<h1>Unsupervised Pretraining Using Stacked Autoencoders</h1>

<p>As we discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>, if you are tackling a complex supervised task but you do not have a lot of labeled training data, one solution is to find a neural network that performs a similar task, and then reuse its lower layers. This makes it possible to train a high-performance model using little training data because your neural network won’t have to learn all the low-level features; it will just reuse the feature detectors learned by the existing network.</p>

<p>Similarly, if you have a large dataset but most of it is unlabeled, you can first train a stacked autoencoder using all the data, then reuse the lower layers to create a neural network for your actual task, and train it using the labeled data. For example, <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#unsupervised_pretraining_autoencoders_diagram">Figure&nbsp;17-6</a> shows how to use a stacked autoencoder to perform unsupervised pretraining for a classification neural network. When training the classifier, if you really don’t have much labeled training data, you may want to freeze the pretrained layers (at least the lower ones).</p>

<figure class="smallereighty"><div id="unsupervised_pretraining_autoencoders_diagram" class="figure">
<img src="./Chapter17_files/mls2_1706.png" alt="mls2 1706" width="1440" height="877" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1706.png">
<h6><span class="label">Figure 17-6. </span>Unsupervised pretraining using autoencoders</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Having plenty of unlabeled data and little labeled data is common: indeed, building a large unlabeled dataset is often cheap (e.g., a simple script can download millions of images off the internet), but labeling those images can usually be done reliably only by humans (e.g., classifying them as cute or not). Labeling instances is time-consuming and costly, so it’s normal to have only a few thousand human-labeled instances.</p>
</div>

<p>There is nothing special about the implementation: just train an autoencoder using all the training data (labeled plus unlabeled), then reuse its encoder layers to create a new neural network (see the exercises at the end of this chapter for an example).</p>

<p>Next, let’s look at a few techniques for training stacked autoencoders.</p>








<section data-type="sect2" data-pdf-bookmark="Tying Weights"><div class="sect2" id="idm46263490209912">
<h2>Tying Weights</h2>

<p>When an autoencoder is neatly symmetrical, like the one we just built, a common technique is to <em>tie the weights</em> of the decoder layers to the weights of the encoder layers. This halves the number of weights in the model, speeding up training and limiting the risk of overfitting. Specifically, if the autoencoder has a total of <em>N</em> layers (not counting the input layer), and <strong>W</strong><sub><em>L</em></sub> represents the connection weights of the <em>L</em><sup>th</sup> layer (e.g., layer 1 is the first hidden layer, layer <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-153-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;StartFraction upper N Over 2 EndFraction&quot;&gt;&lt;mfrac&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mfrac&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-6841" aria-label="StartFraction upper N Over 2 EndFraction" style="width: 1.031em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.082em, 1000.99em, 2.625em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-6842"><span class="mfrac" id="MathJax-Span-6843"><span style="display: inline-block; position: relative; width: 0.774em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.396em, 1000.63em, 4.167em, -1000.01em); top: -4.419em; left: 50%; margin-left: -0.306em;"><span class="mi" id="MathJax-Span-6844" style="font-size: 70.7%; font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1000.32em, 4.167em, -1000.01em); top: -3.648em; left: 50%; margin-left: -0.152em;"><span class="mn" id="MathJax-Span-6845" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1000.73em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.774em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.421em; border-left: 0px solid; width: 0px; height: 1.432em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="StartFraction upper N Over 2 EndFraction"><mfrac><mi>N</mi><mn>2</mn></mfrac></math></span></span><script type="math/mml" id="MathJax-Element-153"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="StartFraction upper N Over 2 EndFraction">
  <mfrac><mi>N</mi> <mn>2</mn></mfrac>
</math></script> is the coding layer, and layer <em>N</em> is the output layer), then the decoder layer weights can be defined simply as: <strong>W</strong><sub><em>N–L</em>+1</sub> = <strong>W</strong><sub><em>L</em></sub><sup><em>T</em></sup> (with  <em>L</em> = 1, 2, <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-154-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;comma comma StartFraction upper N Over 2 EndFraction&quot;&gt;&lt;mrow&gt;&lt;mo&gt;&amp;#x22EF;&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mfrac&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-6846" aria-label="comma comma StartFraction upper N Over 2 EndFraction" style="width: 2.83em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.728em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.082em, 1002.74em, 2.625em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-6847"><span class="mrow" id="MathJax-Span-6848"><span class="mo" id="MathJax-Span-6849" style="font-family: MathJax_Main;">⋯</span><span class="mo" id="MathJax-Span-6850" style="font-family: MathJax_Main; padding-left: 0.157em;">,</span><span class="mfrac" id="MathJax-Span-6851" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 0.774em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.396em, 1000.63em, 4.167em, -1000.01em); top: -4.419em; left: 50%; margin-left: -0.306em;"><span class="mi" id="MathJax-Span-6852" style="font-size: 70.7%; font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1000.32em, 4.167em, -1000.01em); top: -3.648em; left: 50%; margin-left: -0.152em;"><span class="mn" id="MathJax-Span-6853" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1000.73em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.774em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.421em; border-left: 0px solid; width: 0px; height: 1.432em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="comma comma StartFraction upper N Over 2 EndFraction"><mrow><mo>⋯</mo><mo>,</mo><mfrac><mi>N</mi><mn>2</mn></mfrac></mrow></math></span></span><script type="math/mml" id="MathJax-Element-154"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="comma comma StartFraction upper N Over 2 EndFraction">
  <mrow>
    <mo>⋯</mo>
    <mo>,</mo>
    <mfrac><mi>N</mi> <mn>2</mn></mfrac>
  </mrow>
</math></script>).</p>

<p>To tie weights between layers using Keras, let’s define a custom layer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">DenseTranspose</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Layer</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">dense</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">dense</code> <code class="o">=</code> <code class="n">dense</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">activation</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">activations</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">activation</code><code class="p">)</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="nf-Magic">__init__</code><code class="p">(</code><code class="o">**</code><code class="n">kwargs</code><code class="p">)</code>
    <code class="k">def</code> <code class="nf">build</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">batch_input_shape</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">biases</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">add_weight</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s2">"bias"</code><code class="p">,</code> <code class="n">initializer</code><code class="o">=</code><code class="s2">"zeros"</code><code class="p">,</code>
                                      <code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">dense</code><code class="o">.</code><code class="n">input_shape</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]])</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="n">build</code><code class="p">(</code><code class="n">batch_input_shape</code><code class="p">)</code>
    <code class="k">def</code> <code class="nf">call</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">):</code>
        <code class="n">z</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">dense</code><code class="o">.</code><code class="n">weights</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">transpose_b</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">activation</code><code class="p">(</code><code class="n">z</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">biases</code><code class="p">)</code></pre>

<p>This custom layer acts like a regular <code>Dense</code> layer, but it uses another <code>Dense</code> layer’s weights, transposed (setting <code>transpose_b=True</code> is equivalent to transposing the second argument, but it’s more efficient as it performs the transposition on the fly within the <code>matmul()</code> operation). However, it uses its own bias vector. Next, we can build a new stacked autoencoder, much like the previous one, but with the decoder’s dense layers tied to the encoder’s dense layers:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">dense_1</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">)</code>
<code class="n">dense_2</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">30</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">)</code>

<code class="n">tied_encoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(</code><code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">]),</code>
    <code class="n">dense_1</code><code class="p">,</code>
    <code class="n">dense_2</code>
<code class="p">])</code>

<code class="n">tied_decoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">DenseTranspose</code><code class="p">(</code><code class="n">dense_2</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">),</code>
    <code class="n">DenseTranspose</code><code class="p">(</code><code class="n">dense_1</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Reshape</code><code class="p">([</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">])</code>
<code class="p">])</code>

<code class="n">tied_ae</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="n">tied_encoder</code><code class="p">,</code> <code class="n">tied_decoder</code><code class="p">])</code></pre>

<p>This model achieves a very slightly lower <em>reconstruction error</em> than the previous model, with almost half the number of parameters.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Training One Autoencoder at a Time"><div class="sect2" id="idm46263490051368">
<h2>Training One Autoencoder at a Time</h2>

<p>Rather than training the whole stacked autoencoder in one go like we just did, it is possible to train one shallow autoencoder at a time, then stack all of them into a single stacked autoencoder (hence the name), as shown on <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#stacking_autoencoders_diagram">Figure&nbsp;17-7</a>. This technique is not used as much these days, but you may still run into papers that talk about “greedy layerwise training,” so it’s good to know what it means.</p>

<figure><div id="stacking_autoencoders_diagram" class="figure">
<img src="./Chapter17_files/mls2_1707.png" alt="mls2 1707" width="1440" height="689" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1707.png">
<h6><span class="label">Figure 17-7. </span>Training one autoencoder at a time</h6>
</div></figure>

<p>During the first phase of training, the first autoencoder learns to reconstruct the inputs. Then we encode the whole training set using this first autoencoder, and this gives us a new (compressed) training set. We then train a second autoencoder on this new dataset. This is the second phase of training. Finally, we build a big sandwich using all these autoencoders, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#stacking_autoencoders_diagram">Figure&nbsp;17-7</a> (i.e., we first stack the hidden layers of each autoencoder, then the output layers in reverse order). This gives us the final stacked autoencoder (see the notebook for an implementation). We could easily train more autoencoders this way, building a very deep stacked autoencoder.</p>

<p>As we discussed earlier, one of the triggers of the current Deep Learning tsunami is the discovery in 2006 by Geoffrey Hinton et al. that deep neural networks can be pretrained in an unsupervised fashion, using this greedy layerwise approach. They used restricted Boltzmann machines for that (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/app05.html#other_ann_appendix">Appendix&nbsp;E</a>). In <a href="https://homl.info/112">2007 Yoshua Bengio et al. showed</a><sup><a data-type="noteref" id="idm46263490002056-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263490002056" class="totri-footnote">3</a></sup> that autoencoders worked just as well. For several years it was the only efficient way to train deep nets, until many of the techniques introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a> made it possible to just train a deep net in one shot.</p>

<p>Autoencoders are not limited to dense networks: you can also build convolutional autoencoders, or even recurrent autoencoders. Let’s look at these now.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Convolutional Autoencoders"><div class="sect1" id="idm46263489999624">
<h1>Convolutional Autoencoders</h1>

<p>If you are dealing with images, then the autoencoders we have seen so far will not work well (unless the images are very small): as we saw in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cnn_chapter">Chapter&nbsp;14</a>, convolutional neural networks are far better suited than dense networks to deal with images. So if you want to build an autoencoder for images (e.g., for unsupervised pretraining or dimensionality reduction), you will need to build a <a href="https://homl.info/convae"><em>convolutional autoencoder</em></a><sup><a data-type="noteref" id="idm46263489996376-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263489996376" class="totri-footnote">4</a></sup> The encoder is a regular CNN composed of convolutional layers and pooling layers. It typically reduces the spatial dimensionality of the inputs (i.e., height and width) while increasing its depth (i.e., the number of feature maps). The decoder must do the reverse (upscale the image and reduce its depth back to the original dimensions), and for this you can use transpose convolutional layers (alternatively, you could combine upsampling layers with convolutional layers). Here is a simple convolutional autoencoder for Fashion MNIST:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">conv_encoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Reshape</code><code class="p">([</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2D</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">MaxPool2D</code><code class="p">(</code><code class="n">pool_size</code><code class="o">=</code><code class="mi">2</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2D</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">MaxPool2D</code><code class="p">(</code><code class="n">pool_size</code><code class="o">=</code><code class="mi">2</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2D</code><code class="p">(</code><code class="mi">64</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">MaxPool2D</code><code class="p">(</code><code class="n">pool_size</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="p">])</code>
<code class="n">conv_decoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2DTranspose</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"valid"</code><code class="p">,</code>
                                 <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">,</code>
                                 <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">64</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2DTranspose</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,</code>
                                 <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2DTranspose</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,</code>
                                 <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Reshape</code><code class="p">([</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">])</code>
<code class="p">])</code>
<code class="n">conv_ae</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="n">conv_encoder</code><code class="p">,</code> <code class="n">conv_decoder</code><code class="p">])</code></pre>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Recurrent Autoencoders"><div class="sect1" id="idm46263489992008">
<h1>Recurrent Autoencoders</h1>

<p>If you want to build an autoencoder for sequences, such as time series or text (e.g., for unsupervised learning or dimensionality reduction), then recurrent neural networks (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#rnn_chapter">Chapter&nbsp;15</a>) may be better suited than dense networks. Building a <em>recurrent autoencoder</em> is straightward: the encoder is typically a sequence-to-vector RNN which compresses the input sequence down to a single vector. The decoder is a vector-to-sequence RNN that does the reverse:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">recurrent_encoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">LSTM</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="bp">None</code><code class="p">,</code> <code class="mi">28</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">LSTM</code><code class="p">(</code><code class="mi">30</code><code class="p">)</code>
<code class="p">])</code>
<code class="n">recurrent_decoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">RepeatVector</code><code class="p">(</code><code class="mi">28</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">30</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">LSTM</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">return_sequences</code><code class="o">=</code><code class="bp">True</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">TimeDistributed</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">28</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">))</code>
<code class="p">])</code>
<code class="n">recurrent_ae</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="n">recurrent_encoder</code><code class="p">,</code> <code class="n">recurrent_decoder</code><code class="p">])</code></pre>

<p>This recurrent autoencoder can process sequences of any length, with 28 dimensions per time step. Conveniently, this means it can process Fashion MNIST images, by treating each image as a sequence of rows: at each time step, the RNN will process a single row of 28 pixels. Obviously, you could use a recurrent autoencoder for any kind of sequence. Note that we use a <code>RepeatVector</code> layer as the first layer of the decoder, to ensure that its input vector gets fed to the decoder at each time step.</p>

<p>OK, let’s step back for a second. So far we have seen various kinds of autoencoders (basic, stacked, convolutional, and recurrent), and we have looked at how to train them (either in one shot or layer by layer). We also looked at a couple applications: data visualization and unsupervised pretraining.</p>

<p>Up to now, in order to force the autoencoder to learn interesting features, we have limited the size of the coding layer, making it undercomplete. There are actually many other kinds of constraints that can be used, including ones that allow the coding layer to be just as large as the inputs, or even larger, resulting in an <em>overcomplete autoencoder</em>. Let’s look at some of those approaches now.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Denoising Autoencoders"><div class="sect1" id="idm46263489525768">
<h1>Denoising Autoencoders</h1>

<p>Another way to force the autoencoder to learn useful features is to add noise to its inputs, training it to recover the original, noise-free inputs. This idea has been around since the 1980s (e.g., it is mentioned in Yann LeCun’s 1987 master’s thesis). In a <a href="https://homl.info/113">2008 paper</a>,<sup><a data-type="noteref" id="idm46263489523080-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263489523080" class="totri-footnote">5</a></sup> Pascal Vincent et al. showed that autoencoders could also be used for feature extraction. In a <a href="https://homl.info/114">2010 paper</a>,<sup><a data-type="noteref" id="idm46263489521432-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263489521432" class="totri-footnote">6</a></sup> Vincent et al. introduced <em>stacked denoising autoencoders</em>.</p>

<p>The noise can be pure Gaussian noise added to the inputs, or it can be randomly switched off inputs, just like in dropout (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>). <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#denoising_autoencoders_diagram">Figure&nbsp;17-8</a> shows both options.</p>

<figure class="smallereighty"><div id="denoising_autoencoders_diagram" class="figure">
<img src="./Chapter17_files/mls2_1708.png" alt="mls2 1708" width="1440" height="881" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1708.png">
<h6><span class="label">Figure 17-8. </span>Denoising autoencoders, with Gaussian noise (left) or dropout (right)</h6>
</div></figure>

<p>The implementation is straightforward: it is a regular stacked autoencoder with an additional <code>Dropout</code> layer applied to the encoder’s inputs (or you could use a <code>GaussianNoise</code> layer instead). Recall that the <code>Dropout</code> layer is only active during training (and so is the <code>GaussianNoise</code> layer).</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">dropout_encoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(</code><code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.5</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">30</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">)</code>
<code class="p">])</code>
<code class="n">dropout_decoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">30</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Reshape</code><code class="p">([</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">])</code>
<code class="p">])</code>
<code class="n">dropout_ae</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="n">dropout_encoder</code><code class="p">,</code> <code class="n">dropout_decoder</code><code class="p">])</code></pre>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#dropout_denoising_plot">Figure&nbsp;17-9</a> shows a few noisy images (with half the pixels turned off), and the images reconstructed by the dropout-based denoising autoencoder. Notice how the autoencoder guesses details that are actually not in the input, such as the top of the white shirt (bottom row, fourth image). As you can see, not only can denoising autoencoders be used for data visualization or unsupervised pretraining like the other autoencoders we discussed so far, but it can also be used quite simply and efficiently to remove noise from images.</p>

<figure class="smallereighty"><div id="dropout_denoising_plot" class="figure">
<img src="./Chapter17_files/mls2_1709.png" alt="mls2 1709" width="1439" height="564" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1709.png">
<h6><span class="label">Figure 17-9. </span>Noisy images (top) and their reconstructions (bottom)</h6>
</div></figure>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Sparse Autoencoders"><div class="sect1" id="idm46263489510632">
<h1>Sparse Autoencoders</h1>

<p>Another kind of constraint that often leads to good feature extraction is <em>sparsity</em>: by adding an appropriate term to the cost function, the autoencoder is pushed to reduce the number of active neurons in the coding layer. For example, it may be pushed to have on average only 5% significantly active neurons in the coding layer. This forces the autoencoder to represent each input as a combination of a small number of activations. As a result, each neuron in the coding layer typically ends up representing a useful feature (if you could speak only a few words per month, you would probably try to make them worth listening to).</p>

<p>A simple approach is to use the sigmoid activation function in the coding layer (to constrain the codings to values between 0 and 1), use a large coding layer (e.g., with 300 units), and add some ℓ<sub>1</sub> regularization to the coding layer’s activations (the decoder is just a regular decoder):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">sparse_l1_encoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(</code><code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">300</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">ActivityRegularization</code><code class="p">(</code><code class="n">l1</code><code class="o">=</code><code class="mf">1e-3</code><code class="p">)</code>
<code class="p">])</code>
<code class="n">sparse_l1_decoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">300</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Reshape</code><code class="p">([</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">])</code>
<code class="p">])</code>
<code class="n">sparse_l1_ae</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="n">sparse_l1_encoder</code><code class="p">,</code> <code class="n">sparse_l1_decoder</code><code class="p">])</code></pre>

<p>This <code>ActivityRegularization</code> layer just returns its inputs, but as a side-effect it adds a training loss equal to the sum of absolute values of its inputs (this layer only has an effect during training). Equivalently, you could remove the <code>ActivityRegularization</code> layer and set <code>activity_regularizer=keras.regularizers.l1(1e-3)</code> in the previous layer. This penalty will encourage the neural network to produce codings close to 0, but since it will also be penalized if it does not reconstruct the inputs correctly, it will have to output at least a few nonzero values. Using the ℓ<sub>1</sub> norm rather than the ℓ<sub>2</sub> norm will push the neural network to preserve the most important codings while eliminating the ones that are not needed for the input image (rather than just reducing all codings).</p>

<p>Another approach, which often yields better results, is to measure the actual sparsity of the coding layer at each training iteration, and penalize the model when the measured sparsity differs from a target sparsity. We do so by computing the average activation of each neuron in the coding layer, over the whole training batch. The batch size must not be too small, or else the mean will not be accurate.</p>

<p>Once we have the mean activation per neuron, we want to penalize the neurons that are too active, or not enough, by adding a <em>sparsity loss</em> to the cost function. For example, if we measure that a neuron has an average activation of 0.3, but the target sparsity is 0.1, it must be penalized to activate less. One approach could be simply adding the squared error (0.3 – 0.1)<sup>2</sup> to the cost function, but in practice a better approach is to use the Kullback–Leibler divergence (briefly discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>), which has much stronger gradients than the mean squared error, as you can see in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#sparsity_loss_plot">Figure&nbsp;17-10</a>.</p>

<figure class="smallereighty"><div id="sparsity_loss_plot" class="figure">
<img src="./Chapter17_files/mls2_1710.png" alt="mls2 1710" width="1439" height="921" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1710.png">
<h6><span class="label">Figure 17-10. </span>Sparsity loss</h6>
</div></figure>

<p>Given two discrete probability distributions <em>P</em> and <em>Q</em>, the KL divergence between these distributions, noted <em>D</em><sub>KL</sub>(<em>P</em> ∥ <em>Q</em>), can be computed using <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#kl_divergence_equation">Equation 17-1</a>.</p>
<div data-type="equation" id="kl_divergence_equation">
<h5><span class="label">Equation 17-1. </span>Kullback–Leibler divergence</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-155-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;KL&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;&amp;#x2225;&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/munder&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-6854" style="width: 14.913em; display: inline-block;"><span style="display: inline-block; position: relative; width: 14.45em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(0.465em, 1014.46em, 3.447em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-6855"><span class="mrow" id="MathJax-Span-6856"><span class="msub" id="MathJax-Span-6857"><span style="display: inline-block; position: relative; width: 1.905em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6858" style="font-family: MathJax_Math-italic;">D</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.825em;"><span class="mi" id="MathJax-Span-6859" style="font-size: 70.7%; font-family: MathJax_Main;">KL</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-6860" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-6861" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6862" style="font-family: MathJax_Math-italic;">P<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mo" id="MathJax-Span-6863" style="font-family: MathJax_Main; padding-left: 0.26em;">∥</span><span class="mi" id="MathJax-Span-6864" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">Q</span><span class="mo" id="MathJax-Span-6865" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-6866" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="munder" id="MathJax-Span-6867" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1001.4em, 4.63em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-6868" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1000.22em, 4.27em, -1000.01em); top: -2.928em; left: 0.62em;"><span class="mi" id="MathJax-Span-6869" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mi" id="MathJax-Span-6870" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">P<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mrow" id="MathJax-Span-6871" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-6872" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6873" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-6874" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-6875" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mstyle" id="MathJax-Span-6876"><span class="mrow" id="MathJax-Span-6877"><span class="mfrac" id="MathJax-Span-6878"><span style="display: inline-block; position: relative; width: 2.059em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.087em, 1001.76em, 4.424em, -1000.01em); top: -4.728em; left: 50%; margin-left: -0.923em;"><span class="mrow" id="MathJax-Span-6879"><span class="mi" id="MathJax-Span-6880" style="font-family: MathJax_Math-italic;">P<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mo" id="MathJax-Span-6881" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6882" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-6883" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1001.81em, 4.424em, -1000.01em); top: -3.288em; left: 50%; margin-left: -0.974em;"><span class="mrow" id="MathJax-Span-6884"><span class="mi" id="MathJax-Span-6885" style="font-family: MathJax_Math-italic;">Q</span><span class="mo" id="MathJax-Span-6886" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6887" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-6888" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1002.02em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.059em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.268em; border-left: 0px solid; width: 0px; height: 2.862em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>D</mi><mi>KL</mi></msub><mrow><mo>(</mo><mi>P</mi><mo>∥</mo><mi>Q</mi><mo>)</mo></mrow><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mi>P</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow><mo form="prefix">log</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>P</mi><mo>(</mo><mi>i</mi><mo>)</mo></mrow><mrow><mi>Q</mi><mo>(</mo><mi>i</mi><mo>)</mo></mrow></mfrac></mstyle></mrow></math></span></span></div><script type="math/mml" id="MathJax-Element-155"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <msub><mi>D</mi> <mi> KL </mi> </msub>
    <mrow>
      <mo>(</mo>
      <mi>P</mi>
      <mo>∥</mo>
      <mi>Q</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <munder><mo>∑</mo> <mi>i</mi> </munder>
    <mi>P</mi>
    <mrow>
      <mo>(</mo>
      <mi>i</mi>
      <mo>)</mo>
    </mrow>
    <mo form="prefix">log</mo>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac><mrow><mi>P</mi><mo>(</mo><mi>i</mi><mo>)</mo></mrow> <mrow><mi>Q</mi><mo>(</mo><mi>i</mi><mo>)</mo></mrow></mfrac>
    </mstyle>
  </mrow>
</math></script>
</div>

<p>In our case, we want to measure the divergence between the target probability <em>p</em> that a neuron in the coding layer will activate and the actual probability <em>q</em> (i.e., the mean activation over the training batch). So the KL divergence simplifies to <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#kl_divergence_equation_simplified">Equation 17-2</a>.</p>
<div data-type="equation" id="kl_divergence_equation_simplified">
<h5><span class="label">Equation 17-2. </span>KL divergence between the target sparsity <em>p</em> and the actual sparsity <em>q</em></h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-156-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;KL&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;&amp;#x2225;&lt;/mo&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-6889" style="width: 18.615em; display: inline-block;"><span style="display: inline-block; position: relative; width: 18.049em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(0.62em, 1018.06em, 3.139em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-6890"><span class="mrow" id="MathJax-Span-6891"><span class="msub" id="MathJax-Span-6892"><span style="display: inline-block; position: relative; width: 1.905em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6893" style="font-family: MathJax_Math-italic;">D</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.825em;"><span class="mi" id="MathJax-Span-6894" style="font-size: 70.7%; font-family: MathJax_Main;">KL</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-6895" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-6896" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-6897" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-6898" style="font-family: MathJax_Main; padding-left: 0.26em;">∥</span><span class="mi" id="MathJax-Span-6899" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-6900" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-6901" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mi" id="MathJax-Span-6902" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">p</span><span class="mspace" id="MathJax-Span-6903" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-6904" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mstyle" id="MathJax-Span-6905"><span class="mrow" id="MathJax-Span-6906"><span class="mfrac" id="MathJax-Span-6907"><span style="display: inline-block; position: relative; width: 0.62em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.373em, -1000.01em); top: -4.676em; left: 50%; margin-left: -0.255em;"><span class="mi" id="MathJax-Span-6908" style="font-family: MathJax_Math-italic;">p</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.373em, -1000.01em); top: -3.339em; left: 50%; margin-left: -0.203em;"><span class="mi" id="MathJax-Span-6909" style="font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1000.63em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.62em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-6910" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mrow" id="MathJax-Span-6911" style="padding-left: 0.208em;"><span class="mo" id="MathJax-Span-6912" style="font-family: MathJax_Main;">(</span><span class="mn" id="MathJax-Span-6913" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-6914" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mi" id="MathJax-Span-6915" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">p</span><span class="mo" id="MathJax-Span-6916" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-6917" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mstyle" id="MathJax-Span-6918"><span class="mrow" id="MathJax-Span-6919"><span class="mfrac" id="MathJax-Span-6920"><span style="display: inline-block; position: relative; width: 2.316em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.19em, 1002.17em, 4.373em, -1000.01em); top: -4.676em; left: 50%; margin-left: -1.077em;"><span class="mrow" id="MathJax-Span-6921"><span class="mn" id="MathJax-Span-6922" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-6923" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mi" id="MathJax-Span-6924" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">p</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1002.17em, 4.373em, -1000.01em); top: -3.339em; left: 50%; margin-left: -1.077em;"><span class="mrow" id="MathJax-Span-6925"><span class="mn" id="MathJax-Span-6926" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-6927" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mi" id="MathJax-Span-6928" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1002.33em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.316em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.951em; border-left: 0px solid; width: 0px; height: 2.386em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>D</mi><mi>KL</mi></msub><mrow><mo>(</mo><mi>p</mi><mo>∥</mo><mi>q</mi><mo>)</mo></mrow><mo>=</mo><mi>p</mi><mspace width="0.166667em"></mspace><mo form="prefix">log</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mi>p</mi><mi>q</mi></mfrac></mstyle><mo>+</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>p</mi><mo>)</mo></mrow><mo form="prefix">log</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow><mrow><mn>1</mn><mo>-</mo><mi>q</mi></mrow></mfrac></mstyle></mrow></math></span></span></div><script type="math/mml" id="MathJax-Element-156"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <msub><mi>D</mi> <mi> KL </mi> </msub>
    <mrow>
      <mo>(</mo>
      <mi>p</mi>
      <mo>∥</mo>
      <mi>q</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mi>p</mi>
    <mspace width="0.166667em"></mspace>
    <mo form="prefix">log</mo>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac><mi>p</mi> <mi>q</mi></mfrac>
    </mstyle>
    <mo>+</mo>
    <mrow>
      <mo>(</mo>
      <mn>1</mn>
      <mo>-</mo>
      <mi>p</mi>
      <mo>)</mo>
    </mrow>
    <mo form="prefix">log</mo>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac><mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow> <mrow><mn>1</mn><mo>-</mo><mi>q</mi></mrow></mfrac>
    </mstyle>
  </mrow>
</math></script>
</div>

<p>Once we have computed the sparsity loss for each neuron in the coding layer, we sum up these losses and add the result to the cost function. In order to control the relative importance of the sparsity loss and the reconstruction loss, we can multiply the sparsity loss by a sparsity weight hyperparameter. If this weight is too high, the model will stick closely to the target sparsity, but it may not reconstruct the inputs properly, making the model useless. Conversely, if it is too low, the model will mostly ignore the sparsity objective and will not learn any interesting features.</p>

<p>We now have all we need to implement a sparse autoencoder based on the KL divergence. First, let’s create a custom regularizer to apply KL divergence regularization:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">K</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">backend</code>
<code class="n">kl_divergence</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">losses</code><code class="o">.</code><code class="n">kullback_leibler_divergence</code>

<code class="k">class</code> <code class="nc">KLDivergenceRegularizer</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">regularizers</code><code class="o">.</code><code class="n">Regularizer</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">weight</code><code class="p">,</code> <code class="n">target</code><code class="o">=</code><code class="mf">0.1</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">weight</code> <code class="o">=</code> <code class="n">weight</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">target</code> <code class="o">=</code> <code class="n">target</code>
    <code class="k">def</code> <code class="nf-Magic">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">):</code>
        <code class="n">mean_activities</code> <code class="o">=</code> <code class="n">K</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">weight</code> <code class="o">*</code> <code class="p">(</code>
            <code class="n">kl_divergence</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">mean_activities</code><code class="p">)</code> <code class="o">+</code>
            <code class="n">kl_divergence</code><code class="p">(</code><code class="mf">1.</code> <code class="o">-</code> <code class="bp">self</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="mf">1.</code> <code class="o">-</code> <code class="n">mean_activities</code><code class="p">))</code></pre>

<p>Now we can build the sparse autoencoder, using the <code>KLDivergenceRegularizer</code> for the coding layer’s activations:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">kld_reg</code> <code class="o">=</code> <code class="n">KLDivergenceRegularizer</code><code class="p">(</code><code class="n">weight</code><code class="o">=</code><code class="mf">0.05</code><code class="p">,</code> <code class="n">target</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>
<code class="n">sparse_kl_encoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(</code><code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">300</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">,</code> <code class="n">activity_regularizer</code><code class="o">=</code><code class="n">kld_reg</code><code class="p">)</code>
<code class="p">])</code>
<code class="n">sparse_kl_decoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">300</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Reshape</code><code class="p">([</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">])</code>
<code class="p">])</code>
<code class="n">sparse_kl_ae</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="n">sparse_kl_encoder</code><code class="p">,</code> <code class="n">sparse_kl_decoder</code><code class="p">])</code></pre>

<p>After training this sparse autoencoder on Fashion MNIST, the activations of the neurons in the coding layer are mostly close to 0 (about 70% of all activations are lower than 0.1), and all neurons have a mean activation around 0.1 (about 90% of all neurons have a mean activation between 0.1 and 0.2), as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#sparse_autoencoder_plot">Figure&nbsp;17-11</a>.</p>

<figure><div id="sparse_autoencoder_plot" class="figure">
<img src="./Chapter17_files/mls2_1711.png" alt="mls2 1711" width="1439" height="391" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1711.png">
<h6><span class="label">Figure 17-11. </span>Distribution of all the activations in the coding layer (left) and distribution of the mean activation per neuron (right)</h6>
</div></figure>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Variational Autoencoders"><div class="sect1" id="idm46263489276984">
<h1>Variational Autoencoders</h1>

<p>Another important category of autoencoders was <a href="https://homl.info/115">introduced in 2014</a> by Diederik Kingma and Max Welling and quickly became one of the most popular types of autoencoders: <em>variational autoencoders</em>.<sup><a data-type="noteref" id="idm46263488743288-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263488743288" class="totri-footnote">7</a></sup></p>

<p>They are quite different from all the autoencoders we have discussed so far, in these particular ways:</p>

<ul>
<li>
<p>They are <em>probabilistic autoencoders</em>, meaning that their outputs are partly determined by chance, even after training (as opposed to denoising autoencoders, which use randomness only during training).</p>
</li>
<li>
<p>Most importantly, they are <em>generative autoencoders</em>, meaning that they can generate new instances that look like they were sampled from the training set.</p>
</li>
</ul>

<p>Both these properties make them rather similar to Restricted Boltzmann Machines (RBMs, see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/app05.html#other_ann_appendix">Appendix&nbsp;E</a>), but they are easier to train, and the sampling process is much faster (with RBMs you need to wait for the network to stabilize into a “thermal equilibrium” before you can sample a new instance). Indeed, as their name suggests, variational autoencoders perform variational Bayesian inference (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#unsupervised_learning_chapter">Chapter&nbsp;9</a>), which is an efficient way to perform approximate Bayesian inference.</p>

<p>Let’s take a look at how they work. <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#variational_autoencoders_diagram">Figure&nbsp;17-12</a> (left) shows a variational autoencoder. You can recognize the basic structure of all autoencoders, with an encoder followed by a decoder (in this example, they both have two hidden layers), but there is a twist: instead of directly producing a coding for a given input, the encoder produces a <em>mean coding</em> <strong>μ</strong> and a standard deviation <strong>σ</strong>. The actual coding is then sampled randomly from a Gaussian distribution with mean <strong>μ</strong> and standard deviation <strong>σ</strong>. After that the decoder decodes the sampled coding normally. The right part of the diagram shows a training instance going through this autoencoder. First, the encoder produces <strong>μ</strong> and <strong>σ</strong>, then a coding is sampled randomly (notice that it is not exactly located at <strong>μ</strong>), and finally this coding is decoded, and the final output resembles the training instance.</p>

<figure class="smallereighty"><div id="variational_autoencoders_diagram" class="figure">
<img src="./Chapter17_files/mls2_1712.png" alt="mls2 1712" width="1440" height="1164" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1712.png">
<h6><span class="label">Figure 17-12. </span>Variational autoencoder (left) and an instance going through it (right)</h6>
</div></figure>

<p>As you can see on the diagram, although the inputs may have a very convoluted distribution, a variational autoencoder tends to produce codings that look as though they were sampled from a simple Gaussian distribution:<sup><a data-type="noteref" id="idm46263488728168-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263488728168" class="totri-footnote">8</a></sup> during training, the cost function (discussed next) pushes the codings to gradually migrate within the coding space (also called the <em>latent space</em>) to end up looking like a cloud of Gaussian points. One great consequence is that after training a variational autoencoder, you can very easily generate a new instance: just sample a random coding from the Gaussian distribution, decode it, and voilà!</p>

<p>So let’s look at the cost function. It is composed of two parts. The first is the usual reconstruction loss that pushes the autoencoder to reproduce its inputs (we can use cross entropy for this, as discussed earlier). The second is the <em>latent loss</em> that pushes the autoencoder to have codings that look as though they were sampled from a simple Gaussian distribution: it is the KL divergence between the target distribution (i.e., the Gaussian distribution) and the actual distribution of the codings. The math is a bit more complex than with the sparse autoencoder, in particular because of the Gaussian noise, which limits the amount of information that can be transmitted to the coding layer (thus pushing the autoencoder to learn useful features). Luckily, the equations simplify, so the latent loss can be computed quite simply using <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#var_ae_latent_loss_equation">Equation 17-3</a>:<sup><a data-type="noteref" id="idm46263488724008-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263488724008" class="totri-footnote">9</a></sup></p>
<div id="var_ae_latent_loss_equation" data-type="equation">
<h5><span class="label">Equation 17-3. </span>Variational autoencoder’s latent loss</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-157-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;script upper L equals minus one-half sigma-summation Underscript i equals 1 Overscript upper K Endscripts 1 plus log left-parenthesis sigma Subscript i Baseline Superscript 2 Baseline right-parenthesis minus sigma Subscript i Baseline Superscript 2 Baseline minus mu Subscript i Baseline Superscript 2&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mi&gt;&amp;#x2112;&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mfrac&gt;&lt;munderover&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3C3;&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3C3;&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3BC;&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-6929" aria-label="script upper L equals minus one-half sigma-summation Underscript i equals 1 Overscript upper K Endscripts 1 plus log left-parenthesis sigma Subscript i Baseline Superscript 2 Baseline right-parenthesis minus sigma Subscript i Baseline Superscript 2 Baseline minus mu Subscript i Baseline Superscript 2" style="width: 16.866em; display: inline-block;"><span style="display: inline-block; position: relative; width: 16.352em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(0.311em, 1016.36em, 3.55em, -1000.01em); top: -2.208em; left: 0em;"><span class="mrow" id="MathJax-Span-6930"><span class="mrow" id="MathJax-Span-6931"><span class="mi" id="MathJax-Span-6932" style="font-family: MathJax_Script;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.157em;"></span></span><span class="mo" id="MathJax-Span-6933" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mo" id="MathJax-Span-6934" style="font-family: MathJax_Main; padding-left: 0.26em;">−</span><span class="mfrac" id="MathJax-Span-6935"><span style="display: inline-block; position: relative; width: 0.62em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.19em, 1000.42em, 4.167em, -1000.01em); top: -4.676em; left: 50%; margin-left: -0.255em;"><span class="mn" id="MathJax-Span-6936" style="font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1000.47em, 4.167em, -1000.01em); top: -3.339em; left: 50%; margin-left: -0.255em;"><span class="mn" id="MathJax-Span-6937" style="font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1000.63em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.62em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span><span class="munderover" id="MathJax-Span-6938" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1001.4em, 4.63em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-6939" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.09em, 4.27em, -1000.01em); top: -2.928em; left: 0.157em;"><span class="mrow" id="MathJax-Span-6940"><span class="mi" id="MathJax-Span-6941" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-6942" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="mn" id="MathJax-Span-6943" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.293em, 1000.63em, 4.167em, -1000.01em); top: -5.139em; left: 0.414em;"><span class="mi" id="MathJax-Span-6944" style="font-size: 70.7%; font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-6945" style="padding-left: 0.157em;"><span class="mn" id="MathJax-Span-6946" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-6947" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mo" id="MathJax-Span-6948" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mrow" id="MathJax-Span-6949"><span class="mo" id="MathJax-Span-6950" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="msup" id="MathJax-Span-6951"><span style="display: inline-block; position: relative; width: 1.339em; height: 0px;"><span style="position: absolute; clip: rect(3.447em, 1000.88em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6952"><span class="msub" id="MathJax-Span-6953"><span style="display: inline-block; position: relative; width: 0.877em; height: 0px;"><span style="position: absolute; clip: rect(3.447em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6954" style="font-family: MathJax_Math-italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mi" id="MathJax-Span-6955" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 0.877em;"><span class="mn" id="MathJax-Span-6956" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6957" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span><span class="mo" id="MathJax-Span-6958" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="msup" id="MathJax-Span-6959" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.339em; height: 0px;"><span style="position: absolute; clip: rect(3.447em, 1000.88em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6960"><span class="msub" id="MathJax-Span-6961"><span style="display: inline-block; position: relative; width: 0.877em; height: 0px;"><span style="position: absolute; clip: rect(3.447em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6962" style="font-family: MathJax_Math-italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mi" id="MathJax-Span-6963" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 0.877em;"><span class="mn" id="MathJax-Span-6964" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6965" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="msup" id="MathJax-Span-6966" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.339em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.94em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-6967"><span class="msub" id="MathJax-Span-6968"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6969" style="font-family: MathJax_Math-italic;">μ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mi" id="MathJax-Span-6970" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 0.928em;"><span class="mn" id="MathJax-Span-6971" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.213em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.268em; border-left: 0px solid; width: 0px; height: 3.127em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="script upper L equals minus one-half sigma-summation Underscript i equals 1 Overscript upper K Endscripts 1 plus log left-parenthesis sigma Subscript i Baseline Superscript 2 Baseline right-parenthesis minus sigma Subscript i Baseline Superscript 2 Baseline minus mu Subscript i Baseline Superscript 2" display="block"><mrow><mi>ℒ</mi><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mn>1</mn><mo>+</mo><mo form="prefix">log</mo><mrow><mo>(</mo><msup><mrow><msub><mi>σ</mi><mi>i</mi></msub></mrow><mn>2</mn></msup><mo>)</mo></mrow><mo>-</mo><msup><mrow><msub><mi>σ</mi><mi>i</mi></msub></mrow><mn>2</mn></msup><mo>-</mo><msup><mrow><msub><mi>μ</mi><mi>i</mi></msub></mrow><mn>2</mn></msup></mrow></mrow></math></span></span></div><script type="math/mml" id="MathJax-Element-157"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="script upper L equals minus one-half sigma-summation Underscript i equals 1 Overscript upper K Endscripts 1 plus log left-parenthesis sigma Subscript i Baseline Superscript 2 Baseline right-parenthesis minus sigma Subscript i Baseline Superscript 2 Baseline minus mu Subscript i Baseline Superscript 2" display="block">
  <mrow>
    <mi>ℒ</mi>
    <mo>=</mo>
    <mo>-</mo>
    <mfrac><mn>1</mn> <mn>2</mn></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>K</mi> </munderover>
    <mrow>
      <mn>1</mn>
      <mo>+</mo>
      <mo form="prefix">log</mo>
      <mrow>
        <mo>(</mo>
        <msup><mrow><msub><mi>σ</mi> <mi>i</mi> </msub></mrow> <mn>2</mn> </msup>
        <mo>)</mo>
      </mrow>
      <mo>-</mo>
      <msup><mrow><msub><mi>σ</mi> <mi>i</mi> </msub></mrow> <mn>2</mn> </msup>
      <mo>-</mo>
      <msup><mrow><msub><mi>μ</mi> <mi>i</mi> </msub></mrow> <mn>2</mn> </msup>
    </mrow>
  </mrow>
</math></script>
</div>

<p><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-158-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;script upper L&quot;&gt;&lt;mi&gt;&amp;#x2112;&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-6972" aria-label="script upper L" style="width: 1.082em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.288em, 1001.04em, 2.316em, -1000.01em); top: -2.157em; left: 0em;"><span class="mrow" id="MathJax-Span-6973"><span class="mi" id="MathJax-Span-6974" style="font-family: MathJax_Script;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.157em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.162em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.05em; border-left: 0px solid; width: 0px; height: 0.85em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="script upper L"><mi>ℒ</mi></math></span></span><script type="math/mml" id="MathJax-Element-158"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="script upper L">
  <mi>ℒ</mi>
</math></script> is the latent loss, <em>n</em> is the codings’ dimensionality, and <em>μ</em><sub>i</sub> and <em>σ</em><sub>i</sub> are the mean and standard deviation of the <em>i</em><sup>th</sup> component of the codings. The vectors <strong>μ</strong> and <strong>σ</strong> (which contain all the <em>μ</em><sub>i</sub> and <em>σ</em><sub>i</sub>) are output by the encoder, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#variational_autoencoders_diagram">Figure&nbsp;17-12</a> (left).</p>

<p>A common tweak to the variational autoencoder’s architecture is to make the encoder output <strong>γ</strong> = log(<strong>σ</strong><sup>2</sup>) rather than <strong>σ</strong>. The latent loss can then be computed as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#var_ae_latent_loss_equation_2">Equation 17-4</a>. This approach is more numerically stable and speeds up training.</p>
<div id="var_ae_latent_loss_equation_2" data-type="equation">
<h5><span class="label">Equation 17-4. </span>Variational autoencoder’s latent loss, rewritten using <strong>γ</strong> = log(<strong>σ</strong><sup>2</sup>)</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-159-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;script upper L equals minus one-half sigma-summation Underscript i equals 1 Overscript upper K Endscripts 1 plus gamma Subscript i Baseline minus exp left-parenthesis gamma Subscript i Baseline right-parenthesis minus mu Subscript i Baseline Superscript 2&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mi&gt;&amp;#x2112;&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mfrac&gt;&lt;munderover&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B3;&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mo form=&quot;prefix&quot;&gt;exp&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B3;&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3BC;&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-6975" aria-label="script upper L equals minus one-half sigma-summation Underscript i equals 1 Overscript upper K Endscripts 1 plus gamma Subscript i Baseline minus exp left-parenthesis gamma Subscript i Baseline right-parenthesis minus mu Subscript i Baseline Superscript 2" style="width: 15.89em; display: inline-block;"><span style="display: inline-block; position: relative; width: 15.427em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(0.26em, 1015.43em, 3.499em, -1000.01em); top: -2.157em; left: 0em;"><span class="mrow" id="MathJax-Span-6976"><span class="mrow" id="MathJax-Span-6977"><span class="mi" id="MathJax-Span-6978" style="font-family: MathJax_Script;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.157em;"></span></span><span class="mo" id="MathJax-Span-6979" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mo" id="MathJax-Span-6980" style="font-family: MathJax_Main; padding-left: 0.26em;">−</span><span class="mfrac" id="MathJax-Span-6981"><span style="display: inline-block; position: relative; width: 0.62em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.19em, 1000.42em, 4.167em, -1000.01em); top: -4.676em; left: 50%; margin-left: -0.255em;"><span class="mn" id="MathJax-Span-6982" style="font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1000.47em, 4.167em, -1000.01em); top: -3.339em; left: 50%; margin-left: -0.255em;"><span class="mn" id="MathJax-Span-6983" style="font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1000.63em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.62em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span><span class="munderover" id="MathJax-Span-6984" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1001.4em, 4.63em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-6985" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.09em, 4.27em, -1000.01em); top: -2.928em; left: 0.157em;"><span class="mrow" id="MathJax-Span-6986"><span class="mi" id="MathJax-Span-6987" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-6988" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="mn" id="MathJax-Span-6989" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.293em, 1000.63em, 4.167em, -1000.01em); top: -5.139em; left: 0.414em;"><span class="mi" id="MathJax-Span-6990" style="font-size: 70.7%; font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-6991" style="padding-left: 0.157em;"><span class="mn" id="MathJax-Span-6992" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-6993" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msub" id="MathJax-Span-6994" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 0.825em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.52em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-6995" style="font-family: MathJax_Math-italic;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.517em;"><span class="mi" id="MathJax-Span-6996" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-6997" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mo" id="MathJax-Span-6998" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">exp</span><span class="mrow" id="MathJax-Span-6999"><span class="mo" id="MathJax-Span-7000" style="font-family: MathJax_Main;">(</span><span class="msub" id="MathJax-Span-7001"><span style="display: inline-block; position: relative; width: 0.825em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.52em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7002" style="font-family: MathJax_Math-italic;">γ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.517em;"><span class="mi" id="MathJax-Span-7003" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-7004" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-7005" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="msup" id="MathJax-Span-7006" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.339em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.94em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-7007"><span class="msub" id="MathJax-Span-7008"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7009" style="font-family: MathJax_Math-italic;">μ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mi" id="MathJax-Span-7010" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 0.928em;"><span class="mn" id="MathJax-Span-7011" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.162em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.268em; border-left: 0px solid; width: 0px; height: 3.127em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="script upper L equals minus one-half sigma-summation Underscript i equals 1 Overscript upper K Endscripts 1 plus gamma Subscript i Baseline minus exp left-parenthesis gamma Subscript i Baseline right-parenthesis minus mu Subscript i Baseline Superscript 2" display="block"><mrow><mi>ℒ</mi><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mrow><mn>1</mn><mo>+</mo><msub><mi>γ</mi><mi>i</mi></msub><mo>-</mo><mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>γ</mi><mi>i</mi></msub><mo>)</mo></mrow><mo>-</mo><msup><mrow><msub><mi>μ</mi><mi>i</mi></msub></mrow><mn>2</mn></msup></mrow></mrow></math></span></span></div><script type="math/mml" id="MathJax-Element-159"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="script upper L equals minus one-half sigma-summation Underscript i equals 1 Overscript upper K Endscripts 1 plus gamma Subscript i Baseline minus exp left-parenthesis gamma Subscript i Baseline right-parenthesis minus mu Subscript i Baseline Superscript 2" display="block">
  <mrow>
    <mi>ℒ</mi>
    <mo>=</mo>
    <mo>-</mo>
    <mfrac><mn>1</mn> <mn>2</mn></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>K</mi> </munderover>
    <mrow>
      <mn>1</mn>
      <mo>+</mo>
      <msub><mi>γ</mi> <mi>i</mi> </msub>
      <mo>-</mo>
      <mo form="prefix">exp</mo>
      <mrow>
        <mo>(</mo>
        <msub><mi>γ</mi> <mi>i</mi> </msub>
        <mo>)</mo>
      </mrow>
      <mo>-</mo>
      <msup><mrow><msub><mi>μ</mi> <mi>i</mi> </msub></mrow> <mn>2</mn> </msup>
    </mrow>
  </mrow>
</math></script>
</div>

<p>OK, so let’s start building a variational autoencoder for Fashion MNIST (as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#variational_autoencoders_diagram">Figure&nbsp;17-12</a>, but using the <strong>γ</strong> tweak). First, we will need a custom layer to sample the codings, given <strong>μ</strong> and <strong>γ</strong>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">Sampling</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Layer</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf">call</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">):</code>
        <code class="n">mean</code><code class="p">,</code> <code class="n">log_var</code> <code class="o">=</code> <code class="n">inputs</code>
        <code class="k">return</code> <code class="n">K</code><code class="o">.</code><code class="n">random_normal</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">shape</code><code class="p">(</code><code class="n">log_var</code><code class="p">))</code> <code class="o">*</code> <code class="n">K</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">log_var</code> <code class="o">/</code> <code class="mi">2</code><code class="p">)</code> <code class="o">+</code> <code class="n">mean</code></pre>

<p>This <code>Sampling</code> layer takes two inputs: <code>mean</code> (<strong>μ</strong>) and <code>log_var</code> (<strong>γ</strong>). It uses <code>K.random_normal()</code> to sample a random vector (of the same shape as <strong>γ</strong>) from the Normal distribution, with mean 0 and standard deviation 1. Then it multiplies it by exp(<strong>γ</strong>&nbsp;/&nbsp;2) (which is equal to <strong>σ</strong>, as you can verify), and finally it adds <strong>μ</strong> and returns the result. This samples a codings vector from the Normal distribution with mean <strong>μ</strong> and standard deviation <strong>σ</strong>.</p>

<p>Next, we can create the encoder, using the Functional API because the model is not entirely sequential:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">codings_size</code> <code class="o">=</code> <code class="mi">10</code>

<code class="n">inputs</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Input</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">])</code>
<code class="n">z</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">()(</code><code class="n">inputs</code><code class="p">)</code>
<code class="n">z</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">150</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">)(</code><code class="n">z</code><code class="p">)</code>
<code class="n">z</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">)(</code><code class="n">z</code><code class="p">)</code>
<code class="n">codings_mean</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="n">codings_size</code><code class="p">)(</code><code class="n">z</code><code class="p">)</code>  <code class="c1"># μ</code>
<code class="n">codings_log_var</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="n">codings_size</code><code class="p">)(</code><code class="n">z</code><code class="p">)</code>  <code class="c1"># γ</code>
<code class="n">codings</code> <code class="o">=</code> <code class="n">Sampling</code><code class="p">()([</code><code class="n">codings_mean</code><code class="p">,</code> <code class="n">codings_log_var</code><code class="p">])</code>
<code class="n">variational_encoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">Model</code><code class="p">(</code>
    <code class="n">inputs</code><code class="o">=</code><code class="p">[</code><code class="n">inputs</code><code class="p">],</code> <code class="n">outputs</code><code class="o">=</code><code class="p">[</code><code class="n">codings_mean</code><code class="p">,</code> <code class="n">codings_log_var</code><code class="p">,</code> <code class="n">codings</code><code class="p">])</code></pre>

<p>Note that the <code>Dense</code> layers that output <code>codings_mean</code> (<strong>μ</strong>) and <code>codings_log_var</code> (<strong>γ</strong>) have the same inputs (i.e., the outputs of the second <code>Dense</code> layer). We then pass both <code>codings_mean</code> and <code>codings_log_var</code> to the <code>Sampling</code> layer. Finally, the <code>variational_encoder</code> model has three outputs, in case you want to inspect the value of the <code>codings_mean</code> and <code>codings_log_var</code>. The only output we will use is the last one (<code>codings</code>). Now let’s build the decoder:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">decoder_inputs</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Input</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="n">codings_size</code><code class="p">])</code>
<code class="n">x</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">)(</code><code class="n">decoder_inputs</code><code class="p">)</code>
<code class="n">x</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">150</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
<code class="n">x</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
<code class="n">outputs</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Reshape</code><code class="p">([</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">])(</code><code class="n">x</code><code class="p">)</code>
<code class="n">variational_decoder</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">Model</code><code class="p">(</code><code class="n">inputs</code><code class="o">=</code><code class="p">[</code><code class="n">decoder_inputs</code><code class="p">],</code> <code class="n">outputs</code><code class="o">=</code><code class="p">[</code><code class="n">outputs</code><code class="p">])</code></pre>

<p>For this decoder, we could have used the Sequential API instead of the Functional API, since it is really just a simple stack of layers, virtually identical to many of the decoders we have built so far. Finally, let’s build the variational autoencoder model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">_</code><code class="p">,</code> <code class="n">_</code><code class="p">,</code> <code class="n">codings</code> <code class="o">=</code> <code class="n">variational_encoder</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code>
<code class="n">reconstructions</code> <code class="o">=</code> <code class="n">variational_decoder</code><code class="p">(</code><code class="n">codings</code><code class="p">)</code>
<code class="n">variational_ae</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">Model</code><code class="p">(</code><code class="n">inputs</code><code class="o">=</code><code class="p">[</code><code class="n">inputs</code><code class="p">],</code> <code class="n">outputs</code><code class="o">=</code><code class="p">[</code><code class="n">reconstructions</code><code class="p">])</code></pre>

<p>Note that we ignore the first two outputs of the encoder (we only want to feed the codings to the decoder). Lastly, we must add the latent loss and the reconstruction loss:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">latent_loss</code> <code class="o">=</code> <code class="o">-</code><code class="mf">0.5</code> <code class="o">*</code> <code class="n">K</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code>
    <code class="mi">1</code> <code class="o">+</code> <code class="n">codings_log_var</code> <code class="o">-</code> <code class="n">K</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">codings_log_var</code><code class="p">)</code> <code class="o">-</code> <code class="n">K</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="n">codings_mean</code><code class="p">),</code>
    <code class="n">axis</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>
<code class="n">variational_ae</code><code class="o">.</code><code class="n">add_loss</code><code class="p">(</code><code class="n">K</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">latent_loss</code><code class="p">)</code> <code class="o">/</code> <code class="mf">784.</code><code class="p">)</code>
<code class="n">variational_ae</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"binary_crossentropy"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s2">"rmsprop"</code><code class="p">)</code></pre>

<p>We first apply <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#var_ae_latent_loss_equation_2">Equation 17-4</a> to compute the latent loss for each instance in the batch (we sum over the last axis).  Then we compute the mean loss over all the instances in the batch, and we divide the result by 784 to ensure it has the appropriate scale compared to the reconstruction loss. Indeed, the variational autoencoder’s reconstruction loss is supposed to be the sum of the pixel reconstruction errors, but when Keras computes the loss (i.e., the binary cross-entropy), it computes the mean over all 784 pixels, rather than the sum. So the reconstruction loss is 784 times smaller than we need it to be. We could define a custom loss to compute the sum rather than the mean, but it is simpler to divide the latent loss by 784 (the final loss will be 784 times smaller than it should be, but this just means that we should use a larger learning rate).</p>

<p>Note that we use the RMSprop optimizer, which works well in this case. And finally we can train the autoencoder!</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">history</code> <code class="o">=</code> <code class="n">variational_ae</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">128</code><code class="p">,</code>
                             <code class="n">validation_data</code><code class="o">=</code><code class="p">[</code><code class="n">X_valid</code><code class="p">,</code> <code class="n">X_valid</code><code class="p">])</code></pre>








<section data-type="sect2" data-pdf-bookmark="Generating Fashion MNIST Images"><div class="sect2" id="idm46263488114728">
<h2>Generating Fashion MNIST Images</h2>

<p>Now let’s use this variational autoencoder to generate images that look like fashion items. All we need to do is sample random codings from a Gaussian distribution and decode them:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">codings</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">normal</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="mi">12</code><code class="p">,</code> <code class="n">codings_size</code><code class="p">])</code>
<code class="n">images</code> <code class="o">=</code> <code class="n">variational_decoder</code><code class="p">(</code><code class="n">codings</code><code class="p">)</code><code class="o">.</code><code class="n">numpy</code><code class="p">()</code></pre>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#vae_generated_images_plot">Figure&nbsp;17-13</a> shows the 12 generated images.</p>

<figure><div id="vae_generated_images_plot" class="figure">
<img src="./Chapter17_files/mls2_1713.png" alt="mls2 1713" width="928" height="679" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1713.png">
<h6><span class="label">Figure 17-13. </span>Fashion MNIST images generated by the variational autoencoder</h6>
</div></figure>

<p>The majority of these images look fairly convincing, if a bit too fuzzy. The rest are not great, but don’t be too harsh on the autoencoder—it only had a few minutes to learn! Give it a bit more fine tuning and training time, and those images should look better.</p>

<p>Variational autoencoders make it possible to perform <em>semantic interpolation</em>: instead of interpolating two images at the pixel level (which would look as if the two images were overlaid), we can interpolate at the codings level. We first run both images through the encoder, then we interpolate the two codings we get, and finally we decode the interpolated codings to get the final image. It will look like a regular Fashion MNIST image, but it will be an intermediate between the original images. In the following code example, we take the 12 codings we just generated, we organize them in a 3 × 4 grid, and we use TensorFlow’s <code>tf.image.resize()</code> function to resize this grid to 5 × 7. By default, the <code>resize()</code> function will perform bilinear interpolation, so every other row and column will contain interpolated codings. We then use the decoder to produce all the images:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">codings_grid</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">codings</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="n">codings_size</code><code class="p">])</code>
<code class="n">larger_grid</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">image</code><code class="o">.</code><code class="n">resize</code><code class="p">(</code><code class="n">codings_grid</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="p">[</code><code class="mi">5</code><code class="p">,</code> <code class="mi">7</code><code class="p">])</code>
<code class="n">interpolated_codings</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">larger_grid</code><code class="p">,</code> <code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="n">codings_size</code><code class="p">])</code>
<code class="n">images</code> <code class="o">=</code> <code class="n">variational_decoder</code><code class="p">(</code><code class="n">interpolated_codings</code><code class="p">)</code><code class="o">.</code><code class="n">numpy</code><code class="p">()</code></pre>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#semantic_interpolation_plot">Figure&nbsp;17-14</a> shows the resulting images. The original images are framed, and the rest are the result of semantic interpolation between the nearby images. Notice, for example, how the shoe in the fourth row and fifth column is a nice interpolation between the two shoes located above and below it.</p>

<figure><div id="semantic_interpolation_plot" class="figure">
<img src="./Chapter17_files/mls2_1714.png" alt="mls2 1714" width="1442" height="1007" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1714.png">
<h6><span class="label">Figure 17-14. </span>Semantic interpolation</h6>
</div></figure>

<p>For several years, variational autoencoders were quite popular, but GANs eventually took the lead, in particular because they are capable of generating much more realistic and crisp images. So let’s turn our attention to GANs.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Generative Adversarial Networks"><div class="sect1" id="idm46263488745576">
<h1>Generative Adversarial Networks</h1>

<p>Generative Adversarial Networks were proposed in a <a href="https://homl.info/gan">2014 paper</a><sup><a data-type="noteref" id="idm46263487897112-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487897112">10</a></sup> by Ian Goodfellow et al., and although the idea got researchers excited almost instantly, it took a few years to overcome some of the difficulties  of training GANs. Like many great ideas, it seems simple in hindsight: make neural networks compete against each other in the hope that this competition will push them to excel. As shown on <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#gan_diagram">Figure&nbsp;17-15</a>, a GAN is composed of two neural networks:</p>
<dl>
<dt>generator</dt>
<dd>
<p>Takes a random distribution as input (typically Gaussian) and outputs some data — typically, an image. You can think of the random inputs as the latent representations (i.e., codings) of the image to be generated. So, as you can see, the generator offers the same functionality as a decoder in a variational autoencoder, and it can be used in the same way to generate new images (just feed it some Gaussian noise, and it outputs a brand-new image). However, it is trained very differently, as we will soon see.</p>
</dd>
<dt>discriminator</dt>
<dd>
<p>Takes either a fake image from the generator as input, or a real image from the training set, and it must guess whether the input image is fake or real.</p>
</dd>
</dl>

<figure><div id="gan_diagram" class="figure">
<img src="./Chapter17_files/mls2_1715.png" alt="mls2 1715" width="1442" height="937" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1715.png">
<h6><span class="label">Figure 17-15. </span>A Generative Adversarial Network</h6>
</div></figure>

<p>During training, the generator and the discriminator have opposite goals: the discriminator tries to tell fake images from real images, while the generator tries to produce images that look real enough to trick the discriminator. Because the GAN is composed of two networks with different objectives, it cannot be trained like a regular neural network. Each training iteration is divided into two phases:</p>

<ul>
<li>
<p>In the first phase, we train the discriminator. A batch of real images is sampled from the training set and is completed with an equal number of fake images produced by the generator. The labels are set to 0 for fake images and 1 for real images, and the discriminator is trained on this labeled batch for one step, using the binary cross-entropy loss. Importantly, backpropagation only optimizes the weights of the discriminator during this phase.</p>
</li>
<li>
<p>In the second phase, we train the generator. We first use it to produce another batch of fake images, and once again the discriminator is used to tell whether the images are fake or real. This time we do not add real images in the batch, and all the labels are set to 1 (real): in other words, we want the generator to produce images that the discriminator will (wrongly) believe to be real! Crucially, the weights of the discriminator are frozen during this step, so backpropagation only affects the weights of the generator.</p>
</li>
</ul>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The generator never actually sees any real image, yet it gradually learns to produce convincing fake images! All it gets is the gradients flowing back through the discriminator. Fortunately, the better the discriminator gets, the more information about the real images is contained in these secondhand gradients, so the generator can make significant progress.</p>
</div>

<p>Let’s go ahead and build a simple GAN for Fashion MNIST. First, we need to build the generator and the discriminator. The generator is similar to an autoencoder’s decoder, and the discriminator is a regular binary classifier (it takes an image as input and ends with a dense layer containing a single unit and using the sigmoid activation function). For the second phase of each training iteration, we also need the full GAN model containing the generator followed by the discriminator:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">codings_size</code> <code class="o">=</code> <code class="mi">30</code>

<code class="n">generator</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="n">codings_size</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">150</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">28</code> <code class="o">*</code> <code class="mi">28</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Reshape</code><code class="p">([</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">])</code>
<code class="p">])</code>
<code class="n">discriminator</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(</code><code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">150</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">)</code>
<code class="p">])</code>
<code class="n">gan</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="n">generator</code><code class="p">,</code> <code class="n">discriminator</code><code class="p">])</code></pre>

<p>Next, we need to compile these models. As the discriminator is a binary classifier, we can naturally use the binary cross-entropy loss. The generator will only be trained through the <code>gan</code> model, so we do not need to compile it at all. The <code>gan</code> model is also a binary classifier, so it can also use the binary cross-entropy loss. Importantly, the discriminator should not be trained during the second phase, so we make it non-trainable before compiling the <code>gan</code> model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">discriminator</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"binary_crossentropy"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s2">"rmsprop"</code><code class="p">)</code>
<code class="n">discriminator</code><code class="o">.</code><code class="n">trainable</code> <code class="o">=</code> <code class="bp">False</code>
<code class="n">gan</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"binary_crossentropy"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s2">"rmsprop"</code><code class="p">)</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The <code>trainable</code> attribute is taken into account by Keras only when compiling a model, so after running this code, the <code>discriminator</code> <em>is</em> trainable if we call its <code>fit()</code> method or its <code>train_on_batch()</code> method (as we will be using), while it is <em>not</em> trainable when we call these methods on the <code>gan</code> model.</p>
</div>

<p>Since the training loop is unusual, we cannot use the regular <code>fit()</code> method. Instead, we will write a custom training loop. For this, we first need to create a <code>Dataset</code> to iterate through the images:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">batch_size</code> <code class="o">=</code> <code class="mi">32</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">Dataset</code><code class="o">.</code><code class="n">from_tensor_slices</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code><code class="o">.</code><code class="n">shuffle</code><code class="p">(</code><code class="mi">1000</code><code class="p">)</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">batch</code><code class="p">(</code><code class="n">batch_size</code><code class="p">,</code> <code class="n">drop_remainder</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code><code class="o">.</code><code class="n">prefetch</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code></pre>

<p>We are now ready to write the training loop. Let’s wrap it in a <code>train_gan()</code> function:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">train_gan</code><code class="p">(</code><code class="n">gan</code><code class="p">,</code> <code class="n">dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">,</code> <code class="n">codings_size</code><code class="p">,</code> <code class="n">n_epochs</code><code class="o">=</code><code class="mi">50</code><code class="p">):</code>
    <code class="n">generator</code><code class="p">,</code> <code class="n">discriminator</code> <code class="o">=</code> <code class="n">gan</code><code class="o">.</code><code class="n">layers</code>
    <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_epochs</code><code class="p">):</code>
        <code class="k">for</code> <code class="n">X_batch</code> <code class="ow">in</code> <code class="n">dataset</code><code class="p">:</code>
            <code class="c1"># phase 1 - training the discriminator</code>
            <code class="n">noise</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">normal</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="n">batch_size</code><code class="p">,</code> <code class="n">codings_size</code><code class="p">])</code>
            <code class="n">generated_images</code> <code class="o">=</code> <code class="n">generator</code><code class="p">(</code><code class="n">noise</code><code class="p">)</code>
            <code class="n">X_fake_and_real</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">generated_images</code><code class="p">,</code> <code class="n">X_batch</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
            <code class="n">y1</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">constant</code><code class="p">([[</code><code class="mf">0.</code><code class="p">]]</code> <code class="o">*</code> <code class="n">batch_size</code> <code class="o">+</code> <code class="p">[[</code><code class="mf">1.</code><code class="p">]]</code> <code class="o">*</code> <code class="n">batch_size</code><code class="p">)</code>
            <code class="n">discriminator</code><code class="o">.</code><code class="n">trainable</code> <code class="o">=</code> <code class="bp">True</code>
            <code class="n">discriminator</code><code class="o">.</code><code class="n">train_on_batch</code><code class="p">(</code><code class="n">X_fake_and_real</code><code class="p">,</code> <code class="n">y1</code><code class="p">)</code>
            <code class="c1"># phase 2 - training the generator</code>
            <code class="n">noise</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">normal</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="n">batch_size</code><code class="p">,</code> <code class="n">codings_size</code><code class="p">])</code>
            <code class="n">y2</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">constant</code><code class="p">([[</code><code class="mf">1.</code><code class="p">]]</code> <code class="o">*</code> <code class="n">batch_size</code><code class="p">)</code>
            <code class="n">discriminator</code><code class="o">.</code><code class="n">trainable</code> <code class="o">=</code> <code class="bp">False</code>
            <code class="n">gan</code><code class="o">.</code><code class="n">train_on_batch</code><code class="p">(</code><code class="n">noise</code><code class="p">,</code> <code class="n">y2</code><code class="p">)</code>

<code class="n">train_gan</code><code class="p">(</code><code class="n">gan</code><code class="p">,</code> <code class="n">dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">,</code> <code class="n">codings_size</code><code class="p">)</code></pre>

<p>As discussed earlier, you can see the two phases at each iteration:</p>

<ul>
<li>
<p>In phase one we feed Gaussian noise to the generator to produce fake images, and we complete this batch by concatenating an equal number of real images. The targets <code>y1</code> are set to 0 for fake images and 1 for real images. Then we train the discriminator on this batch. Note that we set the discriminator’s <code>trainable</code> attribute to <code>True</code>: this is only to get rid of a warning that Keras displays when it notices that <code>trainable</code> is now <code>False</code> but was <code>True</code> when the model was compiled (or vice versa).</p>
</li>
<li>
<p>In phase two, we feed the GAN some Gaussian noise. Its generator will start by producing fake images, then the discriminator will try to guess whether these images are fake or real. We want the discriminator to believe that the fake images are real, so the targets <code>y2</code> are set to 1. Note that we set the <code>trainable</code> attribute to <code>False</code>, once again to avoid a warning.</p>
</li>
</ul>

<p>That’s it! If you display the generated images (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#gan_generated_images_plot">Figure&nbsp;17-16</a>), you will see that at the end of the first epoch, they already start to look like (very noisy) Fashion MNIST images.</p>

<figure><div id="gan_generated_images_plot" class="figure">
<img src="./Chapter17_files/mls2_1716.png" alt="mls2 1716" width="1440" height="710" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1716.png">
<h6><span class="label">Figure 17-16. </span>Images generated by the GAN after one epoch of training</h6>
</div></figure>

<p>Unfortunately, the images never really get much better than that, and you may even find epochs where the GAN seems to be forgetting what it learned. Why is that? Well, it turns out that training a GAN can be challenging. Let’s see why.</p>








<section data-type="sect2" data-pdf-bookmark="The Difficulties of Training GANs"><div class="sect2" id="idm46263487347720">
<h2>The Difficulties of Training GANs</h2>

<p>During training, the generator and the discriminator constantly try to outsmart each other, in a zero-sum game. As training advances, the game may end up in a state that game theorists call a <em>Nash equilibrium</em>, named after the mathematician John Nash: this is when no player would be better off changing their own strategy (assuming the other players do not change theirs). For example, a Nash equilibrium is reached when everyone drives on the left side of the road: indeed, no driver would be better off being the only one to switch sides. Of course, there is a second possible Nash equilibrium: when everyone drives on the <em>right</em> side of the road. Different initial states and dynamics may lead to one equilibrium or the other. In this example, there is a single optimal strategy once an equilibrium is reached (i.e., driving on the same side as everyone else), but a Nash equilibrium can involve multiple competing strategies (e.g., a predator chases its prey, the prey tries to escape, and neither would be better off changing their strategy).</p>

<p>So how does this apply to GANs? Well, the authors of the paper demonstrated that a GAN can only reach a single Nash equilibrium: that’s when the generator produces perfectly realistic images, and the discriminator is forced to guess (50% real, 50% fake). This fact is very encouraging: it would seem that you just need to train the GAN for long enough, and it will eventually reach this equilibrium, giving you a perfect generator. Unfortunately, it’s not that simple: nothing guarantees that the equilibrium will ever be reached.</p>

<p>The biggest difficulty is called <em>mode collapse</em>: this is when the generator’s outputs gradually become less diverse. Here is how this can happen: suppose that the generator gets better at producing convincing shoes than any other class, so it will fool the discriminator a bit more with shoes, and this will encourage it to produce even more images of shoes. Gradually, it will forget how to produce anything else. Meanwhile, the only fake images that the discriminator will see will be shoes, so it will also forget how to discriminate fake images of other classes. Eventually, when the discriminator manages to discriminate the fake shoes from the real ones, the generator will be forced to move to another class. It may then become good at shirts, forgetting about shoes, and the discriminator will follow. The GAN may gradually cycle across a few classes, never really becoming very good at any of them.</p>

<p>Moreover, because the generator and the discriminator are constantly pushing against each other, their parameters may end up oscillating and becoming unstable. Training may begin properly, then suddenly diverge for no apparent reason, due to these instabilities. And since many factors affect these complex dynamics, GANs are very sensitive to the hyperparameters: you may have to spend a lot of effort fine-tuning them.</p>

<p>These problems have kept researchers very busy since 2014: many papers were published on this topic, some proposing new cost functions<sup><a data-type="noteref" id="idm46263487340152-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487340152">11</a></sup> (but a <a href="https://homl.info/gansequal">2018 paper</a> by Google researchers questions their efficiency) or techniques to stabilize training or to avoid the mode collapse issue. For example, a popular technique called <em>experience replay</em> consists in storing the images produced by the generator at each iteration in a replay buffer (gradually dropping older generated images) and training the discriminator using real images plus fake images drawn from this buffer (rather than just fake images produced by the current generator). This reduces the chances that the discriminator will overfit the latest generator’s outputs. Another common technique is called <em>minibatch discrimination</em>: it measures how similar images are across the batch and provides this statistic to the discriminator, so it can easily reject a whole batch of fake images that lack diversity. This encourages the generator to produce a greater variety of images, reducing the chance of mode collapse. Other papers simply propose specific architectures that happen to perform well.</p>

<p>In short, this is still a very active field of research, and the dynamics of GANs are still not perfectly understood. But the good news is that great progress has been made, and some of the results are truly astounding! So let’s look at some of the most successful architectures, starting with DCGANs, which were the state of the art just a few years ago. Then we will look at two more recent (and more complex) architectures.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Deep Convolutional GANs"><div class="sect2" id="idm46263487334648">
<h2>Deep Convolutional GANs</h2>

<p>The original GAN paper in 2014 experimented with convolutional layers, but only tried to generated small images. Soon after, many researchers tried to build GANs based on deeper convolutional nets for larger images. This proved to be tricky, as training was very unstable, but Alec Radford, Luke Metz, and Soumith Chintala finally succeeded late 2015, after experimenting with many different architectures and hyperparameters. They called their architecture <a href="https://homl.info/dcgan"><em>Deep Convolutional GANs</em></a> (DCGANs).<sup><a data-type="noteref" id="idm46263487331720-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487331720">12</a></sup> Here are the main guidelines they proposed for building a stable convolutional GANs:</p>

<ul>
<li>
<p>Replace any pooling layers with strided convolutions (in the discriminator) and transposed convolutions (in the generator).</p>
</li>
<li>
<p>Use BatchNorm in both the generator and the discriminator, except on the generator’s output layer and the discriminator’s input layer.</p>
</li>
<li>
<p>Remove fully connected hidden layers for deeper architectures.</p>
</li>
<li>
<p>Use ReLU activation in the generator for all layers except for the output, which should use Tanh.</p>
</li>
<li>
<p>Use LeakyReLU activation in the discriminator for all layers.</p>
</li>
</ul>

<p>These guidelines will work in many cases, but not always, so you may still need to experiment with different hyperparameters (in fact, just changing the random seed and training the same model again will sometimes work). For example, here is a small DCGAN that works reasonably well with Fashion MNIST:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">codings_size</code> <code class="o">=</code> <code class="mi">100</code>

<code class="n">generator</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">7</code> <code class="o">*</code> <code class="mi">7</code> <code class="o">*</code> <code class="mi">128</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="n">codings_size</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Reshape</code><code class="p">([</code><code class="mi">7</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="mi">128</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">BatchNormalization</code><code class="p">(),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2DTranspose</code><code class="p">(</code><code class="mi">64</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,</code>
                                 <code class="n">activation</code><code class="o">=</code><code class="s2">"selu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">BatchNormalization</code><code class="p">(),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2DTranspose</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,</code>
                                 <code class="n">activation</code><code class="o">=</code><code class="s2">"tanh"</code><code class="p">)</code>
<code class="p">])</code>
<code class="n">discriminator</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2D</code><code class="p">(</code><code class="mi">64</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,</code>
                        <code class="n">activation</code><code class="o">=</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">LeakyReLU</code><code class="p">(</code><code class="mf">0.2</code><code class="p">),</code>
                        <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">1</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.4</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Conv2D</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"same"</code><code class="p">,</code>
                        <code class="n">activation</code><code class="o">=</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">LeakyReLU</code><code class="p">(</code><code class="mf">0.2</code><code class="p">)),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.4</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"sigmoid"</code><code class="p">)</code>
<code class="p">])</code>
<code class="n">gan</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="n">generator</code><code class="p">,</code> <code class="n">discriminator</code><code class="p">])</code></pre>

<p>The generator takes codings of size 100, and it projects them to 7 * 7 * 128 dimensions, and reshapes the result to get a 7 × 7 × 128 tensor. This tensor is batch normalized and fed to a transposed convolutional layer with a stride of 2, which upsamples it from 7 × 7 to 14 × 14 and reduces its depth from 128 to 64. The result is batch normalized again and fed to another transposed convolutional layer with a stride of 2, which upsamples it from 14 × 14 to 28 × 28 and reduces the depth from 64 to 1. This layer uses the tanh activation function, so the outputs will range from -1 to 1. For this reason, before training the GAN, we need to rescale the training set to that same range. We also need to reshape it to add the channel dimension:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">X_train</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code> <code class="o">*</code> <code class="mf">2.</code> <code class="o">-</code> <code class="mf">1.</code> <code class="c1"># reshape and rescale</code></pre>

<p>The discriminator looks much like a regular CNN for binary classification, except instead of using max pooling layers to downsample the image, we use strided convolutions (<code>strides=2</code>). Also note that we use the leaky ReLU as the activation function.</p>

<p>Overall, we respected the DCGAN guidelines, except we replaced the BatchNorm layers in the discriminator with dropout layers (otherwise training was unstable in this case), and we also replaced ReLU with SELU in the generator. Feel free to tweak this architecture: you will see how sensitive it is to the hyperparameters (especially the relative learning rate of the two networks).</p>

<p>Lastly, to build the dataset, then compile and train this model, we use the exact same code as earlier. After 50 epochs of training, the generator produces images like those shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#dcgan_generated_images_plot">Figure&nbsp;17-17</a>. It’s still not perfect, but many of these images are pretty convincing.</p>

<figure><div id="dcgan_generated_images_plot" class="figure">
<img src="./Chapter17_files/mls2_1717.png" alt="mls2 1717" width="1441" height="715" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1717.png">
<h6><span class="label">Figure 17-17. </span>Images generated by the DCGAN after 50 epochs of training</h6>
</div></figure>

<p>If you scale up this architecture and train it on a large dataset of faces, you can get fairly realistic images. In fact, DCGANs can learn quite meaningful latent representations, as you can see in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#faces_arithmetics_diagram">Figure&nbsp;17-18</a>: many images were generated, and nine of them were picked manually (top left), including three representing men with glasses, three men without glasses and three women without glasses. For each of these categories, the codings that were used to generate the images were averaged, and an image was generated based on the resulting mean codings (lower left). In short, each of the three lower-left images represents the mean of the three images located above it. But this is not a simple mean computed at the pixel level (this would result in three overlapping faces), it is a mean computed in the latent space, so the images still look like normal faces. Amazingly, if you compute men with glasses, minus men without glasses, plus women without glasses—where each term corresponds to one of the mean codings), and you generate the image that corresponds to this coding—you get the image at the center of the 3 × 3 grid of faces on the right: a woman with glasses! The eight other images around it were generated based on the same vector plus a bit of noise, to illustrate the semantic interpolation capabilities of DCGANs. Being able to do arithmetic on faces feels like science fiction!</p>

<figure><div id="faces_arithmetics_diagram" class="figure">
<img src="./Chapter17_files/mls2_1718.png" alt="mls2 1718" width="1439" height="741" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1718.png">
<h6><span class="label">Figure 17-18. </span>Vector arithmetic for visual concepts (part of Figure 7 from the DCGAN paper)<sup><a data-type="noteref" id="idm46263487102408-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487102408">13</a></sup></h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>If you add each image’s class as an extra input to both the generator and the discriminator, they will both learn what each class looks like, and thus you will be able to control the class of each image produced by the generator. This is called a <a href="https://homl.info/cgan"><em>Conditional GAN</em></a><sup><a data-type="noteref" id="idm46263487099560-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487099560">14</a></sup> (CGAN).</p>
</div>

<p>When you try to generate very large images using DCGANs, you often end up with locally convincing features but overall inconsistencies (such as shirts with one sleeve much longer than the other). How can we fix this?</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Progressive Growing of GANs"><div class="sect2" id="idm46263487334024">
<h2>Progressive Growing of GANs</h2>

<p>An important technique was proposed in a <a href="https://homl.info/progan">2018 paper</a><sup><a data-type="noteref" id="idm46263487096088-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487096088">15</a></sup> by Nvidia researchers Tero Karras et al.: they suggested generating small images at the beginning of training, then gradually adding convolutional layers to both the generator and the discriminator to produce larger and larger images (4 × 4, 8 × 8, 16 × 16, …, 512 × 512, 1,024 × 1,024). This approach resembles greedy layer-wise training of stacked autoencoders. The extra layers get added at the end of the generator and at the beginning of the discriminator, and previously trained layers remain trainable.</p>

<p>For example, when growing the generator’s outputs from 4 × 4 to 8 × 8 (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#progressively_growing_gan_diagram">Figure&nbsp;17-19</a>), an upsampling layer (using nearest neighbor filtering) is added to the existing convolutional layer, so it outputs 8 × 8 feature maps, which are then fed to the new convolutional layer (which uses <code>"same"</code> padding and strides of 1, so its outputs are also 8 × 8). This new layer is followed by a new output convolutional layer: this is a regular convolutional layer with kernel size 1 that projects the outputs down to the desired number of color channels (e.g., 3). To avoid breaking the trained weights of the first convolutional layer when the new convolutional layer is added, the final output is a weighted sum of the original output layer (which now outputs 8 × 8 feature maps) and the new output layer. The weight of the new outputs is <em>α</em>, while the weight of the original outputs is 1&nbsp;–&nbsp;_α<em>, and _α</em> is slowly increased from 0 to 1. In other words, the new convolutional layers (represented with dashed lines in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#progressively_growing_gan_diagram">Figure&nbsp;17-19</a>) are gradually faded in, while the original output layer is gradually faded out. A similar fade-in/fade-out technique is used when a new convolutional layer is added to the discriminator (followed by an average pooling layer for downsampling).</p>

<figure><div id="progressively_growing_gan_diagram" class="figure">
<img src="./Chapter17_files/mls2_1719.png" alt="mls2 1719" width="1420" height="1019" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1719.png">
<h6><span class="label">Figure 17-19. </span>Progressively growing GAN: a GAN generator outputs 4 × 4 color images (left); we extend it to output 8 × 8 images (right)</h6>
</div></figure>

<p>The paper also introduced several other techniques aimed at increasing the diversity of the outputs (to avoid mode collapse) and making training more stable:</p>
<dl>
<dt>Minibatch standard deviation layer</dt>
<dd>
<p>Added near the end of the discriminator. For each position in the inputs, it computes the standard deviation across all channels and all instances in the batch (<code>S = tf.math.reduce_std(inputs, axis=[0, -1])</code>). These standard deviations are then averaged across all points to get a single value (<code>v = tf.reduce_mean(S)</code>). Finally, an extra feature map is added to each instance in the batch and filled with the computed value (<code>tf.concat([inputs, tf.fill([batch_size, height, width, 1], v)], axis=-1)</code>). How does this help? Well, if the generator produces images with little variety, then there will be a small standard deviation across feature maps in the discriminator. Thanks to this layer, the discriminator will have an easy access to this statistic, making it less likely to be fooled by a generator that produces too little diversity. This will encourage the generator to produce more diverse outputs, reducing the risk of mode collapse.</p>
</dd>
<dt>Equalized learning rate</dt>
<dd>
<p>Initializes all weights using a simple Gaussian distribution with mean 0 and standard deviation 1 rather than using He initialization. However, they are scaled down at runtime (i.e., every time the layer is executed) by the same factor as in He initialization: they are divided by <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-160-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;StartRoot 2 slash n Subscript inputs Baseline EndRoot&quot;&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mtext&gt;inputs&lt;/mtext&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7012" aria-label="StartRoot 2 slash n Subscript inputs Baseline EndRoot" style="width: 4.784em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.63em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.134em, 1004.64em, 2.625em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-7013"><span class="msqrt" id="MathJax-Span-7014"><span style="display: inline-block; position: relative; width: 4.63em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1003.61em, 4.476em, -1000.01em); top: -4.008em; left: 0.979em;"><span class="mrow" id="MathJax-Span-7015"><span class="mrow" id="MathJax-Span-7016"><span class="mn" id="MathJax-Span-7017" style="font-family: MathJax_Main;">2</span><span class="mo" id="MathJax-Span-7018" style="font-family: MathJax_Main;">/</span><span class="msub" id="MathJax-Span-7019"><span style="display: inline-block; position: relative; width: 2.625em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-7020" style="font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mtext" id="MathJax-Span-7021" style="font-size: 70.7%; font-family: MathJax_Main;">inputs</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.602em, 1003.61em, 3.961em, -1000.01em); top: -4.573em; left: 0.979em;"><span style="display: inline-block; position: relative; width: 3.653em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.1em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 2.933em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.414em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.928em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 1.391em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 1.905em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 2.419em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1001.04em, 4.527em, -1000.01em); top: -4.008em; left: 0em;"><span style="font-family: MathJax_Size1;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.421em; border-left: 0px solid; width: 0px; height: 1.327em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="StartRoot 2 slash n Subscript inputs Baseline EndRoot"><msqrt><mrow><mn>2</mn><mo>/</mo><msub><mi>n</mi><mtext>inputs</mtext></msub></mrow></msqrt></math></span></span><script type="math/mml" id="MathJax-Element-160"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="StartRoot 2 slash n Subscript inputs Baseline EndRoot">
  <msqrt>
    <mrow>
      <mn>2</mn>
      <mo>/</mo>
      <msub><mi>n</mi> <mtext>inputs</mtext> </msub>
    </mrow>
  </msqrt>
</math></script>, where <em>n</em><sub>inputs</sub> is the number of inputs to the layer. The paper demonstrated that this technique significantly improved the GAN’s performance when using RMSProp, Adam, or other adaptive gradient optimizers. Indeed, these optimizers normalize the gradient updates by their estimated standard deviation (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>), so parameters that have a larger dynamic range<sup><a data-type="noteref" id="idm46263487057480-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487057480">16</a></sup> will take longer to train, while parameters with a small dynamic range may be updated too quickly, leading to instabilities. By rescaling the weights as part of the model itself rather than just rescaling them upon initialization, this approach ensures that the dynamic range is the same for all parameters, throughout training, so they all learn at the same speed. This both speeds up and stabilizes training.</p>
</dd>
<dt>Pixelwise normalization layer</dt>
<dd>
<p>Added after each convolutional layer in the generator. It normalizes each activation based on all the activations in the same image and at the same location but across all channels (dividing by the square root of the mean squared activation). In TensorFlow code, this is <code>inputs / tf.sqrt(tf.reduce_mean(tf.square(X), axis=-1, keepdims=True) + 1e-8)</code> (the smoothing term <code>1e-8</code> is needed to avoid division by zero). This technique avoids explosions in the activations due to excessive competition between the generator and the discriminator.</p>
</dd>
</dl>

<p>The combination of all these techniques allowed the authors to generate <a href="https://homl.info/progandemo">extremely convincing high-definition images of faces</a>. But what exactly do we call “convincing”? Evaluation is one of the big challenges when working with GANs: although it is possible to automatically evaluate the diversity of the generated images, judging their quality is a much trickier and subjective task. One technique is to use human raters, but this is costly and time-consuming. So the authors proposed to measure the similarity between the local image structure of the generated images and the training images, considering every scale. This idea led them to another groundbreaking paper in 2019: StyleGANs.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="StyleGANs"><div class="sect2" id="idm46263487051704">
<h2>StyleGANs</h2>

<p>The state of the art in high-resolution image generation was advanced once again by the same Nvidia team in a <a href="https://homl.info/stylegan">2019 paper</a><sup><a data-type="noteref" id="idm46263487049512-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487049512">17</a></sup> that introduced the popular StyleGAN architecture. The authors used <em>style transfer</em> techniques in the generator to ensure that the generated images have the same local structure as the training images, at every scale, greatly improving the quality of the generated images. The discriminator and the loss function were not modified, only the generator. Let’s take a look at it. It is composed of two networks (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#stylegan_diagram">Figure&nbsp;17-20</a>):</p>
<dl>
<dt>mapping network</dt>
<dd>
<p>An eight-layer MLP that maps the latent representations <strong>z</strong> (i.e., the codings) to a vector <strong>w</strong>. This vector is then sent through multiple <em>affine transformations</em> (i.e., dense layers with no activation functions, represented by the “A” boxes in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#stylegan_diagram">Figure&nbsp;17-20</a>), which produces multiple vectors. These vectors control the style of the generated image at different levels, from fine-grained texture (e.g., hair color) to high-level features (e.g., adult or child). In short, the mapping network maps the codings to multiple style vectors.</p>
</dd>
<dt>synthesis network</dt>
<dd>
<p>Responsible for generating the images. It has a constant learned input (to be clear, this input will be constant <em>after</em> training, but <em>during</em> training it keeps getting tweaked by backpropagation). It processes this input through multiple convolutional and upsampling layers, as earlier, but there are two twists: first, some noise is added to the input and to all the outputs of the convolutional layers (before the activation function). Second, each noise layer is followed by an <em>adaptive instance normalization</em> (AdaIN) layer: it standardizes each feature map independently (by subtracting the feature map’s mean and dividing by its standard deviation), then it uses the style vector to determine the scale and offset of each feature map (the style vector contains one scale and one bias term for each feature map).</p>
</dd>
</dl>

<figure><div id="stylegan_diagram" class="figure">
<img src="./Chapter17_files/mls2_1720.png" alt="mls2 1720" width="1104" height="1229" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1720.png">
<h6><span class="label">Figure 17-20. </span>StyleGAN’s generator architecture (part of Figure 1 from the StyleGAN paper)<sup><a data-type="noteref" id="idm46263487037704-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487037704">18</a></sup></h6>
</div></figure>

<p>The idea of adding noise independently from the codings is very important: some parts of an image are quite random, such as the exact position of each freckle or hair. In earlier GANs, this randomness had to either come from the codings or be some pseudorandom noise produced by the generator itself. If it came from the codings, it meant that the generator had to dedicate a significant portion of the codings’ representational power to store noise: this is quite wasteful. Moreover, the noise had to be able to flow through the network and reach the final layers of the generator: this seems like an unnecessary constraint that probably slowed down training. And finally, some visual artifacts may appear because the same noise was used at different levels. If instead the generator tried to produce its own pseudorandom noise, this noise may not look very convincing, leading to more visual artifacts. Plus, part of the generator’s weights would be dedicated to generating pseudorandom noise, which again seems wasteful. By adding extra noise inputs, all these issues are avoided: the GAN is able to use the provided noise to add the right amount of stochasticity to each part of the image.</p>

<p>The added noise is different for each level. Each noise input consists of a single feature map full of Gaussian noise, which is broadcast to all feature maps (of the given level), and scaled using learned per-feature scaling factors (this is represented by the “B” boxes in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#stylegan_diagram">Figure&nbsp;17-20</a>) before it is added.</p>

<p>Finally, StyleGAN uses a technique called <em>mixing regularization</em> (or <em>style mixing</em>): a percentage of the generated images are produced using two different codings. Specifically, the codings <strong>c</strong><sub>1</sub> and <strong>c</strong><sub>2</sub> are sent through the mapping network, giving two style vectors <strong>w</strong><sub>1</sub> and <strong>w</strong><sub>2</sub>. Then the synthesis network generates an image based on the styles <strong>w</strong><sub>1</sub> for the first levels and the styles <strong>w</strong><sub>2</sub> for the remaining levels. The cutoff level is picked randomly. This prevents the network from assuming that styles at adjacent levels are correlated, which in turn encourages locality in GAN, meaning that each style vector only affects a limited number of traits in the generated image.</p>

<p>There is such a wide variety of GANs out there that it would require a whole book to cover them all. Hopefully this introduction has given you the main ideas, and most importantly the desire to learn more. If you’re struggling with a mathematical concept, there are probably blog posts out there that will help you understand it better. Then go ahead and implement your own GAN, and do not get discouraged if it has trouble learning at first: unfortunately, this is normal, and it will require quite a bit of patience before it works, but the result is worth it. If you’re struggling with an implementation detail, there are plenty of Keras or TensorFlow implementations that you can look at. In fact, if all you want is to get some amazing results quickly, then you can just use a pretrained model (e.g., there are pretrained StyleGAN models available for Keras).</p>

<p>In the next chapter we will move to an entirely different branch of Deep Learning: Deep Reinforcement Learning.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="idm46263487898328">
<h1>Exercises</h1>
<ol>
<li>
<p>What are the main tasks that autoencoders are used for?</p>
</li>
<li>
<p>Suppose you want to train a classifier, and you have plenty of unlabeled training data but only a few thousand labeled instances. How can autoencoders help? How would you proceed?</p>
</li>
<li>
<p>If an autoencoder perfectly reconstructs the inputs, is it necessarily a good autoencoder? How can you evaluate the performance of an autoencoder?</p>
</li>
<li>
<p>What are undercomplete and overcomplete autoencoders? What is the main risk of an excessively undercomplete autoencoder? What about the main risk of an overcomplete autoencoder?</p>
</li>
<li>
<p>How do you tie weights in a stacked autoencoder? What is the point of doing so?</p>
</li>
<li>
<p>What is a generative model? Can you name a type of generative autoencoder?</p>
</li>
<li>
<p>What is a GAN? Can you name a few tasks where GANs can shine?</p>
</li>
<li>
<p>What are the main difficulties when training GANs?</p>
</li>
<li>
<p>Let’s use a denoising autoencoder to pretrain an image classifier. You can use MNIST (simplest), or a more complex image dataset such as <a href="https://homl.info/122">CIFAR10</a> if you want a bigger challenge. Regardless of the dataset you’re using, follow these steps:</p>

<ul>
<li>
<p>Split the dataset into a training set and a test set. Train a deep denoising autoencoder on the full training set.</p>
</li>
<li>
<p>Check that the images are fairly well reconstructed. Visualize the images that most activate each neuron in the coding layer.</p>
</li>
<li>
<p>Build a classification DNN, reusing the lower layers of the autoencoder. Train it using only 500 images from the training set. Does it perform better with or without pretraining?</p>
</li>
</ul>
</li>
<li>
<p>Train a variational autoencoder on the image dataset of your choice, and use it to generate images. Alternatively, you can try to find an unlabeled dataset that you are interested in and see if you can generate new samples.</p>
</li>
<li>
<p>Train a DCGAN to tackle the image dataset of your choice, and use it to generate images. Add experience replay and see if this helps. Turn it into a Conditional GAN where you can control the generated class.</p>
</li>

</ol>

<p>Solutions to these exercises are available in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/app01.html#solutions_appendix">Appendix&nbsp;A</a>.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46263490894888"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263490894888-marker" class="totri-footnote">1</a></sup> William G. Chase and Herbert A. Simon, “Perception in Chess,” Cognitive Psychology 4, no. 1 (January 1973): 55–81.</p><p data-type="footnote" id="idm46263490532776"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263490532776-marker" class="totri-footnote">2</a></sup> You might be tempted to use the accuracy metric, but it would not work properly, since this metric expects the labels to be either 0 or 1 for each pixel. You can easily work around this problem by creating a custom metric that computes the accuracy after rounding the targets and predictions to 0 or 1.</p><p data-type="footnote" id="idm46263490002056"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263490002056-marker" class="totri-footnote">3</a></sup> Yoshua Bengio et al., “Greedy Layer-Wise Training of Deep Networks” in proceedsings of NeurIPS 2006: 153–160.</p><p data-type="footnote" id="idm46263489996376"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263489996376-marker" class="totri-footnote">4</a></sup> Jonathan Masci et al., “Stacked Convolutional Auto-Encoders for Hierarchichal Feature Extraction” in <em>International Conference on Artificial Neural Networks</em> 1 (Berlin: Springer-Verlag, 2011): 52–59.</p><p data-type="footnote" id="idm46263489523080"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263489523080-marker" class="totri-footnote">5</a></sup> Pascal Vincent et al., Extracting and Composing Robust Features with Denoising Autoencoders (Montreal: Université de Montréal, February 2008).</p><p data-type="footnote" id="idm46263489521432"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263489521432-marker" class="totri-footnote">6</a></sup> Pascal Vincent et al., “Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion,” Journal of Machine Learning Research 11 (2010): 3371–3408.</p><p data-type="footnote" id="idm46263488743288"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263488743288-marker" class="totri-footnote">7</a></sup> Diederik Kingma and Max Welling, “Auto-Encoding Variational Bayes,” ICLR (2014).</p><p data-type="footnote" id="idm46263488728168"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263488728168-marker" class="totri-footnote">8</a></sup> Variational autoencoders are actually more general; the codings are not limited to Gaussian distributions.</p><p data-type="footnote" id="idm46263488724008"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263488724008-marker" class="totri-footnote">9</a></sup> For more mathematical details, check out the original paper on variational autoencoders, or Carl Doersch’s <a href="https://homl.info/116">great tutorial</a> (2016).</p><p data-type="footnote" id="idm46263487897112"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487897112-marker">10</a></sup> Ian Goodfellow et al., “Generative Adversarial Nets,” in proceedings of NeurIPS 2014: 2672–2680.</p><p data-type="footnote" id="idm46263487340152"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487340152-marker">11</a></sup> For a nice comparison of the main GAN losses, check out this great GitHub project by Hwalsuk Lee: <a href="https://homl.info/ganloss"><em class="hyperlink">https://homl.info/ganloss</em></a>.</p><p data-type="footnote" id="idm46263487331720"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487331720-marker">12</a></sup> Alec Radford et al., “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,” ICLR (2016).</p><p data-type="footnote" id="idm46263487102408"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487102408-marker">13</a></sup> Reproduced with the kind authorization of the authors.</p><p data-type="footnote" id="idm46263487099560"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487099560-marker">14</a></sup> Mehdi Mirza and Simon Osindero, “Conditional Generative Adversarial Nets” (2014).</p><p data-type="footnote" id="idm46263487096088"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487096088-marker">15</a></sup> Tero Karras et al., “Progressive Growing of GANs for Improved Quality, Stability, and Variation,” ICLR (2018).</p><p data-type="footnote" id="idm46263487057480"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487057480-marker">16</a></sup> The dynamic range of a variable is the ratio between the largest and the lowest value it may take.</p><p data-type="footnote" id="idm46263487049512"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487049512-marker">17</a></sup> Tero Karras et al., “A Style-Based Generator Architecture for Generative Adversarial Networks,” CVPR (2019).</p><p data-type="footnote" id="idm46263487037704"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#idm46263487037704-marker">18</a></sup> Reproduced with the kind authorization of the authors.</p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-18" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders">
		
		<li class="copy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#">Add Highlight</a></li>
		<li class="add-note"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#">
			Add Note
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">16. Natural Language Processing with RNNs and Attention</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch18.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">18. Reinforcement Learning</div>
        </a>
    
  
  </div>

</section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    



        
      </div>
      



  <footer class="pagefoot">
    <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#" class="icon-up" style="display: none;"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      <li class="full-support"><a href="https://www.oreilly.com/online-learning/support/">Support</a></li>
      <li><a href="https://www.oreilly.com/online-learning/apps.html">Get the App</a></li>
      
        <li><a href="https://learning.oreilly.com/accounts/logout/">Sign Out</a></li>
      
    </ul>
    <span class="copyright">© 2019 <a href="https://learning.oreilly.com/" target="_blank">Safari</a>.</span>
    <a href="https://learning.oreilly.com/terms/">Terms of Service</a> /
    <a href="https://learning.oreilly.com/membership-agreement/">Membership Agreement</a> /
    <a href="https://www.oreilly.com/privacy.html">Privacy Policy</a>
  </footer>

<script type="text/javascript">window.NREUM||(NREUM={});NREUM.info={"applicationID":"172641827,79672898,93931619","errorBeacon":"bam.nr-data.net","agent":"","applicationTime":451,"licenseKey":"510f1a6865","transactionName":"YgdaZ0NSW0cEB0RdWltNfkZfUEFdCgofXFBHDVYdR1pQQxZeRl1QQj1aWkU=","beacon":"bam.nr-data.net","queueTime":4}</script>


    
    <script src="./Chapter17_files/saved_resource" charset="utf-8"></script><script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","1732687426968531");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=1732687426968531&amp;ev=PageView&amp;noscript=1"></noscript><script type="text/javascript" id="">(function(){window.medalliaUserIdentifier=document.documentElement.dataset.userUuid;window.medalliaUserName=document.documentElement.dataset.username})();</script>
<script type="text/javascript" id="" src="./Chapter17_files/embed.js.download"></script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script>
<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("set","agent","tmgoogletagmanager","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>

<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>
<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.09556792590964802"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.5229133033930915" width="0" height="0" alt="" src="./Chapter17_files/0"></div>
    <script src="./Chapter17_files/saved_resource(1)" charset="utf-8"></script>
  

<script src="./Chapter17_files/saved_resource(2)" type="text/javascript"></script><script type="text/javascript" id="">window._pp=window._pp||[];if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/register/")_pp.targetUrl="/confirm/trial";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/subscribe/")_pp.targetUrl="/confirm/paid";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/signup/")_pp.targetUrl="/confirm/paid";_pp.siteId="2508";
_pp.siteUId="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79";_pp.orderValue="undefined";_pp.orderId="undefined";(function(){var ppjs=document.createElement("script");ppjs.type="text/javascript";ppjs.async=true;ppjs.src=("https:"==document.location.protocol?"https:":"http:")+"//cdn.pbbl.co/r/"+_pp.siteId+".js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(ppjs,s)})();</script><div class="annotator-notice"></div><div class="font-flyout" style="top: 201px; left: 1194px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch17.html#">Reset</a>
</div>
</div><script type="text/javascript" async="" src="./Chapter17_files/generic1566415868241.js.download" charset="UTF-8"></script><div style="display: none; visibility: hidden;"><script>(function(){if(null!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')&&void 0!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')){var a=!1;window.addEventListener("blur",function(){a&&dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"facebook",eventVal:0,nonInteraction:0})});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseover",function(){window.focus();
a=!0});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseout",function(){a=!1})}try{window.twttr=function(b,a,d){var c,e=b.getElementsByTagName(a)[0];if(!b.getElementById(d))return b=b.createElement(a),b.id=d,b.src="//platform.twitter.com/widgets.js",e.parentNode.insertBefore(b,e),window.twttr||(c={_e:[],ready:function(a){c._e.push(a)}})}(document,"script","twitter-wjs"),twttr.ready(function(a){a.events.bind("tweet",trackTwitter)})}catch(b){}})();
null!==document.querySelector(".IN-widget")&&void 0!==document.querySelector(".IN-widget")&&document.querySelector(".IN-widget").addEventListener("click",function(){dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"linkedin",eventVal:0,nonInteraction:0})});
function trackTwitter(a){a&&(a.target&&"IFRAME"==a.target.nodeName&&(opt_target=extractParamFromUri(a.target.src,"url")),dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"twitter",eventVal:0,nonInteraction:0}))}function extractParamFromUri(a,b){if(a){var c=new RegExp("[\\?\x26#]"+b+"\x3d([^\x26#]*)");c=c.exec(a);if(null!=c)return unescape(c[1])}};</script></div><span><div id="KampyleAnimationContainer" style="z-index: 2147483000; border: 0px; position: fixed; display: block; width: 0px; height: 0px;"></div></span><iframe scrolling="no" frameborder="0" allowtransparency="true" src="./Chapter17_files/widget_iframe.097c1f5038f9e8a0d62a39a892838d66.html" title="Twitter settings iframe" style="display: none;"></iframe><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_AMS, sans-serif;"></div></div></body></html>