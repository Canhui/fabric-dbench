<!DOCTYPE html>
<!-- saved from url=(0091)https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html -->
<html class=" js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/hands-on-machine-learning/9781492032632/part01.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="4626953" data-user-uuid="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79" data-username="17481074" data-account-type="B2B" data-activated-trial-date="" data-archive="9781492032632" data-publishers="O&#39;Reilly Media, Inc." data-htmlfile-name="part01.html" data-epub-title="Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781492032632"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><script type="text/javascript" async="" src="./Chapter10_files/cool-2.1.15.min.js.download"></script><script type="text/javascript" src="./Chapter10_files/510f1a6865"></script><script id="twitter-wjs" src="./Chapter10_files/widgets.js.download"></script><script src="./Chapter10_files/nr-1130.min.js.download"></script><script type="text/javascript" async="" src="./Chapter10_files/2508.js.download"></script><script async="" src="./Chapter10_files/fbevents.js.download"></script><script type="text/javascript" async="" src="./Chapter10_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter10_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter10_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter10_files/analytics.js.download"></script><script type="text/javascript" async="" src="./Chapter10_files/ec.js.download"></script><script type="text/javascript" async="" src="./Chapter10_files/bat.js.download"></script><script type="text/javascript" async="" src="./Chapter10_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter10_files/insight.min.js.download"></script><script type="text/javascript" async="" src="./Chapter10_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter10_files/linkid.js.download"></script><script async="" src="./Chapter10_files/gtm.js.download"></script><script async="" src="./Chapter10_files/analytics.js.download"></script><script type="text/javascript">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if("function"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e("handle"),a=e(3),u=e(4),f=e("ee").get("tracer"),c=e("loader"),s=NREUM;"undefined"==typeof window.newrelic&&(newrelic=s);var p=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],d="api-",l=d+"ixn-";a(p,function(e,n){s[n]=o(d+n,!0,"api")}),s.addPageAction=o(d+"addPageAction",!0),s.setCurrentRouteName=o(d+"routeName",!0),n.exports=newrelic,s.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(e,n){var t={},r=this,o="function"==typeof n;return i(l+"tracer",[c.now(),e,t],r),function(){if(f.emit((o?"":"no-")+"fn-start",[c.now(),r,o],t),o)try{return n.apply(this,arguments)}catch(e){throw f.emit("fn-err",[arguments,this,e],t),e}finally{f.emit("fn-end",[c.now()],t)}}}};a("actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(e,n){m[n]=o(l+n)}),newrelic.noticeError=function(e,n){"string"==typeof e&&(e=new Error(e)),i("err",[e,c.now(),!1,n])}},{}],2:[function(e,n,t){function r(e,n){if(!o)return!1;if(e!==o)return!1;if(!n)return!0;if(!i)return!1;for(var t=i.split("."),r=n.split("."),a=0;a<r.length;a++)if(r[a]!==t[a])return!1;return!0}var o=null,i=null,a=/Version\/(\S+)\s+Safari/;if(navigator.userAgent){var u=navigator.userAgent,f=u.match(a);f&&u.indexOf("Chrome")===-1&&u.indexOf("Chromium")===-1&&(o="Safari",i=f[1])}n.exports={agent:o,version:i,match:r}},{}],3:[function(e,n,t){function r(e,n){var t=[],r="",i=0;for(r in e)o.call(e,r)&&(t[i]=n(r,e[r]),i+=1);return t}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],4:[function(e,n,t){function r(e,n,t){n||(n=0),"undefined"==typeof t&&(t=e?e.length:0);for(var r=-1,o=t-n||0,i=Array(o<0?0:o);++r<o;)i[r]=e[n+r];return i}n.exports=r},{}],5:[function(e,n,t){n.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(e,n,t){function r(){}function o(e){function n(e){return e&&e instanceof r?e:e?f(e,u,i):i()}function t(t,r,o,i){if(!d.aborted||i){e&&e(t,r,o);for(var a=n(o),u=v(t),f=u.length,c=0;c<f;c++)u[c].apply(a,r);var p=s[y[t]];return p&&p.push([b,t,r,a]),a}}function l(e,n){h[e]=v(e).concat(n)}function m(e,n){var t=h[e];if(t)for(var r=0;r<t.length;r++)t[r]===n&&t.splice(r,1)}function v(e){return h[e]||[]}function g(e){return p[e]=p[e]||o(t)}function w(e,n){c(e,function(e,t){n=n||"feature",y[t]=n,n in s||(s[n]=[])})}var h={},y={},b={on:l,addEventListener:l,removeEventListener:m,emit:t,get:g,listeners:v,context:n,buffer:w,abort:a,aborted:!1};return b}function i(){return new r}function a(){(s.api||s.feature)&&(d.aborted=!0,s=d.backlog={})}var u="nr@context",f=e("gos"),c=e(3),s={},p={},d=n.exports=o();d.backlog=s},{}],gos:[function(e,n,t){function r(e,n,t){if(o.call(e,n))return e[n];var r=t();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,n,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return e[n]=r,r}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],handle:[function(e,n,t){function r(e,n,t,r){o.buffer([e],r),o.emit(e,n,t)}var o=e("ee").get("handle");n.exports=r,r.ee=o},{}],id:[function(e,n,t){function r(e){var n=typeof e;return!e||"object"!==n&&"function"!==n?-1:e===window?0:a(e,i,function(){return o++})}var o=1,i="nr@id",a=e("gos");n.exports=r},{}],loader:[function(e,n,t){function r(){if(!E++){var e=x.info=NREUM.info,n=l.getElementsByTagName("script")[0];if(setTimeout(s.abort,3e4),!(e&&e.licenseKey&&e.applicationID&&n))return s.abort();c(y,function(n,t){e[n]||(e[n]=t)}),f("mark",["onload",a()+x.offset],null,"api");var t=l.createElement("script");t.src="https://"+e.agent,n.parentNode.insertBefore(t,n)}}function o(){"complete"===l.readyState&&i()}function i(){f("mark",["domContent",a()+x.offset],null,"api")}function a(){return O.exists&&performance.now?Math.round(performance.now()):(u=Math.max((new Date).getTime(),u))-x.offset}var u=(new Date).getTime(),f=e("handle"),c=e(3),s=e("ee"),p=e(2),d=window,l=d.document,m="addEventListener",v="attachEvent",g=d.XMLHttpRequest,w=g&&g.prototype;NREUM.o={ST:setTimeout,SI:d.setImmediate,CT:clearTimeout,XHR:g,REQ:d.Request,EV:d.Event,PR:d.Promise,MO:d.MutationObserver};var h=""+location,y={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-1130.min.js"},b=g&&w&&w[m]&&!/CriOS/.test(navigator.userAgent),x=n.exports={offset:u,now:a,origin:h,features:{},xhrWrappable:b,userAgent:p};e(1),l[m]?(l[m]("DOMContentLoaded",i,!1),d[m]("load",r,!1)):(l[v]("onreadystatechange",o),d[v]("onload",r)),f("mark",["firstbyte",u],null,"api");var E=0,O=e(5)},{}]},{},["loader"]);</script><link rel="apple-touch-icon" href="https://learning.oreilly.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://learning.oreilly.com/favicon.ico" type="image/x-icon"><link href="./Chapter10_files/css" rel="stylesheet" type="text/css"><title>10. Introduction to Artificial Neural Networks with Keras - Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition</title><link rel="stylesheet" href="./Chapter10_files/output.68851547a55f.css" type="text/css"><link rel="stylesheet" type="text/css" href="./Chapter10_files/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="./Chapter10_files/font-awesome.min.css"><style type="text/css" title="ibis-book">@charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000 !important;padding-top:10px !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:2em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content p[data-type="footnote"]{font-size:90% !important;line-height:1.2em !important;margin-left:2.5em !important;text-indent:-2.3em !important}#sbo-rt-content p[data-type="footnote"] sup{display:inline-block !important;position:static !important;width:2em !important;text-align:right !important;font-size:100% !important;padding-right:.5em !important}#sbo-rt-content p[data-type="footnote"] a[href$="-marker"]{font-family:sans-serif !important;font-size:90% !important;color:#8e0012 !important}#sbo-rt-content p[data-type="footnote"] a[data-type="xref"]{margin:0 !important;padding:0 !important;text-indent:0 !important}#sbo-rt-content a[data-type="noteref"]{font-family:sans-serif !important;color:#8e0012;margin-left:0;padding-left:0}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dd{margin-left:1.5em !important;margin-bottom:.25em}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul>li,#sbo-rt-content ol ul,#sbo-rt-content ol ul>li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul>li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul>li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul>li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol>li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol>li,#sbo-rt-content ul ol,#sbo-rt-content ul ol>li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol>li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol>li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:middle;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content table.border tbody>tr:last-child>td{border-bottom:transparent}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:1em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content .width-10{width:10vw !important}#sbo-rt-content .width-20{width:20vw !important}#sbo-rt-content .width-30{width:30vw !important}#sbo-rt-content .width-40{width:40vw !important}#sbo-rt-content .width-50{width:50vw !important}#sbo-rt-content .width-60{width:60vw !important}#sbo-rt-content .width-70{width:70vw !important}#sbo-rt-content .width-80{width:80vw !important}#sbo-rt-content .width-90{width:90vw !important}#sbo-rt-content .width-full,#sbo-rt-content .width-100{width:100vw !important}#sbo-rt-content div[data-type="equation"].fifty-percent img{width:50%}</style><script> // <![CDATA[
    var g = {
      position_cache: {
        
          "chapter": "/api/v1/book/9781492032632/chapter/part01.html",
          "book_id": "9781492032632",
          "chapter_uri": "part01.html",
          "position": 100.0,
          "user_uuid": "d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79",
          "next_chapter_uri": "/library/view/hands-on-machine-learning/9781492032632/ch01.html"
        
      },
      title: "Hands\u002Don Machine Learning with Scikit\u002DLearn, Keras, and TensorFlow, 2nd Edition",
      author_list: "Aurélien Géron",
      format: "book",
      source: "application/epub+zip",
      is_system_book: true,
      is_public: false,
      loaded_from_server: true,
      allow_scripts: false,
      has_mathml: true
    };
    // ]]></script><script src="./Chapter10_files/modernizr.8e35451ddb64.js.download"></script><script>
    
      

      
        
          window.PUBLIC_ANNOTATIONS = true;
        
      

      window.MOBILE_PUBLIC_ANNOTATIONS = false;

    

    
      window.PRIVACY_CONTROL_OVERRIDE = false;
    

      window.PRIVACY_CONTROL_SWITCH = true;

      window.PUBLISHER_PAGES = true;

      window.SBO = {
        "constants": {
          "SITB_ENDPOINT": "/api/v2/sitb/",
          "SEARCH_SELECT_ENDPOINT": "https://learning.oreilly.com/api/v2/search/select/",
          "ENABLE_ONLINE_TRAINING": false
        }
      };
  </script><link rel="canonical" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta name="description" content=" Part I. The Fundamentals of Machine Learning "><meta property="og:title" content="I. The Fundamentals of Machine Learning"><meta itemprop="isPartOf" content="/library/view/hands-on-machine-learning/9781492032632/"><meta itemprop="name" content="I. The Fundamentals of Machine Learning"><meta property="og:url" itemprop="url" content="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://learning.oreilly.com/library/cover/9781492032632/"><meta property="og:description" itemprop="description" content=" Part I. The Fundamentals of Machine Learning "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O&#39;Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781492032649"><meta property="og:book:author" itemprop="author" content="Aurélien Géron"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: &lt;%= font_size %&gt; !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: &lt;%= font_family %&gt; !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: &lt;%= column_width %&gt;% !important; margin: 0 auto !important; }"></style><noscript><meta http-equiv="refresh" content="0; url=/library/no-js/" /></noscript><script type="text/javascript">
    (function(i,s,o,g,r,a,m) {
      i['GoogleAnalyticsObject']=r;
      i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();
      a=s.createElement(o),m=s.getElementsByTagName(o)[0];
      a.async=1;
      a.src=g;
      m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    var matches = document.cookie.match(/BrowserCookie\s*=\s*([a-f0-9\-]{36})/),
      user_uuid = null;

    if (matches && matches.length === 2) {
      user_uuid = matches[1];
    }

  
    ga('create', 'UA-39299553-7', {'userId': 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79' });
  

  
    
      ga('set', 'dimension1', 'B2B');
    
  

  ga('set', 'dimension6', user_uuid);

  
    ga('set', 'dimension2', 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79');
    
      ga('set', 'dimension7', '0012M0000229keZQAQ');
    
  

  

  

  //enable enhanced link tracking
  ga('require', 'linkid', 'linkid.js');

  // reading interface will track pageviews itself
  if (document.location.pathname.indexOf("/library/view") !== 0) {
    ga('send', 'pageview');
  }
  </script><script>
    var dataLayer = window.dataLayer || [];

    
      window.medalliaVsgUserIdentifier = 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79';
      dataLayer.push({userIdentifier: 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79'});
      dataLayer.push({loggedIn: 'yes'});

      
        window.medalliaVsgAccountIdentifier = '21bed0a7-6b7b-470c-8fa0-40a52db0b491';
        
        dataLayer.push({orgID: '21bed0a7-6b7b-470c-8fa0-40a52db0b491'});
        

        window.medalliaVsgIsIndividual = false;
        
          
          dataLayer.push({learningAccountType: 'enterprise'});
          
        

        
          dataLayer.push({learningPaidAccount: 'yes'});
        
      
    

    (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-5P4V6Z');
    (function () {
      var VERSION = 'V1.1';
      var AUTHOR = 'Awwad';
      if (!window.GtmHelper)
        window.GtmHelper = function () {
          var instance = this;
          var loc = document.location;
          this.version = VERSION;
          this.author = AUTHOR;
          this.readCookie = function (name) {
            var nameEQ = name + "=";
            var ca = document.cookie.split(';');
            for (var i = 0; i < ca.length; i++) {
              var c = ca[i];
              while (c.charAt(0) == ' ') c = c.substring(1, c.length);
              if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length, c.length);
            }
            return null;
          };
          this.createCookie = function (name, value, days, cookieDomain) {
            var domain = "";
            var expires = "";

            if (days) {
              var date = new Date();
              date.setTime(date.getTime() + Math.ceil(days * 24 * 60 * 60 * 1000));
              var expires = " expires=" + date.toGMTString() + ";";
            }

            if (typeof (cookieDomain) != 'undefined')
              domain = " domain=" + cookieDomain + "; ";

            document.cookie = name + "=" + value + ";" + expires + domain + "path=/";
          };

          this.isDuplicated = function (currentTransactionId) {
            // the previous transaction id:
            var previousTransIdValue = this.readCookie("previousTransId");

            if (currentTransactionId === previousTransIdValue) {
              return true; // Duplication
            } else {
              return false;
            }
          };
        }
    })()
  </script><script defer="" src="./Chapter10_files/vendor.a48a756c5182.js.download"></script><script defer="" src="./Chapter10_files/reader.f2a0c6bd2fee.js.download"></script><script src="./Chapter10_files/f(1).txt"></script><script src="./Chapter10_files/f(2).txt"></script><script src="./Chapter10_files/f(3).txt"></script><script src="./Chapter10_files/f(4).txt"></script><script async="" src="./Chapter10_files/MathJax.js.download"></script><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 2147483020;
}
.annotator-filter {
  z-index: 2147483010;
}</style><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
</style><script async="true" type="text/javascript" src="./Chapter10_files/roundtrip.js.download"></script><style type="text/css" id="kampyleStyle">.noOutline{outline: none !important;}.wcagOutline:focus{outline: 1px dashed #595959 !important;outline-offset: 2px !important;transition: none !important;}</style><script async="true" type="text/javascript" src="./Chapter10_files/roundtrip.js.download"></script><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Math-bold-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Script; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_AMS; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf?V=2.7.1') format('opentype')}
</style></head>


<body class="reading sidenav  scalefonts library nav-collapsed"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

    
  <noscript> 
    <iframe src="//www.googletagmanager.com/ns.html?id=GTM-5P4V6Z"
            height="0" width="0"
            style="display:none;visibility:hidden">
    </iframe>
  </noscript>



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#container" class="skip">Skip to content</a><header class="topbar t-topbar"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://learning.oreilly.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M11.738 14H9.254v-3.676a.617.617 0 0 0-.621-.613H7.39a.617.617 0 0 0-.62.613V14H4.284a.617.617 0 0 1-.622-.613V10.22c0-.327.132-.64.367-.87l3.547-3.493a.627.627 0 0 1 .875 0l3.54 3.499c.234.229.366.54.367.864v3.167a.617.617 0 0 1-.62.613zM7.57 2.181a.625.625 0 0 1 .882 0l5.77 5.692-.93.92-5.28-5.209-5.28 5.208-.932-.919 5.77-5.692z"></path></svg><span>Safari Home</span></a></li><li><a href="https://learning.oreilly.com/resource-centers/" class="t-resource-centers-nav l0 nav-icn"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="16px" height="16px" viewBox="0 0 16 16" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="Topic-Page-Design" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="Heron-Button" transform="translate(-20.000000, -78.000000)" fill="#4A3A30"><g id="Group-9" transform="translate(20.000000, 78.000000)"><rect id="Rectangle" x="9.6" y="0" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="9.6" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="0" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect></g></g></g></svg><span>Resource Centers</span></a></li><li><a href="https://learning.oreilly.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://learning.oreilly.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://learning.oreilly.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://learning.oreilly.com/learning-paths/" class="l1 nav-icn t-learningpaths-nav js-toggle-menu-item"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="32px" height="32px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 52.5 (67469) - http://www.bohemiancoding.com/sketch --><title>Mask</title><desc>Created with Sketch.</desc><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><path d="M0,16.0214227 C0,15.0387209 0.796453294,14.2411658 1.77779753,14.2411658 C2.75914177,14.2411658 3.55559506,15.0387209 3.55559506,16.0214227 C3.55559506,17.0041246 2.75914177,17.8016797 1.77779753,17.8016797 C0.796453294,17.8016797 0,17.0041246 0,16.0214227 Z M9.77788642,5.22914885 C8.9280992,5.72049977 7.84008711,5.42853763 7.34941499,4.57757479 C6.85874287,3.72661195 7.15030167,2.63709467 8.00008889,2.14574375 C8.84987611,1.65439282 9.9378882,1.94635496 10.4285603,2.7973178 C10.9192324,3.64828064 10.6276736,4.73779792 9.77788642,5.22914885 Z M4.57213969,7.35869225 C5.42192691,7.85004318 5.71348571,8.93956046 5.22281359,9.79052329 C4.73214147,10.6414861 3.64412938,10.9334483 2.79434216,10.4420974 C1.94455494,9.95074642 1.65299614,8.86122915 2.14366826,8.01026631 C2.63434038,7.15930347 3.72235247,6.86734132 4.57213969,7.35869225 Z M2.79434216,21.6007481 C3.64412938,21.1093972 4.73214147,21.4013594 5.22281359,22.2523222 C5.71348571,23.103285 5.42192691,24.1928023 4.57213969,24.6841532 C3.72235247,25.1755042 2.63434038,24.883542 2.14366826,24.0325792 C1.65299614,23.1816163 1.94455494,22.0920991 2.79434216,21.6007481 Z M7.34941499,27.4652707 C7.84008711,26.6143079 8.9280992,26.3223457 9.77788642,26.8136966 C10.6276736,27.3050476 10.9192324,28.3945649 10.4285603,29.2455277 C9.9378882,30.0964905 8.84987611,30.3884527 8.00008889,29.8971017 C7.15030167,29.4057508 6.85874287,28.3162335 7.34941499,27.4652707 Z M18.7118524,11.3165596 C21.3074367,12.8173162 22.1963355,16.1392758 20.6976522,18.738451 C19.1989689,21.3358459 15.8815987,22.2259744 13.2860143,20.726998 C10.6922077,19.2262414 9.80330893,15.9042818 11.3002144,13.3051066 C12.7988978,10.7059314 16.116268,9.81580294 18.7118524,11.3165596 Z M26.7821642,27.8093944 L30.1315348,31.1633985 C30.3982044,31.4304371 30.2097579,31.8844026 29.8346426,31.8844026 L21.5945511,31.8844026 C21.1287681,31.8844026 20.751875,31.5069881 20.751875,31.0405608 L20.751875,22.7890697 C20.751875,22.4134355 21.2052134,22.2247282 21.4701052,22.4899865 L24.2843587,25.3081333 C26.8337204,23.0240636 28.4444049,19.7092251 28.4444049,16.0223129 C28.4444049,9.15052091 22.8621207,3.56051397 15.9998222,3.56051397 L15.9998222,0 C24.8230314,0 32,7.18689745 32,16.0223129 C32,20.6919269 29.9750886,24.8790914 26.7821642,27.8093944 Z" id="Mask" fill="#8B889A"></path></g></svg><span>Learning Paths</span></a></li><li class="nav-highlights"><a href="https://learning.oreilly.com/u/d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" fill="#4A3C31"><path d="M13.325 18.071H8.036c0-6.736 4.324-10.925 14.464-12.477V0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35c5.142 0 9.175-3.515 9.175-8.816 0-4.628-2.367-7.293-6.253-8.113zm27.5 0h-5.26c0-6.736 4.295-10.925 14.435-12.477V0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35c5.113 0 9.146-3.515 9.146-8.816 0-4.628-2.338-7.293-6.253-8.113z" fill-rule="evenodd"></path></svg><span>Highlights</span></a></li><li><a href="https://learning.oreilly.com/u/preferences/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l1 no-icon">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://learning.oreilly.com/u/preferences/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l2">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9781492032632/chapter/part01.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"><div data-reactroot="" class="menu-dropdown-wrapper js-menu-dropdown-wrapper align-right"><img class="hidden" src="./Chapter10_files/ajax-transp.gif" alt="loading spinner"><div class="menu-control"><div class="control "><div class="js-playlists-menu"><button class="js-playlist-icon"><svg class="icon-add-to-playlist-sml" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill-rule="nonzero" fill="#000000"><g transform="translate(-1.000000, 0.000000)"><rect x="5" y="0" width="12" height="2"></rect><title>Playlists</title><path d="M4.5,14 C6.43299662,14 8,12.4329966 8,10.5 C8,8.56700338 6.43299662,7 4.5,7 C2.56700338,7 1,8.56700338 1,10.5 C1,12.4329966 2.56700338,14 4.5,14 Z M2.5,10 L4,10 L4,8.5 L5,8.5 L5,10 L6.5,10 L6.5,11 L5,11 L5,12.5 L4,12.5 L4,11 L2.5,11 L2.5,10 Z"></path><circle cx="2" cy="5" r="1"></circle><circle cx="1.94117647" cy="1" r="1"></circle><rect x="5" y="4" width="12" height="2"></rect><rect x="9" y="8" width="8" height="2"></rect><rect x="9" y="12" width="8" height="2"></rect></g></g></g></svg><div class="js-playlist-addto-label">Add&nbsp;To</div></button></div></div></div></div></div></div></li><li class="js-font-control-panel font-control-activator"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html&amp;text=Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%20I.%20The%20Fundamentals%20of%20Machine%20Learning&amp;body=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html%0D%0Afrom%20Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    
    <section role="document">
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part02.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">II. Neural Networks and Deep Learning</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">11. Training Deep Neural Networks</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content"><div class="annotator-wrapper"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 10. Introduction to Artificial Neural Networks with Keras"><div class="chapter" id="ann_chapter">
<h1><span class="label">Chapter 10. </span>Introduction to Artificial Neural Networks with Keras</h1>


<p>Birds inspired us to fly, burdock plants inspired velcro, and nature inspired countless more inventions. It seems only logical, then, to look at the brain’s architecture for inspiration on how to build an intelligent machine. This is the logic that sparked <em>artificial neural networks</em> (ANNs): an ANN is a Machine Learning model inspired by the networks of biological neurons found in our brains. However, although planes were inspired by birds, they don’t have to flap their wings. Similarly, ANNs have gradually become quite different from their biological cousins. Some researchers even argue that we should drop the biological analogy altogether (e.g., by saying “units” rather than “neurons”), lest we restrict our creativity to biologically plausible systems.<sup><a data-type="noteref" id="idm46263517315512-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517315512" class="totri-footnote">1</a></sup></p>

<p>ANNs are at the very core of Deep Learning. They are versatile, powerful, and scalable, making them ideal to tackle large and highly complex Machine Learning tasks, such as classifying billions of images (e.g., Google Images), powering speech recognition services (e.g., Apple’s Siri), recommending the best videos to watch to hundreds of millions of users every day (e.g., YouTube), or learning to beat the world champion at the game of <em>Go</em> by playing millions of games against  itself (DeepMind’s AlphaZero).</p>

<p>In the first part of this chapter, we will introduce artificial neural networks, starting with a quick tour of the very first ANN architectures. We’ll lead up to <em>Multilayer Perceptrons</em> (MLPs), which are heavily used today (other architectures will be explored in the next chapters). In the second part, we will look at how to implement neural networks using the popular Keras API. This is a beautifully designed and simple high-level API for building, training, evaluating, and running neural networks. But don’t be fooled by its simplicity: it is expressive and flexible enough to let you build a wide variety of neural network architectures. In fact, it will probably be sufficient for most of your use cases. Moreover, should you ever need extra flexibility, you can always write custom Keras components using its lower-level API, as we will see in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch12.html#tensorflow_chapter">Chapter&nbsp;12</a>.</p>

<p>But first, let’s go back in time to see how artificial neural networks came to be!</p>






<section data-type="sect1" data-pdf-bookmark="From Biological to Artificial Neurons"><div class="sect1" id="idm46263517310472">
<h1>From Biological to Artificial Neurons</h1>

<p>Surprisingly, ANNs have been around for quite a while: they were first introduced back in 1943 by the neurophysiologist Warren McCulloch and the mathematician Walter Pitts. In their <a href="https://homl.info/43">landmark paper</a>,<sup><a data-type="noteref" id="idm46263517307848-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517307848" class="totri-footnote">2</a></sup> “A Logical Calculus of Ideas Immanent in Nervous Activity,” McCulloch and Pitts presented a simplified computational model of how biological neurons might work together in animal brains to perform complex computations using <em>propositional logic</em>. This was the first artificial neural network architecture. Since then many other architectures have been invented, as we will see.</p>

<p>The early successes of ANNs led to the widespread belief that we would soon be conversing with truly intelligent machines. When it became clear in the 1960s that this promise would go unfulfilled (at least for quite a while), funding flew elsewhere, and ANNs entered a long winter. In the early 1980s, new architectures were invented and better training techniques were developed, sparking a revival of interest in <em>connectionism</em> (the study of neural networks). But progress was slow, and by the 1990s other powerful Machine Learning techniques were invented, such as Support Vector Machines (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch05.html#svm_chapter">Chapter&nbsp;5</a>). These techniques seemed to offer better results and stronger theoretical foundations than ANNs, so once again the study of neural networks entered a long winter.</p>

<p>We are now witnessing yet another wave of interest in ANNs. Will this wave die out like the previous ones did? Well, here are a few good reasons to believe that this wave is different and that it will have a much more profound impact on our lives:</p>

<ul>
<li>
<p>There is now a huge quantity of data available to train neural networks, and ANNs frequently outperform other ML techniques on very large and complex problems.</p>
</li>
<li>
<p>The tremendous increase in computing power since the 1990s now makes it possible to train large neural networks in a reasonable amount of time. This is in part due to Moore’s law (the number of components in integrated circuits has doubled about every two years over the last 50 years), but also thanks to the gaming industry, which has produced powerful GPU cards by the millions. Moreover, cloud platforms have made this power accessible to everyone.</p>
</li>
<li>
<p>The training algorithms have been improved. To be fair they are only slightly different from the ones used in the 1990s, but these relatively small tweaks have a huge positive impact.</p>
</li>
<li>
<p>Some theoretical limitations of ANNs have turned out to be benign in practice. For example, many people thought that ANN training algorithms were doomed because they were likely to get stuck in local optima, but it turns out that this is rather rare in practice (or when it is the case, they are usually fairly close to the global optimum).</p>
</li>
<li>
<p>ANNs seem to have entered a virtuous circle of funding and progress. Amazing products based on ANNs regularly make the headline news, which pulls more and more attention and funding toward them, resulting in more and more progress, and even more amazing products.</p>
</li>
</ul>








<section data-type="sect2" data-pdf-bookmark="Biological Neurons"><div class="sect2" id="idm46263517296200">
<h2>Biological Neurons</h2>

<p>Before we discuss artificial neurons, let’s take a quick look at a biological neuron (represented in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#biological_neuron_wikipedia">Figure&nbsp;10-1</a>). It is an unusual-looking cell mostly found in animal brains. It’s composed of a <em>cell body</em> containing the nucleus and most of the cell’s complex components, many branching extensions called <em>dendrites</em>, plus one very long extension called the <em>axon</em>. The axon’s length may be just a few times longer than the cell body, or up to tens of thousands of times longer. Near its extremity the axon splits off into many branches called <em>telodendria</em>, and at the tip of these branches are minuscule structures called <em>synaptic terminals</em> (or simply <em>synapses</em>), which are connected to the dendrites or cell body of other neurons.<sup><a data-type="noteref" id="idm46263517290312-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517290312" class="totri-footnote">3</a></sup> Biological neurons produce short electrical impulses called <em>action potentials</em> (APs, or just <em>signals</em>) which travel along the axon and make the synapses release chemical signals called <em>neurotransmitters</em>. When a neuron receives a sufficient amount of these neurotransmitters within a few milliseconds, it fires its own electrical impulses (actually, it depends on the neurotransmitters, as some of them inhibit the neuron from firing).</p>

<figure class="smallerseventy"><div id="biological_neuron_wikipedia" class="figure">
<img src="./Chapter10_files/mls2_1001.png" alt="mls2 1001" width="1440" height="933" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1001.png">
<h6><span class="label">Figure 10-1. </span>Biological neuron<sup><a data-type="noteref" id="idm46263517286152-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517286152" class="totri-footnote">4</a></sup></h6>
</div></figure>

<p>Thus, individual biological neurons seem to behave in a rather simple way, but they are organized in a vast network of billions of neurons, each neuron typically connected to thousands of other neurons. Highly complex computations can be performed by a vast network of fairly simple neurons, much like a complex anthill can emerge from the combined efforts of simple ants. The architecture of biological neural networks (BNN)<sup><a data-type="noteref" id="idm46263517282680-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517282680" class="totri-footnote">5</a></sup> is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, especially in the cerebral cortex (i.e., the outer layer of your brain), as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#biological_neural_network_wikipedia">Figure&nbsp;10-2</a>.</p>

<figure><div id="biological_neural_network_wikipedia" class="figure">
<img src="./Chapter10_files/mls2_1002.png" alt="mls2 1002" width="743" height="238" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1002.png">
<h6><span class="label">Figure 10-2. </span>Multiple layers in a biological neural network (human cortex)<sup><a data-type="noteref" id="idm46263517278920-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517278920" class="totri-footnote">6</a></sup></h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Logical Computations with Neurons"><div class="sect2" id="idm46263517276872">
<h2>Logical Computations with Neurons</h2>

<p>Warren McCulloch and Walter Pitts proposed a very simple model of the biological neuron, which later became known as an <em>artificial neuron</em>: it has one or more binary (on/off) inputs and one binary output. The artificial neuron activates its output when more than a certain number of its inputs are active. McCulloch and Pitts showed that even with such a simplified model it is possible to build a network of artificial neurons that computes any logical proposition you want. To see how such a network works, let’s build a few ANNs that perform various logical computations (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#nn_propositional_logic_diagram">Figure&nbsp;10-3</a>), assuming that a neuron is activated when at least two of its inputs are active.</p>

<figure class="smallereighty"><div id="nn_propositional_logic_diagram" class="figure">
<img src="./Chapter10_files/mls2_1003.png" alt="mls2 1003" width="1439" height="486" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1003.png">
<h6><span class="label">Figure 10-3. </span>ANNs performing simple logical computations</h6>
</div></figure>

<p>Let’s see what these networks do:</p>

<ul>
<li>
<p>The first network on the left is the identity function: if neuron A is activated, then neuron C gets activated as well (since it receives two input signals from neuron A); but if neuron A is off, then neuron C is off as well.</p>
</li>
<li>
<p>The second network performs a logical AND: neuron C is activated only when both neurons A and B are activated (a single input signal is not enough to activate neuron C).</p>
</li>
<li>
<p>The third network performs a logical OR: neuron C gets activated if either neuron A or neuron B is activated (or both).</p>
</li>
<li>
<p>Finally, if we suppose that an input connection can inhibit the neuron’s activity (which is the case with biological neurons), then the fourth network computes a slightly more complex logical proposition: neuron C is activated only if neuron A is active and neuron B is off. If neuron A is active all the time, then you get a logical NOT: neuron C is active when neuron B is off, and vice versa.</p>
</li>
</ul>

<p>You can imagine how these networks can be combined to compute complex logical expressions (see the exercises at the end of the chapter for an example).</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Perceptron"><div class="sect2" id="idm46263517265304">
<h2>The Perceptron</h2>

<p>The <em>Perceptron</em> is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt. It is based on a slightly different artificial neuron (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#artificial_neuron_diagram">Figure&nbsp;10-4</a>) called a <em>threshold logic unit</em> (TLU), or sometimes a <em>linear threshold unit</em> (LTU). The inputs and output are now numbers (instead of binary on/off values), and each input connection is associated with a weight. The TLU computes a weighted sum of its inputs (<em>z</em> = <em>w</em><sub>1</sub> <em>x</em><sub>1</sub> + <em>w</em><sub>2</sub> <em>x</em><sub>2</sub> + ⋯ + <em>w</em><sub><em>n</em></sub> <em>x</em><sub><em>n</em></sub> = <strong>x</strong><sup><em>T</em></sup> <strong>w</strong>), then applies a <em>step function</em> to that sum and outputs the result: <em>h</em><sub><strong>w</strong></sub>(<strong>x</strong>) = step(<em>z</em>), where <em>z</em> = <strong>x</strong><sup><em>T</em></sup> <strong>w</strong>.</p>

<figure class="smallerfiftyfive"><div id="artificial_neuron_diagram" class="figure">
<img src="./Chapter10_files/mls2_1004.png" alt="mls2 1004" width="1119" height="613" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1004.png">
<h6><span class="label">Figure 10-4. </span>Threshold logic unit: an artificial neuron which computes a weighted sum of its inputs then applies a step function</h6>
</div></figure>

<p>The most common step function used in Perceptrons is the <em>Heaviside step function</em> (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#step_functions_equation">Equation 10-1</a>). Sometimes the sign function is used instead.</p>
<div data-type="equation" id="step_functions_equation">
<h5><span class="label">Equation 10-1. </span>Common step functions used in Perceptrons (assuming threshold = 0)</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-119-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mtable displaystyle=&quot;true&quot;&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mo form=&quot;prefix&quot;&gt;heaviside&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced separators=&quot;&quot; open=&quot;{&quot; close=&quot;&quot;&gt;&lt;mtable&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;if&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo&gt;&amp;lt;&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;if&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo&gt;&amp;#x2265;&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;mo form=&quot;prefix&quot;&gt;sgn&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfenced separators=&quot;&quot; open=&quot;{&quot; close=&quot;&quot;&gt;&lt;mtable&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;if&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo&gt;&amp;lt;&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;if&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;if&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo&gt;&amp;gt;&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4777" style="width: 27.818em; display: inline-block;"><span style="display: inline-block; position: relative; width: 26.995em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(-0.203em, 1026.64em, 3.961em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-4778"><span class="mtable" id="MathJax-Span-4779" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 26.687em; height: 0px;"><span style="position: absolute; clip: rect(2.419em, 1012.56em, 5.144em, -1000.01em); top: -4.008em; left: 0em;"><span style="display: inline-block; position: relative; width: 12.753em; height: 0px;"><span style="position: absolute; clip: rect(2.419em, 1012.56em, 5.144em, -1000.01em); top: -4.008em; right: 0em;"><span class="mtd" id="MathJax-Span-4780"><span class="mrow" id="MathJax-Span-4781"><span class="mrow" id="MathJax-Span-4782"><span class="mo" id="MathJax-Span-4783" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">heaviside</span><span class="mrow" id="MathJax-Span-4784"><span class="mo" id="MathJax-Span-4785" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4786" style="font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-4787" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4788" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mfenced" id="MathJax-Span-4789" style="padding-left: 0.26em;"><span class="mo" id="MathJax-Span-4790" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">{</span></span><span class="mtable" id="MathJax-Span-4791" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 4.527em; height: 0px;"><span style="position: absolute; clip: rect(2.522em, 1000.47em, 4.938em, -1000.01em); top: -4.008em; left: 0em;"><span style="display: inline-block; position: relative; width: 0.517em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.47em, 4.167em, -1000.01em); top: -4.676em; left: 0em;"><span class="mtd" id="MathJax-Span-4792"><span class="mrow" id="MathJax-Span-4793"><span class="mn" id="MathJax-Span-4794" style="font-family: MathJax_Main;">0</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1000.42em, 4.167em, -1000.01em); top: -3.237em; left: 0em;"><span class="mtd" id="MathJax-Span-4803"><span class="mrow" id="MathJax-Span-4804"><span class="mn" id="MathJax-Span-4805" style="font-family: MathJax_Main;">1</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.47em, 1003.2em, 5.093em, -1000.01em); top: -4.008em; left: 1.288em;"><span style="display: inline-block; position: relative; width: 3.242em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1003.2em, 4.219em, -1000.01em); top: -4.676em; left: 0em;"><span class="mtd" id="MathJax-Span-4795"><span class="mrow" id="MathJax-Span-4796"><span class="mrow" id="MathJax-Span-4797"><span class="mtext" id="MathJax-Span-4798" style="font-family: MathJax_Main;">if</span><span class="mspace" id="MathJax-Span-4799" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-4800" style="font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-4801" style="font-family: MathJax_Main; padding-left: 0.26em;">&lt;</span><span class="mn" id="MathJax-Span-4802" style="font-family: MathJax_Main; padding-left: 0.26em;">0</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1003.2em, 4.321em, -1000.01em); top: -3.237em; left: 0em;"><span class="mtd" id="MathJax-Span-4806"><span class="mrow" id="MathJax-Span-4807"><span class="mrow" id="MathJax-Span-4808"><span class="mtext" id="MathJax-Span-4809" style="font-family: MathJax_Main;">if</span><span class="mspace" id="MathJax-Span-4810" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-4811" style="font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-4812" style="font-family: MathJax_Main; padding-left: 0.26em;">≥</span><span class="mn" id="MathJax-Span-4813" style="font-family: MathJax_Main; padding-left: 0.26em;">0</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.573em, 1012.97em, 6.738em, -1000.01em); top: -4.882em; left: 13.576em;"><span style="display: inline-block; position: relative; width: 13.165em; height: 0px;"><span style="position: absolute; clip: rect(2.573em, 1012.97em, 6.738em, -1000.01em); top: -4.882em; left: 0em;"><span class="mtd" id="MathJax-Span-4814"><span class="mrow" id="MathJax-Span-4815"><span class="mrow" id="MathJax-Span-4816"><span class="mspace" id="MathJax-Span-4817" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span><span class="mspace" id="MathJax-Span-4818" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-4819" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">sgn</span><span class="mrow" id="MathJax-Span-4820"><span class="mo" id="MathJax-Span-4821" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4822" style="font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-4823" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4824" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mfenced" id="MathJax-Span-4825" style="padding-left: 0.26em;"><span class="mo" id="MathJax-Span-4826" style="vertical-align: 2.162em;"><span style="display: inline-block; position: relative; width: 0.877em; height: 0px;"><span style="position: absolute; font-family: MathJax_Size4; top: -3.134em; left: 0em;">⎧<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Size4; top: -1.026em; left: 0em;">⎩<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Size4; top: -1.848em; left: 0em;">⎨<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Size4; position: absolute; top: -2.877em; left: 0em;">⎪<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Size4; position: absolute; top: -0.974em; left: 0em;">⎪<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mtable" id="MathJax-Span-4827" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 5.35em; height: 0px;"><span style="position: absolute; clip: rect(2.419em, 1001.19em, 6.327em, -1000.01em); top: -4.625em; left: 0em;"><span style="display: inline-block; position: relative; width: 1.288em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1001.19em, 4.27em, -1000.01em); top: -5.396em; left: 0em;"><span class="mtd" id="MathJax-Span-4828"><span class="mrow" id="MathJax-Span-4829"><span class="mrow" id="MathJax-Span-4830"><span class="mo" id="MathJax-Span-4831" style="font-family: MathJax_Main;">−</span><span class="mn" id="MathJax-Span-4832" style="font-family: MathJax_Main;">1</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1000.47em, 4.167em, -1000.01em); top: -3.956em; left: 0em;"><span class="mtd" id="MathJax-Span-4841"><span class="mrow" id="MathJax-Span-4842"><span class="mn" id="MathJax-Span-4843" style="font-family: MathJax_Main;">0</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1001.19em, 4.27em, -1000.01em); top: -2.517em; left: 0em;"><span class="mtd" id="MathJax-Span-4852"><span class="mrow" id="MathJax-Span-4853"><span class="mrow" id="MathJax-Span-4854"><span class="mo" id="MathJax-Span-4855" style="font-family: MathJax_Main;">+</span><span class="mn" id="MathJax-Span-4856" style="font-family: MathJax_Main;">1</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.63em;"></span></span><span style="position: absolute; clip: rect(2.368em, 1003.2em, 6.327em, -1000.01em); top: -4.625em; left: 2.059em;"><span style="display: inline-block; position: relative; width: 3.242em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1003.2em, 4.219em, -1000.01em); top: -5.396em; left: 0em;"><span class="mtd" id="MathJax-Span-4833"><span class="mrow" id="MathJax-Span-4834"><span class="mrow" id="MathJax-Span-4835"><span class="mtext" id="MathJax-Span-4836" style="font-family: MathJax_Main;">if</span><span class="mspace" id="MathJax-Span-4837" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-4838" style="font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-4839" style="font-family: MathJax_Main; padding-left: 0.26em;">&lt;</span><span class="mn" id="MathJax-Span-4840" style="font-family: MathJax_Main; padding-left: 0.26em;">0</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1003.2em, 4.167em, -1000.01em); top: -3.956em; left: 0em;"><span class="mtd" id="MathJax-Span-4844"><span class="mrow" id="MathJax-Span-4845"><span class="mrow" id="MathJax-Span-4846"><span class="mtext" id="MathJax-Span-4847" style="font-family: MathJax_Main;">if</span><span class="mspace" id="MathJax-Span-4848" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-4849" style="font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-4850" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mn" id="MathJax-Span-4851" style="font-family: MathJax_Main; padding-left: 0.26em;">0</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1003.2em, 4.219em, -1000.01em); top: -2.517em; left: 0em;"><span class="mtd" id="MathJax-Span-4857"><span class="mrow" id="MathJax-Span-4858"><span class="mrow" id="MathJax-Span-4859"><span class="mtext" id="MathJax-Span-4860" style="font-family: MathJax_Main;">if</span><span class="mspace" id="MathJax-Span-4861" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-4862" style="font-family: MathJax_Math-italic;">z<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-4863" style="font-family: MathJax_Main; padding-left: 0.26em;">&gt;</span><span class="mn" id="MathJax-Span-4864" style="font-family: MathJax_Main; padding-left: 0.26em;">0</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.63em;"></span></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.887em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.887em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.798em; border-left: 0px solid; width: 0px; height: 4.08em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo form="prefix">heaviside</mo><mrow><mo>(</mo><mi>z</mi><mo>)</mo></mrow><mo>=</mo><mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mn>0</mn></mtd><mtd columnalign="left"><mrow><mtext>if</mtext><mspace width="4.pt"></mspace><mi>z</mi><mo>&lt;</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign="left"><mn>1</mn></mtd><mtd columnalign="left"><mrow><mtext>if</mtext><mspace width="4.pt"></mspace><mi>z</mi><mo>≥</mo><mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></mtd><mtd columnalign="left"><mrow><mspace width="1.em"></mspace><mspace width="1.em"></mspace><mo form="prefix">sgn</mo><mrow><mo>(</mo><mi>z</mi><mo>)</mo></mrow><mo>=</mo><mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mo>-</mo><mn>1</mn></mrow></mtd><mtd columnalign="left"><mrow><mtext>if</mtext><mspace width="4.pt"></mspace><mi>z</mi><mo>&lt;</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn></mtd><mtd columnalign="left"><mrow><mtext>if</mtext><mspace width="4.pt"></mspace><mi>z</mi><mo>=</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mo>+</mo><mn>1</mn></mrow></mtd><mtd columnalign="left"><mrow><mtext>if</mtext><mspace width="4.pt"></mspace><mi>z</mi><mo>&gt;</mo><mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math></span></span></div><script type="math/mml" id="MathJax-Element-119"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mo form="prefix">heaviside</mo>
          <mrow>
            <mo>(</mo>
            <mi>z</mi>
            <mo>)</mo>
          </mrow>
          <mo>=</mo>
          <mfenced separators="" open="{" close="">
            <mtable>
              <mtr>
                <mtd columnalign="left">
                  <mn>0</mn>
                </mtd>
                <mtd columnalign="left">
                  <mrow>
                    <mtext>if</mtext>
                    <mspace width="4.pt"></mspace>
                    <mi>z</mi>
                    <mo>&lt;</mo>
                    <mn>0</mn>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd columnalign="left">
                  <mn>1</mn>
                </mtd>
                <mtd columnalign="left">
                  <mrow>
                    <mtext>if</mtext>
                    <mspace width="4.pt"></mspace>
                    <mi>z</mi>
                    <mo>≥</mo>
                    <mn>0</mn>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
          </mfenced>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mspace width="1.em"></mspace>
          <mspace width="1.em"></mspace>
          <mo form="prefix">sgn</mo>
          <mrow>
            <mo>(</mo>
            <mi>z</mi>
            <mo>)</mo>
          </mrow>
          <mo>=</mo>
          <mfenced separators="" open="{" close="">
            <mtable>
              <mtr>
                <mtd columnalign="left">
                  <mrow>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </mtd>
                <mtd columnalign="left">
                  <mrow>
                    <mtext>if</mtext>
                    <mspace width="4.pt"></mspace>
                    <mi>z</mi>
                    <mo>&lt;</mo>
                    <mn>0</mn>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd columnalign="left">
                  <mn>0</mn>
                </mtd>
                <mtd columnalign="left">
                  <mrow>
                    <mtext>if</mtext>
                    <mspace width="4.pt"></mspace>
                    <mi>z</mi>
                    <mo>=</mo>
                    <mn>0</mn>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd columnalign="left">
                  <mrow>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                </mtd>
                <mtd columnalign="left">
                  <mrow>
                    <mtext>if</mtext>
                    <mspace width="4.pt"></mspace>
                    <mi>z</mi>
                    <mo>&gt;</mo>
                    <mn>0</mn>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></script>
</div>

<p>A single TLU can be used for simple linear binary classification. It computes a linear combination of the inputs, and if the result exceeds a threshold, it outputs the positive class. Otherwise it outputs the negative class (just like a Logistic Regression classifier or a linear SVM). You could, for example, use a single TLU to classify iris flowers based on the petal length and width (also adding an extra bias feature <em>x</em><sub>0</sub> = 1, just like we did in previous chapters). Training a TLU in this case means finding the right values for <em>w</em><sub>0</sub>, <em>w</em><sub>1</sub>, and <em>w</em><sub>2</sub> (the training algorithm is discussed shortly).</p>

<p>A Perceptron is simply composed of a single layer of TLUs,<sup><a data-type="noteref" id="idm46263517197144-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517197144" class="totri-footnote">7</a></sup> with each TLU connected to all the inputs. When all the neurons in a layer are connected to every neuron in the previous layer (i.e., its input neurons), the layer is called a <em>fully connected layer</em>, or a <em>dense layer</em>. The inputs of the Perceptron are fed to special passthrough neurons called <em>input neurons</em>: they output whatever input they are fed. All the input neurons form the <em>input layer</em>. Moreover, an extra bias feature is generally added (<em>x</em><sub>0</sub> = 1): it is typically represented using a special type of neuron called a <em>bias neuron</em>, which outputs 1 all the time. A Perceptron with two inputs and three outputs is represented in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#perceptron_diagram">Figure&nbsp;10-5</a>. This Perceptron can classify instances simultaneously into three different binary classes, which makes it a multioutput classifier.</p>

<figure class="smallersixty"><div id="perceptron_diagram" class="figure">
<img src="./Chapter10_files/mls2_1005.png" alt="mls2 1005" width="1378" height="792" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1005.png">
<h6><span class="label">Figure 10-5. </span>Architecture of a Perceptron with two input neurons, one bias neuron and three output neurons</h6>
</div></figure>

<p>Thanks to the magic of linear algebra, <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#neural_network_layer_equation">Equation 10-2</a> makes it possible to efficiently compute the outputs of a layer of artificial neurons for several instances at once.</p>
<div data-type="equation" id="neural_network_layer_equation">
<h5><span class="label">Equation 10-2. </span>Computing the outputs of a fully connected layer</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-120-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;&amp;#x3D5;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;W&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;b&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4865" style="width: 10.954em; display: inline-block;"><span style="display: inline-block; position: relative; width: 10.645em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.237em, 1010.55em, 2.625em, -1000.01em); top: -2.157em; left: 0em;"><span class="mrow" id="MathJax-Span-4866"><span class="mrow" id="MathJax-Span-4867"><span class="msub" id="MathJax-Span-4868"><span style="display: inline-block; position: relative; width: 2.162em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4869" style="font-family: MathJax_Math-italic;">h</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mrow" id="MathJax-Span-4870"><span class="mi" id="MathJax-Span-4871" style="font-size: 70.7%; font-family: MathJax_Main-bold;">W</span><span class="mo" id="MathJax-Span-4872" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-4873" style="font-size: 70.7%; font-family: MathJax_Main-bold;">b</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-4874" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4875" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4876" style="font-family: MathJax_Main-bold;">X</span><span class="mo" id="MathJax-Span-4877" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4878" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mi" id="MathJax-Span-4879" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">ϕ</span><span class="mrow" id="MathJax-Span-4880" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4881" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4882" style="font-family: MathJax_Main-bold;">X</span><span class="mi" id="MathJax-Span-4883" style="font-family: MathJax_Main-bold;">W</span><span class="mo" id="MathJax-Span-4884" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-4885" style="font-family: MathJax_Main-bold; padding-left: 0.208em;">b</span><span class="mo" id="MathJax-Span-4886" style="font-family: MathJax_Main;">)</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.162em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.368em; border-left: 0px solid; width: 0px; height: 1.168em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>h</mi><mrow><mi mathvariant="bold">W</mi><mo>,</mo><mi mathvariant="bold">b</mi></mrow></msub><mrow><mo>(</mo><mi mathvariant="bold">X</mi><mo>)</mo></mrow><mo>=</mo><mi>ϕ</mi><mrow><mo>(</mo><mi mathvariant="bold">X</mi><mi mathvariant="bold">W</mi><mo>+</mo><mi mathvariant="bold">b</mi><mo>)</mo></mrow></mrow></math></span></span></div><script type="math/mml" id="MathJax-Element-120"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <msub><mi>h</mi> <mrow><mi mathvariant="bold">W</mi><mo>,</mo><mi mathvariant="bold">b</mi></mrow> </msub>
    <mrow>
      <mo>(</mo>
      <mi mathvariant="bold">X</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mi>ϕ</mi>
    <mrow>
      <mo>(</mo>
      <mi mathvariant="bold">X</mi>
      <mi mathvariant="bold">W</mi>
      <mo>+</mo>
      <mi mathvariant="bold">b</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math></script>
</div>

<p>In this equation:</p>

<ul>
<li>
<p>As always, <strong>X</strong> represents the matrix of input features. It has one row per instance and one column per feature.</p>
</li>
<li>
<p>The weight matrix <strong>W</strong> contains all the connection weights except for the ones from the bias neuron. It has one row per input neuron and one column per artificial neuron in the layer.</p>
</li>
<li>
<p>The bias vector <strong>b</strong> contains all the connection weights between the bias neuron and the artificial neurons. It has one bias term per artificial neuron.</p>
</li>
<li>
<p>The function <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-121-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;phi&quot;&gt;&lt;mi&gt;&amp;#x3D5;&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4887" aria-label="phi" style="width: 0.671em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.62em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.288em, 1000.63em, 2.47em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-4888"><span class="mi" id="MathJax-Span-4889" style="font-family: MathJax_Math-italic;">ϕ</span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.262em; border-left: 0px solid; width: 0px; height: 1.009em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="phi"><mi>ϕ</mi></math></span></span><script type="math/mml" id="MathJax-Element-121"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="phi">
  <mi>ϕ</mi>
</math></script> is called the <em>activation function</em>: when the artificial neurons are TLUs, it is a step function (but we will discuss other activation functions shortly).</p>
</li>
</ul>

<p>So, how is a Perceptron trained? The Perceptron training algorithm proposed by Frank Rosenblatt was largely inspired by <em>Hebb’s rule</em>. In his book <em>The Organization of Behavior</em>, published in 1949, Donald Hebb suggested that when a biological neuron triggers another neuron often, the connection between these two neurons grows stronger. Siegrid Löwel later summarized Hebb’s idea in this catchy phrase: “Cells that fire together, wire together”; that is, the connection weight between two neurons tends to increase when they fire simultaneously. This rule later became known as Hebb’s rule (or <em>Hebbian learning</em>). Perceptrons are trained using a variant of this rule that takes into account the error made by the network when it makes a prediction; the Preceptron learning rule reinforces connections that help reduce the error. More specifically, the Perceptron is fed one training instance at a time, and for each instance it makes its predictions. For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction. The rule is shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#perceptron_update_rule">Equation 10-3</a>.</p>
<div data-type="equation" id="perceptron_update_rule">
<h5><span class="label">Equation 10-3. </span>Perceptron learning rule (weight update)</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-122-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mtext&gt;next&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;step&lt;/mtext&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;&amp;#x3B7;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;msub&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4890" style="width: 15.221em; display: inline-block;"><span style="display: inline-block; position: relative; width: 14.758em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.134em, 1014.77em, 2.728em, -1000.01em); top: -2.208em; left: 0em;"><span class="mrow" id="MathJax-Span-4891"><span class="mrow" id="MathJax-Span-4892"><span class="msup" id="MathJax-Span-4893"><span style="display: inline-block; position: relative; width: 5.144em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1001.5em, 4.476em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-4894"><span class="msub" id="MathJax-Span-4895"><span style="display: inline-block; position: relative; width: 1.545em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.68em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4896" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.722em;"><span class="mrow" id="MathJax-Span-4897"><span class="mi" id="MathJax-Span-4898" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-4899" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-4900" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 1.545em;"><span class="mrow" id="MathJax-Span-4901"><span class="mo" id="MathJax-Span-4902" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mtext" id="MathJax-Span-4903" style="font-size: 70.7%; font-family: MathJax_Main;">next</span><span class="mspace" id="MathJax-Span-4904" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-4905" style="font-size: 70.7%; font-family: MathJax_Main;">step</span><span class="mo" id="MathJax-Span-4906" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4907" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="msub" id="MathJax-Span-4908" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 1.545em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.68em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4909" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.722em;"><span class="mrow" id="MathJax-Span-4910"><span class="mi" id="MathJax-Span-4911" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-4912" style="font-size: 70.7%; font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-4913" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4914" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mi" id="MathJax-Span-4915" style="font-family: MathJax_Math-italic; padding-left: 0.208em;">η<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mrow" id="MathJax-Span-4916" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4917" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="msub" id="MathJax-Span-4918"><span style="display: inline-block; position: relative; width: 0.877em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4919" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.517em;"><span class="mi" id="MathJax-Span-4920" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4921" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="msub" id="MathJax-Span-4922" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.52em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mover" id="MathJax-Span-4923"><span style="display: inline-block; position: relative; width: 0.568em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.373em, -1000.01em); top: -4.008em; left: 0.003em;"><span class="mi" id="MathJax-Span-4924" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1000.37em, 3.653em, -1000.01em); top: -4.059em; left: 0.105em;"><span class="mo" id="MathJax-Span-4925" style=""><span style="font-family: MathJax_Main;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.751em; left: 0.568em;"><span class="mi" id="MathJax-Span-4926" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4927" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span><span class="msub" id="MathJax-Span-4928" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 0.877em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.52em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4929" style="font-family: MathJax_Math-italic;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.568em;"><span class="mi" id="MathJax-Span-4930" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.213em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.421em; border-left: 0px solid; width: 0px; height: 1.485em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msup><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow><mrow><mo>(</mo><mtext>next</mtext><mspace width="4.pt"></mspace><mtext>step</mtext><mo>)</mo></mrow></msup><mo>=</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>+</mo><mi>η</mi><mrow><mo>(</mo><msub><mi>y</mi><mi>j</mi></msub><mo>-</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>j</mi></msub><mo>)</mo></mrow><msub><mi>x</mi><mi>i</mi></msub></mrow></math></span></span></div><script type="math/mml" id="MathJax-Element-122"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <msup><mrow><msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow> </msub></mrow> <mrow><mo>(</mo><mtext>next</mtext><mspace width="4.pt"></mspace><mtext>step</mtext><mo>)</mo></mrow> </msup>
    <mo>=</mo>
    <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow> </msub>
    <mo>+</mo>
    <mi>η</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>y</mi> <mi>j</mi> </msub>
      <mo>-</mo>
      <msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>j</mi> </msub>
      <mo>)</mo>
    </mrow>
    <msub><mi>x</mi> <mi>i</mi> </msub>
  </mrow>
</math></script>
</div>

<p>In this equation:</p>

<ul>
<li>
<p><em>w</em><sub><em>i</em>, <em>j</em></sub> is the connection weight between the <em>i</em><sup>th</sup> input neuron and the <em>j</em><sup>th</sup> output neuron.</p>
</li>
<li>
<p><em>x</em><sub><em>i</em></sub> is the <em>i</em><sup>th</sup> input value of the current training instance.</p>
</li>
<li>
<p><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-123-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;ModifyingAbove y With caret Subscript j&quot;&gt;&lt;msub&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4931" aria-label="ModifyingAbove y With caret Subscript j" style="width: 0.979em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.237em, 1000.94em, 2.625em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-4932"><span class="msub" id="MathJax-Span-4933"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.52em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mover" id="MathJax-Span-4934"><span style="display: inline-block; position: relative; width: 0.568em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.373em, -1000.01em); top: -4.008em; left: 0.003em;"><span class="mi" id="MathJax-Span-4935" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1000.37em, 3.653em, -1000.01em); top: -4.059em; left: 0.105em;"><span class="mo" id="MathJax-Span-4936" style=""><span style="font-family: MathJax_Main;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.751em; left: 0.568em;"><span class="mi" id="MathJax-Span-4937" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.421em; border-left: 0px solid; width: 0px; height: 1.274em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="ModifyingAbove y With caret Subscript j"><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>j</mi></msub></math></span></span><script type="math/mml" id="MathJax-Element-123"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="ModifyingAbove y With caret Subscript j">
  <msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>j</mi> </msub>
</math></script> is the output of the <em>j</em><sup>th</sup> output neuron for the current training instance.</p>
</li>
<li>
<p><em>y</em><sub><em>j</em></sub> is the target output of the <em>j</em><sup>th</sup> output neuron for the current training instance.</p>
</li>
<li>
<p><em>η</em> is the learning rate.</p>
</li>
</ul>

<p>The decision boundary of each output neuron is linear, so Perceptrons are incapable of learning complex patterns (just like Logistic Regression classifiers). However, if the training instances are linearly separable, Rosenblatt demonstrated that this algorithm would converge to a solution.<sup><a data-type="noteref" id="idm46263517132120-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517132120" class="totri-footnote">8</a></sup> This is called the <em>Perceptron convergence theorem</em>.</p>

<p>Scikit-Learn provides a <code>Perceptron</code> class that implements a single TLU network. It can be used pretty much as you would expect—for example, on the iris dataset (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>):</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_iris</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">Perceptron</code>

<code class="n">iris</code> <code class="o">=</code> <code class="n">load_iris</code><code class="p">()</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code><code class="p">[:,</code> <code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">)]</code>  <code class="c1"># petal length, petal width</code>
<code class="n">y</code> <code class="o">=</code> <code class="p">(</code><code class="n">iris</code><code class="o">.</code><code class="n">target</code> <code class="o">==</code> <code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">int</code><code class="p">)</code>  <code class="c1"># Iris Setosa?</code>

<code class="n">per_clf</code> <code class="o">=</code> <code class="n">Perceptron</code><code class="p">()</code>
<code class="n">per_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>

<code class="n">y_pred</code> <code class="o">=</code> <code class="n">per_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">([[</code><code class="mi">2</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">]])</code></pre>

<p>You may have noticed that the Perceptron learning algorithm strongly resembles Stochastic Gradient Descent. In fact, Scikit-Learn’s <code>Perceptron</code> class is equivalent to using an <code>SGDClassifier</code> with the following hyperparameters: <code>loss="perceptron"</code>, <code>learning_rate="constant"</code>, <code>eta0=1</code> (the learning rate), and <code>penalty=None</code> (no regularization).</p>

<p>Note that contrary to Logistic Regression classifiers, Perceptrons do not output a class probability; rather, they make predictions based on a hard threshold. This is one reason to prefer Logistic Regression over Perceptrons.</p>

<p>In their 1969 monograph <em>Perceptrons</em>, Marvin Minsky and Seymour Papert highlighted a number of serious weaknesses of Perceptrons, in particular the fact that they are incapable of solving some trivial problems (e.g., the <em>Exclusive OR</em> (XOR) classification problem; see the left side of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#xor_diagram">Figure&nbsp;10-6</a>). This is true of any other linear classification model (such as Logistic Regression classifiers), but researchers had expected much more from Perceptrons, and some were so disappointed that they dropped neural networks altogether in favor of higher-level problems such as logic, problem solving, and search.</p>

<p>It turns out that some of the limitations of Perceptrons can be eliminated by stacking multiple Perceptrons. The resulting ANN is called a <em>Multilayer Perceptron</em> (MLP). In particular, an MLP can solve the XOR problem, as you can verify by computing the output of the MLP represented on the right of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#xor_diagram">Figure&nbsp;10-6</a>: with inputs (0, 0) or (1, 1), the network outputs 0; and with inputs (0, 1) or (1, 0), it outputs 1. All connections have a weight equal to 1, except the four connections where the weight is shown. Try verifying that this network indeed solves the XOR problem!</p>

<figure class="smallersixty"><div id="xor_diagram" class="figure">
<img src="./Chapter10_files/mls2_1006.png" alt="mls2 1006" width="1276" height="775" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1006.png">
<h6><span class="label">Figure 10-6. </span>XOR classification problem and an MLP that solves it</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Multilayer Perceptron and Backpropagation"><div class="sect2" id="idm46263517264360">
<h2>Multilayer Perceptron and Backpropagation</h2>

<p>An MLP is composed of one (passthrough) <em>input layer</em>, one or more layers of TLUs, called <em>hidden layers</em>, and one final layer of TLUs called the <em>output layer</em> (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#mlp_diagram">Figure&nbsp;10-7</a>). The layers close to the input layer are usually called the lower layers, and the ones close to the outputs are usually called the upper layers. Every layer except the output layer includes a bias neuron and is fully connected to the next layer.</p>

<figure class="smallersixty"><div id="mlp_diagram" class="figure">
<img src="./Chapter10_files/mls2_1007.png" alt="mls2 1007" width="1301" height="926" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1007.png">
<h6><span class="label">Figure 10-7. </span>Architecture of a multilayer Perceptron with two inputs, one hidden layer of four neurons, and three output neurons (the bias neurons are shown here, but usually they are implicit)</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The signal flows only in one direction (from the inputs to the outputs), so this architecture is an example of a <em>feedforward neural network</em> (FNN).</p>
</div>

<p>When an ANN contains a deep stack of hidden layers,<sup><a data-type="noteref" id="idm46263517042216-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517042216" class="totri-footnote">9</a></sup> it is called a <em>deep neural network</em> (DNN). The field of Deep Learning studies DNNs, and more generally models containing deep stacks of computations. Even so, many people talk about Deep Learning whenever neural networks are involved (even shallow ones).</p>

<p>For many years researchers struggled to find a way to train MLPs, without success. But in 1986, David Rumelhart, Geoffrey Hinton and Ronald Williams published a <a href="https://homl.info/44">groundbreaking paper</a><sup><a data-type="noteref" id="idm46263517039512-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517039512">10</a></sup> that introduced the <em>backpropagation</em> training algorithm, which is still used today. In short, it is Gradient Descent (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>) using an efficient technique for computing the gradients automatically:<sup><a data-type="noteref" id="idm46263517035800-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517035800">11</a></sup> in just two passes through the network (one forward, one backward), the backpropagation algorithm is able to compute the gradient of the network’s error with regard to every single model parameter. In other words, it can find out how each connection weight and each bias term should be tweaked in order to reduce the error. Once it has these gradients, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Automatically computing gradients is called <em>automatic differentiation</em>, or <em>autodiff</em>. There are various autodiff techniques, with different pros and cons. The one used by backpropagation is called <em>reverse-mode autodiff</em>. It is fast and precise, and is well suited when the function to differentiate has many variables (e.g., connection weights) and few outputs (e.g., one loss). If you want to learn more about autodiff, check out <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/app04.html#autodiff_appendix">Appendix&nbsp;D</a>.</p>
</div>

<p>Let’s run through this algorithm in a bit more detail:</p>

<ul>
<li>
<p>It handles one mini-batch at a time (for example containing 32 instances each), and it goes through the full training set multiple times. Each pass is called an <em>epoch</em>.</p>
</li>
<li>
<p>Each mini-batch is passed to the network’s input layer, which sends it to the first hidden layer. The algorithm then computes the output of all the neurons in this layer (for every instance in the mini-batch). The result is passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the output layer. This is the <em>forward pass</em>: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.</p>
</li>
<li>
<p>Next, the algorithm measures the network’s output error (i.e., it uses a loss function that compares the desired output and the actual output of the network, and returns some measure of the error).</p>
</li>
<li>
<p>Then it computes how much each output connection contributed to the error. This is done analytically by applying the <em>chain rule</em> (perhaps the most fundamental rule in calculus), which makes this step fast and precise.</p>
</li>
<li>
<p>The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain rule—and so on until the algorithm reaches the input layer. As we explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network (hence the name of the algorithm).</p>
</li>
<li>
<p>Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.</p>
</li>
</ul>

<p>This algorithm is so important that it’s worth summarizing it again: for each training instance, the backpropagation algorithm first makes a prediction (forward pass), measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), and finally tweaks the connection weights to reduce the error (Gradient Descent step).</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>It is important to initialize all the hidden layers’ connection weights randomly, or else training will fail. For example, if you initialize all weights and biases to zero, then all neurons in a given layer will be perfectly identical, and thus backpropagation will affect them in exactly the same way, so they will remain identical. In other words, despite having hundreds of neurons per layer, your model will act as if it had only one neuron per layer: it won’t be too smart. If instead you randomly initialize the weights, you <em>break the symmetry</em> and allow backpropagation to train a diverse team of neurons.</p>
</div>

<p>In order for this algorithm to work properly, its authors made a key change to the MLP’s architecture: they replaced the step function with the logistic function (sigmoid function), <em>σ</em>(<em>z</em>) = 1 / (1 + exp(–<em>z</em>)). This was essential because the step function contains only flat segments, so there is no gradient to work with (Gradient Descent cannot move on a flat surface), while the logistic function has a well-defined nonzero derivative everywhere, allowing Gradient Descent to make some progress at every step. In fact, the backpropagation algorithm works well with many other <em>activation functions</em>, not just the logistic function. Here are two other popular activation functions:</p>
<dl>
<dt>The <em>hyperbolic tangent</em> function: tanh(<em>z</em>) = 2<em>σ</em>(2<em>z</em>) – 1</dt>
<dd>
<p>Just like the logistic function, it is S-shaped, continuous, and differentiable, but its output value ranges from –1 to 1 (instead of 0 to 1 in the case of the logistic function). That range tends to make each layer’s output more or less centered around 0 at the beginning of training, which often helps speed up convergence.</p>
</dd>
<dt>The Rectified Linear Unit function: ReLU(<em>z</em>) = max(0, <em>z</em>)</dt>
<dd>
<p>It is continuous but unfortunately not differentiable at <em>z</em> = 0 (the slope changes abruptly, which can make Gradient Descent bounce around), and its derivative is 0 for <em>z</em> &lt; 0. In practice, however, it works very well and has the advantage of being fast to compute, so it has become the default.<sup><a data-type="noteref" id="idm46263517009000-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517009000">12</a></sup> Most importantly, the fact that it does not have a maximum output value helps reduce some issues during Gradient Descent (we will come back to this in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>).</p>
</dd>
</dl>

<p>These popular activation functions and their derivatives are represented in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#activation_functions_plot">Figure&nbsp;10-8</a>. But wait! Why do we need activation functions in the first place? Well, if you chain several linear transformations, all you get is a linear transformation. For example, say f(<em>x</em>) = 2<em>x</em> + 3 and g(<em>x</em>) = 5_x_ – 1, then chaining these two linear functions gives you another linear function: f(g(<em>x</em>)) = 2(5<em>x</em> – 1) + 3 = 10<em>x</em> + 1. So if you don’t have some nonlinearity between layers, then even a deep stack of layers is equivalent to a single layer, and you can’t solve very complex problems with that. Conversely, a large enough DNN with nonlinear activations can theoretically approximate any continuous function.</p>

<figure><div id="activation_functions_plot" class="figure">
<img src="./Chapter10_files/mls2_1008.png" alt="mls2 1008" width="1441" height="493" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1008.png">
<h6><span class="label">Figure 10-8. </span>Activation functions and their derivatives</h6>
</div></figure>

<p>OK! You know where neural nets came from, what their architecture is, and how to compute their outputs. You also learned about the backpropagation algorithm. But what exactly can you do with them?</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Regression MLPs"><div class="sect2" id="idm46263517050504">
<h2>Regression MLPs</h2>

<p>First, MLPs can be used for regression tasks. If you want to predict a single value (e.g., the price of a house, given many of its features), then you just need a single output neuron: its output is the predicted value. For multivariate regression (i.e., to predict multiple values at once), you need one output neuron per output dimension. For example, to locate the center of an object on an image, you need to predict 2D coordinates, so you need two output neurons. If you also want to place a bounding box around the object, then you need two more numbers: the width and the height of the object. So, you end up with four output neurons.</p>

<p>In general, when building an MLP for regression, you do not want to use any activation function for the output neurons, so they are free to output any range of values. If you want to guarantee that the output will always be positive, then you can use the ReLU activation function in the output layer. Alternatively, you can use the <em>softplus</em> activation function, which is a smooth variant of ReLU: softplus(<em>z</em>) = log(1 + exp(<em>z</em>)). It is close to 0 when <em>z</em> is negative, and close to <em>z</em> when <em>z</em> is positive. Finally, if you want to guarantee that the predictions will fall within a given range of values, then you can use the logistic function or the hyperbolic tangent, and then scale the labels to the appropriate range: 0 to 1 for the logistic function and –1 to 1 for the hyperbolic tangent.</p>

<p>The loss function to use during training is typically the mean squared error, but if you have a lot of outliers in the training set, you may prefer to use the mean absolute error instead. Alternatively, you can use the Huber loss, which is a combination of both.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The Huber loss is quadratic when the error is smaller than a threshold δ (typically 1) but linear when the error is larger than δ. The linear part makes it less sensitive to outliers than the mean squared error, and the quadratic part allows it to converge faster and be more precise than the mean absolute error.</p>
</div>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#regression_mlp_architecture">Table&nbsp;10-1</a> summarizes the typical architecture of a regression MLP.</p>
<table id="regression_mlp_architecture">
<caption><span class="label">Table 10-1. </span>Typical regression MLP architecture</caption>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Typical value</th>
</tr>
</thead>
<tbody>
<tr>
<td><p># input neurons</p></td>
<td><p>One per input feature (e.g., 28 x 28 = 784 for MNIST)</p></td>
</tr>
<tr>
<td><p># hidden layers</p></td>
<td><p>Depends on the problem, but typically 1 to 5.</p></td>
</tr>
<tr>
<td><p># neurons per hidden layer</p></td>
<td><p>Depends on the problem, but typically 10 to 100.</p></td>
</tr>
<tr>
<td><p># output neurons</p></td>
<td><p>1 per prediction dimension</p></td>
</tr>
<tr>
<td><p>Hidden activation</p></td>
<td><p>ReLU (or SELU, see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>)</p></td>
</tr>
<tr>
<td><p>Output activation</p></td>
<td><p>None or ReLU/Softplus (if positive outputs) or Logistic/Tanh (if bounded outputs)</p></td>
</tr>
<tr>
<td><p>Loss function</p></td>
<td><p>MSE or MAE/Huber (if outliers)</p></td>
</tr>
</tbody>
</table>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Classification MLPs"><div class="sect2" id="idm46263516935176">
<h2>Classification MLPs</h2>

<p>MLPs can also be used for classification tasks. For a binary classification problem, you just need a single output neuron using the logistic activation function: the output will be a number between 0 and 1, which you can interpret as the estimated probability of the positive class. The estimated probability of the negative class is equal to one minus that number.</p>

<p>MLPs can also easily handle multilabel binary classification tasks (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch03.html#classification_chapter">Chapter&nbsp;3</a>). For example, you could have an email classification system that predicts whether each incoming email is ham or spam, and simultaneously predicts whether it is an urgent or nonurgent email. In this case, you would need two output neurons, both using the logistic activation function: the first would output the probability that the email is spam, and the second would output the probability that it is urgent. More generally, you would dedicate one output neuron for each positive class. Note that the output probabilities do not necessarily add up to 1. This lets the model output any combination of labels: you can have nonurgent ham, urgent ham, nonurgent spam, and perhaps even urgent spam (although that would probably be an error).</p>

<p>If each instance can belong only to a single class, out of three or more possible classes (e.g., classes 0 through 9 for digit image classification), then you need to have one output neuron per class, and you should use the softmax activation function for the whole output layer (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#fnn_for_classification_diagram">Figure&nbsp;10-9</a>). The softmax function (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>) will ensure that all the estimated probabilities are between 0 and 1 and that they add up to 1 (which is required if the classes are exclusive). This is called multiclass classification.</p>

<figure class="smallerseventy"><div id="fnn_for_classification_diagram" class="figure">
<img src="./Chapter10_files/mls2_1009.png" alt="mls2 1009" width="1439" height="1011" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1009.png">
<h6><span class="label">Figure 10-9. </span>A modern MLP (including ReLU and softmax) for classification</h6>
</div></figure>

<p>Regarding the loss function, since we are predicting probability distributions, the cross-entropy loss (also called the log loss, see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>) is generally a good choice.</p>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#classification_mlp_architecture">Table&nbsp;10-2</a> summarizes the typical architecture of a classification MLP.</p>
<table id="classification_mlp_architecture">
<caption><span class="label">Table 10-2. </span>Typical classification MLP architecture</caption>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Binary classification</th>
<th>Multilabel binary classification</th>
<th>Multiclass classification</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Input and hidden layers</p></td>
<td><p>Same as regression</p></td>
<td><p>Same as regression</p></td>
<td><p>Same as regression</p></td>
</tr>
<tr>
<td><p># output neurons</p></td>
<td><p>1</p></td>
<td><p>1 per label</p></td>
<td><p>1 per class</p></td>
</tr>
<tr>
<td><p>Output layer activation</p></td>
<td><p>Logistic</p></td>
<td><p>Logistic</p></td>
<td><p>Softmax</p></td>
</tr>
<tr>
<td><p>Loss function</p></td>
<td><p>Cross-entropy</p></td>
<td><p>Cross-entropy</p></td>
<td><p>Cross-entropy</p></td>
</tr>
</tbody>
</table>
<div data-type="tip"><h6>Tip</h6>
<p>Before we go on, I recommend you go through exercise 1, at the end of this chapter. You will play with various neural network architectures and visualize their outputs using the <em>TensorFlow Playground</em>. This will be very useful to better understand MLPs, including the effects of all the hyperparameters (number of layers and neurons, activation functions, and more).</p>
</div>

<p>Now you have all the concepts you need to start implementing MLPs with Keras!</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Implementing MLPs with Keras"><div class="sect1" id="idm46263516934520">
<h1>Implementing MLPs with Keras</h1>

<p>Keras is a high-level Deep Learning API that allows you to easily build, train, evaluate, and execute all sorts of neural networks. Its documentation (or specification) is available at <a href="https://keras.io/"><em class="hyperlink">https://keras.io/</em></a>. The <a href="https://github.com/keras-team/keras">reference implementation</a> is called Keras as well. It was developed by François Chollet as part of a research project<sup><a data-type="noteref" id="idm46263516902424-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263516902424">13</a></sup> and was released as an open source project in March 2015. It quickly gained popularity, owing to its ease-of-use, flexibility, and beautiful design. To perform the heavy computations required by neural networks, this reference implementation relies on a computation backend. At present, you can choose from three popular open source Deep Learning libraries: TensorFlow, Microsoft Cognitive Toolkit (CNTK), and Theano. Therefore, to avoid any confusion, we will refer to this reference implementation as <em>multibackend Keras</em>.</p>

<p>Moreover, since late 2016, other implementations have been released. You can now run Keras on Apache MXNet, Apple’s Core ML, Javascript or Typescript (to run Keras code in a web browser), and PlaidML (which can run on all sorts of GPU devices, not just Nvidia). Moreover, TensorFlow itself now comes bundled with its own Keras implementation, tf.keras. It only supports TensorFlow as the backend, but it has the advantage of offering some very useful extra features (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#keras_implementations_diagram">Figure&nbsp;10-10</a>): for example, it supports TensorFlow’s Data API, which makes it easy to load and preprocess data efficiently. For this reason, we will use tf.keras in this book. However, in this chapter we will not use any of the TensorFlow-specific features, so the code should run fine on other Keras implementations as well (at least in Python), with only minor modifications, such as changing the imports.</p>

<figure><div id="keras_implementations_diagram" class="figure">
<img src="./Chapter10_files/mls2_1010.png" alt="mls2 1010" width="1440" height="542" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1010.png">
<h6><span class="label">Figure 10-10. </span>Two implementations of the Keras API: the multibackend Keras (left) and tf.keras (right)</h6>
</div></figure>

<p>The most popular Deep Learning library, after Keras and TensorFlow, is Facebook’s <a href="https://pytorch.org/">PyTorch</a> library. The good news is that its API is quite similar to Keras (in part because both APIs were inspired by Scikit-Learn and <a href="https://chainer.org/">Chainer</a>), so once you know Keras, it is not difficult to switch to PyTorch, if you ever want to. PyTorch’s popularity grew exponentially in 2018, largely thanks to its simplicity and excellent documentation, which were not TensorFlow&nbsp;1.x’s main strengths. However, TensorFlow&nbsp;2 is now arguably just as simple as PyTorch, as it has adopted Keras as its official high-level API and has greatly simplified and cleaned up the rest of the API. Plus the documentation has been completely reorganized, and it is much easier to find what you need now. Similarly, PyTorch’s main weaknesses (e.g., limited portability and no computation graph analysis) have been largely addressed in PyTorch&nbsp;1.0. A healthy competition is beneficial to everyone.</p>

<p>All right, it’s time to code! As tf.keras is bundled with TensorFlow, let’s start by installing TensorFlow.</p>








<section data-type="sect2" data-pdf-bookmark="Installing TensorFlow 2"><div class="sect2" id="idm46263516893176">
<h2>Installing TensorFlow&nbsp;2</h2>

<p>Assuming you installed Jupyter and Scikit-Learn by following the installation instructions in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch02.html#project_chapter">Chapter&nbsp;2</a>, use pip to install TensorFlow. If you created an isolated environment using virtualenv, you first need to activate it:</p>
<pre data-type="programlisting" data-code-language="shell-session"><code class="go">$ </code><strong><code class="go">cd $ML_PATH</code></strong><code class="go">              # Your ML working directory (e.g., $HOME/ml)
</code><code class="go">$ </code><strong><code class="go">source my_env/bin/activate</code></strong><code class="go">  # on Linux or MacOSX
</code><code class="go">$ </code><strong><code class="go">.\my_env\Scripts\activate</code></strong><code class="go">   # on Windows
</code></pre>

<p>Next, install TensorFlow&nbsp;2 (if you are not using a virtualenv, you will need administrator rights, or to add the <code>--user</code> option):</p>
<pre data-type="programlisting" data-code-language="shell-session"><code class="go">$ </code><strong><code class="go">python3 -m pip install --upgrade tensorflow</code></strong><code class="go">
</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>For GPU support, at the time of this writing you need to install <code>tensorflow-gpu</code> instead of <code>tensorflow</code>, but the TensorFlow team is working on having a single library that will support both CPU-only and GPU-equipped systems. You will still need to install extra libraries for GPU support (see <a href="https://tensorflow.org/install"><em class="hyperlink">https://tensorflow.org/install</em></a> for more details). We will look at GPUs in more depth in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#deployment_chapter">Chapter&nbsp;19</a>.</p>
</div>

<p>To test your installation, open a Python shell or a Jupyter notebook, then import TensorFlow and tf.keras, and print their versions:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">tensorflow</code> <code class="kn">as</code> <code class="nn">tf</code>
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">tensorflow</code> <code class="kn">import</code> <code class="n">keras</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">tf</code><code class="o">.</code><code class="n">__version__</code>
<code class="go">'2.0.0'</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">keras</code><code class="o">.</code><code class="n">__version__</code>
<code class="go">'2.2.4-tf'</code></pre>

<p>The second version is the version of the Keras API implemented by tf.keras. Note that it ends with <code>-tf</code>, highlighting the fact that tf.keras implements the Keras API, plus some extra TensorFlow-specific features.</p>

<p>Now let’s use tf.keras! Let’s start by building a simple image classifier.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Building an Image Classifier Using the Sequential API"><div class="sect2" id="idm46263516892552">
<h2>Building an Image Classifier Using the Sequential API</h2>

<p>First, we need to load a dataset. We will tackle <em>Fashion MNIST</em>, which is a drop-in replacement of MNIST (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch03.html#classification_chapter">Chapter&nbsp;3</a>). It has the exact same format as MNIST (70,000 grayscale images of 28 × 28 pixels each, with 10 classes), but the images represent fashion items rather than handwritten digits, so each class is more diverse, and the problem turns out to be significantly more challenging than MNIST. For example, a simple linear model reaches about 92% accuracy on MNIST, but only about 83% on Fashion MNIST.</p>










<section data-type="sect3" data-pdf-bookmark="Using Keras to load the dataset"><div class="sect3" id="idm46263516811704">
<h3>Using Keras to load the dataset</h3>

<p>Keras provides some utility functions to fetch and load common datasets, including MNIST, Fashion MNIST, and the original California housing dataset. Let’s load Fashion MNIST:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">fashion_mnist</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">datasets</code><code class="o">.</code><code class="n">fashion_mnist</code>
<code class="p">(</code><code class="n">X_train_full</code><code class="p">,</code> <code class="n">y_train_full</code><code class="p">),</code> <code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code> <code class="o">=</code> <code class="n">fashion_mnist</code><code class="o">.</code><code class="n">load_data</code><code class="p">()</code></pre>

<p>When loading MNIST or Fashion MNIST using Keras rather than Scikit-Learn, one important difference is that every image is represented as a 28 × 28 array rather than a 1D array of size 784. Moreover, the pixel intensities are represented as integers (from 0 to 255) rather than floats (from 0.0 to 255.0). Here is the shape and data type of the training set:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_train_full</code><code class="o">.</code><code class="n">shape</code>
<code class="go">(60000, 28, 28)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_train_full</code><code class="o">.</code><code class="n">dtype</code>
<code class="go">dtype('uint8')</code></pre>

<p>Note that the dataset is already split into a training set and a test set, but there is no validation set, so let’s create one. Moreover, since we are going to train the neural network using Gradient Descent, we must scale the input features. For simplicity, we scale the pixel intensities down to the 0-1 range by dividing them by 255.0 (this also converts them to floats):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">X_valid</code><code class="p">,</code> <code class="n">X_train</code> <code class="o">=</code> <code class="n">X_train_full</code><code class="p">[:</code><code class="mi">5000</code><code class="p">]</code> <code class="o">/</code> <code class="mf">255.0</code><code class="p">,</code> <code class="n">X_train_full</code><code class="p">[</code><code class="mi">5000</code><code class="p">:]</code> <code class="o">/</code> <code class="mf">255.0</code>
<code class="n">y_valid</code><code class="p">,</code> <code class="n">y_train</code> <code class="o">=</code> <code class="n">y_train_full</code><code class="p">[:</code><code class="mi">5000</code><code class="p">],</code> <code class="n">y_train_full</code><code class="p">[</code><code class="mi">5000</code><code class="p">:]</code></pre>

<p>With MNIST, when the label is equal to 5, it means that the image represents the handwritten digit 5. Easy. For Fashion MNIST, however, we need the list of class names to know what we are dealing with:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">class_names</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"T-shirt/top"</code><code class="p">,</code> <code class="s2">"Trouser"</code><code class="p">,</code> <code class="s2">"Pullover"</code><code class="p">,</code> <code class="s2">"Dress"</code><code class="p">,</code> <code class="s2">"Coat"</code><code class="p">,</code>
               <code class="s2">"Sandal"</code><code class="p">,</code> <code class="s2">"Shirt"</code><code class="p">,</code> <code class="s2">"Sneaker"</code><code class="p">,</code> <code class="s2">"Bag"</code><code class="p">,</code> <code class="s2">"Ankle boot"</code><code class="p">]</code></pre>

<p>For example, the first image in the training set represents a coat:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">class_names</code><code class="p">[</code><code class="n">y_train</code><code class="p">[</code><code class="mi">0</code><code class="p">]]</code>
<code class="go">'Coat'</code></pre>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#fashion_mnist_plot">Figure&nbsp;10-11</a> shows samples from the Fashion MNIST dataset.</p>

<figure class="smallerseventy"><div id="fashion_mnist_plot" class="figure">
<img src="./Chapter10_files/mls2_1011.png" alt="mls2 1011" width="1439" height="595" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1011.png">
<h6><span class="label">Figure 10-11. </span>Samples from Fashion MNIST</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Creating the model using the Sequential API"><div class="sect3" id="idm46263516811112">
<h3>Creating the model using the Sequential API</h3>

<p>Now let’s build the neural network! Here is a classification MLP with two hidden layers:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(</code><code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">]))</code>
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">300</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">))</code>
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">))</code>
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"softmax"</code><code class="p">))</code></pre>

<p>Let’s go through this code line by line:</p>

<ul>
<li>
<p>The first line creates a <code>Sequential</code> model. This is the simplest kind of Keras model for neural networks that are just composed of a single stack of layers connected sequentially. This is called the Sequential API.</p>
</li>
<li>
<p>Next, we build the first layer and add it to the model. It is a <code>Flatten</code> layer whose role is to convert each input image into a 1D array: if it receives input data <code>X</code>, it computes <code>X.reshape(-1, 1)</code>. This layer does not have any parameters; it is just there to do some simple preprocessing. Since it is the first layer in the model, you should specify the <code>input_shape</code>, which doesn’t include the batch size, only the shape of the instances. Alternatively, you could add a <code>keras.layers.InputLayer</code> as the first layer, setting <code>input_shape=[28,28]</code>.</p>
</li>
<li>
<p>Next we add a <code>Dense</code> hidden layer with 300 neurons. It will use the ReLU activation function. Each <code>Dense</code> layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of bias terms (one per neuron). When it receives some input data, it computes <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#neural_network_layer_equation">Equation 10-2</a>.</p>
</li>
<li>
<p>Then we add a second <code>Dense</code> hidden layer with 100 neurons, also using the ReLU activation function.</p>
</li>
<li>
<p>Finally, we add a <code>Dense</code> output layer with 10 neurons (one per class), using the softmax activation function (because the classes are exclusive).</p>
</li>
</ul>
<div data-type="tip"><h6>Tip</h6>
<p>Specifying <code>activation="relu"</code> is equivalent to <code>activation=keras.activations.relu</code>. Other activation functions are available in the <code>keras.activations</code> package, we will use many of them in this book. See <a href="https://keras.io/activations/"><em class="hyperlink">https://keras.io/activations/</em></a> for the full list.</p>
</div>

<p>Instead of adding the layers one by one as we just did, you can pass a list of layers when creating the <code>Sequential</code> model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Flatten</code><code class="p">(</code><code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">300</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"softmax"</code><code class="p">)</code>
<code class="p">])</code></pre>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46263516471944">
<h5>Using Code Examples from keras.io</h5>
<p>Code examples documented on keras.io will work fine with tf.keras, but you need to change the imports. For example, consider this keras.io code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">keras.layers</code> <code class="kn">import</code> <code class="n">Dense</code>
<code class="n">output_layer</code> <code class="o">=</code> <code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code></pre>

<p>You must change the imports like this:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tensorflow.keras.layers</code> <code class="kn">import</code> <code class="n">Dense</code>
<code class="n">output_layer</code> <code class="o">=</code> <code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code></pre>

<p>Or simply use full paths, if you prefer:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tensorflow</code> <code class="kn">import</code> <code class="n">keras</code>
<code class="n">output_layer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code></pre>

<p>This approach is more verbose, but I use it in this book so you can easily see which packages to use, and to avoid confusion between standard classes and custom classes. In production code, I prefer the previous approach. Many people also use <code>from tensorflow.keras import layers</code> followed by <code>layers.Dense(10)</code>.</p>
</div></aside>

<p>The model’s <code>summary()</code> method displays all the model’s layers,<sup><a data-type="noteref" id="idm46263516361496-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263516361496">14</a></sup> including each layer’s name (which is automatically generated unless you set it when creating the layer), its output shape (<code>None</code> means the batch size can be anything), and its number of parameters. The summary ends with the total number of parameters, including trainable and nontrainable parameters. Here we only have trainable parameters (we will see examples of non-trainable parameters in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>):</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">model</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code>
<code class="go">Model: "sequential"</code>
<code class="go">_________________________________________________________________</code>
<code class="go">Layer (type)                 Output Shape              Param #</code>
<code class="go">=================================================================</code>
<code class="go">flatten (Flatten)            (None, 784)               0</code>
<code class="go">_________________________________________________________________</code>
<code class="go">dense (Dense)                (None, 300)               235500</code>
<code class="go">_________________________________________________________________</code>
<code class="go">dense_1 (Dense)              (None, 100)               30100</code>
<code class="go">_________________________________________________________________</code>
<code class="go">dense_2 (Dense)              (None, 10)                1010</code>
<code class="go">=================================================================</code>
<code class="go">Total params: 266,610</code>
<code class="go">Trainable params: 266,610</code>
<code class="go">Non-trainable params: 0</code>
<code class="go">_________________________________________________________________</code></pre>

<p>Note that <code>Dense</code> layers often have a <em>lot</em> of parameters. For example, the first hidden layer has 784 × 300 connection weights, plus 300 bias terms, which adds up to 235,500 parameters! This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting, especially when you do not have a lot of training data. We will come back to this later.</p>

<p>You can easily get a model’s list of layers, to fetch a layer by its index, or you can fetch it by name:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">model</code><code class="o">.</code><code class="n">layers</code>
<code class="go">[&lt;tensorflow.python.keras.layers.core.Flatten at 0x132414e48&gt;,</code>
<code class="go"> &lt;tensorflow.python.keras.layers.core.Dense at 0x1324149b0&gt;,</code>
<code class="go"> &lt;tensorflow.python.keras.layers.core.Dense at 0x1356ba8d0&gt;,</code>
<code class="go"> &lt;tensorflow.python.keras.layers.core.Dense at 0x13240d240&gt;]</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">hidden1</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">layers</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">hidden1</code><code class="o">.</code><code class="n">name</code>
<code class="go">'dense'</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">model</code><code class="o">.</code><code class="n">get_layer</code><code class="p">(</code><code class="s">'dense'</code><code class="p">)</code> <code class="ow">is</code> <code class="n">hidden1</code>
<code class="go">True</code></pre>

<p>All the parameters of a layer can be accessed using its <code>get_weights()</code> and <code>set_weights()</code> methods. For a <code>Dense</code> layer, this includes both the connection weights and the bias terms:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">weights</code><code class="p">,</code> <code class="n">biases</code> <code class="o">=</code> <code class="n">hidden1</code><code class="o">.</code><code class="n">get_weights</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">weights</code>
<code class="go">array([[ 0.02448617, -0.00877795, -0.02189048, ..., -0.02766046,</code>
<code class="go">         0.03859074, -0.06889391],</code>
<code class="go">       ...,</code>
<code class="go">       [-0.06022581,  0.01577859, -0.02585464, ..., -0.00527829,</code>
<code class="go">         0.00272203, -0.06793761]], dtype=float32)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">weights</code><code class="o">.</code><code class="n">shape</code>
<code class="go">(784, 300)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">biases</code>
<code class="go">array([0., 0., 0., 0., 0., 0., 0., 0., 0., ...,  0., 0., 0.], dtype=float32)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">biases</code><code class="o">.</code><code class="n">shape</code>
<code class="go">(300,)</code></pre>

<p>Notice that the <code>Dense</code> layer initialized the connection weights randomly (which is needed to break symmetry, as we discussed earlier), and the biases were initialized to zeros, which is fine. If you ever want to use a different initialization method, you can set <code>kernel_initializer</code> (<em>kernel</em> is another name for the matrix of connection weights) or <code>bias_initializer</code> when creating the layer. We will discuss initializers further in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>, but if you want the full list, see <a href="https://keras.io/initializers/"><em class="hyperlink">https://keras.io/initializers/</em></a>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The shape of the weight matrix depends on the number of inputs. This is why it is recommended to specify the <code>input_shape</code> when creating the first layer in a <code>Sequential</code> model. However, if you do not specify the input shape, it’s OK: Keras will simply wait until it knows the input shape before it actually builds the model. This will happen either when you feed it actual data (e.g., during training), or when you call its <code>build()</code> method. Until the model is really built, the layers will not have any weights, and you will not be able to do certain things (such as print the model summary or save the model). So, if you know the input shape when creating the model, it is best to specify it.</p>
</div>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Compiling the Model"><div class="sect3" id="idm46263516613816">
<h3>Compiling the Model</h3>

<p>After a model is created, you must call its <code>compile()</code> method to specify the loss function and the optimizer to use. Optionally, you can specify a list of extra metrics to compute during training and evaluation:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"sparse_categorical_crossentropy"</code><code class="p">,</code>
              <code class="n">optimizer</code><code class="o">=</code><code class="s2">"sgd"</code><code class="p">,</code>
              <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s2">"accuracy"</code><code class="p">])</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Using <code>loss="sparse_categorical_crossentropy"</code> is equivalent to <code>loss=keras.losses.sparse_categorical_crossentropy</code>. Similarly, <code>optimizer="sgd"</code> is equivalent to <code>optimizer=keras.optimizers.SGD()</code>, and <code>metrics=["accuracy"]</code> is equivalent to <code>metrics=[keras.metrics.sparse_categorical_accuracy]</code> (when using this loss). We will use many other losses, optimizers, and metrics in this book, but for the full lists see <a href="https://keras.io/losses/"><em class="hyperlink">https://keras.io/losses/</em></a>, <a href="https://keras.io/optimizers/"><em class="hyperlink">https://keras.io/optimizers/</em></a>, and <a href="https://keras.io/metrics/"><em class="hyperlink">https://keras.io/metrics/</em></a>.</p>
</div>

<p>This code requires some explanation. First, we use the <code>"sparse_categorical_crossentropy"</code> loss because we have sparse labels (i.e., for each instance, there is just a target class index, from 0 to 9 in this case), and the classes are exclusive. If instead we had one target probability per class for each instance (such as one-hot vectors, e.g. <code>[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]</code> to represent class 3), then we would need to use the <code>"categorical_crossentropy"</code> loss instead. If we were doing binary classification (with one or more binary labels), then we would use the <code>"sigmoid"</code> (i.e., logistic) activation function in the output layer instead of the <code>"softmax"</code> activation function, and we would use the <code>"binary_crossentropy"</code> loss.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If you want to convert sparse labels (i.e., class indices) to one-hot vector labels, use the <code>keras.utils.to_categorical()</code> function. To go the other way round, use the <code>np.argmax()</code> function with <code>axis=1</code>.</p>
</div>

<p>Regarding the optimizer, <code>"sgd"</code> means that we will train the model using simple Stochastic Gradient Descent. In other words, Keras will perform the backpropagation algorithm described earlier (i.e., reverse-mode autodiff plus Gradient Descent). We will discuss more efficient optimizers in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a> (they improve the Gradient Descent part, not the autodiff). When using the SGD optimizer, it is important to tune the learning rate. So, you will generally want to use <code>optimizer=keras.optimizers.SGD(lr=???)</code> to set the learning rate, rather than <code>optimizer="sgd"</code>, which defaults to <code>lr=0.01</code>.</p>

<p>Finally, since this is a classifier, it’s useful to measure its <code>"accuracy"</code> during training and evaluation.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Training and evaluating the model"><div class="sect3" id="idm46263516022968">
<h3>Training and evaluating the model</h3>

<p>Now the model is ready to be trained. For this we simply need to call its <code>fit()</code> method:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">30</code><code class="p">,</code>
<code class="gp">... </code>                    <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">X_valid</code><code class="p">,</code> <code class="n">y_valid</code><code class="p">))</code>
<code class="gp">...</code>
<code class="go">Train on 55000 samples, validate on 5000 samples</code>
<code class="go">Epoch 1/30</code>
<code class="go">55000/55000 [======] - 3s 49us/sample - loss: 0.7218     - accuracy: 0.7660</code>
<code class="go">                                      - val_loss: 0.4973 - val_accuracy: 0.8366</code>
<code class="go">Epoch 2/30</code>
<code class="go">55000/55000 [======] - 2s 45us/sample - loss: 0.4840     - accuracy: 0.8327</code>
<code class="go">                                      - val_loss: 0.4456 - val_accuracy: 0.8480</code>
<code class="go">[...]</code>
<code class="go">Epoch 30/30</code>
<code class="go">55000/55000 [======] - 3s 53us/sample - loss: 0.2252     - accuracy: 0.9192</code>
<code class="go">                                      - val_loss: 0.2999 - val_accuracy: 0.8926</code></pre>

<p>We pass it the input features (<code>X_train</code>) and the target classes (<code>y_train</code>), as well as the number of epochs to train (or else it would default to just 1, which would definitely not be enough to converge to a good solution). We also pass a validation set (this is optional). Keras will measure the loss and the extra metrics on this set at the end of each epoch, which is very useful to see how well the model really performs. If the performance on the training set is much better than on the validation set, your model is probably overfitting the training set (or there is a bug, such as a data mismatch between the training set and the validation set).</p>

<p>And that’s it! The neural network is trained.<sup><a data-type="noteref" id="idm46263515961960-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263515961960">15</a></sup> At each epoch during training, Keras displays the number of instances processed so far (along with a progress bar), the mean training time per sample, and the loss and accuracy (or any other extra metrics you asked for) on both the training set and the validation set. You can see that the training loss went down, which is a good sign, and the validation accuracy reached 89.26% after 30 epochs, not too far from the training accuracy, so there does not seem to be much overfitting going on.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Instead of passing a validation set using the <code>validation_data</code> argument, you could set <code>validation_split</code> to the ratio of the training set that you want Keras to use for validation. For example, <code>validation_split=0.1</code> tells Keras to use the last 10% of the data (before shuffling) for validation.</p>
</div>

<p>If the training set was very skewed, with some classes being overrepresented and others underrepresented, it would be useful to set the <code>class_weight</code> argument when calling the <code>fit()</code> method, which would give a larger weight to underrepresented classes and a lower weight to overrepresented classes. These weights would be used by Keras when computing the loss. If you need per-instance weights, set the <code>sample_weight</code> argument (it supersedes <code>class_weight</code>). Per-instance weights could be useful if some instances were labeled by experts while others were labeled using a crowdsourcing platform: you might want to give more weight to the former. You can also provide sample weights (but not class weights) for the validation set by adding them as a third item in the <code>validation_data</code> tuple.</p>

<p>The <code>fit()</code> method returns a <code>History</code> object containing the training parameters (<code>history.params</code>), the list of epochs it went through (<code>history.epoch</code>), and most importantly a dictionary (<code>history.history</code>) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set (if any). If you use this dictionary to create a Pandas DataFrame and call its <code>plot()</code> method, you get the learning curves shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#keras_learning_curves_plot">Figure&nbsp;10-12</a>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="kn">as</code> <code class="nn">plt</code>

<code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">history</code><code class="o">.</code><code class="n">history</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mi">5</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">grid</code><code class="p">(</code><code class="bp">True</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">gca</code><code class="p">()</code><code class="o">.</code><code class="n">set_ylim</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code> <code class="c1"># set the vertical range to [0-1]</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<figure><div id="keras_learning_curves_plot" class="figure">
<img src="./Chapter10_files/mls2_1012.png" alt="mls2 1012" width="1440" height="874" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1012.png">
<h6><span class="label">Figure 10-12. </span>Learning curves: the mean training loss and accuracy measured over each epoch, and the mean validation loss and accuracy measured at the end of each epoch</h6>
</div></figure>

<p>You can see that both the training accuracy and the validation accuracy steadily increase during training, while the training loss and the validation loss decrease. Good! Moreover, the validation curves are close to the training curves, which means that there is not too much overfitting. In this particular case, the model looks like it performed better on the validation set than on the training set at the beginning of training. But that’s not the case: indeed, the validation error is computed at the <em>end</em> of each epoch, while the training error is computed using a running mean <em>during</em> each epoch. So the training curve should be shifted by half an epoch to the left. If you do that, you will see that the training and validation curves overlap almost perfectly at the beginning of training.</p>
<div data-type="tip"><h6>Tip</h6>
<p>When plotting the training curve, it should be shifted by half an epoch to the left.</p>
</div>

<p>The training set performance ends up beating the validation performance, as is generally the case when you train for long enough. You can tell that the model has not quite converged yet, as the validation loss is still going down, so you should probably continue training. It’s as simple as calling the <code>fit()</code> method again, since Keras just continues training where it left off (you should be able to reach close to 89% validation accuracy).</p>

<p>If you are not satisfied with the performance of your model, you should go back and tune the hyperparameters. The first one to check is the learning rate. If that doesn’t help, try another optimizer (and always retune the learning rate after changing any hyperparameter). If the performance is still not great, then try tuning your model’s hyperparameters, for example the number of layers, the number of neurons per layer, and the types of activation functions to use for each hidden layer. You can also try tuning other hyperparameters such the batch size (it can be set in the <code>fit()</code> method using the <code>batch_size</code> argument, which defaults to 32). We will get back to hyperparameter tuning at the end of this chapter. Once you are satisfied with your model’s validation accuracy, you should evaluate it on the test set to estimate the generalization error before you deploy the model to production. You can easily do this using the <code>evaluate()</code> method (it also supports several other arguments, such as <code>batch_size</code> or <code>sample_weight</code>; please check the documentation for more details):</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">model</code><code class="o">.</code><code class="n">evaluate</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="go">10000/10000 [==========] - 0s 29us/sample - loss: 0.3340 - accuracy: 0.8851</code>
<code class="go">[0.3339798209667206, 0.8851]</code></pre>

<p>As we saw in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch02.html#project_chapter">Chapter&nbsp;2</a>, it is common to get slightly lower performance on the test set than on the validation set, because the hyperparameters are tuned on the validation set, not the test set (however, in this example, we did not do any hyperparameter tuning, so the lower accuracy is just bad luck). Remember to resist the temptation to tweak the hyperparameters on the test set, or else your estimate of the generalization error will be too optimistic.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Using the model to make predictions"><div class="sect3" id="idm46263516022344">
<h3>Using the model to make predictions</h3>

<p>Next, we can use the model’s <code>predict()</code> method to make predictions on new instances. Since we don’t have actual new instances, we will just use the first three instances of the test set:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code> <code class="o">=</code> <code class="n">X_test</code><code class="p">[:</code><code class="mi">3</code><code class="p">]</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_proba</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_proba</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>
<code class="go">array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.01, 0.  , 0.96],</code>
<code class="go">       [0.  , 0.  , 0.98, 0.  , 0.02, 0.  , 0.  , 0.  , 0.  , 0.  ],</code>
<code class="go">       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],</code>
<code class="go">      dtype=float32)</code></pre>

<p>As you can see, for each instance the model estimates one probability per class, from class 0 to class 9. For example, for the first image it estimates that the probability of class 9 (ankle boot) is 96%, the probability of class 5 (sandal) is 3%, the probability of class 7 (sneaker) is 1%, and the other classes are negligible. In other words, it “believes” the first image is footwear, most likely ankle boots, but it’s not entirely sure, it might be sandals or sneakers. If you only care about the class with the highest estimated probability (even if that probability is quite low), then you can use the <code>predict_classes()</code> method instead:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict_classes</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code>
<code class="go">array([9, 2, 1])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">class_names</code><code class="p">)[</code><code class="n">y_pred</code><code class="p">]</code>
<code class="go">array(['Ankle boot', 'Pullover', 'Trouser'], dtype='&lt;U11')</code></pre>

<p>And the classifier actually classified all three images correctly (these images are shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#fashion_mnist_images_plot">Figure&nbsp;10-13</a>):</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_new</code> <code class="o">=</code> <code class="n">y_test</code><code class="p">[:</code><code class="mi">3</code><code class="p">]</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_new</code>
<code class="go">array([9, 2, 1])</code></pre>

<figure><div id="fashion_mnist_images_plot" class="figure">
<img src="./Chapter10_files/mls2_1013.png" alt="mls2 1013" width="1440" height="520" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1013.png">
<h6><span class="label">Figure 10-13. </span>Correctly classified Fashion MNIST images</h6>
</div></figure>

<p>Now you know how to use the Sequential API to build, train, evaluate, and use a classification MLP. But what about regression?</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Building a Regression MLP Using the Sequential API"><div class="sect2" id="idm46263515705704">
<h2>Building a Regression MLP Using the Sequential API</h2>

<p>Let’s switch to the California housing problem and tackle it using a regression neural network. For simplicity, we will use Scikit-Learn’s <code>fetch_california_housing()</code> function to load the data. This dataset is simpler than the one we used in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch02.html#project_chapter">Chapter&nbsp;2</a>, since it contains only numerical features (there is no <code>ocean_proximity</code> feature), and there is no missing value. After loading the data, we split it into a training set, a validation set, and a test set, and we scale all the features:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">fetch_california_housing</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>

<code class="n">housing</code> <code class="o">=</code> <code class="n">fetch_california_housing</code><code class="p">()</code>

<code class="n">X_train_full</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train_full</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">housing</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">housing</code><code class="o">.</code><code class="n">target</code><code class="p">)</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_valid</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_valid</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">X_train_full</code><code class="p">,</code> <code class="n">y_train_full</code><code class="p">)</code>

<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">X_train</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>
<code class="n">X_valid</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_valid</code><code class="p">)</code>
<code class="n">X_test</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code></pre>

<p>Using the Sequential API to build, train, evaluate, and use a regression MLP to make predictions is quite similar to what we did for classification. The main differences are the fact that the output layer has a single neuron (since we only want to predict a single value) and uses no activation function, and the loss function is the mean squared error. Since the dataset is quite noisy, we just use a single hidden layer with fewer neurons than before, to avoid overfitting:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">30</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">:]),</code>
    <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="p">])</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"mean_squared_error"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="s2">"sgd"</code><code class="p">)</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code>
                    <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">X_valid</code><code class="p">,</code> <code class="n">y_valid</code><code class="p">))</code>
<code class="n">mse_test</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">evaluate</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="n">X_new</code> <code class="o">=</code> <code class="n">X_test</code><code class="p">[:</code><code class="mi">3</code><code class="p">]</code> <code class="c1"># pretend these are new instances</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code></pre>

<p>As you can see, the Sequential API is quite easy to use. However, although sequential models are extremely common, it is sometimes useful to build neural networks with more complex topologies, or with multiple inputs or outputs. For this purpose, Keras offers the Functional API.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Building Complex Models Using the Functional API"><div class="sect2" id="idm46263515574072">
<h2>Building Complex Models Using the Functional API</h2>

<p>One example of a nonsequential neural network is a <em>Wide &amp; Deep</em> neural network. This neural network architecture was introduced in a <a href="https://homl.info/widedeep">2016 paper</a> by Heng-Tze Cheng et al.<sup><a data-type="noteref" id="idm46263515437528-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263515437528">16</a></sup>. It connects all or part of the inputs directly to the output layer, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#wide_deep_diagram">Figure&nbsp;10-14</a>. This architecture makes it possible for the neural network to learn both deep patterns (using the deep path) and simple rules (through the short path).<sup><a data-type="noteref" id="idm46263515435384-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263515435384">17</a></sup> In contrast, a regular MLP forces all the data to flow through the full stack of layers, thus simple patterns in the data may end up being distorted by this sequence of transformations.</p>

<figure class="smallerseventy"><div id="wide_deep_diagram" class="figure">
<img src="./Chapter10_files/mls2_1014.png" alt="mls2 1014" width="951" height="861" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1014.png">
<h6><span class="label">Figure 10-14. </span>Wide &amp; Deep neural network</h6>
</div></figure>

<p>Let’s build such a neural network to tackle the California housing problem:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">input_</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Input</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">:])</code>
<code class="n">hidden1</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">30</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">)(</code><code class="n">input_</code><code class="p">)</code>
<code class="n">hidden2</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">30</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">)(</code><code class="n">hidden1</code><code class="p">)</code>
<code class="n">concat</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Concatenate</code><code class="p">()([</code><code class="n">input_</code><code class="p">,</code> <code class="n">hidden2</code><code class="p">])</code>
<code class="n">output</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">)(</code><code class="n">concat</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">Model</code><code class="p">(</code><code class="n">inputs</code><code class="o">=</code><code class="p">[</code><code class="n">input_</code><code class="p">],</code> <code class="n">outputs</code><code class="o">=</code><code class="p">[</code><code class="n">output</code><code class="p">])</code></pre>

<p>Let’s go through each line of this code:</p>

<ul>
<li>
<p>First, we need to create an <code>Input</code> object.<sup><a data-type="noteref" id="idm46263515295544-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263515295544">18</a></sup> This is a specification of the kind of input the model will get, including its <code>shape</code> and <code>dtype</code>. A model may actually have multiple inputs, as we will see shortly.</p>
</li>
<li>
<p>Next, we create a <code>Dense</code> layer with 30 neurons and using the ReLU activation function. As soon as it is created, notice that we call it like a function, passing it the input. This is why this is called the Functional API. Note that we are just telling Keras how it should connect the layers together; no actual data is being processed yet.</p>
</li>
<li>
<p>We then create a second hidden layer, and again we use it as a function. Note that we pass it the output of the first hidden layer.</p>
</li>
<li>
<p>Next, we create a <code>Concatenate()</code> layer, and once again we immediately use it like a function, to concatenate the input and the output of the second hidden layer. You may prefer the <code>keras.layers.concatenate()</code> function, which creates a <code>Concatenate</code> layer and immediately calls it with the given inputs.</p>
</li>
<li>
<p>Then we create the output layer, with a single neuron and no activation function, and we call it like a function, passing it the result of the concatenation.</p>
</li>
<li>
<p>Lastly, we create a Keras <code>Model</code>, specifying which inputs and outputs to use.</p>
</li>
</ul>

<p>Once you have built the Keras model, everything is exactly like earlier, so no need to repeat it here: you must compile the model, train it, evaluate it and use it to make predictions.</p>

<p>But what if you want to send a subset of the features through the wide path and a different subset (possibly overlapping) through the deep path (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#multiple_inputs_diagram">Figure&nbsp;10-15</a>)? In this case, one solution is to use multiple inputs. For example, suppose we want to send five features through the deep path (features 0 to 4), and six features through the wide path (features 2 to 7):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">input_A</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Input</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="mi">5</code><code class="p">],</code> <code class="n">name</code><code class="o">=</code><code class="s2">"wide_input"</code><code class="p">)</code>
<code class="n">input_B</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Input</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="mi">6</code><code class="p">],</code> <code class="n">name</code><code class="o">=</code><code class="s2">"deep_input"</code><code class="p">)</code>
<code class="n">hidden1</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">30</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">)(</code><code class="n">input_B</code><code class="p">)</code>
<code class="n">hidden2</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">30</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">)(</code><code class="n">hidden1</code><code class="p">)</code>
<code class="n">concat</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">concatenate</code><code class="p">([</code><code class="n">input_A</code><code class="p">,</code> <code class="n">hidden2</code><code class="p">])</code>
<code class="n">output</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">name</code><code class="o">=</code><code class="s2">"output"</code><code class="p">)(</code><code class="n">concat</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">Model</code><code class="p">(</code><code class="n">inputs</code><code class="o">=</code><code class="p">[</code><code class="n">input_A</code><code class="p">,</code> <code class="n">input_B</code><code class="p">],</code> <code class="n">outputs</code><code class="o">=</code><code class="p">[</code><code class="n">output</code><code class="p">])</code></pre>

<figure class="smallerseventy"><div id="multiple_inputs_diagram" class="figure">
<img src="./Chapter10_files/mls2_1015.png" alt="mls2 1015" width="842" height="863" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1015.png">
<h6><span class="label">Figure 10-15. </span>Handling multiple inputs</h6>
</div></figure>

<p>The code is self-explanatory. You should name at least the most important layers, especially when the model gets a bit complex like this. Note that we specified <code>inputs=[input_A, input_B]</code> when creating the model. Now we can compile the model as usual, but when we call the <code>fit()</code> method, instead of passing a single input matrix <code>X_train</code>, we must pass a pair of matrices <code>(X_train_A, X_train_B)</code>: one per input.<sup><a data-type="noteref" id="idm46263515154968-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263515154968">19</a></sup> The same is true for <code>X_valid</code>, and also for <code>X_test</code> and <code>X_new</code> when you call <code>evaluate()</code> or <code>predict()</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"mse"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">1e-3</code><code class="p">))</code>

<code class="n">X_train_A</code><code class="p">,</code> <code class="n">X_train_B</code> <code class="o">=</code> <code class="n">X_train</code><code class="p">[:,</code> <code class="p">:</code><code class="mi">5</code><code class="p">],</code> <code class="n">X_train</code><code class="p">[:,</code> <code class="mi">2</code><code class="p">:]</code>
<code class="n">X_valid_A</code><code class="p">,</code> <code class="n">X_valid_B</code> <code class="o">=</code> <code class="n">X_valid</code><code class="p">[:,</code> <code class="p">:</code><code class="mi">5</code><code class="p">],</code> <code class="n">X_valid</code><code class="p">[:,</code> <code class="mi">2</code><code class="p">:]</code>
<code class="n">X_test_A</code><code class="p">,</code> <code class="n">X_test_B</code> <code class="o">=</code> <code class="n">X_test</code><code class="p">[:,</code> <code class="p">:</code><code class="mi">5</code><code class="p">],</code> <code class="n">X_test</code><code class="p">[:,</code> <code class="mi">2</code><code class="p">:]</code>
<code class="n">X_new_A</code><code class="p">,</code> <code class="n">X_new_B</code> <code class="o">=</code> <code class="n">X_test_A</code><code class="p">[:</code><code class="mi">3</code><code class="p">],</code> <code class="n">X_test_B</code><code class="p">[:</code><code class="mi">3</code><code class="p">]</code>

<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">((</code><code class="n">X_train_A</code><code class="p">,</code> <code class="n">X_train_B</code><code class="p">),</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code>
                    <code class="n">validation_data</code><code class="o">=</code><code class="p">((</code><code class="n">X_valid_A</code><code class="p">,</code> <code class="n">X_valid_B</code><code class="p">),</code> <code class="n">y_valid</code><code class="p">))</code>
<code class="n">mse_test</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">evaluate</code><code class="p">((</code><code class="n">X_test_A</code><code class="p">,</code> <code class="n">X_test_B</code><code class="p">),</code> <code class="n">y_test</code><code class="p">)</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">((</code><code class="n">X_new_A</code><code class="p">,</code> <code class="n">X_new_B</code><code class="p">))</code></pre>

<p>There are many use cases in which you may want to have multiple outputs:</p>

<ul>
<li>
<p>The task may demand it. For instance, you may want to locate and classify the main object in a picture. This is both a regression task (finding the coordinates of the object’s center, as well as its width and height) and a classification task.</p>
</li>
<li>
<p>Similarly, you may have multiple independent tasks based on the same data. Sure, you could train one neural network per task, but in many cases you will get better results on all tasks by training a single neural network with one output per task. This is because the neural network can learn features in the data that are useful across tasks. For example, you could perform <em>multitask classification</em> on pictures of faces, using one output to classify the person’s facial expression (smiling, surprised, etc.) and another output to identify whether they are wearing glasses or not.</p>
</li>
<li>
<p>Another use case is as a regularization technique (i.e., a training constraint whose objective is to reduce overfitting and thus improve the model’s ability to generalize). For example, you may want to add some auxiliary outputs in a neural network architecture (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#multiple_outputs_diagram">Figure&nbsp;10-16</a>) to ensure that the underlying part of the network learns something useful on its own, without relying on the rest of the network.</p>
</li>
</ul>

<figure><div id="multiple_outputs_diagram" class="figure">
<img src="./Chapter10_files/mls2_1016.png" alt="mls2 1016" width="1302" height="872" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1016.png">
<h6><span class="label">Figure 10-16. </span>Handling multiple outputs, in this example to add an auxiliary output for regularization</h6>
</div></figure>

<p>Adding extra outputs is quite easy: just connect them to the appropriate layers and add them to your model’s list of outputs. For example, the following code builds the network represented in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#multiple_outputs_diagram">Figure&nbsp;10-16</a>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[</code><code class="o">...</code><code class="p">]</code> <code class="c1"># Same as above, up to the main output layer</code>
<code class="n">output</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">name</code><code class="o">=</code><code class="s2">"main_output"</code><code class="p">)(</code><code class="n">concat</code><code class="p">)</code>
<code class="n">aux_output</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">name</code><code class="o">=</code><code class="s2">"aux_output"</code><code class="p">)(</code><code class="n">hidden2</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">Model</code><code class="p">(</code><code class="n">inputs</code><code class="o">=</code><code class="p">[</code><code class="n">input_A</code><code class="p">,</code> <code class="n">input_B</code><code class="p">],</code> <code class="n">outputs</code><code class="o">=</code><code class="p">[</code><code class="n">output</code><code class="p">,</code> <code class="n">aux_output</code><code class="p">])</code></pre>

<p>Each output will need its own loss function. Therefore, when we compile the model, we should pass a list of losses<sup><a data-type="noteref" id="idm46263514960952-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263514960952">20</a></sup> (if we pass a single loss, Keras will assume that the same loss must be used for all outputs). By default, Keras will compute all these losses and simply add them up to get the final loss used for training. We care much more about the main output than about the auxiliary output (as it is just used for regularization), so we want to give the main output’s loss a much greater weight. Fortunately, it is possible to set all the loss weights when compiling the model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="p">[</code><code class="s2">"mse"</code><code class="p">,</code> <code class="s2">"mse"</code><code class="p">],</code> <code class="n">loss_weights</code><code class="o">=</code><code class="p">[</code><code class="mf">0.9</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">],</code> <code class="n">optimizer</code><code class="o">=</code><code class="s2">"sgd"</code><code class="p">)</code></pre>

<p>Now when we train the model, we need to provide labels for each output. In this example, the main output and the auxiliary output should try to predict the same thing, so they should use the same labels. So instead of passing <code>y_train</code>, we need to pass <code>(y_train, y_train)</code> (and the same goes for <code>y_valid</code> and <code>y_test</code>):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code>
    <code class="p">[</code><code class="n">X_train_A</code><code class="p">,</code> <code class="n">X_train_B</code><code class="p">],</code> <code class="p">[</code><code class="n">y_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">],</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code>
    <code class="n">validation_data</code><code class="o">=</code><code class="p">([</code><code class="n">X_valid_A</code><code class="p">,</code> <code class="n">X_valid_B</code><code class="p">],</code> <code class="p">[</code><code class="n">y_valid</code><code class="p">,</code> <code class="n">y_valid</code><code class="p">]))</code></pre>

<p>When we evaluate the model, Keras will return the total loss, as well as all the individual losses:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">total_loss</code><code class="p">,</code> <code class="n">main_loss</code><code class="p">,</code> <code class="n">aux_loss</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">evaluate</code><code class="p">(</code>
    <code class="p">[</code><code class="n">X_test_A</code><code class="p">,</code> <code class="n">X_test_B</code><code class="p">],</code> <code class="p">[</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">])</code></pre>

<p>Similarly, the <code>predict()</code> method will return predictions for each output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">y_pred_main</code><code class="p">,</code> <code class="n">y_pred_aux</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">([</code><code class="n">X_new_A</code><code class="p">,</code> <code class="n">X_new_B</code><code class="p">])</code></pre>

<p>As you can see, you can build any sort of architecture you want quite easily with the Functional API. Let’s look at one last way you can build Keras models.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Using the Subclassing API to Build Dynamic Models"><div class="sect2" id="idm46263515439304">
<h2>Using the Subclassing API to Build Dynamic Models</h2>

<p>Both the Sequential API and the Functional API are declarative: you start by declaring which layers you want to use and how they should be connected, and only then can you start feeding the model some data for training or inference. This has many advantages: the model can easily be saved, cloned, and shared; its structure can be displayed and analyzed; the framework can infer shapes and check types, so errors can be caught early (i.e., before any data ever goes through the model). It’s also fairly easy to debug, since the whole model is a static graph of layers. But the flip side is just that: it’s static. Some models involve loops, varying shapes, conditional branching, and other dynamic behaviors. For such cases, or simply if you prefer a more imperative programming style, the Subclassing API is for you.</p>

<p>Simply subclass the <code>Model</code> class, create the layers you need in the constructor, and use them to perform the computations you want in the <code>call()</code> method. For example, creating an instance of the following <code>WideAndDeepModel</code> class gives us an equivalent model to the one we just built with the Functional API. You can then compile it, evaluate it, and use it to make predictions, exactly like we just did:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">WideAndDeepModel</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">Model</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">units</code><code class="o">=</code><code class="mi">30</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="nf-Magic">__init__</code><code class="p">(</code><code class="o">**</code><code class="n">kwargs</code><code class="p">)</code> <code class="c1"># handles standard args (e.g., name)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">hidden1</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="n">units</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="n">activation</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">hidden2</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="n">units</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="n">activation</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">main_output</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">aux_output</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">call</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">):</code>
        <code class="n">input_A</code><code class="p">,</code> <code class="n">input_B</code> <code class="o">=</code> <code class="n">inputs</code>
        <code class="n">hidden1</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">hidden1</code><code class="p">(</code><code class="n">input_B</code><code class="p">)</code>
        <code class="n">hidden2</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">hidden2</code><code class="p">(</code><code class="n">hidden1</code><code class="p">)</code>
        <code class="n">concat</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">concatenate</code><code class="p">([</code><code class="n">input_A</code><code class="p">,</code> <code class="n">hidden2</code><code class="p">])</code>
        <code class="n">main_output</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">main_output</code><code class="p">(</code><code class="n">concat</code><code class="p">)</code>
        <code class="n">aux_output</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">aux_output</code><code class="p">(</code><code class="n">hidden2</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">main_output</code><code class="p">,</code> <code class="n">aux_output</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">WideAndDeepModel</code><code class="p">()</code></pre>

<p>This example looks very much like the Functional API, except we do not need to create the inputs; we just use the <code>input</code> argument to the <code>call()</code> method, and we separate the creation of the layers<sup><a data-type="noteref" id="idm46263514636936-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263514636936">21</a></sup> in the constructor from their usage in the <code>call()</code> method. The big difference is that you can do pretty much anything you want in the <code>call()</code> method: <code>for</code> loops, <code>if</code> statements, low-level TensorFlow operations—your imagination is the limit (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch12.html#tensorflow_chapter">Chapter&nbsp;12</a>)! This makes it a great API for researchers experimenting with new ideas.</p>

<p>This extra flexibility does come at a cost: your model’s architecture is hidden within the <code>call()</code> method, so Keras cannot easily inspect it; it cannot save or clone it; and when you call the <code>summary()</code> method, you only get a list of layers, without any information on how they are connected to each other. Moreover, Keras cannot check types and shapes ahead of time, and it is easier to make mistakes. So unless you really need that extra flexibility, you should probably stick to the Sequential API or the Functional API.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Keras models can be used just like regular layers, so you can easily combine them to build complex architectures.</p>
</div>

<p>Now that you know how to build and train neural nets using Keras, you will want to save them!</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Saving and Restoring a Model"><div class="sect2" id="idm46263514472696">
<h2>Saving and Restoring a Model</h2>

<p>When using the Sequential API or the Functional API, saving a trained Keras model is as simple as it gets:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Sequential</code><code class="p">([</code><code class="o">...</code><code class="p">])</code> <code class="c1"># or keras.Model([...])</code>
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">([</code><code class="o">...</code><code class="p">])</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">([</code><code class="o">...</code><code class="p">])</code>
<code class="n">model</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="s2">"my_keras_model.h5"</code><code class="p">)</code></pre>

<p>Keras will use the HDF5 format to save both the model’s architecture (including every layer’s hyperparameters) and the value of all the model parameters for every layer (e.g., connection weights and biases). It also saves the optimizer (including its hyperparameters and any state it may have).</p>

<p>You will typically have a script that trains a model and saves it, and one or more scripts (or web services) that load the model and use it to make predictions. Loading the model is just as easy:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">load_model</code><code class="p">(</code><code class="s2">"my_keras_model.h5"</code><code class="p">)</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>This will work when using the Sequential API or the Functional API, but unfortunately not when using Model subclassing. You can use <code>save_weights()</code> and <code>load_weights()</code> to at least save and restore the model parameters (but you will need to save and restore everything else yourself).</p>
</div>

<p>But what if training lasts several hours? This is quite common, especially when training on large datasets. In this case, you should not only save your model at the end of training, but also save checkpoints at regular intervals during training, to avoid losing everything in case your computer crashes. But how can you tell the <code>fit()</code> method to save checkpoints? Use callbacks.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Using Callbacks"><div class="sect2" id="idm46263514381768">
<h2>Using Callbacks</h2>

<p>The <code>fit()</code> method accepts a <code>callbacks</code> argument that lets you specify a list of objects that Keras will call during training at the start and end of training, at the start and end of each epoch, and even before and after processing each batch. For example, the <code>ModelCheckpoint</code> callback saves checkpoints of your model at regular intervals during training, by default at the end of each epoch:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[</code><code class="o">...</code><code class="p">]</code> <code class="c1"># build and compile the model</code>
<code class="n">checkpoint_cb</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">callbacks</code><code class="o">.</code><code class="n">ModelCheckpoint</code><code class="p">(</code><code class="s2">"my_keras_model.h5"</code><code class="p">)</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">callbacks</code><code class="o">=</code><code class="p">[</code><code class="n">checkpoint_cb</code><code class="p">])</code></pre>

<p>Moreover, if you use a validation set during training, you can set <code>save_best_only=True</code> when creating the <code>ModelCheckpoint</code>. In this case, it will only save your model when its performance on the validation set is the best so far. This way, you do not need to worry about training for too long and overfitting the training set: simply restore the last model saved after training, and this will be the best model on the validation set. The following code is a simple way to implement early stopping (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">checkpoint_cb</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">callbacks</code><code class="o">.</code><code class="n">ModelCheckpoint</code><code class="p">(</code><code class="s2">"my_keras_model.h5"</code><code class="p">,</code>
                                                <code class="n">save_best_only</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
                    <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">X_valid</code><code class="p">,</code> <code class="n">y_valid</code><code class="p">),</code>
                    <code class="n">callbacks</code><code class="o">=</code><code class="p">[</code><code class="n">checkpoint_cb</code><code class="p">])</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">load_model</code><code class="p">(</code><code class="s2">"my_keras_model.h5"</code><code class="p">)</code> <code class="c1"># roll back to best model</code></pre>

<p>Another way to implement early stopping is to simply use the <code>EarlyStopping</code> callback. It will interrupt training when it measures no progress on the validation set for a number of epochs (defined by the <code>patience</code> argument), and it will optionally roll back to the best model. You can combine both callbacks to save checkpoints of your model (in case your computer crashes) and interrupt training early when there is no more progress (to avoid wasting time and resources):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">early_stopping_cb</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">callbacks</code><code class="o">.</code><code class="n">EarlyStopping</code><code class="p">(</code><code class="n">patience</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
                                                  <code class="n">restore_best_weights</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>
                    <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">X_valid</code><code class="p">,</code> <code class="n">y_valid</code><code class="p">),</code>
                    <code class="n">callbacks</code><code class="o">=</code><code class="p">[</code><code class="n">checkpoint_cb</code><code class="p">,</code> <code class="n">early_stopping_cb</code><code class="p">])</code></pre>

<p>The number of epochs can be set to a large value since training will stop automatically when there is no more progress. In this case, there is no need to restore the best model saved because the <code>EarlyStopping</code> callback will keep track of the best weights and restore them for us at the end of training.</p>
<div data-type="tip"><h6>Tip</h6>
<p>There are many other callbacks available in the <code>keras.callbacks</code> package. See <a href="https://keras.io/callbacks/"><em class="hyperlink">https://keras.io/callbacks/</em></a>.</p>
</div>

<p>If you need extra control, you can easily write your own custom callbacks. As an example of how to do that, the following custom callback will display the ratio between the validation loss and the training loss during training (e.g., to detect overfitting):</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">PrintValTrainRatioCallback</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">callbacks</code><code class="o">.</code><code class="n">Callback</code><code class="p">):</code>
    <code class="k">def</code> <code class="nf">on_epoch_end</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">epoch</code><code class="p">,</code> <code class="n">logs</code><code class="p">):</code>
        <code class="k">print</code><code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">val/train: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">logs</code><code class="p">[</code><code class="s2">"val_loss"</code><code class="p">]</code> <code class="o">/</code> <code class="n">logs</code><code class="p">[</code><code class="s2">"loss"</code><code class="p">]))</code></pre>

<p>As you might expect, you can implement <code>on_train_begin()</code>, <code>on_train_end()</code>, <code>on_epoch_begin()</code>, <code>on_epoch_begin()</code>, <code>on_batch_end()</code> and <code>on_batch_end()</code>. Callbacks can also be used during evaluation and predictions, should you ever need them (e.g., for debugging). For evaluation, you should implement <code>on_test_begin()</code>, <code>on_test_end()</code>, <code>on_test_batch_begin()</code>, or <code>on_test_batch_end()</code> (called by <code>evaluate()</code>), and for prediction you should implement <code>on_predict_begin()</code>, <code>on_predict_end()</code>, <code>on_predict_batch_begin()</code>, or <code>on_predict_batch_end()</code> (called by <code>predict()</code>).</p>

<p>Now let’s take a look at one more tool you should definitely have in your toolbox when using tf.keras: TensorBoard.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Using TensorBoard for Visualization"><div class="sect2" id="idm46263514381144">
<h2>Using TensorBoard for Visualization</h2>

<p>TensorBoard is a great interactive visualization tool that you can use to view the learning curves during training, compare learning curves between multiple runs, visualize the computation graph, analyze training statistics, view images generated by your model, visualize complex multidimensional data projected down to 3D and automatically clustered for you, and more! This tool is installed automatically when you install TensorFlow, so you already have it.</p>

<p>To use it, you must modify your program so that it outputs the data you want to visualize to special binary log files called <em>event files</em>. Each binary data record is called a <em>summary</em>. The TensorBoard server will monitor the log directory, and it will automatically pick up the changes and update the visualizations: this allows you to visualize live data (with a short delay), such as the learning curves during training. In general, you want to point the TensorBoard server to a root log directory and configure your program so that it writes to a different subdirectory every time it runs. This way, the same TensorBoard server instance will allow you to visualize and compare data from multiple runs of your program, without getting everything mixed up.</p>

<p>So let’s start by defining the root log directory we will use for our TensorBoard logs, plus a small function that will generate a subdirectory path based on the current date and time so that it’s different at every run. You may want to include extra information in the log directory name, such as hyperparameter values that you are testing, to make it easier to know what you are looking at in TensorBoard:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">os</code>
<code class="n">root_logdir</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">os</code><code class="o">.</code><code class="n">curdir</code><code class="p">,</code> <code class="s2">"my_logs"</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">get_run_logdir</code><code class="p">():</code>
    <code class="kn">import</code> <code class="nn">time</code>
    <code class="n">run_id</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">strftime</code><code class="p">(</code><code class="s2">"run_</code><code class="si">%Y</code><code class="s2">_</code><code class="si">%m</code><code class="s2">_</code><code class="si">%d</code><code class="s2">-</code><code class="si">%H</code><code class="s2">_</code><code class="si">%M</code><code class="s2">_</code><code class="si">%S</code><code class="s2">"</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">root_logdir</code><code class="p">,</code> <code class="n">run_id</code><code class="p">)</code>

<code class="n">run_logdir</code> <code class="o">=</code> <code class="n">get_run_logdir</code><code class="p">()</code> <code class="c1"># e.g., './my_logs/run_2019_06_07-15_15_22'</code></pre>

<p>Next, the good news is that Keras provides a nice <code>TensorBoard</code> callback:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[</code><code class="o">...</code><code class="p">]</code> <code class="c1"># Build and compile your model</code>
<code class="n">tensorboard_cb</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">callbacks</code><code class="o">.</code><code class="n">TensorBoard</code><code class="p">(</code><code class="n">run_logdir</code><code class="p">)</code>
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">30</code><code class="p">,</code>
                    <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">X_valid</code><code class="p">,</code> <code class="n">y_valid</code><code class="p">),</code>
                    <code class="n">callbacks</code><code class="o">=</code><code class="p">[</code><code class="n">tensorboard_cb</code><code class="p">])</code></pre>

<p>And that’s all there is to it! It could hardly be easier to use. If you run this code, the <code>TensorBoard</code> callback will take care of creating the log directory for you (along with its parent directories if needed), and during training it will create event files and write summaries to them. After running the program a second time (perhaps changing some hyperparameter value), you will end up with a directory structure similar to this one:</p>

<pre data-type="programlisting" data-code-language="shell-session"><code class="go">my_logs/</code>
<code class="go">├── run_2019_06_07-15_15_22</code>
<code class="go">│&nbsp;&nbsp; ├── train</code>
<code class="go">│&nbsp;&nbsp; │&nbsp;&nbsp; ├── events.out.tfevents.1559891732.mycomputer.local.38511.694049.v2</code>
<code class="go">│&nbsp;&nbsp; │&nbsp;&nbsp; ├── events.out.tfevents.1559891732.mycomputer.local.profile-empty</code>
<code class="go">│&nbsp;&nbsp; │&nbsp;&nbsp; └── plugins/profile/2019-06-07_15-15-32</code>
<code class="go">│&nbsp;&nbsp; │&nbsp;&nbsp;     └── local.trace</code>
<code class="go">│&nbsp;&nbsp; └── validation</code>
<code class="go">│&nbsp;&nbsp;     └── events.out.tfevents.1559891733.mycomputer.local.38511.696430.v2</code>
<code class="go">└── run_2019_06_07-15_15_49</code>
<code class="go">    └── [...]</code></pre>

<p>There’s one directory per run, each containing one subdirectory for training logs and one for validation logs. Both contain event files, but the training logs also include profiling traces: this allows TensorBoard to show you exactly how much time the model spent on each part of your model, across all your devices, which is great for locating performance bottlenecks.</p>

<p>Next you need to start the TensorBoard server. One way to do this is by running a command in a terminal. If you installed TensorFlow within a virtualenv, you should activate it. Next, run the following command at the root of the project (or from anywhere else, as long as you point to the appropriate log directory):</p>
<pre data-type="programlisting" data-code-language="shell-session"><code class="go">$ </code><strong><code class="go">tensorboard --logdir=./my_logs --port=6006</code></strong><code class="go">
</code><code class="go">TensorBoard 2.0.0 at http://mycomputer.local:6006/ (Press CTRL+C to quit)
</code></pre>

<p>If your shell cannot find the <code>tensorboard</code> script, then you must update your <code>PATH</code> environment variable so that it contains the directory in which the script was installed (alternatively, you can just replace <code>tensorboard</code> with <code>python3 -m tensorboard.main</code>). Once the server is up, you can open a web browser and go to <a href="http://localhost:6006/"><em class="hyperlink">http://localhost:6006</em></a>.</p>

<p>Alternatively, you can use TensorBoard directly within Jupyter, by running the following commands. The first line loads the TensorBoard extension, and the second line starts a TensorBoard server on port 6006 (unless it is already started) and connects to it:</p>

<pre data-type="programlisting" data-code-language="python"><code class="o">%</code><code class="n">load_ext</code> <code class="n">tensorboard</code>
<code class="o">%</code><code class="n">tensorboard</code> <code class="o">--</code><code class="n">logdir</code><code class="o">=./</code><code class="n">my_logs</code> <code class="o">--</code><code class="n">port</code><code class="o">=</code><code class="mi">6006</code></pre>

<p>Either way, you should see TensorBoard’s web interface. Click the SCALARS tab to view the learning curves (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#tensorboard_diagram">Figure&nbsp;10-17</a>). At the bottom left, select the logs you want to visualize (e.g., the training logs from the first and second run), and click the <code>epoch_loss</code> scalar. Notice that the training loss went down nicely during both runs, but the second run went down much faster. Indeed, we used a learning rate of 0.05 (<code>optimizer=keras.optimizers.SGD(lr=0.05)</code>) instead of 0.001.</p>

<figure><div id="tensorboard_diagram" class="figure">
<img src="./Chapter10_files/mls2_1017.png" alt="mls2 1017" width="1440" height="969" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1017.png">
<h6><span class="label">Figure 10-17. </span>Visualizing learning curves with TensorBoard</h6>
</div></figure>

<p>You can also visualize the whole graph, the learned weights (projected to 3D), or the profiling traces. The <code>TensorBoard</code> callback also has options to log extra data, such as embeddings (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch13.html#data_chapter">Chapter&nbsp;13</a>).</p>

<p>Moreover, TensorFlow offers a lower-level API in the <code>tf.summary</code> package. The following code creates a <code>SummaryWriter</code> using the <code>create_file_writer()</code> function, and it uses this writer as a context to log scalars, histograms, images, audio, and text, all of which can then be visualized using TensorBoard (give it a try!):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">test_logdir</code> <code class="o">=</code> <code class="n">get_run_logdir</code><code class="p">()</code>
<code class="n">writer</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">summary</code><code class="o">.</code><code class="n">create_file_writer</code><code class="p">(</code><code class="n">test_logdir</code><code class="p">)</code>
<code class="k">with</code> <code class="n">writer</code><code class="o">.</code><code class="n">as_default</code><code class="p">():</code>
    <code class="k">for</code> <code class="n">step</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1000</code> <code class="o">+</code> <code class="mi">1</code><code class="p">):</code>
        <code class="n">tf</code><code class="o">.</code><code class="n">summary</code><code class="o">.</code><code class="n">scalar</code><code class="p">(</code><code class="s2">"my_scalar"</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">sin</code><code class="p">(</code><code class="n">step</code> <code class="o">/</code> <code class="mi">10</code><code class="p">),</code> <code class="n">step</code><code class="o">=</code><code class="n">step</code><code class="p">)</code>
        <code class="n">data</code> <code class="o">=</code> <code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code> <code class="o">+</code> <code class="mi">2</code><code class="p">)</code> <code class="o">*</code> <code class="n">step</code> <code class="o">/</code> <code class="mi">100</code> <code class="c1"># some random data</code>
        <code class="n">tf</code><code class="o">.</code><code class="n">summary</code><code class="o">.</code><code class="n">histogram</code><code class="p">(</code><code class="s2">"my_hist"</code><code class="p">,</code> <code class="n">data</code><code class="p">,</code> <code class="n">buckets</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">step</code><code class="o">=</code><code class="n">step</code><code class="p">)</code>
        <code class="n">images</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">rand</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">32</code><code class="p">,</code> <code class="mi">32</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code> <code class="c1"># random 32×32 RGB images</code>
        <code class="n">tf</code><code class="o">.</code><code class="n">summary</code><code class="o">.</code><code class="n">image</code><code class="p">(</code><code class="s2">"my_images"</code><code class="p">,</code> <code class="n">images</code> <code class="o">*</code> <code class="n">step</code> <code class="o">/</code> <code class="mi">1000</code><code class="p">,</code> <code class="n">step</code><code class="o">=</code><code class="n">step</code><code class="p">)</code>
        <code class="n">texts</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"The step is "</code> <code class="o">+</code> <code class="nb">str</code><code class="p">(</code><code class="n">step</code><code class="p">),</code> <code class="s2">"Its square is "</code> <code class="o">+</code> <code class="nb">str</code><code class="p">(</code><code class="n">step</code><code class="o">**</code><code class="mi">2</code><code class="p">)]</code>
        <code class="n">tf</code><code class="o">.</code><code class="n">summary</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="s2">"my_text"</code><code class="p">,</code> <code class="n">texts</code><code class="p">,</code> <code class="n">step</code><code class="o">=</code><code class="n">step</code><code class="p">)</code>
        <code class="n">sine_wave</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">math</code><code class="o">.</code><code class="n">sin</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">range</code><code class="p">(</code><code class="mi">12000</code><code class="p">)</code> <code class="o">/</code> <code class="mi">48000</code> <code class="o">*</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">pi</code> <code class="o">*</code> <code class="n">step</code><code class="p">)</code>
        <code class="n">audio</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">cast</code><code class="p">(</code><code class="n">sine_wave</code><code class="p">,</code> <code class="n">tf</code><code class="o">.</code><code class="n">float32</code><code class="p">),</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">])</code>
        <code class="n">tf</code><code class="o">.</code><code class="n">summary</code><code class="o">.</code><code class="n">audio</code><code class="p">(</code><code class="s2">"my_audio"</code><code class="p">,</code> <code class="n">audio</code><code class="p">,</code> <code class="n">sample_rate</code><code class="o">=</code><code class="mi">48000</code><code class="p">,</code> <code class="n">step</code><code class="o">=</code><code class="n">step</code><code class="p">)</code></pre>

<p>This is actually a useful visualization tool to have, even beyond TensorFlow or Deep Learning.</p>

<p>Let’s summarize what you learned so far in this chapter: we saw where neural nets came from, what an MLP is and how you can use it for classification and regression, how to use tf.keras’s Sequential API to build MLPs, and how to use the Functional API or the Subclassing API to build more complex model architectures. You learned how to save and restore a model, use callbacks for checkpointing, early stopping, and more. Finally, you learned how to use TensorBoard for visualization. You can already go ahead and use neural networks to tackle many problems! However, you may wonder how to choose the number of hidden layers, the number of neurons in the network, and all the other hyperparameters. Let’s look at this now.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Fine-Tuning Neural Network Hyperparameters"><div class="sect1" id="idm46263516905736">
<h1>Fine-Tuning Neural Network Hyperparameters</h1>

<p>The flexibility of neural networks is also one of their main drawbacks: there are many hyperparameters to tweak. Not only can you use any imaginable network architecture, but even in a simple MLP you can change the number of layers, the number of neurons per layer, the type of activation function to use in each layer, the weight initialization logic, and much more. How do you know what combination of hyperparameters is the best for your task?</p>

<p>One option is to simply try many combinations of hyperparameters and see which one works best on the validation set (or use K-fold cross-validation). To do this, one approach is simply use <code>GridSearchCV</code> or <code>RandomizedSearchCV</code> to explore the hyperparameter space, as we did in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch02.html#project_chapter">Chapter&nbsp;2</a>. We need to wrap our Keras models in objects that mimic regular Scikit-Learn regressors. The first step is to create a function that will build and compile a Keras model, given a set of hyperparameters:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">build_model</code><code class="p">(</code><code class="n">n_hidden</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">n_neurons</code><code class="o">=</code><code class="mi">30</code><code class="p">,</code> <code class="n">learning_rate</code><code class="o">=</code><code class="mf">3e-3</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="p">[</code><code class="mi">8</code><code class="p">]):</code>
    <code class="n">model</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">models</code><code class="o">.</code><code class="n">Sequential</code><code class="p">()</code>
    <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">InputLayer</code><code class="p">(</code><code class="n">input_shape</code><code class="o">=</code><code class="n">input_shape</code><code class="p">))</code>
    <code class="k">for</code> <code class="n">layer</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_hidden</code><code class="p">):</code>
        <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="n">n_neurons</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">))</code>
    <code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">keras</code><code class="o">.</code><code class="n">layers</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">))</code>
    <code class="n">optimizer</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">optimizers</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="n">learning_rate</code><code class="p">)</code>
    <code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s2">"mse"</code><code class="p">,</code> <code class="n">optimizer</code><code class="o">=</code><code class="n">optimizer</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">model</code></pre>

<p>This function creates a simple <code>Sequential</code> model for univariate regression (only one output neuron), with the given input shape and the given number of hidden layers and neurons, and it compiles it using an <code>SGD</code> optimizer configured with the given learning rate. It is good practice to provide reasonable defaults to as many hyperparameters as you can, as Scikit-Learn does.</p>

<p>Next, let’s create a <code>KerasRegressor</code> based on this <code>build_model()</code> function:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">keras_reg</code> <code class="o">=</code> <code class="n">keras</code><code class="o">.</code><code class="n">wrappers</code><code class="o">.</code><code class="n">scikit_learn</code><code class="o">.</code><code class="n">KerasRegressor</code><code class="p">(</code><code class="n">build_model</code><code class="p">)</code></pre>

<p>The <code>KerasRegressor</code> object is a thin wrapper around the Keras model built using <code>build_model()</code>. Since we did not specify any hyperparameter when creating it, it will use the default hyperparameters we defined in <code>build_model()</code>. Now we can use this object like a regular Scikit-Learn regressor: we can train it using its <code>fit()</code> method, then evaluate it using its <code>score()</code> method, and use it to make predictions using its <code>predict()</code> method, as you can see in the following code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">keras_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>
              <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">X_valid</code><code class="p">,</code> <code class="n">y_valid</code><code class="p">),</code>
              <code class="n">callbacks</code><code class="o">=</code><code class="p">[</code><code class="n">keras</code><code class="o">.</code><code class="n">callbacks</code><code class="o">.</code><code class="n">EarlyStopping</code><code class="p">(</code><code class="n">patience</code><code class="o">=</code><code class="mi">10</code><code class="p">)])</code>
<code class="n">mse_test</code> <code class="o">=</code> <code class="n">keras_reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">keras_reg</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code></pre>

<p>Note that any extra parameter you pass to the <code>fit()</code> method will get passed to the underlying Keras model. Also note that the score will be the opposite of the MSE because Scikit-Learn wants scores, not losses (i.e., higher should be better).</p>

<p>We don’t want to train and evaluate a single model like this; we want to train hundreds of variants and see which one performs best on the validation set. Since there are many hyperparameters, it is preferable to use a randomized search rather than grid search (as we discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch02.html#project_chapter">Chapter&nbsp;2</a>). Let’s try to explore the number of hidden layers, the number of neurons, and the learning rate:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">scipy.stats</code> <code class="kn">import</code> <code class="n">reciprocal</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">RandomizedSearchCV</code>

<code class="n">param_distribs</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"n_hidden"</code><code class="p">:</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code>
    <code class="s2">"n_neurons"</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">100</code><code class="p">),</code>
    <code class="s2">"learning_rate"</code><code class="p">:</code> <code class="n">reciprocal</code><code class="p">(</code><code class="mf">3e-4</code><code class="p">,</code> <code class="mf">3e-2</code><code class="p">),</code>
<code class="p">}</code>

<code class="n">rnd_search_cv</code> <code class="o">=</code> <code class="n">RandomizedSearchCV</code><code class="p">(</code><code class="n">keras_reg</code><code class="p">,</code> <code class="n">param_distribs</code><code class="p">,</code> <code class="n">n_iter</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
<code class="n">rnd_search_cv</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>
                  <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">X_valid</code><code class="p">,</code> <code class="n">y_valid</code><code class="p">),</code>
                  <code class="n">callbacks</code><code class="o">=</code><code class="p">[</code><code class="n">keras</code><code class="o">.</code><code class="n">callbacks</code><code class="o">.</code><code class="n">EarlyStopping</code><code class="p">(</code><code class="n">patience</code><code class="o">=</code><code class="mi">10</code><code class="p">)])</code></pre>

<p>This is identical to what we did in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch02.html#project_chapter">Chapter&nbsp;2</a>, except here we pass extra parameters to the <code>fit()</code> method, and they get relayed to the underlying Keras models. Note that <code>RandomizedSearchCV</code> uses K-fold cross-validation, so it does not use <code>X_valid</code> and <code>y_valid</code>, which are only used for early stopping.</p>

<p>The exploration may last many hours, depending on the hardware, the size of the dataset, the complexity of the model, and the value of <code>n_iter</code> and <code>cv</code>. When it’s over, you can access the best parameters found, the best score, and the trained Keras model like this:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">rnd_search_cv</code><code class="o">.</code><code class="n">best_params_</code>
<code class="go">{'learning_rate': 0.0033625641252688094, 'n_hidden': 2, 'n_neurons': 42}</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">rnd_search_cv</code><code class="o">.</code><code class="n">best_score_</code>
<code class="go">-0.3189529188278931</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">model</code> <code class="o">=</code> <code class="n">rnd_search_cv</code><code class="o">.</code><code class="n">best_estimator_</code><code class="o">.</code><code class="n">model</code></pre>

<p>You can now save this model, evaluate it on the test set, and if you are satisfied with its performance, deploy it to production. Using randomized search is not too hard, and it works well for many fairly simple problems. When training is slow, however, (e.g., for more complex problems with larger datasets), this approach will only explore a tiny portion of the hyperparameter space. You can partially alleviate this problem by assisting the search process manually: first run a quick random search using wide ranges of hyperparameter values, then run another search using smaller ranges of values centered on the best ones found during the first run, and so on. This approach will hopefully zoom in to a good set of hyperparameters. However, this is very time consuming, and probably not the best use of your time.</p>

<p>Fortunately, there are many techniques to explore a search space much more efficiently than randomly. Their core idea is simple: when a region of the space turns out to be good, it should be explored more. Such techniques take care of the “zooming” process for you and lead to much better solutions in much less time. Here are some Python libraries you can use to optimize hyperparameters:</p>
<dl>
<dt><a href="https://github.com/hyperopt/hyperopt">Hyperopt</a></dt>
<dd>
<p>A popular Python library for optimizing over all sorts of complex search spaces (including real values, such as the learning rate, and discrete values, such as the number of layers).</p>
</dd>
<dt><a href="https://github.com/maxpumperla/hyperas">Hyperas</a>, <a href="https://github.com/Avsecz/kopt">kopt</a>, or <a href="https://github.com/autonomio/talos">Talos</a></dt>
<dd>
<p>Optimizing hyperparameters for Keras model (the first two are based on Hyperopt).</p>
</dd>
<dt><a href="https://homl.info/kerastuner">Keras Tuner</a></dt>
<dd>
<p>An easy-to-use hyperparameter optimization library Google made for Keras models, with a hosted service for visualization and analysis.</p>
</dd>
<dt><a href="https://scikit-optimize.github.io/">Scikit-Optimize</a> (skopt)</dt>
<dd>
<p>A general-purpose optimization library. The <code>BayesSearchCV</code> class performs Bayesian optimization using an interface similar to <code>GridSearchCV</code>.</p>
</dd>
<dt><a href="https://github.com/JasperSnoek/spearmint">Spearmint</a></dt>
<dd>
<p>A Bayesian optimization library.</p>
</dd>
<dt><a href="https://github.com/zygmuntz/hyperband">Hyperband</a></dt>
<dd>
<p>A fast hyperparameter tuning library based on the recent <a href="https://homl.info/hyperband">Hyperband paper</a><sup><a data-type="noteref" id="idm46263513051448-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263513051448">22</a></sup> by Lisha Li et al.</p>
</dd>
<dt><a href="https://github.com/rsteca/sklearn-deap">Sklearn-Deap</a></dt>
<dd>
<p>A hyperparameter optimization library based on evolutionary algorithms, with a <code>GridSearchCV</code>-like interface.</p>
</dd>
</dl>

<p>Moreover, many companies offer services for hyperparameter optimization. We’ll discuss Google Cloud AI Platform’s <a href="https://homl.info/googletuning">hyperparameter tuning service</a> in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#deployment_chapter">Chapter&nbsp;19</a>. Other companies provide APIs for hyperparameter optimization, such as <a href="https://arimo.com/">Arimo</a>, <a href="https://sigopt.com/">SigOpt</a>, and <a href="http://oscar.calldesk.ai/">Oscar</a>.</p>

<p>Hyperparameter tuning is still an active area of research. Evolutionary algorithms are making a comeback lately. For example, check out DeepMind’s excellent <a href="https://homl.info/pbt">2017 paper</a>,<sup><a data-type="noteref" id="idm46263513042232-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263513042232">23</a></sup> where the authors jointly optimize a population of models and their hyperparameters. Google also used an evolutionary approach, not just to search for hyperparameters, but also to look for the best neural network architecture for the problem. This is called <em>AutoML</em>, and it is already available as a <a href="https://cloud.google.com/automl/">cloud service</a>. Perhaps the days of building neural networks manually will soon be over? Check out Google’s <a href="https://homl.info/automlpost">post</a> on this topic. In fact, evolutionary algorithms have been used successfully to train individual neural networks, replacing the ubiquitous Gradient Descent! See this <a href="https://homl.info/neuroevol">2017 post</a> by Uber where the authors introduce their <em>Deep Neuroevolution</em> technique.</p>

<p>Despite all this exciting progress and all these tools and services, it still helps to have an idea of what values are reasonable for each hyperparameter so that you can build a quick prototype and restrict the search space. The following sections provide guidelines for choosing the number of hidden layers and neurons in an MLP and for selecting good values for some of the main hyperparameters.</p>








<section data-type="sect2" data-pdf-bookmark="Number of Hidden Layers"><div class="sect2" id="idm46263513036696">
<h2>Number of Hidden Layers</h2>

<p>For many problems, you can begin with a single hidden layer and get reasonable results. An MLP with just one hidden layer can theoretically model even the most complex functions, provided it has enough neurons. But for complex problems, deep networks have a much higher <em>parameter efficiency</em> than shallow ones: they can model complex functions using exponentially fewer neurons than shallow nets, allowing them to reach much better performance with the same amount of training data.</p>

<p>To understand why, suppose you are asked to draw a forest using some drawing software, but you are forbidden to copy and paste anything. You would have to draw each tree individually, branch per branch, leaf per leaf. If you could instead draw one leaf, copy and paste it to draw a branch, then copy and paste that branch to create a tree, and finally copy and paste this tree to make a forest, you would be finished in no time. Real-world data is often structured in such a hierarchical way, and deep neural networks automatically take advantage of this fact: lower hidden layers model low-level structures (e.g., line segments of various shapes and orientations), intermediate hidden layers combine these low-level structures to model intermediate-level structures (e.g., squares, circles), and the highest hidden layers and the output layer combine these intermediate structures to model high-level structures (e.g., faces).</p>

<p>Not only does this hierarchical architecture help DNNs converge faster to a good solution, it also improves their ability to generalize to new datasets. For example, if you have already trained a model to recognize faces in pictures, and you now want to train a new neural network to recognize hairstyles, then you can kickstart training by reusing the lower layers of the first network. Instead of randomly initializing the weights and biases of the first few layers of the new neural network, you can initialize them to the value of the weights and biases of the lower layers of the first network. This way the network will not have to learn from scratch all the low-level structures that occur in most pictures; it will only have to learn the higher-level structures (e.g., hairstyles). This is called <em>transfer learning</em>.</p>

<p>In summary, for many problems you can start with just one or two hidden layers and the neural network will work just fine. For instance, you can easily reach above 97% accuracy on the MNIST dataset using just one hidden layer with a few hundred neurons, and above 98% accuracy using two hidden layers with the same total number of neurons, in roughly the same amount of training time. For more complex problems, you can ramp up the number of hidden layers, until you start overfitting the training set. Very complex tasks, such as large image classification or speech recognition, typically require networks with dozens of layers (or even hundreds, but not fully connected ones, as we will see in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cnn_chapter">Chapter&nbsp;14</a>), and they need a huge amount of training data. You will rarely have to train such networks from scratch: it is much more common to reuse parts of a pretrained state-of-the-art network that performs a similar task. Training will be a lot faster and require much less data (we will discuss this in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>).</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Number of Neurons per Hidden Layer"><div class="sect2" id="idm46263512955032">
<h2>Number of Neurons per Hidden Layer</h2>

<p>The number of neurons in the input and output layers is determined by the type of input and output your task requires. For example, the MNIST task requires 28 x 28 = 784 input neurons and 10 output neurons.</p>

<p>As for the hidden layers, it used to be common to size them to form a pyramid, with fewer and fewer neurons at each layer—the rationale being that many low-level features can coalesce into far fewer high-level features. A typical neural network for MNIST may have three hidden layers, the first with 300 neurons, the second with 200, and the third with 100. However, this practice has been largely abandoned because it seems that using the same number of neurons in all hidden layers performs just as well in most cases, or even better. Plus there is one hyperparameter to tune instead of one per layer. That said, depending on the dataset, it can sometimes help to make the first hidden layer bigger than the others.</p>

<p>Just like for the number of layers, you can try increasing the number of neurons gradually until the network starts overfitting. But in practice it’s often simpler and more efficient to pick a model with more layers and neurons than you actually need, then use early stopping and other regularization techniques to prevent it from overfitting. Vincent Vanhoucke, a scientist at Google, has dubbed this the “stretch pants” approach: instead of wasting time looking for pants that perfectly match your size, just use large stretch pants that will shrink down to the right size. With this approach, you avoid bottleneck layers that could ruin your model. On the flip side, if a layer has too few neurons, it will not have enough representational power to preserve all the useful information from the inputs (e.g., a layer with two neurons can only output 2D data, so if it processes 3D data, some information will be lost). No matter how big and powerful the rest of the network is, that information will never be recovered.</p>
<div data-type="tip"><h6>Tip</h6>
<p>In general you will get more bang for your buck by increasing the number of layers instead of the number of neurons per layer.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Learning Rate, Batch Size, and Other Hyperparameters"><div class="sect2" id="idm46263512949544">
<h2>Learning Rate, Batch Size, and Other Hyperparameters</h2>

<p>The number of hidden layers and neurons are not the only hyperparameters you can tweak in an MLP. Here are some of the most important ones, as well as tips on how to set them:</p>

<ul>
<li>
<p>The learning rate is arguably the most important hyperparameter. In general, the optimal learning rate is about half of the maximum learning rate (i.e., the learning rate above which the training algorithm diverges, as we saw in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>). One way to find a good learning rate is to train the model for a few hundred iterations, starting with a very low learning rate (e.g., 10<sup>-5</sup>) and gradually increase it up to a very large value (e.g., 10). This is done by multiplying the learning rate by a constant factor at each iteration (e.g., by exp(log(10<sup>6</sup>)/500) to go from 10<sup>-5</sup> to 10 in 500 iterations). If you plot the loss as a function of the learning rate (using a log scale for the learning rate), you should see it dropping at first. But after a while, the learning rate will be too large, so the loss will shoot back up: the optimal learning rate will be a bit lower than the point at which the loss starts shooting back up (typically about 10 times lower than the turning point). You can then reinitialize your model and train it normally using this good learning rate. We will look at more learning rate techniques in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>.</p>
</li>
<li>
<p>Choosing a better optimizer than plain old Mini-batch Gradient Descent (and tuning its hyperparameters) is also quite important. We will see several advanced optimizers in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>.</p>
</li>
<li>
<p>The batch size can also have a significant impact on your model’s performance and training time. The main benefit of using large batch sizes is that hardware accelerators like GPUs can process them efficiently (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch19.html#deployment_chapter">Chapter&nbsp;19</a>), so the training algorithm will see more instances per second. Therefore many researchers and practitioners recommend using the largest batch size that can fit in GPU RAM. There’s a catch, though: in practice, large batch sizes often lead to training instabilities, especially at the beginning of training, and the resulting model may not generalize as well as a model trained with a small batch size. In April 2018, Yann LeCun even tweeted “Friends don’t let friends use mini-batches larger than 32”, citing a <a href="https://homl.info/smallbatch">2018 paper</a><sup><a data-type="noteref" id="idm46263512937288-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263512937288">24</a></sup> by Dominic Masters and Carlo Luschi which concluded that using small batches (from 2 to 32) was preferable because small batches led to better models in less training time. Other papers point in the opposite direction: a <a href="https://homl.info/largebatch">2017 paper</a><sup><a data-type="noteref" id="idm46263512935448-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263512935448">25</a></sup> by Elad Hoffer et al. and a <a href="https://homl.info/largebatch2">2017 paper</a><sup><a data-type="noteref" id="idm46263512933496-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263512933496">26</a></sup> by Priya Goyal et al. showed that it was possible to use very large batch sizes (up to 8,192) using various techniques such as warming up the learning rate (i.e., starting training with a small learning rate, then ramping it up, as we will see in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html#deep_chapter">Chapter&nbsp;11</a>). This led to a very short training time, without any generalization gap. So, one strategy is to try to use a large batch size, using learning rate warmup, and if training is unstable or the final performance is disappointing, then try using a small batch size instead.</p>
</li>
<li>
<p>We discussed how to choose the activation function earlier in this chapter: in general, the ReLU activation function will be a good default for all hidden layers. For the output layer, it really depends on your task.</p>
</li>
<li>
<p>In most cases, the number of training iterations does not actually need to be tweaked: just use early stopping instead.</p>
</li>
</ul>
<div data-type="tip"><h6>Tip</h6>
<p>The optimal learning rate depends on the other hyperparameters, especially the batch size. So if you modify any hyperparameter, make sure to update the learning rate as well.</p>
</div>

<p>For more best practices regarding tuning neural network hyperparameters, check out the excellent <a href="https://homl.info/1cycle">2018 paper</a><sup><a data-type="noteref" id="idm46263512926616-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263512926616">27</a></sup> by Leslie Smith.</p>

<p>This concludes our introduction to artificial neural networks and their implementation with Keras. In the next few chapters, we will discuss techniques to train very deep nets. We will also see how to customize your models using TensorFlow’s lower-level API and how to load and preprocess data efficiently using the Data API. And we will dive into other popular neural network architectures: convolutional neural networks for image processing, recurrent neural networks for sequential data, autoencoders for representation learning, and generative adversarial networks to model and generate data.<sup><a data-type="noteref" id="idm46263512924344-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263512924344">28</a></sup></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="idm46263513535464">
<h1>Exercises</h1>
<ol>
<li>
<p>The <a href="https://playground.tensorflow.org/">TensorFlow Playground</a> is a nice neural network simulator built by the TensorFlow team. In this exercise, you will train several binary classifiers in just a few clicks, and tweak the model’s architecture and its hyperparameters to gain some intuitions on how neural networks work and what their hyperparameters do:</p>

<ul>
<li>
<p>Exploring the patterns learned by a neural net: try training the default neural network by clicking the run button (top left). Notice how it quickly finds a good solution for the classification task. Notice that the neurons in the first hidden layer have learned simple patterns, while the neurons in the second hidden layer have learned to combine the simple patterns of the first hidden layer into more complex patterns. In general, the more layers, the more complex the patterns can be.</p>
</li>
<li>
<p>Exploring activation functions: try replacing the Tanh activation function with the ReLU activation function, and train the network again. Notice that it finds a solution even faster, but this time the boundaries are linear. This is due to the shape of the ReLU function.</p>
</li>
<li>
<p>The risk of local minima: modify the network architecture to have just one hidden layer with three neurons. Train it multiple times (to reset the network weights, click the reset button next to the play button). Notice that the training time varies a lot, and sometimes it even gets stuck in a local minimum.</p>
</li>
<li>
<p>What happens when neural nets are too small: now remove one neuron to keep just two. Notice that the neural network is now incapable of finding a good solution, even if you try multiple times. The model has too few parameters and systematically underfits the training set.</p>
</li>
<li>
<p>What happens when neural nets are large enough: next, set the number of neurons to eight, and train the network several times. Notice that it is now consistently fast and never gets stuck. This highlights an important finding in neural network theory: large neural networks almost never get stuck in local minima, and even when they do these local optima are almost as good as the global optimum. However, they can still get stuck on long plateaus for a long time.</p>
</li>
<li>
<p>The risk of vanishing gradients in deep networks: now change the dataset to be the spiral (bottom-right dataset under “DATA”). Change the network architecture to have four hidden layers with eight neurons each. Notice that training takes much longer and often gets stuck on plateaus for long periods of time. Also notice that the neurons in the highest layers (i.e., on the right) tend to evolve faster than the neurons in the lowest layers (i.e., on the left). This problem, called the “vanishing gradients” problem, can be alleviated with better weight initialization and other techniques, better optimizers (such as AdaGrad or Adam), or with Batch Normalization.</p>
</li>
<li>
<p>Going further: take at least an hour to play around with other parameters and get a feel for what they do so that you build an intuitive understanding about neural networks.</p>
</li>
</ul>
</li>
<li>
<p>Draw an ANN using the original artificial neurons (like the ones in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#nn_propositional_logic_diagram">Figure&nbsp;10-3</a>) that computes <em>A</em> ⊕ <em>B</em> (where ⊕ represents the XOR operation). Hint: <em>A</em> ⊕ <em>B</em> = (<em>A</em> ∧ ¬ <em>B</em>) ∨ (¬ <em>A</em> ∧ <em>B</em>).</p>
</li>
<li>
<p>Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of threshold logic units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?</p>
</li>
<li>
<p>Why was the logistic activation function a key ingredient in training the first MLPs?</p>
</li>
<li>
<p>Name three popular activation functions. Can you draw them?</p>
</li>
<li>
<p>Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with three artificial neurons. All artificial neurons use the ReLU activation function.</p>

<ul>
<li>
<p>What is the shape of the input matrix <strong>X</strong>?</p>
</li>
<li>
<p>What about the shape of the hidden layer’s weight vector <strong>W</strong><sub><em>h</em></sub> and the shape of its bias vector <strong>b</strong><sub><em>h</em></sub>?</p>
</li>
<li>
<p>What is the shape of the output layer’s weight vector <strong>W</strong><sub><em>o</em></sub> and its bias vector <strong>b</strong><sub><em>o</em></sub>?</p>
</li>
<li>
<p>What is the shape of the network’s output matrix <strong>Y</strong>?</p>
</li>
<li>
<p>Write the equation that computes the network’s output matrix <strong>Y</strong> as a function of <strong>X</strong>, <strong>W</strong><sub><em>h</em></sub>, <strong>b</strong><sub><em>h</em></sub>, <strong>W</strong><sub><em>o</em></sub>, and <strong>b</strong><sub><em>o</em></sub>.</p>
</li>
</ul>
</li>
<li>
<p>How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, and using which activation function? Answer the same questions for getting your network to predict housing prices as in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch02.html#project_chapter">Chapter&nbsp;2</a>.</p>
</li>
<li>
<p>What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?</p>
</li>
<li>
<p>Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?</p>
</li>
<li>
<p>Train a deep MLP on the MNIST dataset (you can load it using <code>keras.datasets.mnist.load_data()</code>. See if you can get over 98% precision. Try searching for the optimal learning rate by using the approach presented in this chapter (i.e., by growing the learning rate exponentially, plotting the error and finding the point where the error shoots up). Try adding all the bells and whistles (i.e., save checkpoints, use early stopping, and plot learning curves using TensorBoard).</p>
</li>

</ol>

<p>Solutions to these exercises are available in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/app01.html#solutions_appendix">Appendix&nbsp;A</a>.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46263517315512"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517315512-marker" class="totri-footnote">1</a></sup> You can get the best of both worlds by being open to biological inspirations without being afraid to create biologically unrealistic models, as long as they work well.</p><p data-type="footnote" id="idm46263517307848"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517307848-marker" class="totri-footnote">2</a></sup> Warren S. McCulloch and Walter Pitts, “A Logical Calculus of the Ideas Immanent in Nervous Activity,” <em>The Bulletin of Mathematrical Biology</em> 5, no. 4 (December 1943): 115–113.</p><p data-type="footnote" id="idm46263517290312"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517290312-marker" class="totri-footnote">3</a></sup> They are not actually attached, just so close that they can very quickly exchange chemical signals.</p><p data-type="footnote" id="idm46263517286152"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517286152-marker" class="totri-footnote">4</a></sup> Image by Bruce Blaus (<a href="https://creativecommons.org/licenses/by/3.0/">Creative Commons 3.0</a>). Reproduced from <a href="https://en.wikipedia.org/wiki/Neuron"><em class="hyperlink">https://en.wikipedia.org/wiki/Neuron</em></a>.</p><p data-type="footnote" id="idm46263517282680"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517282680-marker" class="totri-footnote">5</a></sup> In the context of Machine Learning, the phrase “neural networks” generally refers to ANNs, not BNNs.</p><p data-type="footnote" id="idm46263517278920"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517278920-marker" class="totri-footnote">6</a></sup> Drawing of a cortical lamination by S.&nbsp;Ramon y Cajal (public domain). Reproduced from <a href="https://en.wikipedia.org/wiki/Cerebral_cortex"><em class="hyperlink">https://en.wikipedia.org/wiki/Cerebral_cortex</em></a>.</p><p data-type="footnote" id="idm46263517197144"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517197144-marker" class="totri-footnote">7</a></sup> The name <em>Perceptron</em> is sometimes used to mean a tiny network with a single TLU.</p><p data-type="footnote" id="idm46263517132120"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517132120-marker" class="totri-footnote">8</a></sup> Note that this solution is not unique: when data are linearly separable, there is an infinity of hyperplanes that can separate them.</p><p data-type="footnote" id="idm46263517042216"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517042216-marker" class="totri-footnote">9</a></sup> In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see ANNs with dozens of layers, or even hundreds, so the definition of “deep” is quite fuzzy.</p><p data-type="footnote" id="idm46263517039512"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517039512-marker">10</a></sup> David Rumelhart et al. <em>Learning Internal Representations by Error Propagation</em> (Defense Technical Information Center technical report, September 1985), <a href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf"><em class="hyperlink">https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf</em></a>.</p><p data-type="footnote" id="idm46263517035800"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517035800-marker">11</a></sup> This technique was actually independently invented several times by various researchers in different fields, starting with P.&nbsp;Werbos in 1974.</p><p data-type="footnote" id="idm46263517009000"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263517009000-marker">12</a></sup> Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck to sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is one of the cases where the biological analogy was misleading.</p><p data-type="footnote" id="idm46263516902424"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263516902424-marker">13</a></sup> Project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).</p><p data-type="footnote" id="idm46263516361496"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263516361496-marker">14</a></sup> You can use <code>keras.utils.plot_model()</code> to generate an image of your model.</p><p data-type="footnote" id="idm46263515961960"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263515961960-marker">15</a></sup> If your training or validation data does not match the expected shape, you will get an exception. This is perhaps the most common error, so you should get familiar with the error message. The message is actually quite clear: for example, if you try to train this model with an array containing flattened images (<code>X_train.reshape(-1, 784)</code>), then you will get the following exception: “ValueError: Error when checking input: expected flatten_input to have 3 dimensions, but got array with shape (60000, 784)”.</p><p data-type="footnote" id="idm46263515437528"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263515437528-marker">16</a></sup> Heng-Tze Cheng et al., <em>Wide &amp; Deep Learning for Recommender Systems</em> (Google Inc., June 2016).</p><p data-type="footnote" id="idm46263515435384"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263515435384-marker">17</a></sup> The short path can also be used to provide manually engineered features to the neural network.</p><p data-type="footnote" id="idm46263515295544"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263515295544-marker">18</a></sup> The name <code>input_</code> is used to avoid overshadowing Python’s built-in <code>input()</code> function.</p><p data-type="footnote" id="idm46263515154968"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263515154968-marker">19</a></sup> Alternatively, you can pass a dictionary mapping the input names to the input values, like <code>{"wide_input": X_train_A, "deep_input": X_train_B}</code>. This is especially useful when there are many inputs, to avoid getting the order wrong.</p><p data-type="footnote" id="idm46263514960952"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263514960952-marker">20</a></sup> Alternatively, you can pass a dictionary that maps each output name to the corresponding loss. Just like for the inputs, this is useful when there are multiple outputs, to avoid getting the order wrong. The loss weights and metrics (discussed shortly) can also be set using dictionaries.</p><p data-type="footnote" id="idm46263514636936"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263514636936-marker">21</a></sup> Keras models have an <code>output</code> attribute, so we cannot use that name for the main output layer, which is why we renamed it to <code>main_output</code>.</p><p data-type="footnote" id="idm46263513051448"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263513051448-marker">22</a></sup> Lisha Li et al., “Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization,” <em>Journal of Machine Learning Research</em> 18 (April 2018): 1–52.</p><p data-type="footnote" id="idm46263513042232"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263513042232-marker">23</a></sup> Max Jaderberg et al., <em>Population Based Training of Neural Networks</em> (London: DeepMind, 2017).</p><p data-type="footnote" id="idm46263512937288"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263512937288-marker">24</a></sup> Dominic Masters and Carlo Luschi, <em>Revisiting Small Batch Training for Deep Neural Networks</em> (London: DeepMind, November 2017).</p><p data-type="footnote" id="idm46263512935448"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263512935448-marker">25</a></sup> Elad Hoffer et al., “Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural Networks” in <em>Advances in Neural Information Processing Systems</em> 30 (NIPS 2017), ed. I. Guyon et al.: 1729–1739.</p><p data-type="footnote" id="idm46263512933496"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263512933496-marker">26</a></sup> Priya Goyal et al., <em>Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</em> (Facebook, April 2018).</p><p data-type="footnote" id="idm46263512926616"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263512926616-marker">27</a></sup> Leslie N. Smith, <em>A Disciplined Approach to Neural Network Hyper-Parameters: Part 1 — Learning Rate, Batch Size, Momentum, and Weight Decay</em> (Washington, DC: US Naval Research Laboratory technical report, April 2018).</p><p data-type="footnote" id="idm46263512924344"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#idm46263512924344-marker">28</a></sup> A few extra ANN architectures are presented in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/app05.html#other_ann_appendix">Appendix&nbsp;E</a>.</p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-11" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders">
		
		<li class="copy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#">Add Highlight</a></li>
		<li class="add-note"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#">
			Add Note
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part02.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">II. Neural Networks and Deep Learning</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch11.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">11. Training Deep Neural Networks</div>
        </a>
    
  
  </div>

</section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    



        
      </div>
      



  <footer class="pagefoot">
    <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#" class="icon-up" style="display: none;"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      <li class="full-support"><a href="https://www.oreilly.com/online-learning/support/">Support</a></li>
      <li><a href="https://www.oreilly.com/online-learning/apps.html">Get the App</a></li>
      
        <li><a href="https://learning.oreilly.com/accounts/logout/">Sign Out</a></li>
      
    </ul>
    <span class="copyright">© 2019 <a href="https://learning.oreilly.com/" target="_blank">Safari</a>.</span>
    <a href="https://learning.oreilly.com/terms/">Terms of Service</a> /
    <a href="https://learning.oreilly.com/membership-agreement/">Membership Agreement</a> /
    <a href="https://www.oreilly.com/privacy.html">Privacy Policy</a>
  </footer>

<script type="text/javascript">window.NREUM||(NREUM={});NREUM.info={"applicationID":"172641827,79672898,93931619","errorBeacon":"bam.nr-data.net","agent":"","applicationTime":451,"licenseKey":"510f1a6865","transactionName":"YgdaZ0NSW0cEB0RdWltNfkZfUEFdCgofXFBHDVYdR1pQQxZeRl1QQj1aWkU=","beacon":"bam.nr-data.net","queueTime":4}</script>


    
    <script src="./Chapter10_files/saved_resource" charset="utf-8"></script><script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","1732687426968531");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=1732687426968531&amp;ev=PageView&amp;noscript=1"></noscript><script type="text/javascript" id="">(function(){window.medalliaUserIdentifier=document.documentElement.dataset.userUuid;window.medalliaUserName=document.documentElement.dataset.username})();</script>
<script type="text/javascript" id="" src="./Chapter10_files/embed.js.download"></script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script>
<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("set","agent","tmgoogletagmanager","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>

<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>
<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.09556792590964802"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.5229133033930915" width="0" height="0" alt="" src="./Chapter10_files/0"></div>
    <script src="./Chapter10_files/saved_resource(1)" charset="utf-8"></script>
  

<script src="./Chapter10_files/saved_resource(2)" type="text/javascript"></script><script type="text/javascript" id="">window._pp=window._pp||[];if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/register/")_pp.targetUrl="/confirm/trial";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/subscribe/")_pp.targetUrl="/confirm/paid";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/signup/")_pp.targetUrl="/confirm/paid";_pp.siteId="2508";
_pp.siteUId="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79";_pp.orderValue="undefined";_pp.orderId="undefined";(function(){var ppjs=document.createElement("script");ppjs.type="text/javascript";ppjs.async=true;ppjs.src=("https:"==document.location.protocol?"https:":"http:")+"//cdn.pbbl.co/r/"+_pp.siteId+".js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(ppjs,s)})();</script><div class="annotator-notice"></div><div class="font-flyout" style="top: 201px; left: 1194px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html#">Reset</a>
</div>
</div><script type="text/javascript" async="" src="./Chapter10_files/generic1566415868241.js.download" charset="UTF-8"></script><div style="display: none; visibility: hidden;"><script>(function(){if(null!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')&&void 0!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')){var a=!1;window.addEventListener("blur",function(){a&&dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"facebook",eventVal:0,nonInteraction:0})});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseover",function(){window.focus();
a=!0});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseout",function(){a=!1})}try{window.twttr=function(b,a,d){var c,e=b.getElementsByTagName(a)[0];if(!b.getElementById(d))return b=b.createElement(a),b.id=d,b.src="//platform.twitter.com/widgets.js",e.parentNode.insertBefore(b,e),window.twttr||(c={_e:[],ready:function(a){c._e.push(a)}})}(document,"script","twitter-wjs"),twttr.ready(function(a){a.events.bind("tweet",trackTwitter)})}catch(b){}})();
null!==document.querySelector(".IN-widget")&&void 0!==document.querySelector(".IN-widget")&&document.querySelector(".IN-widget").addEventListener("click",function(){dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"linkedin",eventVal:0,nonInteraction:0})});
function trackTwitter(a){a&&(a.target&&"IFRAME"==a.target.nodeName&&(opt_target=extractParamFromUri(a.target.src,"url")),dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"twitter",eventVal:0,nonInteraction:0}))}function extractParamFromUri(a,b){if(a){var c=new RegExp("[\\?\x26#]"+b+"\x3d([^\x26#]*)");c=c.exec(a);if(null!=c)return unescape(c[1])}};</script></div><span><div id="KampyleAnimationContainer" style="z-index: 2147483000; border: 0px; position: fixed; display: block; width: 0px; height: 0px;"></div></span><iframe scrolling="no" frameborder="0" allowtransparency="true" src="./Chapter10_files/widget_iframe.097c1f5038f9e8a0d62a39a892838d66.html" title="Twitter settings iframe" style="display: none;"></iframe><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_AMS, sans-serif;"></div></div></body></html>