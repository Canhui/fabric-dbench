<!DOCTYPE html>
<!-- saved from url=(0091)https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html -->
<html class=" js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/hands-on-machine-learning/9781492032632/part01.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="4626953" data-user-uuid="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79" data-username="17481074" data-account-type="B2B" data-activated-trial-date="" data-archive="9781492032632" data-publishers="O&#39;Reilly Media, Inc." data-htmlfile-name="part01.html" data-epub-title="Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781492032632"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><script type="text/javascript" async="" src="./Chapter9_files/cool-2.1.15.min.js.download"></script><script type="text/javascript" src="./Chapter9_files/510f1a6865"></script><script id="twitter-wjs" src="./Chapter9_files/widgets.js.download"></script><script src="./Chapter9_files/nr-1130.min.js.download"></script><script type="text/javascript" async="" src="./Chapter9_files/2508.js.download"></script><script async="" src="./Chapter9_files/fbevents.js.download"></script><script type="text/javascript" async="" src="./Chapter9_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter9_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter9_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter9_files/analytics.js.download"></script><script type="text/javascript" async="" src="./Chapter9_files/ec.js.download"></script><script type="text/javascript" async="" src="./Chapter9_files/bat.js.download"></script><script type="text/javascript" async="" src="./Chapter9_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter9_files/insight.min.js.download"></script><script type="text/javascript" async="" src="./Chapter9_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter9_files/linkid.js.download"></script><script async="" src="./Chapter9_files/gtm.js.download"></script><script async="" src="./Chapter9_files/analytics.js.download"></script><script type="text/javascript">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if("function"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e("handle"),a=e(3),u=e(4),f=e("ee").get("tracer"),c=e("loader"),s=NREUM;"undefined"==typeof window.newrelic&&(newrelic=s);var p=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],d="api-",l=d+"ixn-";a(p,function(e,n){s[n]=o(d+n,!0,"api")}),s.addPageAction=o(d+"addPageAction",!0),s.setCurrentRouteName=o(d+"routeName",!0),n.exports=newrelic,s.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(e,n){var t={},r=this,o="function"==typeof n;return i(l+"tracer",[c.now(),e,t],r),function(){if(f.emit((o?"":"no-")+"fn-start",[c.now(),r,o],t),o)try{return n.apply(this,arguments)}catch(e){throw f.emit("fn-err",[arguments,this,e],t),e}finally{f.emit("fn-end",[c.now()],t)}}}};a("actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(e,n){m[n]=o(l+n)}),newrelic.noticeError=function(e,n){"string"==typeof e&&(e=new Error(e)),i("err",[e,c.now(),!1,n])}},{}],2:[function(e,n,t){function r(e,n){if(!o)return!1;if(e!==o)return!1;if(!n)return!0;if(!i)return!1;for(var t=i.split("."),r=n.split("."),a=0;a<r.length;a++)if(r[a]!==t[a])return!1;return!0}var o=null,i=null,a=/Version\/(\S+)\s+Safari/;if(navigator.userAgent){var u=navigator.userAgent,f=u.match(a);f&&u.indexOf("Chrome")===-1&&u.indexOf("Chromium")===-1&&(o="Safari",i=f[1])}n.exports={agent:o,version:i,match:r}},{}],3:[function(e,n,t){function r(e,n){var t=[],r="",i=0;for(r in e)o.call(e,r)&&(t[i]=n(r,e[r]),i+=1);return t}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],4:[function(e,n,t){function r(e,n,t){n||(n=0),"undefined"==typeof t&&(t=e?e.length:0);for(var r=-1,o=t-n||0,i=Array(o<0?0:o);++r<o;)i[r]=e[n+r];return i}n.exports=r},{}],5:[function(e,n,t){n.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(e,n,t){function r(){}function o(e){function n(e){return e&&e instanceof r?e:e?f(e,u,i):i()}function t(t,r,o,i){if(!d.aborted||i){e&&e(t,r,o);for(var a=n(o),u=v(t),f=u.length,c=0;c<f;c++)u[c].apply(a,r);var p=s[y[t]];return p&&p.push([b,t,r,a]),a}}function l(e,n){h[e]=v(e).concat(n)}function m(e,n){var t=h[e];if(t)for(var r=0;r<t.length;r++)t[r]===n&&t.splice(r,1)}function v(e){return h[e]||[]}function g(e){return p[e]=p[e]||o(t)}function w(e,n){c(e,function(e,t){n=n||"feature",y[t]=n,n in s||(s[n]=[])})}var h={},y={},b={on:l,addEventListener:l,removeEventListener:m,emit:t,get:g,listeners:v,context:n,buffer:w,abort:a,aborted:!1};return b}function i(){return new r}function a(){(s.api||s.feature)&&(d.aborted=!0,s=d.backlog={})}var u="nr@context",f=e("gos"),c=e(3),s={},p={},d=n.exports=o();d.backlog=s},{}],gos:[function(e,n,t){function r(e,n,t){if(o.call(e,n))return e[n];var r=t();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,n,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return e[n]=r,r}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],handle:[function(e,n,t){function r(e,n,t,r){o.buffer([e],r),o.emit(e,n,t)}var o=e("ee").get("handle");n.exports=r,r.ee=o},{}],id:[function(e,n,t){function r(e){var n=typeof e;return!e||"object"!==n&&"function"!==n?-1:e===window?0:a(e,i,function(){return o++})}var o=1,i="nr@id",a=e("gos");n.exports=r},{}],loader:[function(e,n,t){function r(){if(!E++){var e=x.info=NREUM.info,n=l.getElementsByTagName("script")[0];if(setTimeout(s.abort,3e4),!(e&&e.licenseKey&&e.applicationID&&n))return s.abort();c(y,function(n,t){e[n]||(e[n]=t)}),f("mark",["onload",a()+x.offset],null,"api");var t=l.createElement("script");t.src="https://"+e.agent,n.parentNode.insertBefore(t,n)}}function o(){"complete"===l.readyState&&i()}function i(){f("mark",["domContent",a()+x.offset],null,"api")}function a(){return O.exists&&performance.now?Math.round(performance.now()):(u=Math.max((new Date).getTime(),u))-x.offset}var u=(new Date).getTime(),f=e("handle"),c=e(3),s=e("ee"),p=e(2),d=window,l=d.document,m="addEventListener",v="attachEvent",g=d.XMLHttpRequest,w=g&&g.prototype;NREUM.o={ST:setTimeout,SI:d.setImmediate,CT:clearTimeout,XHR:g,REQ:d.Request,EV:d.Event,PR:d.Promise,MO:d.MutationObserver};var h=""+location,y={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-1130.min.js"},b=g&&w&&w[m]&&!/CriOS/.test(navigator.userAgent),x=n.exports={offset:u,now:a,origin:h,features:{},xhrWrappable:b,userAgent:p};e(1),l[m]?(l[m]("DOMContentLoaded",i,!1),d[m]("load",r,!1)):(l[v]("onreadystatechange",o),d[v]("onload",r)),f("mark",["firstbyte",u],null,"api");var E=0,O=e(5)},{}]},{},["loader"]);</script><link rel="apple-touch-icon" href="https://learning.oreilly.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://learning.oreilly.com/favicon.ico" type="image/x-icon"><link href="./Chapter9_files/css" rel="stylesheet" type="text/css"><title>9. Unsupervised Learning Techniques - Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition</title><link rel="stylesheet" href="./Chapter9_files/output.68851547a55f.css" type="text/css"><link rel="stylesheet" type="text/css" href="./Chapter9_files/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="./Chapter9_files/font-awesome.min.css"><style type="text/css" title="ibis-book">@charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000 !important;padding-top:10px !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:2em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content p[data-type="footnote"]{font-size:90% !important;line-height:1.2em !important;margin-left:2.5em !important;text-indent:-2.3em !important}#sbo-rt-content p[data-type="footnote"] sup{display:inline-block !important;position:static !important;width:2em !important;text-align:right !important;font-size:100% !important;padding-right:.5em !important}#sbo-rt-content p[data-type="footnote"] a[href$="-marker"]{font-family:sans-serif !important;font-size:90% !important;color:#8e0012 !important}#sbo-rt-content p[data-type="footnote"] a[data-type="xref"]{margin:0 !important;padding:0 !important;text-indent:0 !important}#sbo-rt-content a[data-type="noteref"]{font-family:sans-serif !important;color:#8e0012;margin-left:0;padding-left:0}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dd{margin-left:1.5em !important;margin-bottom:.25em}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul>li,#sbo-rt-content ol ul,#sbo-rt-content ol ul>li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul>li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul>li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul>li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol>li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol>li,#sbo-rt-content ul ol,#sbo-rt-content ul ol>li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol>li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol>li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:middle;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content table.border tbody>tr:last-child>td{border-bottom:transparent}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:1em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content .width-10{width:10vw !important}#sbo-rt-content .width-20{width:20vw !important}#sbo-rt-content .width-30{width:30vw !important}#sbo-rt-content .width-40{width:40vw !important}#sbo-rt-content .width-50{width:50vw !important}#sbo-rt-content .width-60{width:60vw !important}#sbo-rt-content .width-70{width:70vw !important}#sbo-rt-content .width-80{width:80vw !important}#sbo-rt-content .width-90{width:90vw !important}#sbo-rt-content .width-full,#sbo-rt-content .width-100{width:100vw !important}#sbo-rt-content div[data-type="equation"].fifty-percent img{width:50%}</style><script> // <![CDATA[
    var g = {
      position_cache: {
        
          "chapter": "/api/v1/book/9781492032632/chapter/part01.html",
          "book_id": "9781492032632",
          "chapter_uri": "part01.html",
          "position": 100.0,
          "user_uuid": "d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79",
          "next_chapter_uri": "/library/view/hands-on-machine-learning/9781492032632/ch01.html"
        
      },
      title: "Hands\u002Don Machine Learning with Scikit\u002DLearn, Keras, and TensorFlow, 2nd Edition",
      author_list: "Aurélien Géron",
      format: "book",
      source: "application/epub+zip",
      is_system_book: true,
      is_public: false,
      loaded_from_server: true,
      allow_scripts: false,
      has_mathml: true
    };
    // ]]></script><script src="./Chapter9_files/modernizr.8e35451ddb64.js.download"></script><script>
    
      

      
        
          window.PUBLIC_ANNOTATIONS = true;
        
      

      window.MOBILE_PUBLIC_ANNOTATIONS = false;

    

    
      window.PRIVACY_CONTROL_OVERRIDE = false;
    

      window.PRIVACY_CONTROL_SWITCH = true;

      window.PUBLISHER_PAGES = true;

      window.SBO = {
        "constants": {
          "SITB_ENDPOINT": "/api/v2/sitb/",
          "SEARCH_SELECT_ENDPOINT": "https://learning.oreilly.com/api/v2/search/select/",
          "ENABLE_ONLINE_TRAINING": false
        }
      };
  </script><link rel="canonical" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta name="description" content=" Part I. The Fundamentals of Machine Learning "><meta property="og:title" content="I. The Fundamentals of Machine Learning"><meta itemprop="isPartOf" content="/library/view/hands-on-machine-learning/9781492032632/"><meta itemprop="name" content="I. The Fundamentals of Machine Learning"><meta property="og:url" itemprop="url" content="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://learning.oreilly.com/library/cover/9781492032632/"><meta property="og:description" itemprop="description" content=" Part I. The Fundamentals of Machine Learning "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O&#39;Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781492032649"><meta property="og:book:author" itemprop="author" content="Aurélien Géron"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: &lt;%= font_size %&gt; !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: &lt;%= font_family %&gt; !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: &lt;%= column_width %&gt;% !important; margin: 0 auto !important; }"></style><noscript><meta http-equiv="refresh" content="0; url=/library/no-js/" /></noscript><script type="text/javascript">
    (function(i,s,o,g,r,a,m) {
      i['GoogleAnalyticsObject']=r;
      i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();
      a=s.createElement(o),m=s.getElementsByTagName(o)[0];
      a.async=1;
      a.src=g;
      m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    var matches = document.cookie.match(/BrowserCookie\s*=\s*([a-f0-9\-]{36})/),
      user_uuid = null;

    if (matches && matches.length === 2) {
      user_uuid = matches[1];
    }

  
    ga('create', 'UA-39299553-7', {'userId': 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79' });
  

  
    
      ga('set', 'dimension1', 'B2B');
    
  

  ga('set', 'dimension6', user_uuid);

  
    ga('set', 'dimension2', 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79');
    
      ga('set', 'dimension7', '0012M0000229keZQAQ');
    
  

  

  

  //enable enhanced link tracking
  ga('require', 'linkid', 'linkid.js');

  // reading interface will track pageviews itself
  if (document.location.pathname.indexOf("/library/view") !== 0) {
    ga('send', 'pageview');
  }
  </script><script>
    var dataLayer = window.dataLayer || [];

    
      window.medalliaVsgUserIdentifier = 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79';
      dataLayer.push({userIdentifier: 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79'});
      dataLayer.push({loggedIn: 'yes'});

      
        window.medalliaVsgAccountIdentifier = '21bed0a7-6b7b-470c-8fa0-40a52db0b491';
        
        dataLayer.push({orgID: '21bed0a7-6b7b-470c-8fa0-40a52db0b491'});
        

        window.medalliaVsgIsIndividual = false;
        
          
          dataLayer.push({learningAccountType: 'enterprise'});
          
        

        
          dataLayer.push({learningPaidAccount: 'yes'});
        
      
    

    (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-5P4V6Z');
    (function () {
      var VERSION = 'V1.1';
      var AUTHOR = 'Awwad';
      if (!window.GtmHelper)
        window.GtmHelper = function () {
          var instance = this;
          var loc = document.location;
          this.version = VERSION;
          this.author = AUTHOR;
          this.readCookie = function (name) {
            var nameEQ = name + "=";
            var ca = document.cookie.split(';');
            for (var i = 0; i < ca.length; i++) {
              var c = ca[i];
              while (c.charAt(0) == ' ') c = c.substring(1, c.length);
              if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length, c.length);
            }
            return null;
          };
          this.createCookie = function (name, value, days, cookieDomain) {
            var domain = "";
            var expires = "";

            if (days) {
              var date = new Date();
              date.setTime(date.getTime() + Math.ceil(days * 24 * 60 * 60 * 1000));
              var expires = " expires=" + date.toGMTString() + ";";
            }

            if (typeof (cookieDomain) != 'undefined')
              domain = " domain=" + cookieDomain + "; ";

            document.cookie = name + "=" + value + ";" + expires + domain + "path=/";
          };

          this.isDuplicated = function (currentTransactionId) {
            // the previous transaction id:
            var previousTransIdValue = this.readCookie("previousTransId");

            if (currentTransactionId === previousTransIdValue) {
              return true; // Duplication
            } else {
              return false;
            }
          };
        }
    })()
  </script><script defer="" src="./Chapter9_files/vendor.a48a756c5182.js.download"></script><script defer="" src="./Chapter9_files/reader.f2a0c6bd2fee.js.download"></script><script src="./Chapter9_files/f(1).txt"></script><script src="./Chapter9_files/f(2).txt"></script><script src="./Chapter9_files/f(3).txt"></script><script src="./Chapter9_files/f(4).txt"></script><script async="" src="./Chapter9_files/MathJax.js.download"></script><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 2147483020;
}
.annotator-filter {
  z-index: 2147483010;
}</style><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
</style><script async="true" type="text/javascript" src="./Chapter9_files/roundtrip.js.download"></script><style type="text/css" id="kampyleStyle">.noOutline{outline: none !important;}.wcagOutline:focus{outline: 1px dashed #595959 !important;outline-offset: 2px !important;transition: none !important;}</style><script async="true" type="text/javascript" src="./Chapter9_files/roundtrip.js.download"></script><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Math-bold-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Script; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_AMS; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf?V=2.7.1') format('opentype')}
</style></head>


<body class="reading sidenav  scalefonts library nav-collapsed"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

    
  <noscript> 
    <iframe src="//www.googletagmanager.com/ns.html?id=GTM-5P4V6Z"
            height="0" width="0"
            style="display:none;visibility:hidden">
    </iframe>
  </noscript>



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#container" class="skip">Skip to content</a><header class="topbar t-topbar"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://learning.oreilly.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M11.738 14H9.254v-3.676a.617.617 0 0 0-.621-.613H7.39a.617.617 0 0 0-.62.613V14H4.284a.617.617 0 0 1-.622-.613V10.22c0-.327.132-.64.367-.87l3.547-3.493a.627.627 0 0 1 .875 0l3.54 3.499c.234.229.366.54.367.864v3.167a.617.617 0 0 1-.62.613zM7.57 2.181a.625.625 0 0 1 .882 0l5.77 5.692-.93.92-5.28-5.209-5.28 5.208-.932-.919 5.77-5.692z"></path></svg><span>Safari Home</span></a></li><li><a href="https://learning.oreilly.com/resource-centers/" class="t-resource-centers-nav l0 nav-icn"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="16px" height="16px" viewBox="0 0 16 16" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="Topic-Page-Design" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="Heron-Button" transform="translate(-20.000000, -78.000000)" fill="#4A3A30"><g id="Group-9" transform="translate(20.000000, 78.000000)"><rect id="Rectangle" x="9.6" y="0" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="9.6" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="0" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect></g></g></g></svg><span>Resource Centers</span></a></li><li><a href="https://learning.oreilly.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://learning.oreilly.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://learning.oreilly.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://learning.oreilly.com/learning-paths/" class="l1 nav-icn t-learningpaths-nav js-toggle-menu-item"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="32px" height="32px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 52.5 (67469) - http://www.bohemiancoding.com/sketch --><title>Mask</title><desc>Created with Sketch.</desc><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><path d="M0,16.0214227 C0,15.0387209 0.796453294,14.2411658 1.77779753,14.2411658 C2.75914177,14.2411658 3.55559506,15.0387209 3.55559506,16.0214227 C3.55559506,17.0041246 2.75914177,17.8016797 1.77779753,17.8016797 C0.796453294,17.8016797 0,17.0041246 0,16.0214227 Z M9.77788642,5.22914885 C8.9280992,5.72049977 7.84008711,5.42853763 7.34941499,4.57757479 C6.85874287,3.72661195 7.15030167,2.63709467 8.00008889,2.14574375 C8.84987611,1.65439282 9.9378882,1.94635496 10.4285603,2.7973178 C10.9192324,3.64828064 10.6276736,4.73779792 9.77788642,5.22914885 Z M4.57213969,7.35869225 C5.42192691,7.85004318 5.71348571,8.93956046 5.22281359,9.79052329 C4.73214147,10.6414861 3.64412938,10.9334483 2.79434216,10.4420974 C1.94455494,9.95074642 1.65299614,8.86122915 2.14366826,8.01026631 C2.63434038,7.15930347 3.72235247,6.86734132 4.57213969,7.35869225 Z M2.79434216,21.6007481 C3.64412938,21.1093972 4.73214147,21.4013594 5.22281359,22.2523222 C5.71348571,23.103285 5.42192691,24.1928023 4.57213969,24.6841532 C3.72235247,25.1755042 2.63434038,24.883542 2.14366826,24.0325792 C1.65299614,23.1816163 1.94455494,22.0920991 2.79434216,21.6007481 Z M7.34941499,27.4652707 C7.84008711,26.6143079 8.9280992,26.3223457 9.77788642,26.8136966 C10.6276736,27.3050476 10.9192324,28.3945649 10.4285603,29.2455277 C9.9378882,30.0964905 8.84987611,30.3884527 8.00008889,29.8971017 C7.15030167,29.4057508 6.85874287,28.3162335 7.34941499,27.4652707 Z M18.7118524,11.3165596 C21.3074367,12.8173162 22.1963355,16.1392758 20.6976522,18.738451 C19.1989689,21.3358459 15.8815987,22.2259744 13.2860143,20.726998 C10.6922077,19.2262414 9.80330893,15.9042818 11.3002144,13.3051066 C12.7988978,10.7059314 16.116268,9.81580294 18.7118524,11.3165596 Z M26.7821642,27.8093944 L30.1315348,31.1633985 C30.3982044,31.4304371 30.2097579,31.8844026 29.8346426,31.8844026 L21.5945511,31.8844026 C21.1287681,31.8844026 20.751875,31.5069881 20.751875,31.0405608 L20.751875,22.7890697 C20.751875,22.4134355 21.2052134,22.2247282 21.4701052,22.4899865 L24.2843587,25.3081333 C26.8337204,23.0240636 28.4444049,19.7092251 28.4444049,16.0223129 C28.4444049,9.15052091 22.8621207,3.56051397 15.9998222,3.56051397 L15.9998222,0 C24.8230314,0 32,7.18689745 32,16.0223129 C32,20.6919269 29.9750886,24.8790914 26.7821642,27.8093944 Z" id="Mask" fill="#8B889A"></path></g></svg><span>Learning Paths</span></a></li><li class="nav-highlights"><a href="https://learning.oreilly.com/u/d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" fill="#4A3C31"><path d="M13.325 18.071H8.036c0-6.736 4.324-10.925 14.464-12.477V0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35c5.142 0 9.175-3.515 9.175-8.816 0-4.628-2.367-7.293-6.253-8.113zm27.5 0h-5.26c0-6.736 4.295-10.925 14.435-12.477V0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35c5.113 0 9.146-3.515 9.146-8.816 0-4.628-2.338-7.293-6.253-8.113z" fill-rule="evenodd"></path></svg><span>Highlights</span></a></li><li><a href="https://learning.oreilly.com/u/preferences/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l1 no-icon">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://learning.oreilly.com/u/preferences/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l2">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9781492032632/chapter/part01.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"><div data-reactroot="" class="menu-dropdown-wrapper js-menu-dropdown-wrapper align-right"><img class="hidden" src="./Chapter9_files/ajax-transp.gif" alt="loading spinner"><div class="menu-control"><div class="control "><div class="js-playlists-menu"><button class="js-playlist-icon"><svg class="icon-add-to-playlist-sml" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill-rule="nonzero" fill="#000000"><g transform="translate(-1.000000, 0.000000)"><rect x="5" y="0" width="12" height="2"></rect><title>Playlists</title><path d="M4.5,14 C6.43299662,14 8,12.4329966 8,10.5 C8,8.56700338 6.43299662,7 4.5,7 C2.56700338,7 1,8.56700338 1,10.5 C1,12.4329966 2.56700338,14 4.5,14 Z M2.5,10 L4,10 L4,8.5 L5,8.5 L5,10 L6.5,10 L6.5,11 L5,11 L5,12.5 L4,12.5 L4,11 L2.5,11 L2.5,10 Z"></path><circle cx="2" cy="5" r="1"></circle><circle cx="1.94117647" cy="1" r="1"></circle><rect x="5" y="4" width="12" height="2"></rect><rect x="9" y="8" width="8" height="2"></rect><rect x="9" y="12" width="8" height="2"></rect></g></g></g></svg><div class="js-playlist-addto-label">Add&nbsp;To</div></button></div></div></div></div></div></div></li><li class="js-font-control-panel font-control-activator"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html&amp;text=Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%20I.%20The%20Fundamentals%20of%20Machine%20Learning&amp;body=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html%0D%0Afrom%20Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    
    <section role="document">
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch08.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">8. Dimensionality Reduction</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part02.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">II. Neural Networks and Deep Learning</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content"><div class="annotator-wrapper"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 9. Unsupervised Learning Techniques"><div class="chapter" id="unsupervised_learning_chapter">
<h1><span class="label">Chapter 9. </span>Unsupervised Learning Techniques</h1>


<p>Although most of the applications of Machine Learning today are based on supervised learning (and as a result, this is where most of the investments go to), the vast majority of the available data is unlabeled: we have the input features <strong>X</strong>, but we do not have the labels <strong>y</strong>. The computer scientist Yann LeCun famously said that “if intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake”. In other words, there is a huge potential in unsupervised learning that we have only barely started to sink our teeth into.</p>

<p>Say you want to create a system that will take a few pictures of each item on a manufacturing production line and detect which items are defective. You can fairly easily create a system that will take pictures automatically, and this might give you thousands of pictures every day. You can then build a reasonably large dataset in just a few weeks. But wait, there are no labels! If you want to train a regular binary classifier that will predict whether an item is defective or not, you will need to label every single picture as “defective” or “normal”. This will generally require human experts to sit down and manually go through all the pictures. This is a long, costly, and tedious task, so it will usually only be done on a small subset of the available pictures. As a result, the labeled dataset will be quite small, and the classifier’s performance will be disappointing. Moreover, every time the company makes any change to its products, the whole process will need to be started over from scratch. Wouldn’t it be great if the algorithm could just exploit the unlabeled data without needing humans to label every picture? Enter unsupervised learning.</p>

<p>In <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch08.html#dimensionality_chapter">Chapter&nbsp;8</a> we looked at the most common unsupervised learning task: dimensionality reduction. In this chapter we will look at a few more unsupervised learning tasks and algorithms:</p>
<dl>
<dt>Clustering</dt>
<dd>
<p>The goal is to group similar instances together into <em>clusters</em>. Clustering is a great tool for data analysis, customer segmentation, recommender systems, search engines, image segmentation, semi-supervised learning, dimensionality reduction, and more.</p>
</dd>
<dt>Anomaly detection</dt>
<dd>
<p>The objective is to learn what “normal” data looks like, and then use that to detect abnormal instances, such as defective items on a production line or a new trend in a time series.</p>
</dd>
<dt>Density estimation</dt>
<dd>
<p>This is the task of estimating the <em>probability density function</em> (PDF) of the random process that generated the dataset. Density estimation is commonly used for anomaly detection: instances located in very low-density regions are likely to be anomalies. It is also useful for data analysis and visualization.</p>
</dd>
</dl>

<p>Ready for some cake? We will start with clustering, using K-Means and DBSCAN, and then we will discuss Gaussian mixture models and see how they can be used for density estimation, clustering, and anomaly detection.</p>






<section data-type="sect1" data-pdf-bookmark="Clustering"><div class="sect1" id="idm46263520289752">
<h1>Clustering</h1>

<p>As you enjoy a hike in the mountains, you stumble upon a plant you have never seen before. You look around and you notice a few more. They are not identical, yet they are sufficiently similar for you to know that they most likely belong to the same species (or at least the same genus). You may need a botanist to tell you what species that is, but you certainly don’t need an expert to identify groups of similar-looking objects. This is called <em>clustering</em>: it is the task of identifying similar instances and assigning them to <em>clusters</em>, that is, groups of similar instances.</p>

<p>Just like in classification, each instance gets assigned to a group. However, unlike classification, clustering is an unsupervised task. Consider <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#classification_vs_clustering_plot">Figure&nbsp;9-1</a>: on the left is the iris dataset (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>), where each instance’s species (i.e., its class) is represented with a different marker. It is a labeled dataset, for which classification algorithms such as Logistic Regression, SVMs, or Random Forest classifiers are well suited. On the right is the same dataset, but without the labels, so you cannot use a classification algorithm anymore. This is where clustering algorithms step in: many of them can easily detect the top-left cluster. It is also quite easy to see with our own eyes, but it is not so obvious that the lower-right cluster is composed of two distinct sub-clusters. That said, the dataset has two additional features (sepal length and width), not represented here, and clustering algorithms can make good use of all features, so in fact they identify the three clusters fairly well (e.g., using a Gaussian mixture model, only 5 instances out of 150 are assigned to the wrong cluster).</p>

<figure class="smallerseventy"><div id="classification_vs_clustering_plot" class="figure">
<img src="./Chapter9_files/mls2_0901.png" alt="mls2 0901" width="1441" height="521" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0901.png">
<h6><span class="label">Figure 9-1. </span>Classification (left) versus clustering (right)</h6>
</div></figure>

<p>Clustering is used in a wide variety of applications, including these:</p>
<dl>
<dt>For customer segmentation</dt>
<dd>
<p>You can cluster your customers based on their purchases, and their activity on your website. This is useful to understand who your customers are and what they need, so you can adapt your products and marketing campaigns to each segment. For example, customer segmentation can be useful in <em>recommender systems</em> to suggest content that other users in the same cluster enjoyed.</p>
</dd>
<dt>For data analysis</dt>
<dd>
<p>When you analyze a new dataset, it can be helpful to run a clustering algorithm, and then analyze each cluster separately.</p>
</dd>
<dt>As a dimensionality reduction technique</dt>
<dd>
<p>Once a dataset has been clustered, it is usually possible to measure each instance’s <em>affinity</em> with each cluster (affinity is any measure of how well an instance fits into a cluster). Each instance’s feature vector <strong>x</strong> can then be replaced with the vector of its cluster affinities. If there are <em>k</em> clusters, then this vector is <em>k</em> dimensional. This vector is typically much lower dimensional than the original feature vector, but it can preserve enough information for further processing.
For <em>anomaly detection</em> (also called <em>outlier detection</em>):
Any instance that has a low affinity to all the clusters is likely to be an anomaly. For example, if you have clustered the users of your website based on their behavior, you can detect users with unusual behavior, such as an unusual number of requests per second. Anomaly detection is particularly useful in detecting defects in manufacturing, or for <em>fraud detection</em>.</p>
</dd>
<dt>For semi-supervised learning</dt>
<dd>
<p>If you only have a few labels, you could perform clustering and propagate the labels to all the instances in the same cluster. This technique can greatly increase the number of labels available for a subsequent supervised learning algorithm, and thus improve its performance.</p>
</dd>
<dt>For search engines</dt>
<dd>
<p>Some search engines let you search for images that are similar to a reference image. To build such a system, you would first apply a clustering algorithm to all the images in your database; similar images would end up in the same cluster. Then when a user provides a reference image, all you need to do is use the trained clustering model to find this image’s cluster, and you can then simply return all the images from this cluster.</p>

<ul>
<li>
<p>To segment an image: by clustering pixels according to their color, then replacing each pixel’s color with the mean color of its cluster, it is possible to considerably reduce the number of different colors in the image. Segmenting an image is used in many object detection and tracking systems, as it makes it easier to detect the contour of each object.</p>
</li>
</ul>
</dd>
</dl>

<p>There is no universal definition of what a cluster is: it really depends on the context, and different algorithms will capture different kinds of clusters. Some algorithms look for instances centered around a particular point, called a <em>centroid</em>. Others look for continuous regions of densely packed instances: these clusters can take on any shape. Some algorithms are hierarchical, looking for clusters of clusters. And the list goes on.</p>

<p>In this section, we will look at two popular clustering algorithms: K-Means and DBSCAN, and we will show some of their applications, such as nonlinear dimensionality reduction, semi-supervised learning, and anomaly detection.</p>








<section data-type="sect2" data-pdf-bookmark="K-Means"><div class="sect2" id="idm46263520264536">
<h2>K-Means</h2>

<p>Consider the unlabeled dataset represented in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#blobs_plot">Figure&nbsp;9-2</a>: you can clearly see five blobs of instances. The K-Means algorithm is a simple algorithm capable of clustering this kind of dataset very quickly and efficiently, often in just a few iterations. It was proposed by Stuart Lloyd at the Bell Labs in 1957 as a technique for pulse-code modulation, but it was only published outside of the company in 1982, in a paper titled <a href="https://homl.info/36">“Least square quantization in PCM”</a>.<sup><a data-type="noteref" id="idm46263520260616-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#idm46263520260616" class="totri-footnote">1</a></sup> By then, in 1965, Edward W. Forgy had published virtually the same algorithm, so K-Means is sometimes referred to as Lloyd-Forgy.</p>

<figure class="smallerseventy"><div id="blobs_plot" class="figure">
<img src="./Chapter9_files/mls2_0902.png" alt="mls2 0902" width="1441" height="680" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0902.png">
<h6><span class="label">Figure 9-2. </span>An unlabeled dataset composed of five blobs of instances</h6>
</div></figure>

<p>Let’s train a K-Means clusterer on this dataset. It will try to find each blob’s center and assign each instance to the closest blob:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">KMeans</code>
<code class="n">k</code> <code class="o">=</code> <code class="mi">5</code>
<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="n">k</code><code class="p">)</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">fit_predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

<p>Note that you have to specify the number of clusters <em>k</em> that the algorithm must find. In this example, it is pretty obvious from looking at the data that <em>k</em> should be set to 5, but in general it is not that easy. We will discuss this shortly.</p>

<p>Each instance was assigned to one of the five clusters. In the context of clustering, an instance’s <em>label</em> is the index of the cluster that this instance gets assigned to by the algorithm: this is not to be confused with the class labels in classification (remember that clustering is an unsupervised learning task). The <code>KMeans</code> instance preserves a copy of the labels of the instances it was trained on, available via the <code>labels_</code> instance variable:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code>
<code class="go">array([4, 0, 1, ..., 2, 1, 0], dtype=int32)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code> <code class="ow">is</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code>
<code class="go">True</code></pre>

<p>We can also take a look at the five centroids that the algorithm found:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">kmeans</code><code class="o">.</code><code class="n">cluster_centers_</code>
<code class="go">array([[-2.80389616,  1.80117999],</code>
<code class="go">       [ 0.20876306,  2.25551336],</code>
<code class="go">       [-2.79290307,  2.79641063],</code>
<code class="go">       [-1.46679593,  2.28585348],</code>
<code class="go">       [-2.80037642,  1.30082566]])</code></pre>

<p>You can easily assign new instances to the cluster whose centroid is closest:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mf">2.5</code><code class="p">]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">kmeans</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>
<code class="go">array([1, 1, 2, 2], dtype=int32)</code></pre>

<p>If you plot the cluster’s decision boundaries, you get a Voronoi tessellation (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#voronoi_plot">Figure&nbsp;9-3</a>, where each centroid is represented with an X).</p>

<figure class="smallerseventy"><div id="voronoi_plot" class="figure">
<img src="./Chapter9_files/mls2_0903.png" alt="mls2 0903" width="1441" height="680" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0903.png">
<h6><span class="label">Figure 9-3. </span>K-Means decision boundaries (Voronoi tessellation)</h6>
</div></figure>

<p>The vast majority of the instances were clearly assigned to the appropriate cluster, but a few instances were probably mislabeled (especially near the boundary between the top-left cluster and the central cluster). Indeed, the K-Means algorithm does not behave very well when the blobs have very different diameters because all it cares about when assigning an instance to a cluster is the distance to the centroid.</p>

<p>Instead of assigning each instance to a single cluster, which is called <em>hard clustering</em>, it can be useful to give each instance a score per cluster, which is called <em>soft clustering</em>. The score can be the distance between the instance and the centroid; conversely, it can be a similarity score (or affinity), such as the Gaussian Radial Basis Function (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch05.html#svm_chapter">Chapter&nbsp;5</a>). In the <code>KMeans</code> class, the <code>transform()</code> method measures the distance from each instance to every centroid:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">kmeans</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>
<code class="go">array([[2.81093633, 0.32995317, 2.9042344 , 1.49439034, 2.88633901],</code>
<code class="go">       [5.80730058, 2.80290755, 5.84739223, 4.4759332 , 5.84236351],</code>
<code class="go">       [1.21475352, 3.29399768, 0.29040966, 1.69136631, 1.71086031],</code>
<code class="go">       [0.72581411, 3.21806371, 0.36159148, 1.54808703, 1.21567622]])</code></pre>

<p>In this example, the first instance in <code>X_new</code> is located at a distance of 2.81 from the first centroid, 0.33 from the second centroid, 2.90 from the third centroid, 1.49 from the fourth centroid and 2.89 from the fifth centroid. If you have a high-dimensional dataset and you transform it this way, you end up with a <em>k</em>-dimensional dataset: this transformation can be a very efficient nonlinear dimensionality reduction technique.</p>










<section data-type="sect3" data-pdf-bookmark="The K-Means algorithm"><div class="sect3" id="idm46263520070840">
<h3>The K-Means algorithm</h3>

<p>So, how does the algorithm work? Well, suppose you were given the centroids. You could easily label all the instances in the dataset by assigning each of them to the cluster whose centroid is closest. Conversely, if you were given all the instance labels, you could easily locate all the centroids by computing the mean of the instances for each cluster. But you are given neither the labels nor the centroids, so how can you proceed? Well, just start by placing the centroids randomly (e.g., by picking <em>k</em> instances at random and using their locations as centroids). Then label the instances, update the centroids, label the instances, update the centroids, and so on until the centroids stop moving. The algorithm is guaranteed to converge in a finite number of steps (usually quite small); it will not oscillate forever.<sup><a data-type="noteref" id="idm46263520068088-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#idm46263520068088" class="totri-footnote">2</a></sup> You can see the algorithm in action in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#kmeans_algorithm_plot">Figure&nbsp;9-4</a>: the centroids are initialized randomly (top left), then the instances are labeled (top right), then the centroids are updated (center left), the instances are relabeled (center right), and so on. As you can see, in just three iterations, the algorithm has reached a clustering that seems close to optimal.</p>

<figure class="smallerseventy"><div id="kmeans_algorithm_plot" class="figure">
<img src="./Chapter9_files/mls2_0904.png" alt="mls2 0904" width="1441" height="1142" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0904.png">
<h6><span class="label">Figure 9-4. </span>The K-Means algorithm</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The computational complexity of the algorithm is generally linear with regard to the number of instances <em>m</em>, the number of clusters <em>k</em>, and the number of dimensions <em>n</em>. However, this is only true when the data has a clustering structure. If it does not, then in the worst-case scenario the complexity can increase exponentially with the number of instances. In practice, this rarely happens, and K-Means is generally one of the fastest clustering algorithms.</p>
</div>

<p>Although the algorithm is guaranteed to converge, it may not converge to the right solution (i.e., it may converge to a local optimum): whether it does or not depends on the centroid initialization. <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#kmeans_variability_plot">Figure&nbsp;9-5</a> shows two suboptimal solutions that the algorithm can converge to if you are not lucky with the random initialization step.</p>

<figure class="smallerseventy"><div id="kmeans_variability_plot" class="figure">
<img src="./Chapter9_files/mls2_0905.png" alt="mls2 0905" width="1441" height="432" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0905.png">
<h6><span class="label">Figure 9-5. </span>Suboptimal solutions due to unlucky centroid initializations</h6>
</div></figure>

<p>Let’s look at a few ways you can mitigate this risk by improving the centroid <span class="keep-together">initialization</span>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Centroid initialization methods"><div class="sect3" id="idm46263520032472">
<h3>Centroid initialization methods</h3>

<p>If you happen to know approximately where the centroids should be (e.g., if you ran another clustering algorithm earlier), then you can set the <code>init</code> hyperparameter to a NumPy array containing the list of centroids, and set <code>n_init</code> to 1:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">good_init</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">]])</code>
<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">init</code><code class="o">=</code><code class="n">good_init</code><code class="p">,</code> <code class="n">n_init</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>

<p>Another solution is to run the algorithm multiple times with different random initializations and keep the best solution. The number of random initializations is controlled by the <code>n_init</code> hyperparameter: by default, it is equal to 10, which means that the whole algorithm described earlier runs 10 times when you call <code>fit()</code>, and Scikit-Learn keeps the best solution. But how exactly does it know which solution is the best? It uses a performance metric! That metric is called the model’s <em>inertia</em>, which is the mean squared distance between each instance and its closest centroid. It is roughly equal to 223.3 for the model on the left in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#kmeans_variability_plot">Figure&nbsp;9-5</a>, 237.5 for the model on the right in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#kmeans_variability_plot">Figure&nbsp;9-5</a>, and 211.6 for the model in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#voronoi_plot">Figure&nbsp;9-3</a>. The <code>KMeans</code> class runs the algorithm <code>n_init</code> times and keeps the model with the lowest inertia. In this example, the model in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#voronoi_plot">Figure&nbsp;9-3</a> will be selected (unless we are very unlucky with <code>n_init</code> consecutive random initializations). If you are curious, a model’s inertia is accessible via the <code>inertia_</code> instance variable:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">kmeans</code><code class="o">.</code><code class="n">inertia_</code>
<code class="go">211.59853725816856</code></pre>

<p>The <code>score()</code> method returns the negative inertia. Why negative? Because a predictor’s <code>score()</code> method must always respect Scikit-Learn’s “greater is better” rule: if a predictor is better than another, its score() method should return a greater score.</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">kmeans</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="go">-211.59853725816856</code></pre>

<p>An important improvement to the K-Means algorithm, <em>K-Means++</em>, was proposed in a <a href="https://homl.info/37">2006 paper</a> by David Arthur and Sergei Vassilvitskii.:<sup><a data-type="noteref" id="idm46263519924344-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#idm46263519924344" class="totri-footnote">3</a></sup> They introduced a smarter initialization step that tends to select centroids that are distant from one another, and this improvement makes the K-Means algorithm much less likely to converge to a suboptimal solution. They showed that the additional computation required for the smarter initialization step is well worth it because it makes it possible to drastically reduce the number of times the algorithm needs to be run to find the optimal solution. Here is the K-Means++ initialization algorithm:</p>
<ol>
<li>
<p>Take one centroid <strong>c</strong><sup>(1)</sup>, chosen uniformly at random from the dataset.</p>
</li>
<li>
<p>Take a new centroid <strong>c</strong><sup>(<em>i</em>)</sup>, choosing an instance <strong>x</strong><sup>(<em>i</em>)</sup> with probability <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-104-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x1D431;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4239" style="width: 3.756em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.653em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(0.877em, 1003.66em, 2.625em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-4240"><span class="mrow" id="MathJax-Span-4241"><span class="mi" id="MathJax-Span-4242" style="font-family: MathJax_Math-italic;">D</span><span class="msup" id="MathJax-Span-4243"><span style="display: inline-block; position: relative; width: 2.83em; height: 0px;"><span style="position: absolute; clip: rect(2.985em, 1002.22em, 4.527em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-4244"><span class="mo" id="MathJax-Span-4245" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="msup" id="MathJax-Span-4246"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4247" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.62em;"><span class="mrow" id="MathJax-Span-4248"><span class="mo" id="MathJax-Span-4249" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4250" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-4251" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4252" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.625em; left: 2.419em;"><span class="mn" id="MathJax-Span-4253" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.421em; border-left: 0px solid; width: 0px; height: 1.591em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msup><mrow><mo>(</mo><msup><mi>𝐱</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo></mrow><mn>2</mn></msup></mrow></math></span></span><script type="math/mml" id="MathJax-Element-104"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow>
    <mi>D</mi>
    <msup><mrow><mo>(</mo><msup><mi>𝐱</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup><mo>)</mo></mrow> <mn>2</mn> </msup>
  </mrow>
</math></script><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-105-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/munderover&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x1D431;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4254" style="width: 6.378em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.172em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(0.877em, 1006.18em, 2.676em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-4255"><span class="mrow" id="MathJax-Span-4256"><span class="munderover" id="MathJax-Span-4257"><span style="display: inline-block; position: relative; width: 2.316em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1000.99em, 4.424em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-4258" style="font-family: MathJax_Size1; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.55em, 1000.68em, 4.167em, -1000.01em); top: -4.47em; left: 1.082em;"><span class="mi" id="MathJax-Span-4259" style="font-size: 70.7%; font-family: MathJax_Math-italic;">m</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.3em, 4.321em, -1000.01em); top: -3.699em; left: 1.082em;"><span class="mrow" id="MathJax-Span-4260"><span class="mi" id="MathJax-Span-4261" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span class="mo" id="MathJax-Span-4262" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="mn" id="MathJax-Span-4263" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msup" id="MathJax-Span-4264" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 3.704em; height: 0px;"><span style="position: absolute; clip: rect(2.985em, 1003.1em, 4.527em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-4265"><span class="mi" id="MathJax-Span-4266" style="font-family: MathJax_Math-italic;">D</span><span class="mo" id="MathJax-Span-4267" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="msup" id="MathJax-Span-4268"><span style="display: inline-block; position: relative; width: 1.545em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4269" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.62em;"><span class="mrow" id="MathJax-Span-4270"><span class="mo" id="MathJax-Span-4271" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4272" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span class="mo" id="MathJax-Span-4273" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4274" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.625em; left: 3.293em;"><span class="mn" id="MathJax-Span-4275" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.474em; border-left: 0px solid; width: 0px; height: 1.697em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mrow><mi>D</mi><mo>(</mo><msup><mi>𝐱</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup><mo>)</mo></mrow><mn>2</mn></msup></mrow></math></span></span><script type="math/mml" id="MathJax-Element-105"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow>
    <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </munderover>
    <msup><mrow><mi>D</mi><mo>(</mo><msup><mi>𝐱</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msup><mo>)</mo></mrow> <mn>2</mn> </msup>
  </mrow>
</math></script>, where D(<strong>x</strong><sup>(<em>i</em>)</sup>) is the distance between the instance <strong>x</strong><sup>(<em>i</em>)</sup> and the closest centroid that was already chosen. This probability distribution ensures that instances farther away from already chosen centroids are much more likely be selected as centroids.</p>
</li>
<li>
<p>Repeat the previous step until all <em>k</em> centroids have been chosen.</p>
</li>

</ol>

<p>The <code>KMeans</code> class uses this initialization method by default. If you want to force it to use the original method (i.e., picking <em>k</em> instances randomly to define the initial centroids), then you can set the <code>init</code> hyperparameter to <code>"random"</code>. You will rarely need to do this.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Accelerated K-Means and mini-batch K-Means"><div class="sect3" id="idm46263520031816">
<h3>Accelerated K-Means and mini-batch K-Means</h3>

<p>Another important improvement to the K-Means algorithm was proposed in a <a href="https://homl.info/38">2003 paper</a> by Charles Elkan.<sup><a data-type="noteref" id="idm46263519845480-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#idm46263519845480" class="totri-footnote">4</a></sup> It considerably accelerates the algorithm by avoiding many unnecessary distance calculations. Elkan achieved this by exploiting the triangle inequality (i.e., the straight line is always the shortest<sup><a data-type="noteref" id="idm46263519844344-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#idm46263519844344" class="totri-footnote">5</a></sup>) and by keeping track of lower and upper bounds for distances between instances and centroids. This is the algorithm the <code>KMeans</code> class uses by default (but you can force it to use the original algorithm by setting the <code>algorithm</code> hyperparameter to <code>"full"</code>, although you probably will never need to).</p>

<p>Yet another important variant of the K-Means algorithm was proposed in a <a href="https://homl.info/39">2010 paper</a> by David Sculley.<sup><a data-type="noteref" id="idm46263519841000-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#idm46263519841000" class="totri-footnote">6</a></sup> Instead of using the full dataset at each iteration, the algorithm is capable of using mini-batches, moving the centroids just slightly at each iteration. This speeds up the algorithm typically by a factor of three or four and makes it possible to cluster huge datasets that do not fit in memory. Scikit-Learn implements this algorithm in the <code>MiniBatchKMeans</code> class. You can just use this class like the <code>KMeans</code> class:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">MiniBatchKMeans</code>

<code class="n">minibatch_kmeans</code> <code class="o">=</code> <code class="n">MiniBatchKMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
<code class="n">minibatch_kmeans</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

<p>If the dataset does not fit in memory, the simplest option is to use the <code>memmap</code> class, as we did for incremental PCA in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch08.html#dimensionality_chapter">Chapter&nbsp;8</a>. Alternatively, you can pass one mini-batch at a time to the <code>partial_fit()</code> method, but this will require much more work, since you will need to perform multiple initializations and select the best one yourself (see the notebook for an example).</p>

<p>Although the Mini-batch K-Means algorithm is much faster than the regular K-Means algorithm, its inertia is generally slightly worse, especially as the number of clusters increases. You can see this in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#minibatch_kmeans_vs_kmeans_plot">Figure&nbsp;9-6</a>: the plot on the left compares the inertias of Mini-batch K-Means and regular K-Means models trained on the previous dataset using various numbers of clusters <em>k</em>. The difference between the two curves remains fairly constant, but this difference becomes more and more significant as <em>k</em> increases, since the inertia becomes smaller and smaller. In the plot on the right, you can see that Mini-batch K-Means is much faster than regular K-Means, and this difference increases with <em>k</em>.</p>

<figure class="smallerseventy"><div id="minibatch_kmeans_vs_kmeans_plot" class="figure">
<img src="./Chapter9_files/mls2_0906.png" alt="mls2 0906" width="1440" height="540" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0906.png">
<h6><span class="label">Figure 9-6. </span>Mini-batch K-Means has a higher inertia than K-Means (left) but it is much faster (right), especially as <em>k</em> increases</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Finding the optimal number of clusters"><div class="sect3" id="idm46263519811432">
<h3>Finding the optimal number of clusters</h3>

<p>So far, we have set the number of clusters <em>k</em> to 5 because it was obvious by looking at the data that this was the correct number of clusters. But in general, it will not be so easy to know how to set <em>k</em>, and the result might be quite bad if you set it to the wrong value. As you can see in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#bad_n_clusters_plot">Figure&nbsp;9-7</a>, setting <em>k</em> to 3 or 8 results in fairly bad models.</p>

<figure class="smallerseventy"><div id="bad_n_clusters_plot" class="figure">
<img src="./Chapter9_files/mls2_0907.png" alt="mls2 0907" width="1441" height="424" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0907.png">
<h6><span class="label">Figure 9-7. </span>Bad choices for the number of clusters: when k is too small, separate clusters get merged (left), and when k is too large, some clusters get chopped into multiple pieces (right)</h6>
</div></figure>

<p>You might be thinking that we could just pick the model with the lowest inertia, right? Unfortunately, it is not that simple. The inertia for <em>k</em>=3 is 653.2, which is much higher than for <em>k</em>=5 (which was 211.6). But with <em>k</em>=8, the inertia is just 119.1. The inertia is not a good performance metric when trying to choose <em>k</em> because it keeps getting lower as we increase <em>k</em>. Indeed, the more clusters there are, the closer each instance will be to its closest centroid, and therefore the lower the inertia will be. Let’s plot the inertia as a function of <em>k</em> (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#inertia_vs_k_plot">Figure&nbsp;9-8</a>).</p>

<figure class="smallerseventy"><div id="inertia_vs_k_plot" class="figure">
<img src="./Chapter9_files/mls2_0908.png" alt="mls2 0908" width="1439" height="579" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0908.png">
<h6><span class="label">Figure 9-8. </span>When plotting the inertia as a function of the number of clusters <em>k</em>, the curve often contains an inflexion point called the “elbow”</h6>
</div></figure>

<p>As you can see, the inertia drops very quickly as we increase <em>k</em> up to 4, but then it decreases much more slowly as we keep increasing <em>k</em>. This curve has roughly the shape of an arm, and there is an “elbow” at <em>k</em> = 4. So, if we did not know better, 4 would be a good choice: any lower value would be dramatic, while any higher value would not help much, and we might just be splitting perfectly good clusters in half for no good reason.</p>

<p>This technique for choosing the best value for the number of clusters is rather coarse. A more precise approach (but also more computationally expensive) is to use the <em>silhouette score</em>, which is the mean <em>silhouette coefficient</em> over all the instances. An instance’s silhouette coefficient is equal to  (<em>b</em> – <em>a</em>) / max(<em>a</em>, <em>b</em>), where <em>a</em> is the mean distance to the other instances in the same cluster (it is the mean intra-cluster distance), and <em>b</em> is the mean nearest-cluster distance, that is, the mean distance to the instances of the next closest cluster (defined as the one that minimizes <em>b</em>, excluding the instance’s own cluster). The silhouette coefficient can vary between -1 and +1. A coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to -1 means that the instance may have been assigned to the wrong cluster. To compute the silhouette score, you can use Scikit-Learn’s <code>silhouette_score()</code> function, giving it all the instances in the dataset and the labels they were assigned:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">silhouette_score</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">silhouette_score</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code><code class="p">)</code>
<code class="go">0.655517642572828</code></pre>

<p>Let’s compare the silhouette scores for different numbers of clusters (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#silhouette_score_vs_k_plot">Figure&nbsp;9-9</a>).</p>

<figure class="smallerseventy"><div id="silhouette_score_vs_k_plot" class="figure">
<img src="./Chapter9_files/mls2_0909.png" alt="mls2 0909" width="1440" height="491" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0909.png">
<h6><span class="label">Figure 9-9. </span>Selecting the number of clusters <em>k</em> using the silhouette score</h6>
</div></figure>

<p>As you can see, this visualization is much richer than the previous one: although it confirms that <em>k</em> = 4 is a very good choice, it also underlines the fact that <em>k</em> = 5 is quite good as well, and much better than <em>k</em> = 6 or 7. This was not visible when comparing inertias.</p>

<p>An even more informative visualization is obtained when you plot every instance’s silhouette coefficient, sorted by the cluster they are assigned to and by the value of the coefficient. This is called a <em>silhouette diagram</em> (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#silhouette_analysis_plot">Figure&nbsp;9-10</a>).</p>

<figure class="smallerseventy"><div id="silhouette_analysis_plot" class="figure">
<img src="./Chapter9_files/mls2_0910.png" alt="mls2 0910" width="1440" height="1164" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0910.png">
<h6><span class="label">Figure 9-10. </span>Analyzing the silhouette diagrams for various values of <em>k</em>: each diagram contains one knife shape per cluster; a shape’s height indicates the number of instances the cluster contains; a shape’s width represents the sorted silhouette coefficients of the instances in the cluster (wider is better); the dashed line indicates the mean silhouette coefficient</h6>
</div></figure>

<p>The vertical dashed lines represent the silhouette score for each number of clusters. When most of the instances in a cluster have a lower coefficient than this score (i.e., if many of the instances stop short of the dashed line, ending to the left of it), then the cluster is rather bad since this means its instances are much too close to other clusters. We can see that when <em>k</em> = 3 and when <em>k</em> = 6, we get bad clusters. But when <em>k</em> = 4 or <em>k</em> = 5, the clusters look pretty good: most instances extend beyond the dashed line, to the right and closer to 1.0. When <em>k</em> = 4, the cluster at index 1 (the third from the top), is rather big. When <em>k</em> = 5, all clusters have similar sizes. So, even though the overall silhouette score from <em>k</em> = 4 is slightly greater than for <em>k</em> = 5, it seems like a good idea to use <em>k</em> = 5 to get clusters of similar sizes.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Limits of K-Means"><div class="sect2" id="idm46263520263720">
<h2>Limits of K-Means</h2>

<p>Despite its many merits, most notably being fast and scalable, K-Means is not perfect. As we saw, it is necessary to run the algorithm several times to avoid suboptimal solutions, plus you need to specify the number of clusters, which can be quite a hassle. Moreover, K-Means does not behave very well when the clusters have varying sizes, different densities, or nonspherical shapes. For example, <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#bad_kmeans_plot">Figure&nbsp;9-11</a> shows how K-Means clusters a dataset containing three ellipsoidal clusters of different dimensions, densities and orientations:</p>

<figure class="smallerseventy"><div id="bad_kmeans_plot" class="figure">
<img src="./Chapter9_files/mls2_0911.png" alt="mls2 0911" width="1440" height="426" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0911.png">
<h6><span class="label">Figure 9-11. </span>K-Means fails to cluster these ellipsoidal blobs properly</h6>
</div></figure>

<p>As you can see, neither of these solutions is any good. The solution on the left is better, but it still chops off 25% of the middle cluster and assigns it to the cluster on the right. The solution on the right is just terrible, even though its inertia is lower. So, depending on the data, different clustering algorithms may perform better. On these types of elliptical clusters, Gaussian mixture models work great.</p>
<div data-type="tip"><h6>Tip</h6>
<p>It is important to scale the input features before you run K-Means, or else the clusters may be very stretched, and K-Means will perform poorly. Scaling the features does not guarantee that all the clusters will be nice and spherical, but it generally improves things.</p>
</div>

<p>Now let’s look at a few ways we can benefit from clustering. We will use K-Means, but feel free to experiment with other clustering algorithms.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Using Clustering for Image Segmentation"><div class="sect2" id="idm46263519724296">
<h2>Using Clustering for Image Segmentation</h2>

<p><em>Image segmentation</em> is the task of partitioning an image into multiple segments. In <em>semantic segmentation</em>, all pixels that are part of the same object type get assigned to the same segment. For example, in a self-driving car’s vision system, all pixels that are part of a pedestrian’s image might be assigned to the “pedestrian” segment (there would be one segment containing all the pedestrians). In <em>instance segmentation</em>, all pixels that are part of the same individual object are assigned to the same segment. In this case there would be a different segment for each pedestrian. The state of the art in semantic or instance segmentation today is achieved using complex architectures based on convolutional neural networks (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch14.html#cnn_chapter">Chapter&nbsp;14</a>). Here, we are going to do something much simpler: <em>color segmentation</em>. We will simply assign pixels to the same segment if they have a similar color. In some applications, this may be sufficient. For example, if you want to analyze satellite images to measure how much total forest area there is in a region, color segmentation may be just fine.</p>

<p>First, use Matplotlib’s imread() function to load the image (see the upper left image in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#image_segmentation_plot">Figure&nbsp;9-12</a>):</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">matplotlib.image</code> <code class="kn">import</code> <code class="n">imread</code>  <code class="c"># or `from imageio import imread`</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">image</code> <code class="o">=</code> <code class="n">imread</code><code class="p">(</code><code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="s">"images"</code><code class="p">,</code><code class="s">"unsupervised_learning"</code><code class="p">,</code><code class="s">"ladybug.png"</code><code class="p">))</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">image</code><code class="o">.</code><code class="n">shape</code>
<code class="go">(533, 800, 3)</code></pre>

<p>The image is represented as a 3D array. The first dimension’s size is the height; the second is the width; and the third is the number of color channels, in this case red, green, and blue (RGB). In other words, for each pixel there is a 3D vector containing the intensities of red, green and blue, each between 0.0 and 1.0 (or between 0 and 255, if you use <code>imageio.imread()</code>). Some images may have fewer channels, such as grayscale images (one channel). And some images may have more channels, such as images with an additional <em>alpha channel</em> for transparency or satellite images, which often contain channels for many light frequencies (e.g., infrared). The following code reshapes the array to get a long list of RGB colors, then it clusters these colors using K-Means.</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">X</code> <code class="o">=</code> <code class="n">image</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">8</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="n">segmented_img</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">cluster_centers_</code><code class="p">[</code><code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code><code class="p">]</code>
<code class="n">segmented_img</code> <code class="o">=</code> <code class="n">segmented_img</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">image</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre>

<p>For example, it may identify a color cluster for all shades of green. Next, for each color (e.g., dark green), it looks for the mean color of the pixel’s color cluster. For example, all shades of green may be replaced with the same light green color (assuming the mean color of the green cluster is light green). Finally it reshapes this long list of colors to get the same shape as the original image. And we’re done!</p>

<p>This outputs the image shown in the upper right of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#image_segmentation_plot">Figure&nbsp;9-12</a>. You can experiment with various numbers of clusters, as shown in the figure. When you use fewer than eight clusters, notice that the ladybug’s flashy red color fails to get a cluster of its own: it gets merged with colors from the environment. This is because K-Means prefers clusters of similar sizes. The lady bug is small, much smaller than the rest of the image; so even though its color is flashy, K-Means fails to dedicate a cluster to it.</p>

<figure class="smallerseventy"><div id="image_segmentation_plot" class="figure">
<img src="./Chapter9_files/mls2_0912.png" alt="mls2 0912" width="1441" height="717" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0912.png">
<h6><span class="label">Figure 9-12. </span>Image segmentation using K-Means with various numbers of color clusters</h6>
</div></figure>

<p>That was not too hard, was it? Now let’s look at another application of clustering: preprocessing.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Using Clustering for Preprocessing"><div class="sect2" id="idm46263519723672">
<h2>Using Clustering for Preprocessing</h2>

<p>Clustering can be an efficient approach to dimensionality reduction, in particular as a preprocessing step before a supervised learning algorithm. As an example of using clustering for dimensionality reduction, let’s tackle the <em>digits dataset</em>, which is a simple MNIST-like dataset containing 1,797 grayscale 8 × 8 images representing digits 0 to 9. First, load the dataset:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_digits</code>

<code class="n">X_digits</code><code class="p">,</code> <code class="n">y_digits</code> <code class="o">=</code> <code class="n">load_digits</code><code class="p">(</code><code class="n">return_X_y</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code></pre>

<p>Now, split it into a training set and a test set:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X_digits</code><code class="p">,</code> <code class="n">y_digits</code><code class="p">)</code></pre>

<p>Next, fit a Logistic Regression model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>

<code class="n">log_reg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>
<code class="n">log_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>

<p>Let’s evaluate its accuracy on the test set:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="go">0.9688888888888889</code></pre>

<p>OK, that’s our baseline: 96.9% accuracy. Let’s see if we can do better by using K-Means as a preprocessing step. We will create a pipeline that will first cluster the training set into 50 clusters and replace the images with their distances to these 50 clusters, then apply a logistic regression model.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Although it is tempting to define the number of clusters to 10, since there are 10 different digits, it is unlikely to perform well, because there are several different ways to write each digit.</p>
</div>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">Pipeline</code>

<code class="n">pipeline</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">([</code>
    <code class="p">(</code><code class="s2">"kmeans"</code><code class="p">,</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">50</code><code class="p">)),</code>
    <code class="p">(</code><code class="s2">"log_reg"</code><code class="p">,</code> <code class="n">LogisticRegression</code><code class="p">()),</code>
<code class="p">])</code>
<code class="n">pipeline</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>

<p>Now let’s evaluate this classification pipeline:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">pipeline</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="go">0.9777777777777777</code></pre>

<p>How about that? We reduced the error rate by almost 30% (from about 3.1% to about 2.2%)!</p>

<p>But we chose the number of clusters <em>k</em> arbitrarily; we can surely do better. Since K-Means is just a preprocessing step in a classification pipeline, finding a good value for <em>k</em> is much simpler than earlier: there’s no need to perform silhouette analysis or minimize the inertia; the best value of <em>k</em> is simply the one that results in the best classification performance during cross-validation. Use <code>GridSearchCV</code> to find the optimal number of clusters:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">GridSearchCV</code>

<code class="n">param_grid</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">(</code><code class="n">kmeans__n_clusters</code><code class="o">=</code><code class="nb">range</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">100</code><code class="p">))</code>
<code class="n">grid_clf</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">pipeline</code><code class="p">,</code> <code class="n">param_grid</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">grid_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>

<p>Let’s look at best value for <em>k</em> and the performance of the resulting pipeline:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">grid_clf</code><code class="o">.</code><code class="n">best_params_</code>
<code class="go">{'kmeans__n_clusters': 99}</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">grid_clf</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="go">0.9822222222222222</code></pre>

<p>With <em>k</em>&nbsp;=&nbsp;99 clusters, we get a significant accuracy boost, reaching 98.22% accuracy on the test set. Cool! You may want to keep exploring higher values for <em>k</em>, since 99 was the largest value in the range we explored.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Using Clustering for Semi-Supervised Learning"><div class="sect2" id="idm46263519589528">
<h2>Using Clustering for Semi-Supervised Learning</h2>

<p>Another use case for clustering is in semi-supervised learning, when we have plenty of unlabeled instances and very few labeled instances. Let’s train a logistic regression model on a sample of 50 labeled instances from the digits dataset:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">n_labeled</code> <code class="o">=</code> <code class="mi">50</code>
<code class="n">log_reg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>
<code class="n">log_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:</code><code class="n">n_labeled</code><code class="p">],</code> <code class="n">y_train</code><code class="p">[:</code><code class="n">n_labeled</code><code class="p">])</code></pre>

<p>What is the performance of this model on the test set?</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="go">0.8333333333333334</code></pre>

<p>The accuracy is just 83.3%. It should come as no surprise that this is much lower than earlier, when we trained the model on the full training set. Let’s see how we can do better. First, let’s cluster the training set into 50 clusters. Then for each cluster, let’s find the image closest to the centroid. We will call these images the representative images:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">k</code> <code class="o">=</code> <code class="mi">50</code>
<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="n">k</code><code class="p">)</code>
<code class="n">X_digits_dist</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>
<code class="n">representative_digit_idx</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">argmin</code><code class="p">(</code><code class="n">X_digits_dist</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">X_representative_digits</code> <code class="o">=</code> <code class="n">X_train</code><code class="p">[</code><code class="n">representative_digit_idx</code><code class="p">]</code></pre>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#representative_images_plot">Figure&nbsp;9-13</a> shows these 50 representative images:</p>

<figure class="smallerseventy"><div id="representative_images_plot" class="figure">
<img src="./Chapter9_files/mls2_0913.png" alt="mls2 0913" width="1440" height="367" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0913.png">
<h6><span class="label">Figure 9-13. </span>Fifty representative digit images (one per cluster)</h6>
</div></figure>

<p>Let’s look at each image and manually label it:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">y_representative_digits</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">4</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="o">...</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">])</code></pre>

<p>Now we have a dataset with just 50 labeled instances, but instead of being random instances, each of them is a representative image of its cluster. Let’s see if the performance is any better:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_representative_digits</code><code class="p">,</code> <code class="n">y_representative_digits</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="go">0.9222222222222223</code></pre>

<p>Wow! We jumped from 83.3% accuracy to 92.2%, although we are still only training the model on 50 instances. Since it is often costly and painful to label instances, especially when it has to be done manually by experts, it is a good idea to label representative instances rather than just random instances.</p>

<p>But perhaps we can go one step further: what if we propagated the labels to all the other instances in the same cluster? This is called <em>label propagation</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">y_train_propagated</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">empty</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">),</code> <code class="n">dtype</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">int32</code><code class="p">)</code>
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">k</code><code class="p">):</code>
    <code class="n">y_train_propagated</code><code class="p">[</code><code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code><code class="o">==</code><code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="n">y_representative_digits</code><code class="p">[</code><code class="n">i</code><code class="p">]</code></pre>

<p>Now let’s train the model again and look at its performance:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train_propagated</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="go">0.9333333333333333</code></pre>

<p>We got a reasonable accuracy boost, but nothing absolutely astounding. The problem is that we propagated each representative instance’s label to all the instances in the same cluster, including the instances located close to the cluster boundaries, which are more likely to be mislabeled. Let’s see what happens if we only propagate the labels to the 20% of the instances that are closest to the centroids:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">percentile_closest</code> <code class="o">=</code> <code class="mi">20</code>
<code class="err">​</code>
<code class="n">X_cluster_dist</code> <code class="o">=</code> <code class="n">X_digits_dist</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)),</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code><code class="p">]</code>
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">k</code><code class="p">):</code>
    <code class="n">in_cluster</code> <code class="o">=</code> <code class="p">(</code><code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code> <code class="o">==</code> <code class="n">i</code><code class="p">)</code>
    <code class="n">cluster_dist</code> <code class="o">=</code> <code class="n">X_cluster_dist</code><code class="p">[</code><code class="n">in_cluster</code><code class="p">]</code>
    <code class="n">cutoff_distance</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">percentile</code><code class="p">(</code><code class="n">cluster_dist</code><code class="p">,</code> <code class="n">percentile_closest</code><code class="p">)</code>
    <code class="n">above_cutoff</code> <code class="o">=</code> <code class="p">(</code><code class="n">X_cluster_dist</code> <code class="o">&gt;</code> <code class="n">cutoff_distance</code><code class="p">)</code>
    <code class="n">X_cluster_dist</code><code class="p">[</code><code class="n">in_cluster</code> <code class="o">&amp;</code> <code class="n">above_cutoff</code><code class="p">]</code> <code class="o">=</code> <code class="o">-</code><code class="mi">1</code>

<code class="n">partially_propagated</code> <code class="o">=</code> <code class="p">(</code><code class="n">X_cluster_dist</code> <code class="o">!=</code> <code class="o">-</code><code class="mi">1</code><code class="p">)</code>
<code class="n">X_train_partially_propagated</code> <code class="o">=</code> <code class="n">X_train</code><code class="p">[</code><code class="n">partially_propagated</code><code class="p">]</code>
<code class="n">y_train_partially_propagated</code> <code class="o">=</code> <code class="n">y_train_propagated</code><code class="p">[</code><code class="n">partially_propagated</code><code class="p">]</code></pre>

<p>Now let’s train the model again on this partially propagated dataset:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_partially_propagated</code><code class="p">,</code> <code class="n">y_train_partially_propagated</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="go">0.94</code></pre>

<p>Nice! With just 50 labeled instances (only 5 examples per class on average!), we got 94.0% performance, which is pretty close to the performance of logistic regression on the fully labeled digits dataset (which was 96.9%). This good performance is due to the fact that the propagated labels are actually pretty good—their accuracy is very close to 99%, as the following code shows:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">y_train_partially_propagated</code> <code class="o">==</code> <code class="n">y_train</code><code class="p">[</code><code class="n">partially_propagated</code><code class="p">])</code>
<code class="go">0.9896907216494846</code></pre>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46263518688104">
<h5>Active Learning</h5>
<p>To continue improving your model and your training set, the next step could be to do a few rounds of <em>active learning</em>, which is when a human expert interacts with the learning algorithm, providing labels for specific instances when the algorithm requests them. There are many different strategies for active learning, but one of the most common ones is called <em>uncertainty sampling</em>. Here is how it works:</p>
<ol>
<li>
<p>The model is trained on the labeled instances gathered so far, and this model is used to make predictions on all the unlabeled instances.</p>
</li>
<li>
<p>The instances for which the model is most uncertain (i.e., when its estimated probability is lowest) must be labeled by the expert.</p>
</li>
<li>
<p>Then you iterate this process until the performance improvement stops being worth the labeling effort.</p>
</li>

</ol>

<p>Other strategies include labeling the instances that would result in the largest model change, or the largest drop in the model’s validation error, or the instances that different models disagree on (e.g., an SVM, or a Random Forest).</p>
</div></aside>

<p>Before we move on to Gaussian mixture models, let’s take a look at DBSCAN, another popular clustering algorithm that illustrates a very different approach based on local density estimation. This approach allows the algorithm to identify clusters of arbitrary shapes.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="DBSCAN"><div class="sect2" id="idm46263519249336">
<h2>DBSCAN</h2>

<p>This algorithm defines clusters as continuous regions of high density. Here is how it works:</p>

<ul>
<li>
<p>For each instance, the algorithm counts how many instances are located within a small distance ε (epsilon) from it. This region is called the instance’s <em>ε-neighborhood</em>.</p>
</li>
<li>
<p>If an instance has at least <code>min_samples</code> instances in its ε-neighborhood (including itself), then it is considered a <em>core instance</em>. In other words, core instances are those that are located in dense regions.</p>
</li>
<li>
<p>All instances in the neighborhood of a core instance belong to the same cluster. This neighborhood may include other core instances, therefore a long sequence of neighboring core instances forms a single cluster.</p>
</li>
<li>
<p>Any instance that is not a core instance and does not have one in its neighborhood is considered an anomaly.</p>
</li>
</ul>

<p>This algorithm works well if all the clusters are dense enough and if they are well separated by low-density regions. The <code>DBSCAN</code> class in Scikit-Learn is as simple to use as you might expect. Let’s test it on the moons dataset, introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch05.html#svm_chapter">Chapter&nbsp;5</a>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">DBSCAN</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_moons</code>

<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_moons</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">noise</code><code class="o">=</code><code class="mf">0.05</code><code class="p">)</code>
<code class="n">dbscan</code> <code class="o">=</code> <code class="n">DBSCAN</code><code class="p">(</code><code class="n">eps</code><code class="o">=</code><code class="mf">0.05</code><code class="p">,</code> <code class="n">min_samples</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
<code class="n">dbscan</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

<p>The labels of all the instances are now available in the <code>labels_</code> instance variable:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">dbscan</code><code class="o">.</code><code class="n">labels_</code>
<code class="go">array([ 0,  2, -1, -1,  1,  0,  0,  0, ...,  3,  2,  3,  3,  4,  2,  6,  3])</code></pre>

<p>Notice that some instances have a cluster index equal to -1, which means that they are considered as anomalies by the algorithm. The indices of the core instances are available in the <code>core_sample_indices_</code> instance variable, and the core instances themselves are available in the <code>components_</code> instance variable:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="nb">len</code><code class="p">(</code><code class="n">dbscan</code><code class="o">.</code><code class="n">core_sample_indices_</code><code class="p">)</code>
<code class="go">808</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">dbscan</code><code class="o">.</code><code class="n">core_sample_indices_</code>
<code class="go">array([ 0,  4,  5,  6,  7,  8, 10, 11, ..., 992, 993, 995, 997, 998, 999])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">dbscan</code><code class="o">.</code><code class="n">components_</code>
<code class="go">array([[-0.02137124,  0.40618608],</code>
<code class="go">       [-0.84192557,  0.53058695],</code>
<code class="go">                  ...</code>
<code class="go">       [-0.94355873,  0.3278936 ],</code>
<code class="go">       [ 0.79419406,  0.60777171]])</code></pre>

<p>This clustering is represented in the left plot of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#dbscan_plot">Figure&nbsp;9-14</a>. As you can see, it identified quite a lot of anomalies, plus seven different clusters. How disappointing! Fortunately, if we widen each instance’s neighborhood by increasing <code>eps</code> to 0.2, we get the clustering on the right, which looks perfect. Let’s continue with this model.</p>

<figure class="smallerseventy"><div id="dbscan_plot" class="figure">
<img src="./Chapter9_files/mls2_0914.png" alt="mls2 0914" width="1440" height="475" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0914.png">
<h6><span class="label">Figure 9-14. </span>DBSCAN clustering using two different neighborhood radiuses</h6>
</div></figure>

<p>Somewhat surprisingly, the DBSCAN class does not have a <code>predict()</code> method, although it has a <code>fit_predict()</code> method. In other words, it cannot predict which cluster a new instance belongs to. Why didn’t the authors of the DBSCAN class implement the predict() method? The reason is that different classification algorithms can be better for different tasks, so the authors decided to let the user choose which one to use. Moreover, it’s not hard to implement. For example, let’s train a <code>KNeighborsClassifier</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="n">KNeighborsClassifier</code>

<code class="n">knn</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">50</code><code class="p">)</code>
<code class="n">knn</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">dbscan</code><code class="o">.</code><code class="n">components_</code><code class="p">,</code> <code class="n">dbscan</code><code class="o">.</code><code class="n">labels_</code><code class="p">[</code><code class="n">dbscan</code><code class="o">.</code><code class="n">core_sample_indices_</code><code class="p">])</code></pre>

<p>Now, given a few new instances, we can predict which cluster they most likely belong to and even estimate a probability for each cluster:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="o">-</code><code class="mf">0.5</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">],</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.1</code><code class="p">],</code> <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">knn</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>
<code class="go">array([1, 0, 1, 0])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">knn</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>
<code class="go">array([[0.18, 0.82],</code>
<code class="go">       [1.  , 0.  ],</code>
<code class="go">       [0.12, 0.88],</code>
<code class="go">       [1.  , 0.  ]])</code></pre>

<p>Note that we only trained them on the core instances, but we could also have chosen to train them on all the instances, or all but the anomalies: this choice depends on the final task.</p>

<p>The decision boundary is represented in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#cluster_classification_plot">Figure&nbsp;9-15</a> (the crosses represent the four instances in <code>X_new</code>). Notice that since there is no anomaly in the KNN’s training set, the classifier always chooses a cluster, even when that cluster is far away. It is fairly straightforward to introduce a maximum distance, in which case the two instances that are far away from both clusters are classified as anomalies. To do this, use the <code>kneighbors()</code> method of the <code>KNeighborsClassifier</code>. Given a set of instances, it returns the distances and the indices of the <em>k</em> nearest neighbors in the training set (two matrices, each with <em>k</em> columns):</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_dist</code><code class="p">,</code> <code class="n">y_pred_idx</code> <code class="o">=</code> <code class="n">knn</code><code class="o">.</code><code class="n">kneighbors</code><code class="p">(</code><code class="n">X_new</code><code class="p">,</code> <code class="n">n_neighbors</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code> <code class="o">=</code> <code class="n">dbscan</code><code class="o">.</code><code class="n">labels_</code><code class="p">[</code><code class="n">dbscan</code><code class="o">.</code><code class="n">core_sample_indices_</code><code class="p">][</code><code class="n">y_pred_idx</code><code class="p">]</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code><code class="p">[</code><code class="n">y_dist</code> <code class="o">&gt;</code> <code class="mf">0.2</code><code class="p">]</code> <code class="o">=</code> <code class="o">-</code><code class="mi">1</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code><code class="o">.</code><code class="n">ravel</code><code class="p">()</code>
<code class="go">array([-1,  0,  1, -1])</code></pre>

<figure class="smallerseventy"><div id="cluster_classification_plot" class="figure">
<img src="./Chapter9_files/mls2_0915.png" alt="mls2 0915" width="1441" height="666" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0915.png">
<h6><span class="label">Figure 9-15. </span>Decision boundary between two clusters</h6>
</div></figure>

<p>In short, DBSCAN is a very simple yet powerful algorithm capable of identifying any number of clusters and of any shape. It is robust to outliers, and it has just two hyperparameters (<code>eps</code> and <code>min_samples</code>). If the density varies significantly across the clusters, however, it can be impossible for it to capture all the clusters properly. Its computational complexity is roughly O(<em>m</em> log <em>m</em>), making it pretty close to linear with regard to the number of instances, but Scikit-Learn’s implementation can require up to O(<em>m</em><sup>2</sup>) memory if <code>eps</code> is large.</p>
<div data-type="tip"><h6>Tip</h6>
<p>You may also want to try <em>Hierarchical DBSCAN</em> (HDBSCAN), which is implemented in the scikit-learn-contrib project (see <a href="https://github.com/scikit-learn-contrib/hdbscan/"><em class="hyperlink">https://github.com/scikit-learn-contrib/hdbscan/</em></a>).</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Other Clustering Algorithms"><div class="sect2" id="idm46263518618728">
<h2>Other Clustering Algorithms</h2>

<p>Scikit-Learn implements several more clustering algorithms that you should take a look at. We cannot cover them all in detail here, but here is a brief overview:</p>
<dl>
<dt>Agglomerative clustering</dt>
<dd>
<p>A hierarchy of clusters is built from the bottom up. Think of many tiny bubbles floating on water and gradually attaching to each other until there’s one big group of bubbles. Similarly, at each iteration, agglomerative clustering connects the nearest pair of clusters (starting with individual instances). If you draw a tree with a branch for every pair of clusters that merged, you get a binary tree of clusters, where the leaves are the individual instances. This approach scales very well to large numbers of instances or clusters. It can capture clusters of various shapes, it produces a flexible and informative cluster tree instead of forcing you to choose a particular cluster scale, and it can be used with any pairwise distance. It can scale nicely to large numbers of instances if you provide a connectivity matrix, which is a sparse <em>m</em> x <em>m</em> matrix that indicates which pairs of instances are neighbors (e.g., returned by <code>sklearn.neighbors.kneighbors_graph()</code>). Without a connectivity matrix, the algorithm does not scale well to large datasets.</p>
</dd>
<dt>Birch</dt>
<dd>
<p>This algorithm was designed specifically for very large datasets, and it can be faster than batch K-Means, with similar results, as long as the number of features is not too large (&lt;20). During training, it builds a tree structure containing just enough information to quickly assign each new instance to a cluster, without having to store all the instances in the tree: this approach allows it to use limited memory, while handling huge datasets.</p>
</dd>
<dt>Mean-shift</dt>
<dd>
<p>This algorithm starts by placing a circle centered on each instance, then for each circle it computes the mean of all the instances located within it, and it shifts the circle so that it is centered on the mean. Next, it iterates this mean-shift step until all the circles stop moving (i.e., until each of them is centered on the mean of the instances it contains). Mean-shift shifts the circles in the direction of higher density, until each of them has found a local density maximum. Finally, all the instances whose circles have settled in the same place (or close enough) are assigned to the same cluster. Mean-shift has some of the same features as DBSCAN, like how it can find any number of clusters of any shape, it has just one hyperparameter (the radius of the circles, called the bandwidth), and it relies on local density estimation. But unlike DBSCAN, mean-shift tends to chop clusters into pieces when they have internal density variations. Unfortunately, its computational complexity is O(<em>m</em><sup>2</sup>), so it is not suited for large datasets.</p>
</dd>
<dt>Affinity propagation</dt>
<dd>
<p>This algorithm uses a voting system, where instances vote for similar instances to be their representatives, and once the algorithm converges, each representative and its voters form a cluster. Affinity propagation can detect any number of clusters of different sizes. Unfortunately, this algorithm has a computational complexity of O(<em>m</em><sup>2</sup>), so it too is not suited for large datasets.</p>
</dd>
<dt>Spectral clustering</dt>
<dd>
<p>This algorithm takes a similarity matrix between the instances and creates a low-dimensional embedding from it (i.e., it reduces its dimensionality), then it uses another clustering algorithm in this low-dimensional space (Scikit-Learn’s implementation uses K-Means). Spectral clustering can capture complex cluster structures, and it can also be used to cut graphs (e.g., to identify clusters of friends on a social network). It does not scale well to large number of instances, and it does not behave well when the clusters have very different sizes.</p>
</dd>
</dl>

<p>Now let’s dive into Gaussian mixture models, which can be used for density estimation, clustering, and anomaly detection.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Gaussian Mixtures"><div class="sect1" id="idm46263520289128">
<h1>Gaussian Mixtures</h1>

<p>A <em>Gaussian mixture model</em> (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown. All the instances generated from a single Gaussian distribution form a cluster that typically looks like an ellipsoid. Each cluster can have a different ellipsoidal shape, size, density, and orientation, just like in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#bad_kmeans_plot">Figure&nbsp;9-11</a>. When you observe an instance, you know it was generated from one of the Gaussian distributions, but you are not told which one, and you do not know what the parameters of these distributions are.</p>

<p>There are several GMM variants. In the simplest variant, implemented in the <code>GaussianMixture</code> class, you must know in advance the number <em>k</em> of Gaussian distributions. The dataset <strong>X</strong> is assumed to have been generated through the following probabilistic process:</p>

<ul>
<li>
<p>For each instance, a cluster is picked randomly among <em>k</em> clusters. The probability of choosing the <em>j</em><sup>th</sup> cluster is defined by the cluster’s weight <em>ϕ</em><sup>(<em>j</em>)</sup>.<sup><a data-type="noteref" id="idm46263518236840-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#idm46263518236840" class="totri-footnote">7</a></sup> The index of the cluster chosen for the <em>i</em><sup>th</sup> instance is noted <em>z</em><sup>(<em>i</em>)</sup>.</p>
</li>
<li>
<p>If <em>z</em><sup>(<em>i</em>)</sup>=<em>j</em>, meaning the <em>i</em><sup>th</sup> instance has been assigned to the <em>j</em><sup>th</sup> cluster, the location <strong>x</strong><sup>(<em>i</em>)</sup> of this instance is sampled randomly from the Gaussian distribution with mean <strong>μ</strong><sup>(<em>j</em>)</sup> and covariance matrix <strong>Σ</strong><sup>(<em>j</em>)</sup>. This is noted <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-106-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;bold x Superscript left-parenthesis i right-parenthesis Baseline tilde script upper N left-parenthesis mu Superscript left-parenthesis j right-parenthesis Baseline comma bold upper Sigma Superscript left-parenthesis j right-parenthesis Baseline right-parenthesis&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x1D431;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;mi&gt;&amp;#x1D4A9;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mi&gt;&amp;#x3BC;&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mi&gt;&amp;#x3A3;&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4276" aria-label="bold x Superscript left-parenthesis i right-parenthesis Baseline tilde script upper N left-parenthesis mu Superscript left-parenthesis j right-parenthesis Baseline comma bold upper Sigma Superscript left-parenthesis j right-parenthesis Baseline right-parenthesis" style="width: 9.051em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.794em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.905em, 1008.6em, 4.013em, -1000.01em); top: -3.185em; left: 0em;"><span class="mrow" id="MathJax-Span-4277"><span class="mrow" id="MathJax-Span-4278"><span class="msup" id="MathJax-Span-4279"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4280" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.62em;"><span class="mrow" id="MathJax-Span-4281"><span class="mo" id="MathJax-Span-4282" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4283" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-4284" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4285" style="font-family: MathJax_Main; padding-left: 0.26em;">∼</span><span class="mi" id="MathJax-Span-4286" style="font-family: MathJax_Script; padding-left: 0.26em;">N</span><span class="mrow" id="MathJax-Span-4287" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4288" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">(</span></span><span class="msup" id="MathJax-Span-4289"><span style="display: inline-block; position: relative; width: 1.545em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-4290"><span class="mi" id="MathJax-Span-4291" style="font-family: MathJax_Math-italic;">μ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.62em;"><span class="mrow" id="MathJax-Span-4292"><span class="mo" id="MathJax-Span-4293" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4294" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span class="mo" id="MathJax-Span-4295" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4296" style="font-family: MathJax_Main;">,</span><span class="msup" id="MathJax-Span-4297" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.802em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.83em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mrow" id="MathJax-Span-4298"><span class="mi" id="MathJax-Span-4299" style="font-family: MathJax_Math-italic;">Σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 0.877em;"><span class="mrow" id="MathJax-Span-4300"><span class="mo" id="MathJax-Span-4301" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4302" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span class="mo" id="MathJax-Span-4303" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4304" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.19em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.739em; border-left: 0px solid; width: 0px; height: 1.962em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="bold x Superscript left-parenthesis i right-parenthesis Baseline tilde script upper N left-parenthesis mu Superscript left-parenthesis j right-parenthesis Baseline comma bold upper Sigma Superscript left-parenthesis j right-parenthesis Baseline right-parenthesis"><mrow><msup><mi>𝐱</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>∼</mo><mi>𝒩</mi><mrow><mo>(</mo><msup><mrow><mi>μ</mi></mrow><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup><mo>,</mo><msup><mrow><mi>Σ</mi></mrow><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup><mo>)</mo></mrow></mrow></math></span></span><script type="math/mml" id="MathJax-Element-106"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="bold x Superscript left-parenthesis i right-parenthesis Baseline tilde script upper N left-parenthesis mu Superscript left-parenthesis j right-parenthesis Baseline comma bold upper Sigma Superscript left-parenthesis j right-parenthesis Baseline right-parenthesis">
  <mrow>
    <msup><mi>𝐱</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
    <mo>∼</mo>
    <mi>𝒩</mi>
    <mrow>
      <mo>(</mo>
      <msup><mrow><mi>μ</mi></mrow> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msup>
      <mo>,</mo>
      <msup><mrow><mi>Σ</mi></mrow> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msup>
      <mo>)</mo>
    </mrow>
  </mrow>
</math></script>.</p>
</li>
</ul>

<p>This generative process can be represented as a <em>graphical model</em>. <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#gmm_plate_diagram">Figure&nbsp;9-16</a> represents the structure of the conditional dependencies between random variables.</p>

<figure class="smallerseventy"><div id="gmm_plate_diagram" class="figure">
<img src="./Chapter9_files/mls2_0916.png" alt="mls2 0916" width="1440" height="759" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0916.png">
<h6><span class="label">Figure 9-16. </span>A graphical representation of a Gaussian mixture model, including its parameters (squares), random variables (circles), and their conditional dependencies (solid arrows)</h6>
</div></figure>

<p>Here is how to interpret the figure:<sup><a data-type="noteref" id="idm46263518185576-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#idm46263518185576" class="totri-footnote">8</a></sup></p>

<ul>
<li>
<p>The circles represent random variables.</p>
</li>
<li>
<p>The squares represent fixed values (i.e., parameters of the model).</p>
</li>
<li>
<p>The large rectangles are called <em>plates</em>. They indicate that their content is repeated several times.</p>
</li>
<li>
<p>The number indicated at the bottom right of each plate indicates how many times its content is repeated. So, there are <em>m</em> random variables <em>z</em><sup>(<em>i</em>)</sup> (from <em>z</em><sup>(1)</sup> to <em>z</em><sup>(<em>m</em>)</sup>) and <em>m</em> random variables <strong>x</strong><sup>(<em>i</em>)</sup>. There are also <em>k</em> means <strong>μ</strong><sup>(<em>j</em>)</sup> and <em>k</em> covariance matrices <strong>Σ</strong><sup>(<em>j</em>)</sup>. Lastly, there is just one weight vector <strong>ϕ</strong> (containing all the weights <em>ϕ</em><sup>(1)</sup> to <em>ϕ</em><sup>(<em>k</em>)</sup>).</p>
</li>
<li>
<p>Each variable <em>z</em><sup>(<em>i</em>)</sup> is drawn from the <em>categorical distribution</em> with weights <strong>ϕ</strong>. Each variable <strong>x</strong><sup>(<em>i</em>)</sup> is drawn from the normal distribution, with the mean and covariance matrix defined by its cluster <em>z</em><sup>(<em>i</em>)</sup>.</p>
</li>
<li>
<p>The solid arrows represent conditional dependencies. For example, the probability distribution for each random variable <em>z</em><sup>(<em>i</em>)</sup> depends on the weight vector <strong>ϕ</strong>. Note that when an arrow crosses a plate boundary, it means that it applies to all the repetitions of that plate. For example, the weight vector <strong>ϕ</strong> conditions the probability distributions of all the random variables <strong>x</strong><sup>(1)</sup> to <strong>x</strong><sup>(<em>m</em>)</sup>.</p>
</li>
<li>
<p>The squiggly arrow from <em>z</em><sup>(<em>i</em>)</sup> to <strong>x</strong><sup>(<em>i</em>)</sup> represents a switch: depending on the value of <em>z</em><sup>(<em>i</em>)</sup>, the instance <strong>x</strong><sup>(<em>i</em>)</sup> will be sampled from a different Gaussian distribution. For example, if <em>z</em><sup>(<em>i</em>)</sup>=<em>j</em>, then <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-107-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;bold x Superscript left-parenthesis i right-parenthesis Baseline tilde script upper N left-parenthesis mu Superscript left-parenthesis j right-parenthesis Baseline comma bold upper Sigma Superscript left-parenthesis j right-parenthesis Baseline right-parenthesis&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x1D431;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x223C;&lt;/mo&gt;&lt;mi&gt;&amp;#x1D4A9;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x3BC;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;&amp;#x3A3;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4305" aria-label="bold x Superscript left-parenthesis i right-parenthesis Baseline tilde script upper N left-parenthesis mu Superscript left-parenthesis j right-parenthesis Baseline comma bold upper Sigma Superscript left-parenthesis j right-parenthesis Baseline right-parenthesis" style="width: 8.794em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.537em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.185em, 1008.39em, 2.728em, -1000.01em); top: -2.208em; left: 0em;"><span class="mrow" id="MathJax-Span-4306"><span class="mrow" id="MathJax-Span-4307"><span class="msup" id="MathJax-Span-4308"><span style="display: inline-block; position: relative; width: 1.494em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4309" style="font-family: MathJax_Main-bold;">x</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.62em;"><span class="mrow" id="MathJax-Span-4310"><span class="mo" id="MathJax-Span-4311" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4312" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-4313" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4314" style="font-family: MathJax_Main; padding-left: 0.26em;">∼</span><span class="mi" id="MathJax-Span-4315" style="font-family: MathJax_Script; padding-left: 0.26em;">N</span><span class="mrow" id="MathJax-Span-4316" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4317" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">(</span></span><span class="msup" id="MathJax-Span-4318"><span style="display: inline-block; position: relative; width: 1.545em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.58em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4319" style="font-family: MathJax_Math-italic;">μ</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.62em;"><span class="mrow" id="MathJax-Span-4320"><span class="mo" id="MathJax-Span-4321" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4322" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span class="mo" id="MathJax-Span-4323" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4324" style="font-family: MathJax_Main;">,</span><span class="msup" id="MathJax-Span-4325" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.802em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.83em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4326" style="font-family: MathJax_Math-italic;">Σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.877em;"><span class="mrow" id="MathJax-Span-4327"><span class="mo" id="MathJax-Span-4328" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4329" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span class="mo" id="MathJax-Span-4330" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4331" style="vertical-align: 0em;"><span style="font-family: MathJax_Size1;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.213em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.421em; border-left: 0px solid; width: 0px; height: 1.38em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="bold x Superscript left-parenthesis i right-parenthesis Baseline tilde script upper N left-parenthesis mu Superscript left-parenthesis j right-parenthesis Baseline comma bold upper Sigma Superscript left-parenthesis j right-parenthesis Baseline right-parenthesis"><mrow><msup><mi>𝐱</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>∼</mo><mi>𝒩</mi><mrow><mo>(</mo><msup><mi>μ</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup><mo>,</mo><msup><mi>Σ</mi><mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup><mo>)</mo></mrow></mrow></math></span></span><script type="math/mml" id="MathJax-Element-107"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="bold x Superscript left-parenthesis i right-parenthesis Baseline tilde script upper N left-parenthesis mu Superscript left-parenthesis j right-parenthesis Baseline comma bold upper Sigma Superscript left-parenthesis j right-parenthesis Baseline right-parenthesis">
  <mrow>
    <msup><mi>𝐱</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
    <mo>∼</mo>
    <mi>𝒩</mi>
    <mrow>
      <mo>(</mo>
      <msup><mi>μ</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msup>
      <mo>,</mo>
      <msup><mi>Σ</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msup>
      <mo>)</mo>
    </mrow>
  </mrow>
</math></script>.</p>
</li>
<li>
<p>Shaded nodes indicate that the value is known. So, in this case, only the random variables <strong>x</strong><sup>(<em>i</em>)</sup> have known values: they are called <em>observed variables</em>. The unknown random variables <em>z</em><sup>(<em>i</em>)</sup> are called <em>latent variables</em>.</p>
</li>
</ul>

<p>So, what can you do with such a model? Well, given the dataset <strong>X</strong>, you typically want to start by estimating the weights <strong>ϕ</strong> and all the distribution parameters <strong>μ</strong><sup>(1)</sup> to <strong>μ</strong><sup>(<em>k</em>)</sup> and <strong>Σ</strong><sup>(1)</sup> to <strong>Σ</strong><sup>(<em>k</em>)</sup>. Scikit-Learn’s <code>GaussianMixture</code> class makes this super easy:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.mixture</code> <code class="kn">import</code> <code class="n">GaussianMixture</code>

<code class="n">gm</code> <code class="o">=</code> <code class="n">GaussianMixture</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">n_init</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code>
<code class="n">gm</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

<p>Let’s look at the parameters that the algorithm estimated:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">weights_</code>
<code class="go">array([0.20965228, 0.4000662 , 0.39028152])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">means_</code>
<code class="go">array([[ 3.39909717,  1.05933727],</code>
<code class="go">       [-1.40763984,  1.42710194],</code>
<code class="go">       [ 0.05135313,  0.07524095]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">covariances_</code>
<code class="go">array([[[ 1.14807234, -0.03270354],</code>
<code class="go">        [-0.03270354,  0.95496237]],</code>

<code class="go">       [[ 0.63478101,  0.72969804],</code>
<code class="go">        [ 0.72969804,  1.1609872 ]],</code>

<code class="go">       [[ 0.68809572,  0.79608475],</code>
<code class="go">        [ 0.79608475,  1.21234145]]])</code></pre>

<p>Great, it worked fine! Indeed, the weights that were used to generate the data were 0.2, 0.4, and 0.4; and similarly, the means and covariance matrices were very close to those found by the algorithm. But how? This class relies on the <em>Expectation-Maximization</em> (EM) algorithm, which has many similarities with the K-Means algorithm: it also initializes the cluster parameters randomly, then it repeats two steps until convergence, first assigning instances to clusters (this is called the <em>expectation step</em>) then updating the clusters (this is called the <em>maximization step</em>). Sounds familiar? Indeed, in the context of clustering you can think of EM as a generalization of K-Means that not only finds the cluster centers (<strong>μ</strong><sup>(1)</sup> to <strong>μ</strong><sup>(<em>k</em>)</sup>), but also their size, shape, and orientation (<strong>Σ</strong><sup>(1)</sup> to <strong>Σ</strong><sup>(<em>k</em>)</sup>), as well as their relative weights (<em>ϕ</em><sup>(1)</sup> to <em>ϕ</em><sup>(<em>k</em>)</sup>). Unlike K-Means, EM uses soft cluster assignments rather than hard assignments: for each instance during the expectation step, the algorithm estimates the probability that it belongs to each cluster (based on the current cluster parameters). Then, during the maximization step, each cluster is updated using <em>all</em> the instances in the dataset, with each instance weighted by the estimated probability that it belongs to that cluster. These probabilities are called the <em>responsibilities</em> of the clusters for the instances. During the maximization step, each cluster’s update will mostly be impacted by the instances it is most responsible for.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Unfortunately, just like K-Means, EM can end up converging to poor solutions, so it needs to be run several times, keeping only the best solution. This is why we set <code>n_init</code> to 10. Be careful: by default <code>n_init</code> is set to 1.</p>
</div>

<p>You can check whether or not the algorithm converged and how many iterations it took:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">converged_</code>
<code class="go">True</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">n_iter_</code>
<code class="go">3</code></pre>

<p>OK, now that you have an estimate of the location, size, shape, orientation, and relative weight of each cluster, the model can easily assign each instance to the most likely cluster (hard clustering) or estimate the probability that it belongs to a particular cluster (soft clustering). Just use the <code>predict()</code> method for hard clustering, or the <code>predict_proba()</code> method for soft clustering:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="go">array([2, 2, 1, ..., 0, 0, 0])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="go">array([[2.32389467e-02, 6.77397850e-07, 9.76760376e-01],</code>
<code class="go">       [1.64685609e-02, 6.75361303e-04, 9.82856078e-01],</code>
<code class="go">       [2.01535333e-06, 9.99923053e-01, 7.49319577e-05],</code>
<code class="go">       ...,</code>
<code class="go">       [9.99999571e-01, 2.13946075e-26, 4.28788333e-07],</code>
<code class="go">       [1.00000000e+00, 1.46454409e-41, 5.12459171e-16],</code>
<code class="go">       [1.00000000e+00, 8.02006365e-41, 2.27626238e-15]])</code></pre>

<p>A Gaussian mixture model is a <em>generative model</em>, meaning you can sample new instances from it (note that they are ordered by cluster index):</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code><code class="p">,</code> <code class="n">y_new</code> <code class="o">=</code> <code class="n">gm</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">6</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code>
<code class="go">array([[ 2.95400315,  2.63680992],</code>
<code class="go">       [-1.16654575,  1.62792705],</code>
<code class="go">       [-1.39477712, -1.48511338],</code>
<code class="go">       [ 0.27221525,  0.690366  ],</code>
<code class="go">       [ 0.54095936,  0.48591934],</code>
<code class="go">       [ 0.38064009, -0.56240465]])</code>

<code class="gp">&gt;&gt;&gt; </code><code class="n">y_new</code>
<code class="go">array([0, 1, 2, 2, 2, 2])</code></pre>

<p>It is also possible to estimate the density of the model at any given location. This is achieved using the <code>score_samples()</code> method: for each instance it is given, this method estimates the log of the <em>probability density function</em> (PDF) at that location. The greater the score, the higher the density:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">score_samples</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="go">array([-2.60782346, -3.57106041, -3.33003479, ..., -3.51352783,</code>
<code class="go">       -4.39802535, -3.80743859])</code></pre>

<p>If you compute the exponential of these scores, you get the value of the PDF at the location of the given instances. These are <em>not</em> probabilities, but probability <em>densities</em>: they can take on any positive value, not just between 0 and 1. To estimate the probability that an instance will fall within a particular region, you would have to integrate the PDF over that region (if you do so over the entire space of possible instance locations, the result will be 1).</p>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#gaussian_mixtures_plot">Figure&nbsp;9-17</a> shows the cluster means, the decision boundaries (dashed lines), and the density contours of this model.</p>

<figure class="smallerseventy"><div id="gaussian_mixtures_plot" class="figure">
<img src="./Chapter9_files/mls2_0917.png" alt="mls2 0917" width="1440" height="682" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0917.png">
<h6><span class="label">Figure 9-17. </span>Cluster means, decision boundaries, and density contours of a trained Gaussian mixture model</h6>
</div></figure>

<p>Nice! The algorithm clearly found an excellent solution. Of course, we made its task easy by generating the data using a set of 2D Gaussian distributions (unfortunately, real-life data is not always so Gaussian and low dimensional. We also gave the algorithm the correct number of clusters. When there are many dimensions, or many clusters, or few instances, EM can struggle to converge to the optimal solution. You might need to reduce the difficulty of the task by limiting the number of parameters that the algorithm has to learn. One way to do this is to limit the range of shapes and orientations that the clusters can have. This can be achieved by imposing constraints on the covariance matrices. To do this, set the <code>covariance_type</code> hyperparameter to one of the following values:</p>
<dl>
<dt><code>"spherical"</code></dt>
<dd>
<p>All clusters must be spherical, but they can have different diameters (i.e., different variances).</p>
</dd>
<dt><code>"diag"</code></dt>
<dd>
<p>Clusters can take on any ellipsoidal shape of any size, but the ellipsoid’s axes must be parallel to the coordinate axes (i.e., the covariance matrices must be diagonal).</p>
</dd>
<dt><code>"tied"</code></dt>
<dd>
<p>All clusters must have the same ellipsoidal shape, size and orientation (i.e., all clusters share the same covariance matrix).</p>
</dd>
</dl>

<p>By default, <code>covariance_type</code> is equal to <code>"full"</code>, which means that each cluster can take on any shape, size, and orientation (it has its own unconstrained covariance matrix). <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#covariance_type_plot">Figure&nbsp;9-18</a> plots the solutions found by the EM algorithm when <code>covariance_type</code> is set to <code>"tied"</code> or "<code>spherical</code>.”</p>

<figure class="smallerseventy"><div id="covariance_type_plot" class="figure">
<img src="./Chapter9_files/mls2_0918.png" alt="mls2 0918" width="1439" height="608" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0918.png">
<h6><span class="label">Figure 9-18. </span>Gaussian mixtures for tied clusters (left) and spherical clusters (right)</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The computational complexity of training a <code>GaussianMixture</code> model depends on the number of instances <em>m</em>, the number of dimensions <em>n</em>, the number of clusters <em>k</em>, and the constraints on the covariance matrices. If <code>covariance_type</code> is <code>"spherical</code> or <code>"diag"</code>, it is O(<em>kmn</em>), assuming the data has a clustering structure. If <code>covariance_type</code> is <code>"tied"</code> or <code>"full"</code>, it is O(<em>kmn</em><sup>2</sup> + <em>kn</em><sup>3</sup>), so it will not scale to large numbers of features.</p>
</div>

<p>Gaussian mixture models can also be used for anomaly detection. Let’s see how.</p>








<section data-type="sect2" data-pdf-bookmark="Anomaly Detection Using Gaussian Mixtures"><div class="sect2" id="idm46263517837320">
<h2>Anomaly Detection Using Gaussian Mixtures</h2>

<p><em>Anomaly detection</em> (also called <em>outlier detection</em>) is the task of detecting instances that deviate strongly from the norm. These instances are called <em>anomalies</em>, or <em>outliers</em>, while the normal instances are called <em>inliers</em>. Anomaly detection is very useful in a wide variety of applications, such as fraud detection detecting defective products in manufacturing, or removing outliers from a dataset before training another model (which can significantly improve the performance of the resulting model).</p>

<p>Using a Gaussian mixture model for anomaly detection is quite simple: any instance located in a low-density region can be considered an anomaly. You must define what density threshold you want to use. For example, in a manufacturing company that tries to detect defective products, the ratio of defective products is usually well known. Say it is equal to 4%. You then set the density threshold to be the value that results in having 4% of the instances located in areas below that threshold density. If you notice that you get too many false positives (i.e., perfectly good products that are flagged as defective), you can lower the threshold. Conversely, if you have too many false negatives (i.e., defective products that the system does not flag as defective), you can increase the threshold. This is the usual precision/recall trade-off (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch03.html#classification_chapter">Chapter&nbsp;3</a>). Here is how you would identify the outliers using the fourth percentile lowest density as the threshold (i.e., approximately 4% of the instances will be flagged as anomalies):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">densities</code> <code class="o">=</code> <code class="n">gm</code><code class="o">.</code><code class="n">score_samples</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="n">density_threshold</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">percentile</code><code class="p">(</code><code class="n">densities</code><code class="p">,</code> <code class="mi">4</code><code class="p">)</code>
<code class="n">anomalies</code> <code class="o">=</code> <code class="n">X</code><code class="p">[</code><code class="n">densities</code> <code class="o">&lt;</code> <code class="n">density_threshold</code><code class="p">]</code></pre>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#mixture_anomaly_detection_plot">Figure&nbsp;9-19</a> represents these anomalies as stars.</p>

<figure class="smallerseventy"><div id="mixture_anomaly_detection_plot" class="figure">
<img src="./Chapter9_files/mls2_0919.png" alt="mls2 0919" width="1440" height="682" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0919.png">
<h6><span class="label">Figure 9-19. </span>Anomaly detection using a Gaussian mixture model</h6>
</div></figure>

<p>A closely related task is <em>novelty detection</em>: it differs from anomaly detection in that the algorithm is assumed to be trained on a “clean” dataset, uncontaminated by outliers, whereas anomaly detection does not make this assumption. Indeed, outlier detection is often used to clean up a dataset.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Gaussian mixture models try to fit all the data, including the outliers, so if you have too many of them, this will bias the model’s view of “normality,” and some outliers may wrongly be considered as normal. If this happens, you can try to fit the model once, use it to detect and remove the most extreme outliers, then fit the model again on the cleaned-up dataset. Another approach is to use robust covariance estimation methods (see the <code>EllipticEnvelope</code> class).</p>
</div>

<p>Just like K-Means, the <code>GaussianMixture</code> algorithm requires you to specify the number of clusters. So, how can you find it?</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Selecting the Number of Clusters"><div class="sect2" id="idm46263517801608">
<h2>Selecting the Number of Clusters</h2>

<p>With K-Means, you could use the inertia or the silhouette score to select the appropriate number of cluster. But with Gaussian mixtures, it is not possible to use these metrics because they are not reliable when the clusters are not spherical or have different sizes. Instead, you can try to find the model that minimizes a <em>theoretical information criterion</em>, such as the <em>Bayesian information criterion</em> (BIC) or the <em>Akaike information criterion</em> (AIC), defined in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#information_criteria_equation">Equation 9-1</a>.</p>

<p>In this equation:</p>
<div data-type="equation" id="information_criteria_equation">
<h5><span class="label">Equation 9-1. </span>Bayesian information criterion (BIC) and Akaike information criterion (AIC)</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-108-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mtable displaystyle=&quot;true&quot;&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4332" style="width: 14.656em; display: inline-block;"><span style="display: inline-block; position: relative; width: 14.193em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(-0.306em, 1013.84em, 4.013em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-4333"><span class="mtable" id="MathJax-Span-4334" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 13.884em; height: 0px;"><span style="position: absolute; clip: rect(2.059em, 1003.25em, 5.298em, -1000.01em); top: -4.008em; left: 0em;"><span style="display: inline-block; position: relative; width: 3.242em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1003.25em, 4.167em, -1000.01em); top: -5.139em; right: 0em;"><span class="mtd" id="MathJax-Span-4335"><span class="mrow" id="MathJax-Span-4336"><span class="mrow" id="MathJax-Span-4337"><span class="mi" id="MathJax-Span-4338" style="font-family: MathJax_Math-italic;">B</span><span class="mi" id="MathJax-Span-4339" style="font-family: MathJax_Math-italic;">I<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mi" id="MathJax-Span-4340" style="font-family: MathJax_Math-italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-4341" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mspace" id="MathJax-Span-4342" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1003.2em, 4.167em, -1000.01em); top: -2.877em; right: 0em;"><span class="mtd" id="MathJax-Span-4362"><span class="mrow" id="MathJax-Span-4363"><span class="mrow" id="MathJax-Span-4364"><span class="mi" id="MathJax-Span-4365" style="font-family: MathJax_Math-italic;">A</span><span class="mi" id="MathJax-Span-4366" style="font-family: MathJax_Math-italic;">I<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mi" id="MathJax-Span-4367" style="font-family: MathJax_Math-italic;">C<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mo" id="MathJax-Span-4368" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mspace" id="MathJax-Span-4369" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.625em, 1009.68em, 6.944em, -1000.01em); top: -5.036em; left: 4.013em;"><span style="display: inline-block; position: relative; width: 9.874em; height: 0px;"><span style="position: absolute; clip: rect(2.728em, 1009.68em, 4.836em, -1000.01em); top: -5.139em; left: 0em;"><span class="mtd" id="MathJax-Span-4343"><span class="mrow" id="MathJax-Span-4344"><span class="mrow" id="MathJax-Span-4345"><span class="mspace" id="MathJax-Span-4346" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-4347" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mrow" id="MathJax-Span-4348"><span class="mo" id="MathJax-Span-4349" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4350" style="font-family: MathJax_Math-italic;">m</span><span class="mo" id="MathJax-Span-4351" style="font-family: MathJax_Main;">)</span></span><span class="mi" id="MathJax-Span-4352" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">p</span><span class="mo" id="MathJax-Span-4353" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mn" id="MathJax-Span-4354" style="font-family: MathJax_Main; padding-left: 0.208em;">2</span><span class="mo" id="MathJax-Span-4355" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mrow" id="MathJax-Span-4356"><span class="mo" id="MathJax-Span-4357" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">(</span></span><span class="mover" id="MathJax-Span-4358"><span style="display: inline-block; position: relative; width: 0.671em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4359" style="font-family: MathJax_Math-italic;">L</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1000.58em, 3.602em, -1000.01em); top: -4.265em; left: 0.105em;"><span class="mo" id="MathJax-Span-4360" style=""><span style="font-family: MathJax_Size1;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4361" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">)</span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.728em, 1006.44em, 4.836em, -1000.01em); top: -2.877em; left: 0em;"><span class="mtd" id="MathJax-Span-4370"><span class="mrow" id="MathJax-Span-4371"><span class="mrow" id="MathJax-Span-4372"><span class="mspace" id="MathJax-Span-4373" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mn" id="MathJax-Span-4374" style="font-family: MathJax_Main;">2</span><span class="mi" id="MathJax-Span-4375" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4376" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mn" id="MathJax-Span-4377" style="font-family: MathJax_Main; padding-left: 0.208em;">2</span><span class="mo" id="MathJax-Span-4378" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mo" id="MathJax-Span-4379" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">(</span></span><span class="mover" id="MathJax-Span-4380"><span style="display: inline-block; position: relative; width: 0.671em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4381" style="font-family: MathJax_Math-italic;">L</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1000.58em, 3.602em, -1000.01em); top: -4.265em; left: 0.105em;"><span class="mo" id="MathJax-Span-4382" style=""><span style="font-family: MathJax_Size1;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-4383" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 5.041em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.851em; border-left: 0px solid; width: 0px; height: 4.239em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>B</mi><mi>I</mi><mi>C</mi><mo>=</mo><mspace width="0.166667em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><mspace width="0.166667em"></mspace><mo form="prefix">log</mo><mrow><mo>(</mo><mi>m</mi><mo>)</mo></mrow><mi>p</mi><mo>-</mo><mn>2</mn><mo form="prefix">log</mo><mrow><mo>(</mo><mover accent="true"><mi>L</mi><mo>^</mo></mover><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign="right"><mrow><mi>A</mi><mi>I</mi><mi>C</mi><mo>=</mo><mspace width="0.166667em"></mspace></mrow></mtd><mtd columnalign="left"><mrow><mspace width="0.166667em"></mspace><mn>2</mn><mi>p</mi><mo>-</mo><mn>2</mn><mo form="prefix">log</mo><mo>(</mo><mover accent="true"><mi>L</mi><mo>^</mo></mover><mo>)</mo></mrow></mtd></mtr></mtable></math></span></span></div><script type="math/mml" id="MathJax-Element-108"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mi>B</mi>
          <mi>I</mi>
          <mi>C</mi>
          <mo>=</mo>
          <mspace width="0.166667em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mspace width="0.166667em"></mspace>
          <mo form="prefix">log</mo>
          <mrow>
            <mo>(</mo>
            <mi>m</mi>
            <mo>)</mo>
          </mrow>
          <mi>p</mi>
          <mo>-</mo>
          <mn>2</mn>
          <mo form="prefix">log</mo>
          <mrow>
            <mo>(</mo>
            <mover accent="true"><mi>L</mi> <mo>^</mo></mover>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mi>A</mi>
          <mi>I</mi>
          <mi>C</mi>
          <mo>=</mo>
          <mspace width="0.166667em"></mspace>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mspace width="0.166667em"></mspace>
          <mn>2</mn>
          <mi>p</mi>
          <mo>-</mo>
          <mn>2</mn>
          <mo form="prefix">log</mo>
          <mo>(</mo>
          <mover accent="true"><mi>L</mi> <mo>^</mo></mover>
          <mo>)</mo>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></script>
</div>

<ul>
<li>
<p><em>m</em> is the number of instances, as always.</p>
</li>
<li>
<p><em>p</em> is the number of parameters learned by the model.</p>
</li>
<li>
<p><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-109-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;ModifyingAbove upper L With caret&quot;&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4384" aria-label="ModifyingAbove upper L With caret" style="width: 0.722em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.671em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(0.928em, 1000.63em, 2.265em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-4385"><span class="mover" id="MathJax-Span-4386"><span style="display: inline-block; position: relative; width: 0.671em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4387" style="font-family: MathJax_Math-italic;">L</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1000.58em, 3.602em, -1000.01em); top: -4.265em; left: 0.105em;"><span class="mo" id="MathJax-Span-4388" style=""><span style="font-family: MathJax_Size1;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.05em; border-left: 0px solid; width: 0px; height: 1.168em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="ModifyingAbove upper L With caret"><mover accent="true"><mi>L</mi><mo>^</mo></mover></math></span></span><script type="math/mml" id="MathJax-Element-109"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="ModifyingAbove upper L With caret">
  <mover accent="true"><mi>L</mi> <mo>^</mo></mover>
</math></script> is the maximized value of the <em>likelihood function</em> of the model.</p>
</li>
</ul>

<p>Both the BIC and the AIC penalize models that have more parameters to learn (e.g., more clusters) and reward models that fit the data well. They often end up selecting the same model. When they differ, the model selected by the BIC tends to be simpler (fewer parameters) than the one selected by the AIC,  but tends to not fit the data quite as well (this is especially true for larger datasets).</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46263517732952">
<h5>Likelihood Function</h5>
<p>The terms “probability” and “likelihood” are often used interchangeably in the English language, but they have very different meanings in statistics. Given a statistical model with some parameters <strong>θ</strong>, the word “probability” is used to describe how plausible a future outcome <strong>x</strong> is (knowing the parameter values <strong>θ</strong>), while the word “likelihood” is used to describe how plausible a particular set of parameter values <strong>θ</strong> are, after the outcome <strong>x</strong> is known.</p>

<p>Consider a 1D mixture model of two Gaussian distributions centered at -4 and +1. For simplicity, this toy model has a single parameter <em>θ</em> that controls the standard deviations of both distributions. The top-left contour plot in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#likelihood_function_plot">Figure&nbsp;9-20</a> shows the entire model <em>f</em>(<em>x</em>; <em>θ</em>) as a function of both <em>x</em> and <em>θ</em>. To estimate the probability distribution of a future outcome <em>x</em>, you need to set the model parameter <em>θ</em>. For example, if you set it to <em>θ</em>=1.3 (the horizontal line), you get the probability density function <em>f</em>(<em>x</em>; <em>θ</em>=1.3) shown in the lower-left plot. Say you want to estimate the probability that <em>x</em> will fall between -2 and +2. You must calculate the integral of the PDF on this range (i.e., the surface of the shaded region). But what if you don’t know <em>θ</em>, and instead if you have observed a single instance <em>x</em>=2.5 (the vertical line in the upper-left plot? In this case, you get the likelihood function <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-110-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;script upper L&quot;&gt;&lt;mi&gt;&amp;#x2112;&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4389" aria-label="script upper L" style="width: 1.277em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.032em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.277em, 1001.03em, 2.306em, -999.998em); top: -2.154em; left: 0em;"><span class="mrow" id="MathJax-Span-4390"><span class="mi" id="MathJax-Span-4391" style="font-family: MathJax_Script;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.15em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.159em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.056em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="script upper L"><mi>ℒ</mi></math></span></span><script type="math/mml" id="MathJax-Element-110"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="script upper L">
  <mi>ℒ</mi>
</math></script>(<em>θ</em>|<em>x</em>=2.5)=f(<em>x</em>=2.5; <em>θ</em>), represented in the upper-right plot.</p>

<p>In short, the PDF is a function of <em>x</em> (with <em>θ</em> fixed), while the likelihood function is a function of <em>θ</em> (with <em>x</em> fixed). It is important to understand that the likelihood function is <em>not</em> a probability distribution: if you integrate a probability distribution over all possible values of <em>x</em>, you always get 1; but if you integrate the likelihood function over all possible values of <em>θ</em>, the result can be any positive value.</p>

<figure class="smallerseventy"><div id="likelihood_function_plot" class="figure">
<img src="./Chapter9_files/mls2_0920.png" alt="mls2 0920" width="1440" height="779" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0920.png">
<h6><span class="label">Figure 9-20. </span>A model’s parametric function (top left), and some derived functions: a PDF (lower left), a likelihood function (top right) and a log likelihood function (lower right)</h6>
</div></figure>

<p>Given a dataset <strong>X</strong>, a common task is to try to estimate the most likely values for the model parameters. To do this, you must find the values that maximize the likelihood function, given <strong>X</strong>. In this example, if you have observed a single instance <em>x</em>=2.5, the <em>maximum likelihood estimate</em> (MLE) of <em>θ</em> is <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-111-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;ModifyingAbove theta With caret&quot;&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4392" aria-label="ModifyingAbove theta With caret" style="width: 0.738em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.591em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.983em, 1000.49em, 2.257em, -999.998em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-4393"><span class="mover" id="MathJax-Span-4394"><span style="display: inline-block; position: relative; width: 0.591em; height: 0px;"><span style="position: absolute; clip: rect(3.189em, 1000.44em, 4.169em, -999.998em); top: -4.017em; left: 0.002em;"><span class="mi" id="MathJax-Span-4395" style="font-family: MathJax_Math-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.022em;"></span></span><span style="position: absolute; clip: rect(3.189em, 1000.39em, 3.63em, -999.998em); top: -4.311em; left: 0.1em;"><span class="mo" id="MathJax-Span-4396" style=""><span style="font-family: MathJax_Main;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.022em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.11em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.056em; border-left: 0px solid; width: 0px; height: 1.297em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="ModifyingAbove theta With caret"><mover accent="true"><mi>θ</mi><mo>^</mo></mover></math></span></span><script type="math/mml" id="MathJax-Element-111"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="ModifyingAbove theta With caret">
  <mover accent="true"><mi>θ</mi> <mo>^</mo></mover>
</math></script>=1.5. If a prior probability distribution <em>g</em> over <em>θ</em> exists, it is possible to take it into account by maximizing <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-112-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;script upper L&quot;&gt;&lt;mi&gt;&amp;#x2112;&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4397" aria-label="script upper L" style="width: 1.277em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.032em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.277em, 1001.03em, 2.306em, -999.998em); top: -2.154em; left: 0em;"><span class="mrow" id="MathJax-Span-4398"><span class="mi" id="MathJax-Span-4399" style="font-family: MathJax_Script;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.15em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.159em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.056em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="script upper L"><mi>ℒ</mi></math></span></span><script type="math/mml" id="MathJax-Element-112"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="script upper L">
  <mi>ℒ</mi>
</math></script>(<em>θ</em>|<em>x</em>)g(<em>θ</em>) rather than just maximizing <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-113-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;script upper L&quot;&gt;&lt;mi&gt;&amp;#x2112;&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4400" aria-label="script upper L" style="width: 1.277em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.032em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.277em, 1001.03em, 2.306em, -999.998em); top: -2.154em; left: 0em;"><span class="mrow" id="MathJax-Span-4401"><span class="mi" id="MathJax-Span-4402" style="font-family: MathJax_Script;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.15em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.159em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.056em; border-left: 0px solid; width: 0px; height: 1.003em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="script upper L"><mi>ℒ</mi></math></span></span><script type="math/mml" id="MathJax-Element-113"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="script upper L">
  <mi>ℒ</mi>
</math></script>(<em>θ</em>|<em>x</em>). This is called <em>maximum a-posteriori</em> (MAP) estimation. Since MAP constrains the parameter values, you can think of it as a regularized version of MLE.</p>

<p>Notice that maximizing the likelihood function is equivalent to maximizing its logarithm (represented in the lower-righthand side of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#likelihood_function_plot">Figure&nbsp;9-20</a>). Indeed the logarithm is a strictly increasing function, so if <em>θ</em> maximizes the log likelihood, it also maximizes the likelihood. It turns out that it is generally easier to maximize the log likelihood. For example, if you observed several independent instances <em>x</em><sup>(1)</sup> to <em>x</em><sup>(<em>m</em>)</sup>, you would need to find the value of <em>θ</em> that maximizes the product of the individual likelihood functions. But it is equivalent, and much simpler, to maximize the sum (not the product) of the log likelihood functions, thanks to the magic of the logarithm which converts products into sums: log(<em>ab</em>)=log(<em>a</em>)+log(<em>b</em>).</p>

<p>Once you have estimated <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-114-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;ModifyingAbove theta With caret&quot;&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4403" aria-label="ModifyingAbove theta With caret" style="width: 0.738em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.591em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.983em, 1000.49em, 2.257em, -999.998em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-4404"><span class="mover" id="MathJax-Span-4405"><span style="display: inline-block; position: relative; width: 0.591em; height: 0px;"><span style="position: absolute; clip: rect(3.189em, 1000.44em, 4.169em, -999.998em); top: -4.017em; left: 0.002em;"><span class="mi" id="MathJax-Span-4406" style="font-family: MathJax_Math-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.022em;"></span></span><span style="position: absolute; clip: rect(3.189em, 1000.39em, 3.63em, -999.998em); top: -4.311em; left: 0.1em;"><span class="mo" id="MathJax-Span-4407" style=""><span style="font-family: MathJax_Main;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.022em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.11em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.056em; border-left: 0px solid; width: 0px; height: 1.297em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="ModifyingAbove theta With caret"><mover accent="true"><mi>θ</mi><mo>^</mo></mover></math></span></span><script type="math/mml" id="MathJax-Element-114"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="ModifyingAbove theta With caret">
  <mover accent="true"><mi>θ</mi> <mo>^</mo></mover>
</math></script>, the value of <em>θ</em> that maximizes the likelihood function, then you are ready to compute <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-115-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;ModifyingAbove upper L With caret equals script upper L left-parenthesis ModifyingAbove theta With caret comma bold upper X right-parenthesis&quot;&gt;&lt;mrow&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;&amp;#x2112;&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;&amp;#x3B8;&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;&amp;#x1D417;&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4408" aria-label="ModifyingAbove upper L With caret equals script upper L left-parenthesis ModifyingAbove theta With caret comma bold upper X right-parenthesis" style="width: 7.6em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.326em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.963em, 1006.13em, 4.022em, -999.998em); top: -3.233em; left: 0em;"><span class="mrow" id="MathJax-Span-4409"><span class="mrow" id="MathJax-Span-4410"><span class="mover" id="MathJax-Span-4411"><span style="display: inline-block; position: relative; width: 0.689em; height: 0px;"><span style="position: absolute; clip: rect(3.189em, 1000.64em, 4.169em, -999.998em); top: -4.017em; left: 0em;"><span class="mi" id="MathJax-Span-4412" style="font-family: MathJax_Math-italic;">L</span><span style="display: inline-block; width: 0px; height: 4.022em;"></span></span><span style="position: absolute; clip: rect(3.14em, 1000.59em, 3.63em, -999.998em); top: -4.262em; left: 0.1em;"><span class="mo" id="MathJax-Span-4413" style=""><span style="font-family: MathJax_Size1;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.022em;"></span></span></span></span><span class="mo" id="MathJax-Span-4414" style="font-family: MathJax_Main; padding-left: 0.297em;">=</span><span class="mi" id="MathJax-Span-4415" style="font-family: MathJax_Script; padding-left: 0.297em;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.15em;"></span></span><span class="mrow" id="MathJax-Span-4416" style="padding-left: 0.15em;"><span class="mo" id="MathJax-Span-4417" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">(</span></span><span class="mover" id="MathJax-Span-4418"><span style="display: inline-block; position: relative; width: 0.591em; height: 0px;"><span style="position: absolute; clip: rect(3.189em, 1000.44em, 4.169em, -999.998em); top: -4.017em; left: 0.002em;"><span class="mi" id="MathJax-Span-4419" style="font-family: MathJax_Math-italic;">θ</span><span style="display: inline-block; width: 0px; height: 4.022em;"></span></span><span style="position: absolute; clip: rect(3.189em, 1000.39em, 3.63em, -999.998em); top: -4.311em; left: 0.1em;"><span class="mo" id="MathJax-Span-4420" style=""><span style="font-family: MathJax_Main;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.022em;"></span></span></span></span><span class="mo" id="MathJax-Span-4421" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-4422" style="font-family: MathJax_Main-bold; padding-left: 0.15em;">X</span><span class="mo" id="MathJax-Span-4423" style="vertical-align: 0em;"><span style="font-family: MathJax_Size2;">)</span></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.238em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.821em; border-left: 0px solid; width: 0px; height: 2.297em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="ModifyingAbove upper L With caret equals script upper L left-parenthesis ModifyingAbove theta With caret comma bold upper X right-parenthesis"><mrow><mover accent="true"><mi>L</mi><mo>^</mo></mover><mo>=</mo><mi>ℒ</mi><mrow><mo>(</mo><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo>,</mo><mi>𝐗</mi><mo>)</mo></mrow></mrow></math></span></span><script type="math/mml" id="MathJax-Element-115"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="ModifyingAbove upper L With caret equals script upper L left-parenthesis ModifyingAbove theta With caret comma bold upper X right-parenthesis">
  <mrow>
    <mover accent="true"><mi>L</mi> <mo>^</mo></mover>
    <mo>=</mo>
    <mi>ℒ</mi>
    <mrow>
      <mo>(</mo>
      <mover accent="true"><mi>θ</mi> <mo>^</mo></mover>
      <mo>,</mo>
      <mi>𝐗</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math></script>, which is the value used to compute the AIC and BIC; you can think of it as a measure of how well the model fits the data.</p>
</div></aside>

<p>To compute the BIC and AIC, call the <code>bic()</code> or <code>aic()</code> methods:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">bic</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="go">8189.74345832983</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">aic</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="go">8102.518178214792</code></pre>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#aic_bic_vs_k_plot">Figure&nbsp;9-21</a> shows the BIC for different numbers of clusters <em>k</em>. As you can see, both the BIC and the AIC are lowest when <em>k</em>=3, so it is most likely the best choice. Note that we could also search for the best value for the <code>covariance_type</code> hyperparameter. For example, if it is <code>"spherical"</code> rather than <code>"full"</code>, then the model has significantly fewer parameters to learn, but it does not fit the data as well.</p>

<figure class="smallerseventy"><div id="aic_bic_vs_k_plot" class="figure">
<img src="./Chapter9_files/mls2_0921.png" alt="mls2 0921" width="1439" height="484" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0921.png">
<h6><span class="label">Figure 9-21. </span>AIC and BIC for different numbers of clusters <em>k</em></h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Bayesian Gaussian Mixture Models"><div class="sect2" id="idm46263517800984">
<h2>Bayesian Gaussian Mixture Models</h2>

<p>Rather than manually searching for the optimal number of clusters, use the <code>BayesianGaussianMixture</code> class, which is capable of giving weights equal (or close) to zero to unnecessary clusters. Set the number of clusters <code>n_components</code> to a value that you have good reason to believe is greater than the optimal number of clusters (this assumes some minimal knowledge about the problem at hand), and the algorithm will eliminate the unnecessary clusters automatically. For example, let’s set the number of clusters to 10 and see what happens:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.mixture</code> <code class="kn">import</code> <code class="n">BayesianGaussianMixture</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bgm</code> <code class="o">=</code> <code class="n">BayesianGaussianMixture</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">n_init</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bgm</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">np</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="n">bgm</code><code class="o">.</code><code class="n">weights_</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
<code class="go">array([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])</code></pre>

<p>Perfect: the algorithm automatically detected that only three clusters are needed, and the resulting clusters are almost identical to the ones in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#gaussian_mixtures_plot">Figure&nbsp;9-17</a>.</p>

<p>In this model, the cluster parameters (including the weights, means, and covariance matrices) are not treated as fixed model parameters anymore, but as latent random variables, like the cluster assignments (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#bgm_plate_diagram">Figure&nbsp;9-22</a>). So <strong>z</strong> now includes both the cluster parameters and the cluster assignments.</p>

<figure class="smallerseventy"><div id="bgm_plate_diagram" class="figure">
<img src="./Chapter9_files/mls2_0922.png" alt="mls2 0922" width="1440" height="777" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0922.png">
<h6><span class="label">Figure 9-22. </span>Bayesian Gaussian mixture model</h6>
</div></figure>

<p>Prior knowledge about the latent variables <strong>z</strong> can be encoded in a probability distribution <em>p</em>(<strong>z</strong>) called the <em>prior</em>. For example, we may have a prior belief that the clusters are likely to be few (low concentration), or conversely, that they are more likely to be plentiful (high concentration). This prior belief about the number of clusters  can be adjusted using the <code>weight_concentration_prior</code> hyperparameter. Setting it to 0.01 or 10,000 gives very different clusterings (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#mixture_concentration_prior_plot">Figure&nbsp;9-23</a>). The more data we have, however, the more data we have, the less the priors matter. In fact, to plot diagrams with such large differences, you must use very strong priors and little data.</p>

<figure class="smallerseventy"><div id="mixture_concentration_prior_plot" class="figure">
<img src="./Chapter9_files/mls2_0923.png" alt="mls2 0923" width="1440" height="618" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0923.png">
<h6><span class="label">Figure 9-23. </span>Using different concentration priors on the same data results in different numbers of clusters</h6>
</div></figure>

<p>Bayes’ theorem (<a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#bayes_theorem_equation">Equation 9-2</a>) tells us how to update the probability distribution over the latent variables after we observe some data <strong>X</strong>. It computes the <em>posterior</em> distribution <em>p</em>(<strong>z</strong>|<strong>X</strong>), which is the conditional probability of <strong>z</strong> given <strong>X</strong>.</p>
<div data-type="equation" id="bayes_theorem_equation">
<h5><span class="label">Equation 9-2. </span>Bayes’ theorem</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-116-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;Posterior&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mtext&gt;Likelihood&lt;/mtext&gt;&lt;mo&gt;&amp;#xD7;&lt;/mo&gt;&lt;mtext&gt;Prior&lt;/mtext&gt;&lt;/mrow&gt;&lt;mtext&gt;Evidence&lt;/mtext&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4424" style="width: 24.99em; display: inline-block;"><span style="display: inline-block; position: relative; width: 24.27em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(0.517em, 1024.28em, 3.293em, -1000.01em); top: -2.157em; left: 0em;"><span class="mrow" id="MathJax-Span-4425"><span class="mrow" id="MathJax-Span-4426"><span class="mi" id="MathJax-Span-4427" style="font-family: MathJax_Math-italic;">p</span><span class="mrow" id="MathJax-Span-4428" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4429" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4430" style="font-family: MathJax_Main-bold;">z</span><span class="mo" id="MathJax-Span-4431" style="vertical-align: 0em;"><span style="font-family: MathJax_Main;">|</span></span><span class="mi" id="MathJax-Span-4432" style="font-family: MathJax_Main-bold;">X</span><span class="mo" id="MathJax-Span-4433" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4434" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mtext" id="MathJax-Span-4435" style="font-family: MathJax_Main; padding-left: 0.26em;">Posterior</span><span class="mo" id="MathJax-Span-4436" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mstyle" id="MathJax-Span-4437"><span class="mrow" id="MathJax-Span-4438"><span class="mfrac" id="MathJax-Span-4439"><span style="display: inline-block; position: relative; width: 8.126em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.19em, 1007.93em, 4.167em, -1000.01em); top: -4.676em; left: 50%; margin-left: -4.008em;"><span class="mrow" id="MathJax-Span-4440"><span class="mtext" id="MathJax-Span-4441" style="font-family: MathJax_Main;">Likelihood</span><span class="mo" id="MathJax-Span-4442" style="font-family: MathJax_Main; padding-left: 0.208em;">×</span><span class="mtext" id="MathJax-Span-4443" style="font-family: MathJax_Main; padding-left: 0.208em;">Prior</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1003.92em, 4.167em, -1000.01em); top: -3.339em; left: 50%; margin-left: -1.951em;"><span class="mtext" id="MathJax-Span-4444" style="font-family: MathJax_Main;">Evidence</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1008.08em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 8.126em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-4445" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mstyle" id="MathJax-Span-4446" style="padding-left: 0.26em;"><span class="mrow" id="MathJax-Span-4447"><span class="mfrac" id="MathJax-Span-4448"><span style="display: inline-block; position: relative; width: 5.041em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.087em, 1004.79em, 4.424em, -1000.01em); top: -4.728em; left: 50%; margin-left: -2.465em;"><span class="mrow" id="MathJax-Span-4449"><span class="mi" id="MathJax-Span-4450" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4451" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4452" style="font-family: MathJax_Main-bold;">X</span><span class="mo" id="MathJax-Span-4453" style="vertical-align: 0em;"><span style="font-family: MathJax_Main;">|</span></span><span class="mi" id="MathJax-Span-4454" style="font-family: MathJax_Main-bold;">z</span><span class="mo" id="MathJax-Span-4455" style="font-family: MathJax_Main;">)</span><span class="mspace" id="MathJax-Span-4456" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-4457" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4458" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4459" style="font-family: MathJax_Main-bold;">z</span><span class="mo" id="MathJax-Span-4460" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1002.07em, 4.424em, -1000.01em); top: -3.288em; left: 50%; margin-left: -1.077em;"><span class="mrow" id="MathJax-Span-4461"><span class="mi" id="MathJax-Span-4462" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4463" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4464" style="font-family: MathJax_Main-bold;">X</span><span class="mo" id="MathJax-Span-4465" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1005em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 5.041em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.162em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.056em; border-left: 0px solid; width: 0px; height: 2.598em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>p</mi><mrow><mo>(</mo><mi mathvariant="bold">z</mi><mo>|</mo><mi mathvariant="bold">X</mi><mo>)</mo></mrow><mo>=</mo><mtext>Posterior</mtext><mo>=</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mtext>Likelihood</mtext><mo>×</mo><mtext>Prior</mtext></mrow><mtext>Evidence</mtext></mfrac></mstyle><mo>=</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>p</mi><mo>(</mo><mi mathvariant="bold">X</mi><mo>|</mo><mi mathvariant="bold">z</mi><mo>)</mo><mspace width="0.166667em"></mspace><mi>p</mi><mo>(</mo><mi mathvariant="bold">z</mi><mo>)</mo></mrow><mrow><mi>p</mi><mo>(</mo><mi mathvariant="bold">X</mi><mo>)</mo></mrow></mfrac></mstyle></mrow></math></span></span></div><script type="math/mml" id="MathJax-Element-116"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <mi>p</mi>
    <mrow>
      <mo>(</mo>
      <mi mathvariant="bold">z</mi>
      <mo>|</mo>
      <mi mathvariant="bold">X</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mtext>Posterior</mtext>
    <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac><mrow><mtext>Likelihood</mtext><mo>×</mo><mtext>Prior</mtext></mrow> <mtext>Evidence</mtext></mfrac>
    </mstyle>
    <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac><mrow><mi>p</mi><mo>(</mo><mi mathvariant="bold">X</mi><mo>|</mo><mi mathvariant="bold">z</mi><mo>)</mo><mspace width="0.166667em"></mspace><mi>p</mi><mo>(</mo><mi mathvariant="bold">z</mi><mo>)</mo></mrow> <mrow><mi>p</mi><mo>(</mo><mi mathvariant="bold">X</mi><mo>)</mo></mrow></mfrac>
    </mstyle>
  </mrow>
</math></script>
</div>

<p>Unfortunately, in a Gaussian mixture model (and many other problems), the denominator <em>p</em>(<strong>x</strong>) is intractable, as it requires integrating over all the possible values of <strong>z</strong> (<a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#evidence_integral_equation">Equation 9-3</a>), which would require considering all possible combinations of cluster parameters and cluster assignments.</p>
<div data-type="equation" id="evidence_integral_equation">
<h5><span class="label">Equation 9-3. </span>The evidence <em>p</em>(<strong>X</strong>) is often intractable</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-117-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;&amp;#x222B;&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4466" style="width: 11.365em; display: inline-block;"><span style="display: inline-block; position: relative; width: 11.005em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.751em, 1010.96em, 4.424em, -1000.01em); top: -3.339em; left: 0em;"><span class="mrow" id="MathJax-Span-4467"><span class="mrow" id="MathJax-Span-4468"><span class="mi" id="MathJax-Span-4469" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4470" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">(</span></span><span class="mi" id="MathJax-Span-4471" style="font-family: MathJax_Main-bold;">X</span><span class="mo" id="MathJax-Span-4472" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">)</span></span><span class="mo" id="MathJax-Span-4473" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mo" id="MathJax-Span-4474" style="font-family: MathJax_Size2; vertical-align: 0.003em; padding-left: 0.26em;">∫<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.414em;"></span></span><span class="mrow" id="MathJax-Span-4475" style="padding-left: 0.157em;"><span class="mi" id="MathJax-Span-4476" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4477" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4478" style="font-family: MathJax_Main-bold;">X</span><span class="mo" id="MathJax-Span-4479" style="vertical-align: 0em;"><span style="font-family: MathJax_Main;">|</span></span><span class="mi" id="MathJax-Span-4480" style="font-family: MathJax_Main-bold;">z</span><span class="mo" id="MathJax-Span-4481" style="font-family: MathJax_Main;">)</span><span class="mi" id="MathJax-Span-4482" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4483" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4484" style="font-family: MathJax_Main-bold;">z</span><span class="mo" id="MathJax-Span-4485" style="font-family: MathJax_Main;">)</span><span class="mi" id="MathJax-Span-4486" style="font-family: MathJax_Math-italic;">d<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mi" id="MathJax-Span-4487" style="font-family: MathJax_Main-bold;">z</span></span></span></span><span style="display: inline-block; width: 0px; height: 3.345em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.004em; border-left: 0px solid; width: 0px; height: 2.598em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>p</mi><mo>(</mo><mi mathvariant="bold">X</mi><mo>)</mo><mo>=</mo><mo>∫</mo><mrow><mi>p</mi><mo>(</mo><mi mathvariant="bold">X</mi><mo>|</mo><mi mathvariant="bold">z</mi><mo>)</mo><mi>p</mi><mo>(</mo><mi mathvariant="bold">z</mi><mo>)</mo><mi>d</mi><mi mathvariant="bold">z</mi></mrow></mrow></math></span></span></div><script type="math/mml" id="MathJax-Element-117"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <mi>p</mi>
    <mo>(</mo>
    <mi mathvariant="bold">X</mi>
    <mo>)</mo>
    <mo>=</mo>
    <mo>∫</mo>
    <mrow>
      <mi>p</mi>
      <mo>(</mo>
      <mi mathvariant="bold">X</mi>
      <mo>|</mo>
      <mi mathvariant="bold">z</mi>
      <mo>)</mo>
      <mi>p</mi>
      <mo>(</mo>
      <mi mathvariant="bold">z</mi>
      <mo>)</mo>
      <mi>d</mi>
      <mi mathvariant="bold">z</mi>
    </mrow>
  </mrow>
</math></script>
</div>

<p>This intractability is one of the central problems in Bayesian statistics, and there are several approaches to solving it. One of them is <em>variational inference</em>, which picks a family of distributions <em>q</em>(<strong>z</strong>; <strong>λ</strong>) with its own <em>variational parameters</em> <strong>λ</strong> (lambda) then optimizes these parameters to make <em>q</em>(<strong>z</strong>) a good approximation of <em>p</em>(<strong>z</strong>|<strong>X</strong>). This is achieved by finding the value of <strong>λ</strong> that minimizes the KL divergence from <em>q</em>(<strong>z</strong>) to <em>p</em>(<strong>z</strong>|<strong>X</strong>), noted D<sub>KL</sub>(<em>q</em>‖<em>p</em>). The KL divergence equation is shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#variational_kl_divergence_equation">Equation 9-4</a>, and it can be rewritten as the log of the evidence (log <em>p</em>(<strong>X</strong>)) minus the <em>evidence lower bound</em> (ELBO). Since the log of the evidence does not depend on <em>q</em>, it is a constant term, so minimizing the KL divergence just requires maximizing the ELBO.</p>
<div data-type="equation" id="variational_kl_divergence_equation">
<h5><span class="label">Equation 9-4. </span>KL divergence from <em>q</em>(<strong>z</strong>) to <em>p</em>(<strong>z</strong>|<strong>X</strong>)</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-118-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mtable displaystyle=&quot;true&quot;&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo&gt;&amp;#x2225;&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D53C;&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced separators=&quot;&quot; open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D53C;&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced separators=&quot;&quot; open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D53C;&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced separators=&quot;&quot; open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D53C;&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced separators=&quot;&quot; open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D53C;&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced separators=&quot;&quot; open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mfenced&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D53C;&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced separators=&quot;&quot; open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mfenced&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D53C;&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced separators=&quot;&quot; open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D53C;&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced separators=&quot;&quot; open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mfenced&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mfenced separators=&quot;&quot; open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D53C;&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced separators=&quot;&quot; open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mfenced&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D53C;&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced separators=&quot;&quot; open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mfenced&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;mtext&gt;ELBO&lt;/mtext&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd /&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;where&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;ELBO&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D53C;&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced separators=&quot;&quot; open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mfenced&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x1D53C;&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced separators=&quot;&quot; open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mspace width=&quot;0.166667em&quot; /&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;z&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-4488" style="width: 29.823em; display: inline-block;"><span style="display: inline-block; position: relative; width: 28.949em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(-5.499em, 1028.7em, 9.206em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-4489"><span class="mtable" id="MathJax-Span-4490" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 28.64em; height: 0px;"><span style="position: absolute; clip: rect(7.458em, 1005.92em, 21.134em, -1000.01em); top: -14.342em; left: 0em;"><span style="display: inline-block; position: relative; width: 5.967em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1005.92em, 4.424em, -1000.01em); top: -9.972em; right: 0em;"><span class="mtd" id="MathJax-Span-4491"><span class="mrow" id="MathJax-Span-4492"><span class="mrow" id="MathJax-Span-4493"><span class="mrow" id="MathJax-Span-4494"><span class="msub" id="MathJax-Span-4495"><span style="display: inline-block; position: relative; width: 2.008em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.78em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4496" style="font-family: MathJax_Math-italic;">D</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.825em;"><span class="mrow" id="MathJax-Span-4497"><span class="mi" id="MathJax-Span-4498" style="font-size: 70.7%; font-family: MathJax_Math-italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span class="mi" id="MathJax-Span-4499" style="font-size: 70.7%; font-family: MathJax_Math-italic;">L</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-4500" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4501" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4502" style="font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-4503" style="font-family: MathJax_Main; padding-left: 0.26em;">∥</span><span class="mi" id="MathJax-Span-4504" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">p</span><span class="mo" id="MathJax-Span-4505" style="font-family: MathJax_Main;">)</span></span></span><span class="mo" id="MathJax-Span-4506" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.499em, 1000.73em, 4.013em, -1000.01em); top: -7.761em; right: 0em;"><span class="mtd" id="MathJax-Span-4535"><span class="mrow" id="MathJax-Span-4536"><span class="mo" id="MathJax-Span-4537" style="font-family: MathJax_Main;">=</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.499em, 1000.73em, 4.013em, -1000.01em); top: -5.602em; right: 0em;"><span class="mtd" id="MathJax-Span-4563"><span class="mrow" id="MathJax-Span-4564"><span class="mo" id="MathJax-Span-4565" style="font-family: MathJax_Main;">=</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.499em, 1000.73em, 4.013em, -1000.01em); top: -3.391em; right: 0em;"><span class="mtd" id="MathJax-Span-4599"><span class="mrow" id="MathJax-Span-4600"><span class="mo" id="MathJax-Span-4601" style="font-family: MathJax_Main;">=</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.499em, 1000.73em, 4.013em, -1000.01em); top: -1.848em; right: 0em;"><span class="mtd" id="MathJax-Span-4631"><span class="mrow" id="MathJax-Span-4632"><span class="mo" id="MathJax-Span-4633" style="font-family: MathJax_Main;">=</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.499em, 1000.73em, 4.013em, -1000.01em); top: -0.357em; right: 0em;"><span class="mtd" id="MathJax-Span-4675"><span class="mrow" id="MathJax-Span-4676"><span class="mo" id="MathJax-Span-4677" style="font-family: MathJax_Main;">=</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.499em, 1000.73em, 4.013em, -1000.01em); top: 1.185em; right: 0em;"><span class="mtd" id="MathJax-Span-4725"><span class="mrow" id="MathJax-Span-4726"><span class="mo" id="MathJax-Span-4727" style="font-family: MathJax_Main;">=</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.859em, 1000.01em, 4.167em, -1000.01em); top: 2.625em; left: 50%; margin-left: 0em;"><span class="mtd" id="MathJax-Span-4741"><span class="mrow" id="MathJax-Span-4742"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 14.347em;"></span></span><span style="position: absolute; clip: rect(7.766em, 1021.76em, 22.471em, -1000.01em); top: -15.37em; left: 6.789em;"><span style="display: inline-block; position: relative; width: 21.854em; height: 0px;"><span style="position: absolute; clip: rect(2.368em, 1007.72em, 5.144em, -1000.01em); top: -9.972em; left: 0em;"><span class="mtd" id="MathJax-Span-4507"><span class="mrow" id="MathJax-Span-4508"><span class="mrow" id="MathJax-Span-4509"><span class="mspace" id="MathJax-Span-4510" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="msub" id="MathJax-Span-4511"><span style="display: inline-block; position: relative; width: 1.082em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4512" style="font-family: MathJax_AMS;">E</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.671em;"><span class="mi" id="MathJax-Span-4513" style="font-size: 70.7%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mfenced" id="MathJax-Span-4514" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4515" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">[</span></span><span class="mo" id="MathJax-Span-4516" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mstyle" id="MathJax-Span-4517"><span class="mrow" id="MathJax-Span-4518"><span class="mfrac" id="MathJax-Span-4519"><span style="display: inline-block; position: relative; width: 3.396em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.087em, 1001.66em, 4.424em, -1000.01em); top: -4.728em; left: 50%; margin-left: -0.871em;"><span class="mrow" id="MathJax-Span-4520"><span class="mi" id="MathJax-Span-4521" style="font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-4522" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4523" style="font-family: MathJax_Main-bold;">z</span><span class="mo" id="MathJax-Span-4524" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1003.2em, 4.424em, -1000.01em); top: -3.288em; left: 50%; margin-left: -1.643em;"><span class="mrow" id="MathJax-Span-4525"><span class="mi" id="MathJax-Span-4526" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4527" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4528" style="font-family: MathJax_Main-bold;">z</span><span class="mspace" id="MathJax-Span-4529" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-4530" style="vertical-align: 0em;"><span style="font-family: MathJax_Main;">|</span></span><span class="mspace" id="MathJax-Span-4531" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-4532" style="font-family: MathJax_Main-bold;">X</span><span class="mo" id="MathJax-Span-4533" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1003.4em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.396em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-4534" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">]</span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1011.63em, 4.476em, -1000.01em); top: -7.761em; left: 0em;"><span class="mtd" id="MathJax-Span-4538"><span class="mrow" id="MathJax-Span-4539"><span class="mrow" id="MathJax-Span-4540"><span class="mspace" id="MathJax-Span-4541" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="msub" id="MathJax-Span-4542"><span style="display: inline-block; position: relative; width: 1.082em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4543" style="font-family: MathJax_AMS;">E</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.671em;"><span class="mi" id="MathJax-Span-4544" style="font-size: 70.7%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mfenced" id="MathJax-Span-4545" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4546" style=""><span style="font-family: MathJax_Main;">[</span></span><span class="mo" id="MathJax-Span-4547" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mi" id="MathJax-Span-4548" style="font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-4549" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mi" id="MathJax-Span-4550" style="font-family: MathJax_Main-bold;">z</span><span class="mo" id="MathJax-Span-4551" style=""><span style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4552" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mo" id="MathJax-Span-4553" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mi" id="MathJax-Span-4554" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4555" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mi" id="MathJax-Span-4556" style="font-family: MathJax_Main-bold;">z</span><span class="mspace" id="MathJax-Span-4557" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-4558" style="vertical-align: 0em;"><span style="font-family: MathJax_Main;">|</span></span><span class="mspace" id="MathJax-Span-4559" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-4560" style="font-family: MathJax_Main-bold;">X</span><span class="mo" id="MathJax-Span-4561" style=""><span style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4562" style=""><span style="font-family: MathJax_Main;">]</span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.368em, 1012.35em, 5.144em, -1000.01em); top: -5.602em; left: 0em;"><span class="mtd" id="MathJax-Span-4566"><span class="mrow" id="MathJax-Span-4567"><span class="mrow" id="MathJax-Span-4568"><span class="mspace" id="MathJax-Span-4569" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="msub" id="MathJax-Span-4570"><span style="display: inline-block; position: relative; width: 1.082em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4571" style="font-family: MathJax_AMS;">E</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.671em;"><span class="mi" id="MathJax-Span-4572" style="font-size: 70.7%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mfenced" id="MathJax-Span-4573" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4574" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">[</span></span><span class="mo" id="MathJax-Span-4575" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mi" id="MathJax-Span-4576" style="font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mrow" id="MathJax-Span-4577" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4578" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4579" style="font-family: MathJax_Main-bold;">z</span><span class="mo" id="MathJax-Span-4580" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4581" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mo" id="MathJax-Span-4582" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mstyle" id="MathJax-Span-4583"><span class="mrow" id="MathJax-Span-4584"><span class="mfrac" id="MathJax-Span-4585"><span style="display: inline-block; position: relative; width: 3.242em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.087em, 1002.99em, 4.424em, -1000.01em); top: -4.728em; left: 50%; margin-left: -1.54em;"><span class="mrow" id="MathJax-Span-4586"><span class="mi" id="MathJax-Span-4587" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4588" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4589" style="font-family: MathJax_Main-bold;">z</span><span class="mo" id="MathJax-Span-4590" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-4591" style="font-family: MathJax_Main-bold; padding-left: 0.157em;">X</span><span class="mo" id="MathJax-Span-4592" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1002.07em, 4.424em, -1000.01em); top: -3.288em; left: 50%; margin-left: -1.077em;"><span class="mrow" id="MathJax-Span-4593"><span class="mi" id="MathJax-Span-4594" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4595" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4596" style="font-family: MathJax_Main-bold;">X</span><span class="mo" id="MathJax-Span-4597" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1003.2em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.242em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span><span class="mo" id="MathJax-Span-4598" style="vertical-align: 0em;"><span style="font-family: MathJax_Size3;">]</span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1016.51em, 4.476em, -1000.01em); top: -3.391em; left: 0em;"><span class="mtd" id="MathJax-Span-4602"><span class="mrow" id="MathJax-Span-4603"><span class="mrow" id="MathJax-Span-4604"><span class="mspace" id="MathJax-Span-4605" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="msub" id="MathJax-Span-4606"><span style="display: inline-block; position: relative; width: 1.082em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4607" style="font-family: MathJax_AMS;">E</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.671em;"><span class="mi" id="MathJax-Span-4608" style="font-size: 70.7%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mfenced" id="MathJax-Span-4609" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4610" style=""><span style="font-family: MathJax_Main;">[</span></span><span class="mo" id="MathJax-Span-4611" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mi" id="MathJax-Span-4612" style="font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-4613" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mi" id="MathJax-Span-4614" style="font-family: MathJax_Main-bold;">z</span><span class="mo" id="MathJax-Span-4615" style=""><span style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4616" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mo" id="MathJax-Span-4617" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mi" id="MathJax-Span-4618" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4619" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mi" id="MathJax-Span-4620" style="font-family: MathJax_Main-bold;">z</span><span class="mo" id="MathJax-Span-4621" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-4622" style="font-family: MathJax_Main-bold; padding-left: 0.157em;">X</span><span class="mo" id="MathJax-Span-4623" style=""><span style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4624" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="mo" id="MathJax-Span-4625" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mi" id="MathJax-Span-4626" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4627" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mi" id="MathJax-Span-4628" style="font-family: MathJax_Main-bold;">X</span><span class="mo" id="MathJax-Span-4629" style=""><span style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4630" style=""><span style="font-family: MathJax_Main;">]</span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1020.47em, 4.476em, -1000.01em); top: -1.848em; left: 0em;"><span class="mtd" id="MathJax-Span-4634"><span class="mrow" id="MathJax-Span-4635"><span class="mrow" id="MathJax-Span-4636"><span class="mspace" id="MathJax-Span-4637" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="msub" id="MathJax-Span-4638"><span style="display: inline-block; position: relative; width: 1.082em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4639" style="font-family: MathJax_AMS;">E</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.671em;"><span class="mi" id="MathJax-Span-4640" style="font-size: 70.7%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mfenced" id="MathJax-Span-4641" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4642" style=""><span style="font-family: MathJax_Main;">[</span></span><span class="mo" id="MathJax-Span-4643" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mi" id="MathJax-Span-4644" style="font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-4645" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mi" id="MathJax-Span-4646" style="font-family: MathJax_Main-bold;">z</span><span class="mo" id="MathJax-Span-4647" style=""><span style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4648" style=""><span style="font-family: MathJax_Main;">]</span></span></span><span class="mo" id="MathJax-Span-4649" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="msub" id="MathJax-Span-4650" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.082em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4651" style="font-family: MathJax_AMS;">E</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.671em;"><span class="mi" id="MathJax-Span-4652" style="font-size: 70.7%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mfenced" id="MathJax-Span-4653" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4654" style=""><span style="font-family: MathJax_Main;">[</span></span><span class="mo" id="MathJax-Span-4655" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mi" id="MathJax-Span-4656" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4657" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mi" id="MathJax-Span-4658" style="font-family: MathJax_Main-bold;">z</span><span class="mo" id="MathJax-Span-4659" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-4660" style="font-family: MathJax_Main-bold; padding-left: 0.157em;">X</span><span class="mo" id="MathJax-Span-4661" style=""><span style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4662" style=""><span style="font-family: MathJax_Main;">]</span></span></span><span class="mo" id="MathJax-Span-4663" style="font-family: MathJax_Main; padding-left: 0.208em;">+</span><span class="msub" id="MathJax-Span-4664" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.082em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4665" style="font-family: MathJax_AMS;">E</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.671em;"><span class="mi" id="MathJax-Span-4666" style="font-size: 70.7%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mfenced" id="MathJax-Span-4667" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4668" style=""><span style="font-family: MathJax_Main;">[</span></span><span class="mo" id="MathJax-Span-4669" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mi" id="MathJax-Span-4670" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4671" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mi" id="MathJax-Span-4672" style="font-family: MathJax_Main-bold;">X</span><span class="mo" id="MathJax-Span-4673" style=""><span style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4674" style=""><span style="font-family: MathJax_Main;">]</span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1021.76em, 4.476em, -1000.01em); top: -0.357em; left: 0em;"><span class="mtd" id="MathJax-Span-4678"><span class="mrow" id="MathJax-Span-4679"><span class="mrow" id="MathJax-Span-4680"><span class="mspace" id="MathJax-Span-4681" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="msub" id="MathJax-Span-4682"><span style="display: inline-block; position: relative; width: 1.082em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4683" style="font-family: MathJax_AMS;">E</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.671em;"><span class="mi" id="MathJax-Span-4684" style="font-size: 70.7%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mfenced" id="MathJax-Span-4685" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4686" style=""><span style="font-family: MathJax_Main;">[</span></span><span class="mo" id="MathJax-Span-4687" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mspace" id="MathJax-Span-4688" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-4689" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4690" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mi" id="MathJax-Span-4691" style="font-family: MathJax_Main-bold;">X</span><span class="mo" id="MathJax-Span-4692" style=""><span style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4693" style=""><span style="font-family: MathJax_Main;">]</span></span></span><span class="mo" id="MathJax-Span-4694" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mfenced" id="MathJax-Span-4695" style="padding-left: 0.208em;"><span class="mo" id="MathJax-Span-4696" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="msub" id="MathJax-Span-4697"><span style="display: inline-block; position: relative; width: 1.082em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4698" style="font-family: MathJax_AMS;">E</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.671em;"><span class="mi" id="MathJax-Span-4699" style="font-size: 70.7%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mfenced" id="MathJax-Span-4700" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4701" style=""><span style="font-family: MathJax_Main;">[</span></span><span class="mo" id="MathJax-Span-4702" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mspace" id="MathJax-Span-4703" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-4704" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4705" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mi" id="MathJax-Span-4706" style="font-family: MathJax_Main-bold;">z</span><span class="mo" id="MathJax-Span-4707" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-4708" style="font-family: MathJax_Main-bold; padding-left: 0.157em;">X</span><span class="mo" id="MathJax-Span-4709" style=""><span style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4710" style=""><span style="font-family: MathJax_Main;">]</span></span></span><span class="mo" id="MathJax-Span-4711" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="msub" id="MathJax-Span-4712" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.082em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4713" style="font-family: MathJax_AMS;">E</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.671em;"><span class="mi" id="MathJax-Span-4714" style="font-size: 70.7%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mfenced" id="MathJax-Span-4715" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4716" style=""><span style="font-family: MathJax_Main;">[</span></span><span class="mo" id="MathJax-Span-4717" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mspace" id="MathJax-Span-4718" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-4719" style="font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-4720" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mi" id="MathJax-Span-4721" style="font-family: MathJax_Main-bold;">z</span><span class="mo" id="MathJax-Span-4722" style=""><span style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4723" style=""><span style="font-family: MathJax_Main;">]</span></span></span><span class="mo" id="MathJax-Span-4724" style=""><span style="font-family: MathJax_Main;">)</span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1009.37em, 4.424em, -1000.01em); top: 1.185em; left: 0em;"><span class="mtd" id="MathJax-Span-4728"><span class="mrow" id="MathJax-Span-4729"><span class="mrow" id="MathJax-Span-4730"><span class="mspace" id="MathJax-Span-4731" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mo" id="MathJax-Span-4732" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mspace" id="MathJax-Span-4733" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-4734" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4735" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-4736" style="font-family: MathJax_Main-bold;">X</span><span class="mo" id="MathJax-Span-4737" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-4738" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="mtext" id="MathJax-Span-4739" style="font-family: MathJax_Main; padding-left: 0.208em;">ELBO</span><span class="mspace" id="MathJax-Span-4740" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1020.68em, 4.476em, -1000.01em); top: 2.625em; left: 0em;"><span class="mtd" id="MathJax-Span-4743"><span class="mrow" id="MathJax-Span-4744"><span class="mrow" id="MathJax-Span-4745"><span class="mtext" id="MathJax-Span-4746" style="font-family: MathJax_Main;">where</span><span class="mspace" id="MathJax-Span-4747" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-4748" style="font-family: MathJax_Main;">ELBO</span><span class="mo" id="MathJax-Span-4749" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="msub" id="MathJax-Span-4750" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 1.082em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4751" style="font-family: MathJax_AMS;">E</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.671em;"><span class="mi" id="MathJax-Span-4752" style="font-size: 70.7%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mfenced" id="MathJax-Span-4753" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4754" style=""><span style="font-family: MathJax_Main;">[</span></span><span class="mo" id="MathJax-Span-4755" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mspace" id="MathJax-Span-4756" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-4757" style="font-family: MathJax_Math-italic;">p</span><span class="mo" id="MathJax-Span-4758" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mi" id="MathJax-Span-4759" style="font-family: MathJax_Main-bold;">z</span><span class="mo" id="MathJax-Span-4760" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-4761" style="font-family: MathJax_Main-bold; padding-left: 0.157em;">X</span><span class="mo" id="MathJax-Span-4762" style=""><span style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4763" style=""><span style="font-family: MathJax_Main;">]</span></span></span><span class="mo" id="MathJax-Span-4764" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="msub" id="MathJax-Span-4765" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 1.082em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-4766" style="font-family: MathJax_AMS;">E</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.671em;"><span class="mi" id="MathJax-Span-4767" style="font-size: 70.7%; font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mfenced" id="MathJax-Span-4768" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-4769" style=""><span style="font-family: MathJax_Main;">[</span></span><span class="mo" id="MathJax-Span-4770" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mspace" id="MathJax-Span-4771" style="height: 0em; vertical-align: 0em; width: 0.157em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-4772" style="font-family: MathJax_Math-italic;">q<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-4773" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="mi" id="MathJax-Span-4774" style="font-family: MathJax_Main-bold;">z</span><span class="mo" id="MathJax-Span-4775" style=""><span style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-4776" style=""><span style="font-family: MathJax_Main;">]</span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 15.375em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -7.199em; border-left: 0px solid; width: 0px; height: 14.936em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mrow><mo>(</mo><mi>q</mi><mo>∥</mo><mi>p</mi><mo>)</mo></mrow></mrow><mo>=</mo></mrow></mtd><mtd columnalign="left"><mrow><mspace width="0.166667em"></mspace><msub><mi>𝔼</mi><mi>q</mi></msub><mfenced separators="" open="[" close="]"><mo form="prefix">log</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>q</mi><mo>(</mo><mi mathvariant="bold">z</mi><mo>)</mo></mrow><mrow><mi>p</mi><mo>(</mo><mi mathvariant="bold">z</mi><mspace width="0.166667em"></mspace><mo>|</mo><mspace width="0.166667em"></mspace><mi mathvariant="bold">X</mi><mo>)</mo></mrow></mfrac></mstyle></mfenced></mrow></mtd></mtr><mtr><mtd columnalign="right"><mo>=</mo></mtd><mtd columnalign="left"><mrow><mspace width="0.166667em"></mspace><msub><mi>𝔼</mi><mi>q</mi></msub><mfenced separators="" open="[" close="]"><mo form="prefix">log</mo><mi>q</mi><mo>(</mo><mi mathvariant="bold">z</mi><mo>)</mo><mo>-</mo><mo form="prefix">log</mo><mi>p</mi><mo>(</mo><mi mathvariant="bold">z</mi><mspace width="0.166667em"></mspace><mo>|</mo><mspace width="0.166667em"></mspace><mi mathvariant="bold">X</mi><mo>)</mo></mfenced></mrow></mtd></mtr><mtr><mtd columnalign="right"><mo>=</mo></mtd><mtd columnalign="left"><mrow><mspace width="0.166667em"></mspace><msub><mi>𝔼</mi><mi>q</mi></msub><mfenced separators="" open="[" close="]"><mo form="prefix">log</mo><mi>q</mi><mrow><mo>(</mo><mi mathvariant="bold">z</mi><mo>)</mo></mrow><mo>-</mo><mo form="prefix">log</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>p</mi><mo>(</mo><mi mathvariant="bold">z</mi><mo>,</mo><mi mathvariant="bold">X</mi><mo>)</mo></mrow><mrow><mi>p</mi><mo>(</mo><mi mathvariant="bold">X</mi><mo>)</mo></mrow></mfrac></mstyle></mfenced></mrow></mtd></mtr><mtr><mtd columnalign="right"><mo>=</mo></mtd><mtd columnalign="left"><mrow><mspace width="0.166667em"></mspace><msub><mi>𝔼</mi><mi>q</mi></msub><mfenced separators="" open="[" close="]"><mo form="prefix">log</mo><mi>q</mi><mo>(</mo><mi mathvariant="bold">z</mi><mo>)</mo><mo>-</mo><mo form="prefix">log</mo><mi>p</mi><mo>(</mo><mi mathvariant="bold">z</mi><mo>,</mo><mi mathvariant="bold">X</mi><mo>)</mo><mo>+</mo><mo form="prefix">log</mo><mi>p</mi><mo>(</mo><mi mathvariant="bold">X</mi><mo>)</mo></mfenced></mrow></mtd></mtr><mtr><mtd columnalign="right"><mo>=</mo></mtd><mtd columnalign="left"><mrow><mspace width="0.166667em"></mspace><msub><mi>𝔼</mi><mi>q</mi></msub><mfenced separators="" open="[" close="]"><mo form="prefix">log</mo><mi>q</mi><mo>(</mo><mi mathvariant="bold">z</mi><mo>)</mo></mfenced><mo>-</mo><msub><mi>𝔼</mi><mi>q</mi></msub><mfenced separators="" open="[" close="]"><mo form="prefix">log</mo><mi>p</mi><mo>(</mo><mi mathvariant="bold">z</mi><mo>,</mo><mi mathvariant="bold">X</mi><mo>)</mo></mfenced><mo>+</mo><msub><mi>𝔼</mi><mi>q</mi></msub><mfenced separators="" open="[" close="]"><mo form="prefix">log</mo><mi>p</mi><mo>(</mo><mi mathvariant="bold">X</mi><mo>)</mo></mfenced></mrow></mtd></mtr><mtr><mtd columnalign="right"><mo>=</mo></mtd><mtd columnalign="left"><mrow><mspace width="0.166667em"></mspace><msub><mi>𝔼</mi><mi>q</mi></msub><mfenced separators="" open="[" close="]"><mo form="prefix">log</mo><mspace width="0.166667em"></mspace><mi>p</mi><mo>(</mo><mi mathvariant="bold">X</mi><mo>)</mo></mfenced><mo>-</mo><mfenced separators="" open="(" close=")"><msub><mi>𝔼</mi><mi>q</mi></msub><mfenced separators="" open="[" close="]"><mo form="prefix">log</mo><mspace width="0.166667em"></mspace><mi>p</mi><mo>(</mo><mi mathvariant="bold">z</mi><mo>,</mo><mi mathvariant="bold">X</mi><mo>)</mo></mfenced><mo>-</mo><msub><mi>𝔼</mi><mi>q</mi></msub><mfenced separators="" open="[" close="]"><mo form="prefix">log</mo><mspace width="0.166667em"></mspace><mi>q</mi><mo>(</mo><mi mathvariant="bold">z</mi><mo>)</mo></mfenced></mfenced></mrow></mtd></mtr><mtr><mtd columnalign="right"><mo>=</mo></mtd><mtd columnalign="left"><mrow><mspace width="0.166667em"></mspace><mo form="prefix">log</mo><mspace width="0.166667em"></mspace><mi>p</mi><mo>(</mo><mi mathvariant="bold">X</mi><mo>)</mo><mo>-</mo><mtext>ELBO</mtext><mspace width="1.em"></mspace></mrow></mtd></mtr><mtr><mtd></mtd><mtd columnalign="left"><mrow><mtext>where</mtext><mspace width="4.pt"></mspace><mtext>ELBO</mtext><mo>=</mo><msub><mi>𝔼</mi><mi>q</mi></msub><mfenced separators="" open="[" close="]"><mo form="prefix">log</mo><mspace width="0.166667em"></mspace><mi>p</mi><mo>(</mo><mi mathvariant="bold">z</mi><mo>,</mo><mi mathvariant="bold">X</mi><mo>)</mo></mfenced><mo>-</mo><msub><mi>𝔼</mi><mi>q</mi></msub><mfenced separators="" open="[" close="]"><mo form="prefix">log</mo><mspace width="0.166667em"></mspace><mi>q</mi><mo>(</mo><mi mathvariant="bold">z</mi><mo>)</mo></mfenced></mrow></mtd></mtr></mtable></math></span></span></div><script type="math/mml" id="MathJax-Element-118"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mrow>
            <msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow> </msub>
            <mrow>
              <mo>(</mo>
              <mi>q</mi>
              <mo>∥</mo>
              <mi>p</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
        </mrow>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mspace width="0.166667em"></mspace>
          <msub><mi>𝔼</mi> <mi>q</mi> </msub>
          <mfenced separators="" open="[" close="]">
            <mo form="prefix">log</mo>
            <mstyle scriptlevel="0" displaystyle="true">
              <mfrac><mrow><mi>q</mi><mo>(</mo><mi mathvariant="bold">z</mi><mo>)</mo></mrow> <mrow><mi>p</mi><mo>(</mo><mi mathvariant="bold">z</mi><mspace width="0.166667em"></mspace><mo>|</mo><mspace width="0.166667em"></mspace><mi mathvariant="bold">X</mi><mo>)</mo></mrow></mfrac>
            </mstyle>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mo>=</mo>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mspace width="0.166667em"></mspace>
          <msub><mi>𝔼</mi> <mi>q</mi> </msub>
          <mfenced separators="" open="[" close="]">
            <mo form="prefix">log</mo>
            <mi>q</mi>
            <mo>(</mo>
            <mi mathvariant="bold">z</mi>
            <mo>)</mo>
            <mo>-</mo>
            <mo form="prefix">log</mo>
            <mi>p</mi>
            <mo>(</mo>
            <mi mathvariant="bold">z</mi>
            <mspace width="0.166667em"></mspace>
            <mo>|</mo>
            <mspace width="0.166667em"></mspace>
            <mi mathvariant="bold">X</mi>
            <mo>)</mo>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mo>=</mo>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mspace width="0.166667em"></mspace>
          <msub><mi>𝔼</mi> <mi>q</mi> </msub>
          <mfenced separators="" open="[" close="]">
            <mo form="prefix">log</mo>
            <mi>q</mi>
            <mrow>
              <mo>(</mo>
              <mi mathvariant="bold">z</mi>
              <mo>)</mo>
            </mrow>
            <mo>-</mo>
            <mo form="prefix">log</mo>
            <mstyle scriptlevel="0" displaystyle="true">
              <mfrac><mrow><mi>p</mi><mo>(</mo><mi mathvariant="bold">z</mi><mo>,</mo><mi mathvariant="bold">X</mi><mo>)</mo></mrow> <mrow><mi>p</mi><mo>(</mo><mi mathvariant="bold">X</mi><mo>)</mo></mrow></mfrac>
            </mstyle>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mo>=</mo>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mspace width="0.166667em"></mspace>
          <msub><mi>𝔼</mi> <mi>q</mi> </msub>
          <mfenced separators="" open="[" close="]">
            <mo form="prefix">log</mo>
            <mi>q</mi>
            <mo>(</mo>
            <mi mathvariant="bold">z</mi>
            <mo>)</mo>
            <mo>-</mo>
            <mo form="prefix">log</mo>
            <mi>p</mi>
            <mo>(</mo>
            <mi mathvariant="bold">z</mi>
            <mo>,</mo>
            <mi mathvariant="bold">X</mi>
            <mo>)</mo>
            <mo>+</mo>
            <mo form="prefix">log</mo>
            <mi>p</mi>
            <mo>(</mo>
            <mi mathvariant="bold">X</mi>
            <mo>)</mo>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mo>=</mo>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mspace width="0.166667em"></mspace>
          <msub><mi>𝔼</mi> <mi>q</mi> </msub>
          <mfenced separators="" open="[" close="]">
            <mo form="prefix">log</mo>
            <mi>q</mi>
            <mo>(</mo>
            <mi mathvariant="bold">z</mi>
            <mo>)</mo>
          </mfenced>
          <mo>-</mo>
          <msub><mi>𝔼</mi> <mi>q</mi> </msub>
          <mfenced separators="" open="[" close="]">
            <mo form="prefix">log</mo>
            <mi>p</mi>
            <mo>(</mo>
            <mi mathvariant="bold">z</mi>
            <mo>,</mo>
            <mi mathvariant="bold">X</mi>
            <mo>)</mo>
          </mfenced>
          <mo>+</mo>
          <msub><mi>𝔼</mi> <mi>q</mi> </msub>
          <mfenced separators="" open="[" close="]">
            <mo form="prefix">log</mo>
            <mi>p</mi>
            <mo>(</mo>
            <mi mathvariant="bold">X</mi>
            <mo>)</mo>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mo>=</mo>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mspace width="0.166667em"></mspace>
          <msub><mi>𝔼</mi> <mi>q</mi> </msub>
          <mfenced separators="" open="[" close="]">
            <mo form="prefix">log</mo>
            <mspace width="0.166667em"></mspace>
            <mi>p</mi>
            <mo>(</mo>
            <mi mathvariant="bold">X</mi>
            <mo>)</mo>
          </mfenced>
          <mo>-</mo>
          <mfenced separators="" open="(" close=")">
            <msub><mi>𝔼</mi> <mi>q</mi> </msub>
            <mfenced separators="" open="[" close="]">
              <mo form="prefix">log</mo>
              <mspace width="0.166667em"></mspace>
              <mi>p</mi>
              <mo>(</mo>
              <mi mathvariant="bold">z</mi>
              <mo>,</mo>
              <mi mathvariant="bold">X</mi>
              <mo>)</mo>
            </mfenced>
            <mo>-</mo>
            <msub><mi>𝔼</mi> <mi>q</mi> </msub>
            <mfenced separators="" open="[" close="]">
              <mo form="prefix">log</mo>
              <mspace width="0.166667em"></mspace>
              <mi>q</mi>
              <mo>(</mo>
              <mi mathvariant="bold">z</mi>
              <mo>)</mo>
            </mfenced>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd columnalign="right">
        <mo>=</mo>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mspace width="0.166667em"></mspace>
          <mo form="prefix">log</mo>
          <mspace width="0.166667em"></mspace>
          <mi>p</mi>
          <mo>(</mo>
          <mi mathvariant="bold">X</mi>
          <mo>)</mo>
          <mo>-</mo>
          <mtext>ELBO</mtext>
          <mspace width="1.em"></mspace>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd></mtd>
      <mtd columnalign="left">
        <mrow>
          <mtext>where</mtext>
          <mspace width="4.pt"></mspace>
          <mtext>ELBO</mtext>
          <mo>=</mo>
          <msub><mi>𝔼</mi> <mi>q</mi> </msub>
          <mfenced separators="" open="[" close="]">
            <mo form="prefix">log</mo>
            <mspace width="0.166667em"></mspace>
            <mi>p</mi>
            <mo>(</mo>
            <mi mathvariant="bold">z</mi>
            <mo>,</mo>
            <mi mathvariant="bold">X</mi>
            <mo>)</mo>
          </mfenced>
          <mo>-</mo>
          <msub><mi>𝔼</mi> <mi>q</mi> </msub>
          <mfenced separators="" open="[" close="]">
            <mo form="prefix">log</mo>
            <mspace width="0.166667em"></mspace>
            <mi>q</mi>
            <mo>(</mo>
            <mi mathvariant="bold">z</mi>
            <mo>)</mo>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></script>
</div>

<p>In practice, there are different techniques to maximize the ELBO. In <em>mean field variational inference</em>, it is necessary to pick the family of distributions <em>q</em>(<strong>z</strong>; <strong>λ</strong>) and the prior <em>p</em>(<em>z</em>) very carefully to ensure that the equation for the ELBO simplifies to a form that can be computed. Unfortunately, there is no general way to do this. Picking the right family of distributions and the right prior depends on the task and requires some mathematical skills. For example, the distributions and lower-bound equations used in Scikit-Learn’s <code>BayesianGaussianMixture</code> class are presented in the <a href="https://homl.info/40">documentation</a>. From these equations it is possible to derive update equations for the cluster parameters and assignment variables: these are then used very much like in the Expectation-Maximization algorithm. In fact, the computational complexity of the <code>BayesianGaussianMixture</code> class is similar to that of the <code>GaussianMixture</code> class (but generally significantly slower). A simpler approach to maximizing the ELBO is called <em>black box stochastic variational inference</em> (BBSVI): at each iteration, a few samples are drawn from <em>q</em>, and they are used to estimate the gradients of the ELBO with regard to the variational parameters <strong>λ</strong>, which are then used in a gradient ascent step. This approach makes it possible to use Bayesian inference with any kind of model (provided it is differentiable), even deep neural networks: using Bayesian inference with deep neural networks is called Bayesian Deep Learning.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If you want to dive deeper into Bayesian statistics, check out the book <a href="https://homl.info/bda"><em>Bayesian Data Analysis</em></a> by Andrew Gelman et al. (Chapman and Hall).</p>
</div>

<p>Gaussian mixture models work great on clusters with ellipsoidal shapes, but if you try to fit a dataset with different shapes, you may have bad surprises. For example, let’s see what happens if we use a Bayesian Gaussian mixture model to cluster the moons dataset (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#moons_vs_bgm_plot">Figure&nbsp;9-24</a>).</p>

<figure class="smallerseventy"><div id="moons_vs_bgm_plot" class="figure">
<img src="./Chapter9_files/mls2_0924.png" alt="mls2 0924" width="1440" height="468" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0924.png">
<h6><span class="label">Figure 9-24. </span>Fitting a Gaussian mixture to nonellipsoidal clusters</h6>
</div></figure>

<p>Oops, the algorithm desperately searched for ellipsoids, so it found eight different clusters instead of two. The density estimation is not too bad, so this model could perhaps be used for anomaly detection, but it failed to identify the two moons. Let’s now look at a few clustering algorithms capable of dealing with arbitrarily shaped clusters.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Other Algorithms for Anomaly and Novelty Detection"><div class="sect2" id="idm46263517656744">
<h2>Other Algorithms for Anomaly and Novelty Detection</h2>

<p>Scikit-Learn implements other algorithms dedicated to anomaly detection or novelty detection:</p>
<dl>
<dt>PCA_ (and other dimensionality reduction techniques with an <code>inverse_transform()</code> method)</dt>
<dd>
<p>If you compare the reconstruction error of a normal instance with the reconstruction error of an anomaly, the latter will usually be much larger. This is a simple and often quite efficient anomaly detection approach (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#exercises-ch9">“Exercises”</a> for an application of this approach).</p>
</dd>
<dt>Fast-MCD (minimum covariance determinant)</dt>
<dd>
<p>Implemented by the <code>EllipticEnvelope</code> class: this algorithm is useful for outlier detection, in particular to cleanup a dataset. It assumes that the normal instances (inliers) are generated from a single Gaussian distribution (not a mixture). It also assumes that the dataset is contaminated with outliers that were not generated from this Gaussian distribution. When the algorithm estimates the parameters of the Gaussian distribution (i.e., the shape of the elliptic envelope around the inliers), it is careful to ignore the instances that are most likely outliers. This technique gives a better estimation of the elliptic envelope and thus makes it better at identifying the outliers.</p>
</dd>
<dt>Isolation forest</dt>
<dd>
<p>This is an efficient algorithm for outlier detection, especially in high-dimensional datasets. The algorithm builds a Random Forest in which each Decision Tree is grown randomly: at each node, it picks a feature randomly, then it picks a random threshold value (between the min and max value) to split the dataset in two. The dataset gradually gets chopped into pieces this way, until all instances end up isolated from the other instances. An anomaly is usually far from other instances, so on average (across all the Decision Trees) it tends to get isolated in fewer steps than normal instances.</p>
</dd>
<dt>Local outlier factor (LOF)</dt>
<dd>
<p>This algorithm is also good for outlier detection. It compares the density of instances around a given instance to the density around its neighbors. An anomaly is often more isolated than its <em>k</em> nearest neighbors.</p>
</dd>
<dt>One-class SVM</dt>
<dd>
<p>This algorithm is better suited for novelty detection. Recall that a kernelized SVM classifier separates two classes by first (implicitly) mapping all the instances to a high-dimensional space, then separating the two classes using a linear SVM classifier within this high-dimensional space (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch05.html#svm_chapter">Chapter&nbsp;5</a>). Since we just have one class of instances, the one-class SVM algorithm instead tries to separate the instances in high-dimensional space from the origin. In the original space, this will correspond to finding a small region that encompasses all the instances. If a new instance does not fall within this region, it is an anomaly. There are a few hyperparameters to tweak: the usual ones for a kernelized SVM, plus a margin hyperparameter that corresponds to the probability of a new instance being mistakenly considered as novel, when it is in fact normal. It works great, especially with high-dimensional datasets. But just like all SVMs, it does not scale to large datasets.</p>
</dd>
</dl>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="exercises-ch9">
<h1>Exercises</h1>
<ol>
<li>
<p>How would you define clustering? Can you name a few clustering algorithms?</p>
</li>
<li>
<p>What are some of the main applications of clustering algorithms?</p>
</li>
<li>
<p>Describe two techniques to select the right number of clusters when using K-Means.</p>
</li>
<li>
<p>What is label propagation? Why would you implement it, and how?</p>
</li>
<li>
<p>Can you name two clustering algorithms that can scale to large datasets? And two that look for regions of high density?</p>
</li>
<li>
<p>Can you think of a use case where active learning would be useful? How would you implement it?</p>
</li>
<li>
<p>What is the difference between anomaly detection and novelty detection?</p>
</li>
<li>
<p>What is a Gaussian Mixture? What tasks can you use it for?</p>
</li>
<li>
<p>Can you name two techniques to find the right number of clusters when using a Gaussian Mixture model?</p>
</li>
<li>
<p>The classic Olivetti faces dataset contains 400 grayscale 64 × 64 pixel images of faces. Each image is flattened to a 1D vector of size 4,096. 40 different people were photographed (10 times each), and the usual task is to train a model that can predict which person is represented in each picture. Load the dataset using the <code>sklearn.datasets.fetch_olivetti_faces()</code> function, then split it into a training set, a validation set, and a test set (note that the dataset is already scaled between 0 and 1). Since the dataset is quite small, you probably want to use stratified sampling to ensure that there are the same number of images per person in each set. Next, cluster the images using K-Means, and ensure that you have a good number of clusters (using one of the techniques discussed in this chapter). Visualize the clusters: do you see similar faces in each cluster?</p>
</li>
<li>
<p>Continuing with the Olivetti faces dataset, train a classifier to predict which person is represented on each picture, and evaluate it on the validation set. Next, use K-Means as a dimensionality reduction tool, and train a classifier on the reduced set. Search for the number of clusters that allows the classifier to get the best performance: what performance can you reach? What if you append the features from the reduced set to the original features (again, searching for the best number of clusters)?</p>
</li>
<li>
<p>Train a Gaussian Mixture model on the Olivetti faces dataset. To speed up the algorithm, you should probably reduce the dataset’s dimensionality (e.g., use PCA, preserving 99% of the variance). Use the model to generate some new faces (using the <code>sample()</code> method), and visualize them (if you used PCA, you will need to use its <code>inverse_transform()</code> method). Try to modify some images (e.g., rotate, flip, darken) and see if the model can detect the anomaly (i.e., compare the output of the <code>score_samples()</code> method for normal images and for anomalies).</p>
</li>
<li>
<p>Some dimensionality reduction techniques can also be used for anomaly detection. For example, take the Olivetti faces dataset and reduce it with PCA, preserving 99% of the variance. Then compute the reconstruction error for each image. Next, take some of the modified images you built in the previous exercise, and look at their reconstruction error: notice how much larger the reconstruction error is. If you plot the reconstructed image, you will see why: it tries to reconstruct a normal face.</p>
</li>

</ol>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46263520260616"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#idm46263520260616-marker" class="totri-footnote">1</a></sup> Stuart P. Lloyd, “Least Squares Quantization in PCM,” IEEE Transactions on Information Theory 28, no. 2 (March 1982): 129–137.</p><p data-type="footnote" id="idm46263520068088"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#idm46263520068088-marker" class="totri-footnote">2</a></sup> That’s because pointing out that the mean squared distance between the instances and their closest centroid can only go down at each step.</p><p data-type="footnote" id="idm46263519924344"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#idm46263519924344-marker" class="totri-footnote">3</a></sup> David Arthur and Sergei Vassilvitskii, “k-means+\+: The Advantages of Careful Seeding” in proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms 2007: 1027–1035.</p><p data-type="footnote" id="idm46263519845480"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#idm46263519845480-marker" class="totri-footnote">4</a></sup> Charles Elkan, “Using the Triangle Inequality to Accelerate k-Means,” in Proceedings of the Twentieth International Conference on Machine Learning  (Washington, DC, 2003).</p><p data-type="footnote" id="idm46263519844344"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#idm46263519844344-marker" class="totri-footnote">5</a></sup> The triangle inequality is AC ≤ AB + BC where A, B and C are three points, and AB, AC and BC are the distances between these points.</p><p data-type="footnote" id="idm46263519841000"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#idm46263519841000-marker" class="totri-footnote">6</a></sup> David Sculley, “Web-Scale K-Means Clustering,” <em>Proceedings of the 19th International Conference on World Wide Web</em> (Raleigh, North Carolina, April 26–30, 2010): 1177–1178.</p><p data-type="footnote" id="idm46263518236840"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#idm46263518236840-marker" class="totri-footnote">7</a></sup> Phi (ϕ or φ) is the 21st letter of the Greek alphabet.</p><p data-type="footnote" id="idm46263518185576"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#idm46263518185576-marker" class="totri-footnote">8</a></sup> Most of these notations are standard, but a few additional notations were taken from the Wikipedia article on <a href="https://en.wikipedia.org/wiki/Plate_notation">plate notation</a>.</p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-9" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders">
		
		<li class="copy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#">Add Highlight</a></li>
		<li class="add-note"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#">
			Add Note
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch08.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">8. Dimensionality Reduction</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part02.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">II. Neural Networks and Deep Learning</div>
        </a>
    
  
  </div>

</section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    



        
      </div>
      



  <footer class="pagefoot">
    <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#" class="icon-up" style="display: none;"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      <li class="full-support"><a href="https://www.oreilly.com/online-learning/support/">Support</a></li>
      <li><a href="https://www.oreilly.com/online-learning/apps.html">Get the App</a></li>
      
        <li><a href="https://learning.oreilly.com/accounts/logout/">Sign Out</a></li>
      
    </ul>
    <span class="copyright">© 2019 <a href="https://learning.oreilly.com/" target="_blank">Safari</a>.</span>
    <a href="https://learning.oreilly.com/terms/">Terms of Service</a> /
    <a href="https://learning.oreilly.com/membership-agreement/">Membership Agreement</a> /
    <a href="https://www.oreilly.com/privacy.html">Privacy Policy</a>
  </footer>

<script type="text/javascript">window.NREUM||(NREUM={});NREUM.info={"applicationID":"172641827,79672898,93931619","errorBeacon":"bam.nr-data.net","agent":"","applicationTime":451,"licenseKey":"510f1a6865","transactionName":"YgdaZ0NSW0cEB0RdWltNfkZfUEFdCgofXFBHDVYdR1pQQxZeRl1QQj1aWkU=","beacon":"bam.nr-data.net","queueTime":4}</script>


    
    <script src="./Chapter9_files/saved_resource" charset="utf-8"></script><script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","1732687426968531");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=1732687426968531&amp;ev=PageView&amp;noscript=1"></noscript><script type="text/javascript" id="">(function(){window.medalliaUserIdentifier=document.documentElement.dataset.userUuid;window.medalliaUserName=document.documentElement.dataset.username})();</script>
<script type="text/javascript" id="" src="./Chapter9_files/embed.js.download"></script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script>
<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("set","agent","tmgoogletagmanager","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>

<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>
<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.09556792590964802"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.5229133033930915" width="0" height="0" alt="" src="./Chapter9_files/0"></div>
    <script src="./Chapter9_files/saved_resource(1)" charset="utf-8"></script>
  

<script src="./Chapter9_files/saved_resource(2)" type="text/javascript"></script><script type="text/javascript" id="">window._pp=window._pp||[];if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/register/")_pp.targetUrl="/confirm/trial";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/subscribe/")_pp.targetUrl="/confirm/paid";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/signup/")_pp.targetUrl="/confirm/paid";_pp.siteId="2508";
_pp.siteUId="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79";_pp.orderValue="undefined";_pp.orderId="undefined";(function(){var ppjs=document.createElement("script");ppjs.type="text/javascript";ppjs.async=true;ppjs.src=("https:"==document.location.protocol?"https:":"http:")+"//cdn.pbbl.co/r/"+_pp.siteId+".js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(ppjs,s)})();</script><div class="annotator-notice"></div><div class="font-flyout" style="top: 201px; left: 1194px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch09.html#">Reset</a>
</div>
</div><script type="text/javascript" async="" src="./Chapter9_files/generic1566415868241.js.download" charset="UTF-8"></script><div style="display: none; visibility: hidden;"><script>(function(){if(null!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')&&void 0!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')){var a=!1;window.addEventListener("blur",function(){a&&dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"facebook",eventVal:0,nonInteraction:0})});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseover",function(){window.focus();
a=!0});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseout",function(){a=!1})}try{window.twttr=function(b,a,d){var c,e=b.getElementsByTagName(a)[0];if(!b.getElementById(d))return b=b.createElement(a),b.id=d,b.src="//platform.twitter.com/widgets.js",e.parentNode.insertBefore(b,e),window.twttr||(c={_e:[],ready:function(a){c._e.push(a)}})}(document,"script","twitter-wjs"),twttr.ready(function(a){a.events.bind("tweet",trackTwitter)})}catch(b){}})();
null!==document.querySelector(".IN-widget")&&void 0!==document.querySelector(".IN-widget")&&document.querySelector(".IN-widget").addEventListener("click",function(){dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"linkedin",eventVal:0,nonInteraction:0})});
function trackTwitter(a){a&&(a.target&&"IFRAME"==a.target.nodeName&&(opt_target=extractParamFromUri(a.target.src,"url")),dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"twitter",eventVal:0,nonInteraction:0}))}function extractParamFromUri(a,b){if(a){var c=new RegExp("[\\?\x26#]"+b+"\x3d([^\x26#]*)");c=c.exec(a);if(null!=c)return unescape(c[1])}};</script></div><span><div id="KampyleAnimationContainer" style="z-index: 2147483000; border: 0px; position: fixed; display: block; width: 0px; height: 0px;"></div></span><iframe scrolling="no" frameborder="0" allowtransparency="true" src="./Chapter9_files/widget_iframe.097c1f5038f9e8a0d62a39a892838d66.html" title="Twitter settings iframe" style="display: none;"></iframe><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_AMS, sans-serif;"></div></div></body></html>