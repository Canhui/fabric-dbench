<!DOCTYPE html>
<!-- saved from url=(0091)https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html -->
<html class=" js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/hands-on-machine-learning/9781492032632/part01.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="4626953" data-user-uuid="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79" data-username="17481074" data-account-type="B2B" data-activated-trial-date="" data-archive="9781492032632" data-publishers="O&#39;Reilly Media, Inc." data-htmlfile-name="part01.html" data-epub-title="Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781492032632"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><script type="text/javascript" async="" src="./Chapter7_files/cool-2.1.15.min.js.download"></script><script type="text/javascript" src="./Chapter7_files/510f1a6865"></script><script id="twitter-wjs" src="./Chapter7_files/widgets.js.download"></script><script src="./Chapter7_files/nr-1130.min.js.download"></script><script type="text/javascript" async="" src="./Chapter7_files/2508.js.download"></script><script async="" src="./Chapter7_files/fbevents.js.download"></script><script type="text/javascript" async="" src="./Chapter7_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter7_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter7_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter7_files/analytics.js.download"></script><script type="text/javascript" async="" src="./Chapter7_files/ec.js.download"></script><script type="text/javascript" async="" src="./Chapter7_files/bat.js.download"></script><script type="text/javascript" async="" src="./Chapter7_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter7_files/insight.min.js.download"></script><script type="text/javascript" async="" src="./Chapter7_files/f.txt"></script><script type="text/javascript" async="" src="./Chapter7_files/linkid.js.download"></script><script async="" src="./Chapter7_files/gtm.js.download"></script><script async="" src="./Chapter7_files/analytics.js.download"></script><script type="text/javascript">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if("function"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e("handle"),a=e(3),u=e(4),f=e("ee").get("tracer"),c=e("loader"),s=NREUM;"undefined"==typeof window.newrelic&&(newrelic=s);var p=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],d="api-",l=d+"ixn-";a(p,function(e,n){s[n]=o(d+n,!0,"api")}),s.addPageAction=o(d+"addPageAction",!0),s.setCurrentRouteName=o(d+"routeName",!0),n.exports=newrelic,s.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(e,n){var t={},r=this,o="function"==typeof n;return i(l+"tracer",[c.now(),e,t],r),function(){if(f.emit((o?"":"no-")+"fn-start",[c.now(),r,o],t),o)try{return n.apply(this,arguments)}catch(e){throw f.emit("fn-err",[arguments,this,e],t),e}finally{f.emit("fn-end",[c.now()],t)}}}};a("actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(e,n){m[n]=o(l+n)}),newrelic.noticeError=function(e,n){"string"==typeof e&&(e=new Error(e)),i("err",[e,c.now(),!1,n])}},{}],2:[function(e,n,t){function r(e,n){if(!o)return!1;if(e!==o)return!1;if(!n)return!0;if(!i)return!1;for(var t=i.split("."),r=n.split("."),a=0;a<r.length;a++)if(r[a]!==t[a])return!1;return!0}var o=null,i=null,a=/Version\/(\S+)\s+Safari/;if(navigator.userAgent){var u=navigator.userAgent,f=u.match(a);f&&u.indexOf("Chrome")===-1&&u.indexOf("Chromium")===-1&&(o="Safari",i=f[1])}n.exports={agent:o,version:i,match:r}},{}],3:[function(e,n,t){function r(e,n){var t=[],r="",i=0;for(r in e)o.call(e,r)&&(t[i]=n(r,e[r]),i+=1);return t}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],4:[function(e,n,t){function r(e,n,t){n||(n=0),"undefined"==typeof t&&(t=e?e.length:0);for(var r=-1,o=t-n||0,i=Array(o<0?0:o);++r<o;)i[r]=e[n+r];return i}n.exports=r},{}],5:[function(e,n,t){n.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(e,n,t){function r(){}function o(e){function n(e){return e&&e instanceof r?e:e?f(e,u,i):i()}function t(t,r,o,i){if(!d.aborted||i){e&&e(t,r,o);for(var a=n(o),u=v(t),f=u.length,c=0;c<f;c++)u[c].apply(a,r);var p=s[y[t]];return p&&p.push([b,t,r,a]),a}}function l(e,n){h[e]=v(e).concat(n)}function m(e,n){var t=h[e];if(t)for(var r=0;r<t.length;r++)t[r]===n&&t.splice(r,1)}function v(e){return h[e]||[]}function g(e){return p[e]=p[e]||o(t)}function w(e,n){c(e,function(e,t){n=n||"feature",y[t]=n,n in s||(s[n]=[])})}var h={},y={},b={on:l,addEventListener:l,removeEventListener:m,emit:t,get:g,listeners:v,context:n,buffer:w,abort:a,aborted:!1};return b}function i(){return new r}function a(){(s.api||s.feature)&&(d.aborted=!0,s=d.backlog={})}var u="nr@context",f=e("gos"),c=e(3),s={},p={},d=n.exports=o();d.backlog=s},{}],gos:[function(e,n,t){function r(e,n,t){if(o.call(e,n))return e[n];var r=t();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,n,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return e[n]=r,r}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],handle:[function(e,n,t){function r(e,n,t,r){o.buffer([e],r),o.emit(e,n,t)}var o=e("ee").get("handle");n.exports=r,r.ee=o},{}],id:[function(e,n,t){function r(e){var n=typeof e;return!e||"object"!==n&&"function"!==n?-1:e===window?0:a(e,i,function(){return o++})}var o=1,i="nr@id",a=e("gos");n.exports=r},{}],loader:[function(e,n,t){function r(){if(!E++){var e=x.info=NREUM.info,n=l.getElementsByTagName("script")[0];if(setTimeout(s.abort,3e4),!(e&&e.licenseKey&&e.applicationID&&n))return s.abort();c(y,function(n,t){e[n]||(e[n]=t)}),f("mark",["onload",a()+x.offset],null,"api");var t=l.createElement("script");t.src="https://"+e.agent,n.parentNode.insertBefore(t,n)}}function o(){"complete"===l.readyState&&i()}function i(){f("mark",["domContent",a()+x.offset],null,"api")}function a(){return O.exists&&performance.now?Math.round(performance.now()):(u=Math.max((new Date).getTime(),u))-x.offset}var u=(new Date).getTime(),f=e("handle"),c=e(3),s=e("ee"),p=e(2),d=window,l=d.document,m="addEventListener",v="attachEvent",g=d.XMLHttpRequest,w=g&&g.prototype;NREUM.o={ST:setTimeout,SI:d.setImmediate,CT:clearTimeout,XHR:g,REQ:d.Request,EV:d.Event,PR:d.Promise,MO:d.MutationObserver};var h=""+location,y={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-1130.min.js"},b=g&&w&&w[m]&&!/CriOS/.test(navigator.userAgent),x=n.exports={offset:u,now:a,origin:h,features:{},xhrWrappable:b,userAgent:p};e(1),l[m]?(l[m]("DOMContentLoaded",i,!1),d[m]("load",r,!1)):(l[v]("onreadystatechange",o),d[v]("onload",r)),f("mark",["firstbyte",u],null,"api");var E=0,O=e(5)},{}]},{},["loader"]);</script><link rel="apple-touch-icon" href="https://learning.oreilly.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://learning.oreilly.com/favicon.ico" type="image/x-icon"><link href="./Chapter7_files/css" rel="stylesheet" type="text/css"><title>7. Ensemble Learning and Random Forests - Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition</title><link rel="stylesheet" href="./Chapter7_files/output.68851547a55f.css" type="text/css"><link rel="stylesheet" type="text/css" href="./Chapter7_files/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="./Chapter7_files/font-awesome.min.css"><style type="text/css" title="ibis-book">@charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000 !important;padding-top:10px !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:2em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content p[data-type="footnote"]{font-size:90% !important;line-height:1.2em !important;margin-left:2.5em !important;text-indent:-2.3em !important}#sbo-rt-content p[data-type="footnote"] sup{display:inline-block !important;position:static !important;width:2em !important;text-align:right !important;font-size:100% !important;padding-right:.5em !important}#sbo-rt-content p[data-type="footnote"] a[href$="-marker"]{font-family:sans-serif !important;font-size:90% !important;color:#8e0012 !important}#sbo-rt-content p[data-type="footnote"] a[data-type="xref"]{margin:0 !important;padding:0 !important;text-indent:0 !important}#sbo-rt-content a[data-type="noteref"]{font-family:sans-serif !important;color:#8e0012;margin-left:0;padding-left:0}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dd{margin-left:1.5em !important;margin-bottom:.25em}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul>li,#sbo-rt-content ol ul,#sbo-rt-content ol ul>li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul>li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul>li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul>li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol>li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol>li,#sbo-rt-content ul ol,#sbo-rt-content ul ol>li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol>li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol>li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:middle;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content table.border tbody>tr:last-child>td{border-bottom:transparent}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:1em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content .width-10{width:10vw !important}#sbo-rt-content .width-20{width:20vw !important}#sbo-rt-content .width-30{width:30vw !important}#sbo-rt-content .width-40{width:40vw !important}#sbo-rt-content .width-50{width:50vw !important}#sbo-rt-content .width-60{width:60vw !important}#sbo-rt-content .width-70{width:70vw !important}#sbo-rt-content .width-80{width:80vw !important}#sbo-rt-content .width-90{width:90vw !important}#sbo-rt-content .width-full,#sbo-rt-content .width-100{width:100vw !important}#sbo-rt-content div[data-type="equation"].fifty-percent img{width:50%}</style><script> // <![CDATA[
    var g = {
      position_cache: {
        
          "chapter": "/api/v1/book/9781492032632/chapter/part01.html",
          "book_id": "9781492032632",
          "chapter_uri": "part01.html",
          "position": 100.0,
          "user_uuid": "d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79",
          "next_chapter_uri": "/library/view/hands-on-machine-learning/9781492032632/ch01.html"
        
      },
      title: "Hands\u002Don Machine Learning with Scikit\u002DLearn, Keras, and TensorFlow, 2nd Edition",
      author_list: "Aurélien Géron",
      format: "book",
      source: "application/epub+zip",
      is_system_book: true,
      is_public: false,
      loaded_from_server: true,
      allow_scripts: false,
      has_mathml: true
    };
    // ]]></script><script src="./Chapter7_files/modernizr.8e35451ddb64.js.download"></script><script>
    
      

      
        
          window.PUBLIC_ANNOTATIONS = true;
        
      

      window.MOBILE_PUBLIC_ANNOTATIONS = false;

    

    
      window.PRIVACY_CONTROL_OVERRIDE = false;
    

      window.PRIVACY_CONTROL_SWITCH = true;

      window.PUBLISHER_PAGES = true;

      window.SBO = {
        "constants": {
          "SITB_ENDPOINT": "/api/v2/sitb/",
          "SEARCH_SELECT_ENDPOINT": "https://learning.oreilly.com/api/v2/search/select/",
          "ENABLE_ONLINE_TRAINING": false
        }
      };
  </script><link rel="canonical" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta name="description" content=" Part I. The Fundamentals of Machine Learning "><meta property="og:title" content="I. The Fundamentals of Machine Learning"><meta itemprop="isPartOf" content="/library/view/hands-on-machine-learning/9781492032632/"><meta itemprop="name" content="I. The Fundamentals of Machine Learning"><meta property="og:url" itemprop="url" content="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://learning.oreilly.com/library/cover/9781492032632/"><meta property="og:description" itemprop="description" content=" Part I. The Fundamentals of Machine Learning "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O&#39;Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781492032649"><meta property="og:book:author" itemprop="author" content="Aurélien Géron"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: &lt;%= font_size %&gt; !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: &lt;%= font_family %&gt; !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: &lt;%= column_width %&gt;% !important; margin: 0 auto !important; }"></style><noscript><meta http-equiv="refresh" content="0; url=/library/no-js/" /></noscript><script type="text/javascript">
    (function(i,s,o,g,r,a,m) {
      i['GoogleAnalyticsObject']=r;
      i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();
      a=s.createElement(o),m=s.getElementsByTagName(o)[0];
      a.async=1;
      a.src=g;
      m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    var matches = document.cookie.match(/BrowserCookie\s*=\s*([a-f0-9\-]{36})/),
      user_uuid = null;

    if (matches && matches.length === 2) {
      user_uuid = matches[1];
    }

  
    ga('create', 'UA-39299553-7', {'userId': 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79' });
  

  
    
      ga('set', 'dimension1', 'B2B');
    
  

  ga('set', 'dimension6', user_uuid);

  
    ga('set', 'dimension2', 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79');
    
      ga('set', 'dimension7', '0012M0000229keZQAQ');
    
  

  

  

  //enable enhanced link tracking
  ga('require', 'linkid', 'linkid.js');

  // reading interface will track pageviews itself
  if (document.location.pathname.indexOf("/library/view") !== 0) {
    ga('send', 'pageview');
  }
  </script><script>
    var dataLayer = window.dataLayer || [];

    
      window.medalliaVsgUserIdentifier = 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79';
      dataLayer.push({userIdentifier: 'd59baa21-c0cd-4fcf-9c68-a2b8d4f52a79'});
      dataLayer.push({loggedIn: 'yes'});

      
        window.medalliaVsgAccountIdentifier = '21bed0a7-6b7b-470c-8fa0-40a52db0b491';
        
        dataLayer.push({orgID: '21bed0a7-6b7b-470c-8fa0-40a52db0b491'});
        

        window.medalliaVsgIsIndividual = false;
        
          
          dataLayer.push({learningAccountType: 'enterprise'});
          
        

        
          dataLayer.push({learningPaidAccount: 'yes'});
        
      
    

    (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-5P4V6Z');
    (function () {
      var VERSION = 'V1.1';
      var AUTHOR = 'Awwad';
      if (!window.GtmHelper)
        window.GtmHelper = function () {
          var instance = this;
          var loc = document.location;
          this.version = VERSION;
          this.author = AUTHOR;
          this.readCookie = function (name) {
            var nameEQ = name + "=";
            var ca = document.cookie.split(';');
            for (var i = 0; i < ca.length; i++) {
              var c = ca[i];
              while (c.charAt(0) == ' ') c = c.substring(1, c.length);
              if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length, c.length);
            }
            return null;
          };
          this.createCookie = function (name, value, days, cookieDomain) {
            var domain = "";
            var expires = "";

            if (days) {
              var date = new Date();
              date.setTime(date.getTime() + Math.ceil(days * 24 * 60 * 60 * 1000));
              var expires = " expires=" + date.toGMTString() + ";";
            }

            if (typeof (cookieDomain) != 'undefined')
              domain = " domain=" + cookieDomain + "; ";

            document.cookie = name + "=" + value + ";" + expires + domain + "path=/";
          };

          this.isDuplicated = function (currentTransactionId) {
            // the previous transaction id:
            var previousTransIdValue = this.readCookie("previousTransId");

            if (currentTransactionId === previousTransIdValue) {
              return true; // Duplication
            } else {
              return false;
            }
          };
        }
    })()
  </script><script defer="" src="./Chapter7_files/vendor.a48a756c5182.js.download"></script><script defer="" src="./Chapter7_files/reader.f2a0c6bd2fee.js.download"></script><script src="./Chapter7_files/f(1).txt"></script><script src="./Chapter7_files/f(2).txt"></script><script src="./Chapter7_files/f(3).txt"></script><script src="./Chapter7_files/f(4).txt"></script><script async="" src="./Chapter7_files/MathJax.js.download"></script><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 2147483020;
}
.annotator-filter {
  z-index: 2147483010;
}</style><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
</style><script async="true" type="text/javascript" src="./Chapter7_files/roundtrip.js.download"></script><style type="text/css" id="kampyleStyle">.noOutline{outline: none !important;}.wcagOutline:focus{outline: 1px dashed #595959 !important;outline-offset: 2px !important;transition: none !important;}</style><script async="true" type="text/javascript" src="./Chapter7_files/roundtrip.js.download"></script><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Main; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-bold; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Main-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Math-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Caligraphic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size1; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size2; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size3; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf?V=2.7.1') format('opentype')}
@font-face {font-family: MathJax_Size4; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf?V=2.7.1') format('opentype')}
</style><style type="text/css">@font-face {font-family: MathJax_Math-bold-italic; src: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff?V=2.7.1') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf?V=2.7.1') format('opentype')}
</style></head>


<body class="reading sidenav  scalefonts library nav-collapsed"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

    
  <noscript> 
    <iframe src="//www.googletagmanager.com/ns.html?id=GTM-5P4V6Z"
            height="0" width="0"
            style="display:none;visibility:hidden">
    </iframe>
  </noscript>



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#container" class="skip">Skip to content</a><header class="topbar t-topbar"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://learning.oreilly.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M11.738 14H9.254v-3.676a.617.617 0 0 0-.621-.613H7.39a.617.617 0 0 0-.62.613V14H4.284a.617.617 0 0 1-.622-.613V10.22c0-.327.132-.64.367-.87l3.547-3.493a.627.627 0 0 1 .875 0l3.54 3.499c.234.229.366.54.367.864v3.167a.617.617 0 0 1-.62.613zM7.57 2.181a.625.625 0 0 1 .882 0l5.77 5.692-.93.92-5.28-5.209-5.28 5.208-.932-.919 5.77-5.692z"></path></svg><span>Safari Home</span></a></li><li><a href="https://learning.oreilly.com/resource-centers/" class="t-resource-centers-nav l0 nav-icn"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="16px" height="16px" viewBox="0 0 16 16" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="Topic-Page-Design" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="Heron-Button" transform="translate(-20.000000, -78.000000)" fill="#4A3A30"><g id="Group-9" transform="translate(20.000000, 78.000000)"><rect id="Rectangle" x="9.6" y="0" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="9.6" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect><rect id="Rectangle" x="0" y="9.6" width="6.4" height="6.4" rx="0.503118"></rect></g></g></g></svg><span>Resource Centers</span></a></li><li><a href="https://learning.oreilly.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://learning.oreilly.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://learning.oreilly.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://learning.oreilly.com/learning-paths/" class="l1 nav-icn t-learningpaths-nav js-toggle-menu-item"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="32px" height="32px" viewBox="0 0 32 32" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 52.5 (67469) - http://www.bohemiancoding.com/sketch --><title>Mask</title><desc>Created with Sketch.</desc><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><path d="M0,16.0214227 C0,15.0387209 0.796453294,14.2411658 1.77779753,14.2411658 C2.75914177,14.2411658 3.55559506,15.0387209 3.55559506,16.0214227 C3.55559506,17.0041246 2.75914177,17.8016797 1.77779753,17.8016797 C0.796453294,17.8016797 0,17.0041246 0,16.0214227 Z M9.77788642,5.22914885 C8.9280992,5.72049977 7.84008711,5.42853763 7.34941499,4.57757479 C6.85874287,3.72661195 7.15030167,2.63709467 8.00008889,2.14574375 C8.84987611,1.65439282 9.9378882,1.94635496 10.4285603,2.7973178 C10.9192324,3.64828064 10.6276736,4.73779792 9.77788642,5.22914885 Z M4.57213969,7.35869225 C5.42192691,7.85004318 5.71348571,8.93956046 5.22281359,9.79052329 C4.73214147,10.6414861 3.64412938,10.9334483 2.79434216,10.4420974 C1.94455494,9.95074642 1.65299614,8.86122915 2.14366826,8.01026631 C2.63434038,7.15930347 3.72235247,6.86734132 4.57213969,7.35869225 Z M2.79434216,21.6007481 C3.64412938,21.1093972 4.73214147,21.4013594 5.22281359,22.2523222 C5.71348571,23.103285 5.42192691,24.1928023 4.57213969,24.6841532 C3.72235247,25.1755042 2.63434038,24.883542 2.14366826,24.0325792 C1.65299614,23.1816163 1.94455494,22.0920991 2.79434216,21.6007481 Z M7.34941499,27.4652707 C7.84008711,26.6143079 8.9280992,26.3223457 9.77788642,26.8136966 C10.6276736,27.3050476 10.9192324,28.3945649 10.4285603,29.2455277 C9.9378882,30.0964905 8.84987611,30.3884527 8.00008889,29.8971017 C7.15030167,29.4057508 6.85874287,28.3162335 7.34941499,27.4652707 Z M18.7118524,11.3165596 C21.3074367,12.8173162 22.1963355,16.1392758 20.6976522,18.738451 C19.1989689,21.3358459 15.8815987,22.2259744 13.2860143,20.726998 C10.6922077,19.2262414 9.80330893,15.9042818 11.3002144,13.3051066 C12.7988978,10.7059314 16.116268,9.81580294 18.7118524,11.3165596 Z M26.7821642,27.8093944 L30.1315348,31.1633985 C30.3982044,31.4304371 30.2097579,31.8844026 29.8346426,31.8844026 L21.5945511,31.8844026 C21.1287681,31.8844026 20.751875,31.5069881 20.751875,31.0405608 L20.751875,22.7890697 C20.751875,22.4134355 21.2052134,22.2247282 21.4701052,22.4899865 L24.2843587,25.3081333 C26.8337204,23.0240636 28.4444049,19.7092251 28.4444049,16.0223129 C28.4444049,9.15052091 22.8621207,3.56051397 15.9998222,3.56051397 L15.9998222,0 C24.8230314,0 32,7.18689745 32,16.0223129 C32,20.6919269 29.9750886,24.8790914 26.7821642,27.8093944 Z" id="Mask" fill="#8B889A"></path></g></svg><span>Learning Paths</span></a></li><li class="nav-highlights"><a href="https://learning.oreilly.com/u/d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" fill="#4A3C31"><path d="M13.325 18.071H8.036c0-6.736 4.324-10.925 14.464-12.477V0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35c5.142 0 9.175-3.515 9.175-8.816 0-4.628-2.367-7.293-6.253-8.113zm27.5 0h-5.26c0-6.736 4.295-10.925 14.435-12.477V0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35c5.113 0 9.146-3.515 9.146-8.816 0-4.628-2.338-7.293-6.253-8.113z" fill-rule="evenodd"></path></svg><span>Highlights</span></a></li><li><a href="https://learning.oreilly.com/u/preferences/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l1 no-icon">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://learning.oreilly.com/u/preferences/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l2">Support</a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9781492032632/chapter/part01.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"><div data-reactroot="" class="menu-dropdown-wrapper js-menu-dropdown-wrapper align-right"><img class="hidden" src="./Chapter7_files/ajax-transp.gif" alt="loading spinner"><div class="menu-control"><div class="control "><div class="js-playlists-menu"><button class="js-playlist-icon"><svg class="icon-add-to-playlist-sml" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill-rule="nonzero" fill="#000000"><g transform="translate(-1.000000, 0.000000)"><rect x="5" y="0" width="12" height="2"></rect><title>Playlists</title><path d="M4.5,14 C6.43299662,14 8,12.4329966 8,10.5 C8,8.56700338 6.43299662,7 4.5,7 C2.56700338,7 1,8.56700338 1,10.5 C1,12.4329966 2.56700338,14 4.5,14 Z M2.5,10 L4,10 L4,8.5 L5,8.5 L5,10 L6.5,10 L6.5,11 L5,11 L5,12.5 L4,12.5 L4,11 L2.5,11 L2.5,10 Z"></path><circle cx="2" cy="5" r="1"></circle><circle cx="1.94117647" cy="1" r="1"></circle><rect x="5" y="4" width="12" height="2"></rect><rect x="9" y="8" width="8" height="2"></rect><rect x="9" y="12" width="8" height="2"></rect></g></g></g></svg><div class="js-playlist-addto-label">Add&nbsp;To</div></button></div></div></div></div></div></div></li><li class="js-font-control-panel font-control-activator"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html&amp;text=Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%20I.%20The%20Fundamentals%20of%20Machine%20Learning&amp;body=https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/part01.html%0D%0Afrom%20Hands-on%20Machine%20Learning%20with%20Scikit-Learn%2C%20Keras%2C%20and%20TensorFlow%2C%202nd%20Edition%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    
    <section role="document">
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch06.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">6. Decision Trees</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch08.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">8. Dimensionality Reduction</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content"><div class="annotator-wrapper"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 7. Ensemble Learning and Random Forests"><div class="chapter" id="ensembles_chapter">
<h1><span class="label">Chapter 7. </span>Ensemble Learning and Random Forests</h1>


<p>Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert’s answer. This is called the <em>wisdom of the crowd</em>. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of predictors is called an <em>ensemble</em>; thus, this technique is called <em>Ensemble Learning</em>, and an Ensemble Learning algorithm is called an <em>Ensemble method</em>.</p>

<p>As an example of the Ensemble Method, you can train a group of Decision Tree classifiers, each on a different random subset of the training set. To make predictions, you obtain the predictions of all individual trees, then predict the class that gets the most votes (see the last exercise in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch06.html#trees_chapter">Chapter&nbsp;6</a>). Such an ensemble of Decision Trees is called a <em>Random Forest</em>, and despite its simplicity, this is one of the most powerful Machine Learning algorithms available today.</p>

<p>Moreover, as we discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch02.html#project_chapter">Chapter&nbsp;2</a>, you will often use Ensemble methods near the end of a project, once you have already built a few good predictors, to combine them into an even better predictor. In fact, the winning solutions in Machine Learning competitions often involve several Ensemble methods (most famously in the <a href="http://netflixprize.com/">Netflix Prize competition</a>).</p>

<p>In this chapter we will discuss the most popular Ensemble methods, including <em>bagging</em>, <em>boosting</em>, and <em>stacking</em>. We will also explore Random Forests.</p>






<section data-type="sect1" data-pdf-bookmark="Voting Classifiers"><div class="sect1" id="idm46263523226488">
<h1>Voting Classifiers</h1>

<p>Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#voting_classifier_training_diagram">Figure&nbsp;7-1</a>).</p>

<figure><div id="voting_classifier_training_diagram" class="figure">
<img src="./Chapter7_files/mls2_0701.png" alt="mls2 0701" width="1441" height="618" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0701.png">
<h6><span class="label">Figure 7-1. </span>Training diverse classifiers</h6>
</div></figure>

<p>A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classifier is called a <em>hard voting</em> classifier (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#voting_classifier_prediction_diagram">Figure&nbsp;7-2</a>).</p>

<figure><div id="voting_classifier_prediction_diagram" class="figure">
<img src="./Chapter7_files/mls2_0702.png" alt="mls2 0702" width="1441" height="767" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0702.png">
<h6><span class="label">Figure 7-2. </span>Hard voting classifier predictions</h6>
</div></figure>

<p>Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the best classifier in the ensemble. In fact, even if each classifier is a <em>weak learner</em> (meaning it does only slightly better than random guessing), the ensemble can still be a <em>strong learner</em> (achieving high accuracy), provided there are a sufficient number of weak learners and they are sufficiently diverse.</p>

<p>How is this possible? The following analogy can help shed some light on this mystery. Suppose you have a slightly biased coin that has a 51% chance of coming up heads and 49% chance of coming up tails. If you toss it 1,000 times, you will generally get more or less 510 heads and 490 tails, and hence a majority of heads. If you do the math, you will find that the probability of obtaining a majority of heads after 1,000 tosses is close to 75%. The more you toss the coin, the higher the probability (e.g., with 10,000 tosses, the probability climbs over 97%). This is due to the <em>law of large numbers</em>: as you keep tossing the coin, the ratio of heads gets closer and closer to the probability of heads (51%). <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#law_of_large_numbers_plot">Figure&nbsp;7-3</a> shows 10 series of biased coin tosses. You can see that as the number of tosses increases, the ratio of heads approaches 51%. Eventually all 10 series end up so close to 51% that they are consistently above 50%.</p>

<figure><div id="law_of_large_numbers_plot" class="figure">
<img src="./Chapter7_files/mls2_0703.png" alt="mls2 0703" width="1441" height="579" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0703.png">
<h6><span class="label">Figure 7-3. </span>The law of large numbers</h6>
</div></figure>

<p>Similarly, suppose you build an ensemble containing 1,000 classifiers that are individually correct only 51% of the time (barely better than random guessing). If you predict the majority voted class, you can hope for up to 75% accuracy! However, this is only true if all classifiers are perfectly independent, making uncorrelated errors, which is clearly not the case because they are trained on the same data. They are likely to make the same types of errors, so there will be many majority votes for the wrong class, reducing the ensemble’s accuracy.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy.</p>
</div>

<p>The following code creates and trains a voting classifier in Scikit-Learn, composed of three diverse classifiers (the training set is the moons dataset, introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch05.html#svm_chapter">Chapter&nbsp;5</a>):</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">VotingClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVC</code>

<code class="n">log_clf</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>
<code class="n">rnd_clf</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">()</code>
<code class="n">svm_clf</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">()</code>

<code class="n">voting_clf</code> <code class="o">=</code> <code class="n">VotingClassifier</code><code class="p">(</code>
    <code class="n">estimators</code><code class="o">=</code><code class="p">[(</code><code class="s1">'lr'</code><code class="p">,</code> <code class="n">log_clf</code><code class="p">),</code> <code class="p">(</code><code class="s1">'rf'</code><code class="p">,</code> <code class="n">rnd_clf</code><code class="p">),</code> <code class="p">(</code><code class="s1">'svc'</code><code class="p">,</code> <code class="n">svm_clf</code><code class="p">)],</code>
    <code class="n">voting</code><code class="o">=</code><code class="s1">'hard'</code><code class="p">)</code>
<code class="n">voting_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>

<p>Let’s look at each classifier’s accuracy on the test set:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">accuracy_score</code>
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">clf</code> <code class="ow">in</code> <code class="p">(</code><code class="n">log_clf</code><code class="p">,</code> <code class="n">rnd_clf</code><code class="p">,</code> <code class="n">svm_clf</code><code class="p">,</code> <code class="n">voting_clf</code><code class="p">):</code>
<code class="gp">... </code>    <code class="n">clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="gp">... </code>    <code class="n">y_pred</code> <code class="o">=</code> <code class="n">clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>
<code class="gp">... </code>    <code class="k">print</code><code class="p">(</code><code class="n">clf</code><code class="o">.</code><code class="n">__class__</code><code class="o">.</code><code class="n">__name__</code><code class="p">,</code> <code class="n">accuracy_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">))</code>
<code class="gp">...</code>
<code class="go">LogisticRegression 0.864</code>
<code class="go">RandomForestClassifier 0.896</code>
<code class="go">SVC 0.888</code>
<code class="go">VotingClassifier 0.904</code></pre>

<p>There you have it! The voting classifier slightly outperforms all the individual classifiers.</p>

<p>If all classifiers are able to estimate class probabilities (i.e., they have a <code>predict_proba()</code> method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called <em>soft voting</em>. It often achieves higher performance than hard voting because it gives more weight to highly confident votes. All you need to do is replace <code>voting="hard"</code> with <code>voting="soft"</code> and ensure that all classifiers can estimate class probabilities. This is not the case of the <code>SVC</code> class by default, so you need to set its <code>probability</code> hyperparameter to <code>True</code> (this will make the <code>SVC</code> class use cross-validation to estimate class probabilities, slowing down training, and it will add a <code>predict_proba()</code> method). If you modify the preceding code to use soft voting, you will find that the voting classifier achieves over 91.2% accuracy!</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Bagging and Pasting"><div class="sect1" id="idm46263523225864">
<h1>Bagging and Pasting</h1>

<p>One way to get a diverse set of classifiers is to use very different training algorithms, as just discussed. Another approach is to use the same training algorithm for every predictor and train them on different random subsets of the training set. When sampling is performed <em>with</em> replacement, this method is called <a href="https://homl.info/20"><em>bagging</em></a><sup><a data-type="noteref" id="idm46263523035048-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263523035048" class="totri-footnote">1</a></sup> (short for <em>bootstrap aggregating</em><sup><a data-type="noteref" id="idm46263523033688-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263523033688" class="totri-footnote">2</a></sup>). When sampling is performed <em>without</em> replacement, it is called <a href="https://homl.info/21"><em>pasting</em></a>.<sup><a data-type="noteref" id="idm46263523031320-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263523031320" class="totri-footnote">3</a></sup></p>

<p>In other words, both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor. This sampling and training process is represented in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#bagging_training_diagram">Figure&nbsp;7-4</a>.</p>

<figure><div id="bagging_training_diagram" class="figure">
<img src="./Chapter7_files/mls2_0704.png" alt="mls2 0704" width="1440" height="727" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0704.png">
<h6><span class="label">Figure 7-4. </span>Bagging and pasting involves training several predictors on different random samples of the training set</h6>
</div></figure>

<p>Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the <em>statistical mode</em> (i.e., the most frequent prediction, just like a hard voting classifier) for classification, or the average for regression. Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias and variance.<sup><a data-type="noteref" id="idm46263523025816-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263523025816" class="totri-footnote">4</a></sup> Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set.</p>

<p>As you can see in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#bagging_training_diagram">Figure&nbsp;7-4</a>, predictors can all be trained in parallel, via different CPU cores or even different servers. Similarly, predictions can be made in parallel. This is one of the reasons bagging and pasting are such popular methods: they scale very well.</p>








<section data-type="sect2" data-pdf-bookmark="Bagging and Pasting in Scikit-Learn"><div class="sect2" id="idm46263523022440">
<h2>Bagging and Pasting in Scikit-Learn</h2>

<p>Scikit-Learn offers a simple API for both bagging and pasting with the <code>BaggingClassifier</code> class (or <code>BaggingRegressor</code> for regression). The following code trains an ensemble of 500 Decision Tree classifiers,<sup><a data-type="noteref" id="idm46263523019736-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263523019736" class="totri-footnote">5</a></sup> each trained on 100 training instances randomly sampled from the training set with replacement (this is an example of bagging, but if you want to use pasting instead, just set <code>bootstrap=False</code>). The <code>n_jobs</code> parameter tells Scikit-Learn the number of CPU cores to use for training and predictions (<span class="keep-together">–1</span> tells Scikit-Learn to use all available cores):</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">BaggingClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeClassifier</code>

<code class="n">bag_clf</code> <code class="o">=</code> <code class="n">BaggingClassifier</code><code class="p">(</code>
    <code class="n">DecisionTreeClassifier</code><code class="p">(),</code> <code class="n">n_estimators</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code>
    <code class="n">max_samples</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">bootstrap</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>
<code class="n">bag_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">bag_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The <code>BaggingClassifier</code> automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities (i.e., if it has a <code>predict_proba()</code> method), which is the case with Decision Trees classifiers.</p>
</div>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#decision_tree_without_and_with_bagging_plot">Figure&nbsp;7-5</a> compares the decision boundary of a single Decision Tree with the decision boundary of a bagging ensemble of 500 trees (from the preceding code), both trained on the moons dataset. As you can see, the ensemble’s predictions will likely generalize much better than the single Decision Tree’s predictions: the ensemble has a comparable bias but a smaller variance (it makes roughly the same number of errors on the training set, but the decision boundary is less irregular).</p>

<figure><div id="decision_tree_without_and_with_bagging_plot" class="figure">
<img src="./Chapter7_files/mls2_0705.png" alt="mls2 0705" width="1441" height="489" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0705.png">
<h6><span class="label">Figure 7-5. </span>A single Decision Tree (left) versus a bagging ensemble of 500 trees (right)</h6>
</div></figure>

<p>Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting, but the extra diversity also means that predictors end up being less correlated, so the ensemble’s variance is reduced. Overall, bagging often results in better models, which explains why it is generally preferred. However, if you have spare time and CPU power, you can use cross-validation to evaluate both bagging and pasting and select the one that works best.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Out-of-Bag Evaluation"><div class="sect2" id="idm46263522917112">
<h2>Out-of-Bag Evaluation</h2>

<p>With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. By default a <code>BaggingClassifier</code> samples <em>m</em> training instances with replacement (<code>bootstrap=True</code>), where <em>m</em> is the size of the training set. This means that only about 63% of the training instances are sampled on average for each predictor.<sup><a data-type="noteref" id="idm46263522913128-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522913128" class="totri-footnote">6</a></sup> The remaining 37% of the training instances that are not sampled are called <em>out-of-bag</em> (oob) instances. Note that they are not the same 37% for all predictors.</p>

<p>Since a predictor never sees the oob instances during training, it can be evaluated on these instances, without the need for a separate validation set. You can evaluate the ensemble itself by averaging out the oob evaluations of each predictor.</p>

<p>In Scikit-Learn, you can set <code>oob_score=True</code> when creating a <code>BaggingClassifier</code> to request an automatic oob evaluation after training. The following code demonstrates this. The resulting evaluation score is available through the <code>oob_score_</code> variable:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">bag_clf</code> <code class="o">=</code> <code class="n">BaggingClassifier</code><code class="p">(</code>
<code class="gp">... </code>    <code class="n">DecisionTreeClassifier</code><code class="p">(),</code> <code class="n">n_estimators</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code>
<code class="gp">... </code>    <code class="n">bootstrap</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">,</code> <code class="n">oob_score</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="gp">...</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bag_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bag_clf</code><code class="o">.</code><code class="n">oob_score_</code>
<code class="go">0.90133333333333332</code></pre>

<p>According to this oob evaluation, this <code>BaggingClassifier</code> is likely to achieve about 90.1% accuracy on the test set. Let’s verify this:</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">accuracy_score</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code> <code class="o">=</code> <code class="n">bag_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">)</code>
<code class="go">0.91200000000000003</code></pre>

<p>We get 91.2% accuracy on the test set—close enough!</p>

<p>The oob decision function for each training instance is also available through the <code>oob_decision_function_</code> variable. In this case (since the base estimator has a <code>predict_proba()</code> method), the decision function returns the class probabilities for each training instance. For example, the oob evaluation estimates that the first training instance has a 68.25% probability of belonging to the positive class (and 31.75% of belonging to the negative class):</p>

<pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">bag_clf</code><code class="o">.</code><code class="n">oob_decision_function_</code>
<code class="go">array([[0.31746032, 0.68253968],</code>
<code class="go">       [0.34117647, 0.65882353],</code>
<code class="go">       [1.        , 0.        ],</code>
<code class="go">       ...</code>
<code class="go">       [1.        , 0.        ],</code>
<code class="go">       [0.03108808, 0.96891192],</code>
<code class="go">       [0.57291667, 0.42708333]])</code></pre>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Random Patches and Random Subspaces"><div class="sect1" id="idm46263522916616">
<h1>Random Patches and Random Subspaces</h1>

<p>The <code>BaggingClassifier</code> class supports sampling the features as well. Sampling the features is controlled by two hyperparameters: <code>max_features</code> and <code>bootstrap_features</code>. They work the same way as <code>max_samples</code> and <code>bootstrap</code>, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features.</p>

<p>This technique is particularly useful when you are dealing with high-dimensional inputs (such as images). Sampling both training instances and features is called the <a href="https://homl.info/22"><em>Random Patches</em> method</a>.<sup><a data-type="noteref" id="idm46263522820664-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522820664" class="totri-footnote">7</a></sup> Keeping all training instances (i.e., <code>bootstrap=False</code> and <code>max_samples=1.0</code>) but sampling features (i.e., <code>bootstrap_features=True</code> and/or <code>max_features</code> smaller than 1.0) is called the <a href="https://homl.info/23"><em>Random Subspaces</em> method</a>.<sup><a data-type="noteref" id="idm46263522745080-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522745080" class="totri-footnote">8</a></sup></p>

<p>Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Random Forests"><div class="sect1" id="idm46263522742232">
<h1>Random Forests</h1>

<p>As   we have discussed, a <a href="https://homl.info/24">Random Forest</a><sup><a data-type="noteref" id="idm46263522739912-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522739912" class="totri-footnote">9</a></sup> is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with <code>max_samples</code> set to the size of the training set. Instead of building a <code>BaggingClassifier</code> and passing it a <code>DecisionTreeClassifier</code>, you can instead use the <code>RandomForestClassifier</code> class, which is more convenient and optimized for Decision Trees<sup><a data-type="noteref" id="idm46263522737000-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522737000">10</a></sup> (similarly, there is a <code>RandomForestRegressor</code> class for regression tasks). The following code uses all available CPU cores to train a Random Forest classifier with 500 trees (each limited to maximum 16 nodes):</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>

<code class="n">rnd_clf</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code> <code class="n">max_leaf_nodes</code><code class="o">=</code><code class="mi">16</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>
<code class="n">rnd_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="n">y_pred_rf</code> <code class="o">=</code> <code class="n">rnd_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code></pre>

<p>With a few exceptions, a <code>RandomForestClassifier</code> has all the hyperparameters of a <code>DecisionTreeClassifier</code> (to control how trees are grown), plus all the hyperparameters of a <code>BaggingClassifier</code> to control the ensemble itself.<sup><a data-type="noteref" id="idm46263522681544-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522681544">11</a></sup></p>

<p>The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch06.html#trees_chapter">Chapter&nbsp;6</a>), it searches for the best feature among a random subset of features. The algorithm results in a greater tree diversity, which (once again) trades a higher bias for a lower variance, generally yielding an overall better model. The following <code>BaggingClassifier</code> is roughly equivalent to the previous <code>RandomForestClassifier</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">bag_clf</code> <code class="o">=</code> <code class="n">BaggingClassifier</code><code class="p">(</code>
    <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">splitter</code><code class="o">=</code><code class="s2">"random"</code><code class="p">,</code> <code class="n">max_leaf_nodes</code><code class="o">=</code><code class="mi">16</code><code class="p">),</code>
    <code class="n">n_estimators</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code> <code class="n">max_samples</code><code class="o">=</code><code class="mf">1.0</code><code class="p">,</code> <code class="n">bootstrap</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code></pre>








<section data-type="sect2" data-pdf-bookmark="Extra-Trees"><div class="sect2" id="idm46263522654264">
<h2>Extra-Trees</h2>

<p>When you are growing a tree in a Random Forest, at each node only a random subset of the features is considered for splitting (as discussed earlier). It is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds (like regular Decision Trees do).</p>

<p>A forest of such extremely random trees is simply called an <a href="https://homl.info/25"><em>Extremely Randomized Trees</em></a> ensemble<sup><a data-type="noteref" id="idm46263522611672-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522611672">12</a></sup> (or <em>Extra-Trees</em> for short). Once again, this technique trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests because finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree.</p>

<p>You can create an Extra-Trees classifier using Scikit-Learn’s <code>ExtraTreesClassifier</code> class. Its API is identical to the <code>RandomForestClassifier</code> class. Similarly, the <code>ExtraTreesRegressor</code> class has the same API as the <code>RandomForestRegressor</code> class.</p>
<div data-type="tip"><h6>Tip</h6>
<p>It is hard to tell in advance whether a <code>RandomForestClassifier</code> will perform better or worse than an <code>ExtraTreesClassifier</code>. Generally, the only way to know is to try both and compare them using cross-validation (and tuning the hyperparameters using grid search).</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Feature Importance"><div class="sect2" id="idm46263522605416">
<h2>Feature Importance</h2>

<p>Yet another great quality of Random Forests is that they make it easy to measure the relative importance of each feature. Scikit-Learn measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). More precisely, it is a weighted average, where each node’s weight is equal to the number of training samples that are associated with it (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch06.html#trees_chapter">Chapter&nbsp;6</a>).</p>

<p>Scikit-Learn computes this score automatically for each feature after training, then it scales the results so that the sum of all importances is equal to 1. You can access the result using the <code>feature_importances_</code> variable. For example, the following code trains a <code>RandomForestClassifier</code> on the iris dataset (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>) and outputs each feature’s importance. It seems that the most important features are the petal length (44%) and width (42%), while sepal length and width are rather unimportant in comparison (11% and 2%, respectively).</p>

<pre data-type="programlisting" data-code-language="pycon" class="pagebreak-before"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_iris</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">iris</code> <code class="o">=</code> <code class="n">load_iris</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">rnd_clf</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">rnd_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">iris</code><code class="p">[</code><code class="s">"data"</code><code class="p">],</code> <code class="n">iris</code><code class="p">[</code><code class="s">"target"</code><code class="p">])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">score</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">iris</code><code class="p">[</code><code class="s">"feature_names"</code><code class="p">],</code> <code class="n">rnd_clf</code><code class="o">.</code><code class="n">feature_importances_</code><code class="p">):</code>
<code class="gp">... </code>    <code class="k">print</code><code class="p">(</code><code class="n">name</code><code class="p">,</code> <code class="n">score</code><code class="p">)</code>
<code class="gp">...</code>
<code class="go">sepal length (cm) 0.112492250999</code>
<code class="go">sepal width (cm) 0.0231192882825</code>
<code class="go">petal length (cm) 0.441030464364</code>
<code class="go">petal width (cm) 0.423357996355</code></pre>

<p>Similarly, if you train a Random Forest classifier on the MNIST dataset (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch03.html#classification_chapter">Chapter&nbsp;3</a>) and plot each pixel’s importance, you get the image represented in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#mnist_feature_importance_plot">Figure&nbsp;7-6</a>.</p>

<figure class="smallereighty"><div id="mnist_feature_importance_plot" class="figure">
<img src="./Chapter7_files/mls2_0706.png" alt="mls2 0706" width="1440" height="988" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0706.png">
<h6><span class="label">Figure 7-6. </span>MNIST pixel importance (according to a Random Forest classifier)</h6>
</div></figure>

<p>Random Forests are very handy to get a quick understanding of what features actually matter, in particular if you need to perform feature selection.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Boosting"><div class="sect1" id="idm46263522500840">
<h1>Boosting</h1>

<p><em>Boosting</em> (originally called <em>hypothesis boosting</em>) refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. There are many Boosting methods available, but by far the most popular are <a href="https://homl.info/26"><em>AdaBoost</em></a><sup><a data-type="noteref" id="idm46263522497528-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522497528">13</a></sup> (short for <em>Adaptive Boosting</em>) and <em>Gradient Boosting</em>. Let’s start with AdaBoost.</p>








<section data-type="sect2" data-pdf-bookmark="AdaBoost"><div class="sect2" id="idm46263522494376">
<h2>AdaBoost</h2>

<p>One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases. This is the technique used by AdaBoost.</p>

<p>For example, when training an AdaBoost classifier, the algorithm first trains a base classifier (such as a Decision Tree) and uses it to make predictions on the training set. The algorithm then increases the relative weight of misclassified training instances. Then it trains a second classifier, using the updated weights, and again makes predictions on the training set, updates the instance weights, and so on (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#adaboost_training_diagram">Figure&nbsp;7-7</a>).</p>

<figure><div id="adaboost_training_diagram" class="figure">
<img src="./Chapter7_files/mls2_0707.png" alt="mls2 0707" width="1439" height="842" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0707.png">
<h6><span class="label">Figure 7-7. </span>AdaBoost sequential training with instance weight updates</h6>
</div></figure>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#boosting_plot">Figure&nbsp;7-8</a> shows the decision boundaries of five consecutive predictors on the moons dataset (in this example, each predictor is a highly regularized SVM classifier with an RBF kernel<sup><a data-type="noteref" id="idm46263522487064-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522487064">14</a></sup>). The first classifier gets many instances wrong, so their weights get boosted. The second classifier therefore does a better job on these instances, and so on. The plot on the right represents the same sequence of predictors, except that the learning rate is halved (i.e., the misclassified instance weights are boosted half as much at every iteration). As you can see, this sequential learning technique has some similarities with Gradient Descent, except that instead of tweaking a single predictor’s parameters to minimize a cost function, AdaBoost adds predictors to the ensemble, gradually making it better.</p>

<figure><div id="boosting_plot" class="figure">
<img src="./Chapter7_files/mls2_0708.png" alt="mls2 0708" width="1441" height="491" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0708.png">
<h6><span class="label">Figure 7-8. </span>Decision boundaries of consecutive predictors</h6>
</div></figure>

<p>Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, except that predictors have different weights, depending on their overall accuracy on the weighted training set.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>There is one important drawback to this sequential learning technique: it cannot be parallelized (or only partially), since each predictor can only be trained after the previous predictor has been trained and evaluated. As a result, it does not scale as well as bagging or pasting.</p>
</div>

<p>Let’s take a closer look at the AdaBoost algorithm. Each instance weight <em>w</em><sup><em>(i)</em></sup> is initially set to <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-88-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;StartFraction 1 Over m EndFraction&quot;&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mfrac&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-3587" aria-label="StartFraction 1 Over m EndFraction" style="width: 0.979em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.928em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.082em, 1000.94em, 2.625em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-3588"><span class="mfrac" id="MathJax-Span-3589"><span style="display: inline-block; position: relative; width: 0.722em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.396em, 1000.32em, 4.167em, -1000.01em); top: -4.419em; left: 50%; margin-left: -0.152em;"><span class="mn" id="MathJax-Span-3590" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.55em, 1000.63em, 4.167em, -1000.01em); top: -3.648em; left: 50%; margin-left: -0.306em;"><span class="mi" id="MathJax-Span-3591" style="font-size: 70.7%; font-family: MathJax_Math-italic;">m</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1000.73em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.722em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.421em; border-left: 0px solid; width: 0px; height: 1.38em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="StartFraction 1 Over m EndFraction"><mfrac><mn>1</mn><mi>m</mi></mfrac></math></span></span><script type="math/mml" id="MathJax-Element-88"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="StartFraction 1 Over m EndFraction">
  <mfrac><mn>1</mn> <mi>m</mi></mfrac>
</math></script>. A first predictor is trained, and its weighted error rate <em>r</em><sub>1</sub> is computed on the training set; see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#weighted_error_rate">Equation 7-1</a>.</p>
<div id="weighted_error_rate" data-type="equation">
<h5><span class="label">Equation 7-1. </span>Weighted error rate of the j<sup>th</sup> predictor</h5>
 <span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-89-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;false&quot;&gt;&lt;mrow&gt;&lt;mfrac linethickness=&quot;0pt&quot;&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo&gt;&amp;#x2260;&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/munderover&gt;&lt;msup&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/munderover&gt;&lt;msup&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;mtext&gt;where&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;msubsup&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;is&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;the&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;msup&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mtext&gt;th&lt;/mtext&gt;&lt;/msup&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;predictor&amp;#x2019;s&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;prediction&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;for&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;the&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;msup&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mtext&gt;th&lt;/mtext&gt;&lt;/msup&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;instance.&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-3592" style="width: 36.969em; display: inline-block;"><span style="display: inline-block; position: relative; width: 35.89em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(-2.517em, 1035.8em, 4.99em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-3593"><span class="mrow" id="MathJax-Span-3594"><span class="msub" id="MathJax-Span-3595"><span style="display: inline-block; position: relative; width: 0.825em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.42em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3596" style="font-family: MathJax_Math-italic;">r</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.465em;"><span class="mi" id="MathJax-Span-3597" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-3598" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mstyle" id="MathJax-Span-3599" style="padding-left: 0.26em;"><span class="mrow" id="MathJax-Span-3600"><span class="mfrac" id="MathJax-Span-3601"><span style="display: inline-block; position: relative; width: 4.733em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.293em, 1004.59em, 7.56em, -1000.01em); top: -7.864em; left: 50%; margin-left: -2.311em;"><span class="mstyle" id="MathJax-Span-3602"><span class="mrow" id="MathJax-Span-3603"><span class="mrow" id="MathJax-Span-3604"><span class="munderover" id="MathJax-Span-3605"><span style="display: inline-block; position: relative; width: 2.882em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1001.4em, 4.63em, -1000.01em); top: -4.008em; left: 0.722em;"><span class="mo" id="MathJax-Span-3606" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.882em, 1002.84em, 4.99em, -1000.01em); top: -2.414em; left: 0em;"><span class="mstyle" id="MathJax-Span-3607"><span class="mrow" id="MathJax-Span-3608"><span class="mrow" id="MathJax-Span-3609"><span class="mfrac" id="MathJax-Span-3610"><span style="display: inline-block; position: relative; width: 2.625em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.396em, 1001.09em, 4.167em, -1000.01em); top: -4.522em; left: 50%; margin-left: -0.563em;"><span class="mrow" id="MathJax-Span-3611"><span class="mi" id="MathJax-Span-3612" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-3613" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="mn" id="MathJax-Span-3614" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.139em, 1002.63em, 4.476em, -1000.01em); top: -3.596em; left: 50%; margin-left: -1.283em;"><span class="mrow" id="MathJax-Span-3615"><span class="msubsup" id="MathJax-Span-3616"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.293em, 1000.37em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mover" id="MathJax-Span-3617"><span style="display: inline-block; position: relative; width: 0.414em; height: 0px;"><span style="position: absolute; clip: rect(3.55em, 1000.37em, 4.321em, -1000.01em); top: -4.008em; left: 0.003em;"><span class="mi" id="MathJax-Span-3618" style="font-size: 70.7%; font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.345em, 1000.27em, 3.807em, -1000.01em); top: -4.059em; left: 0.105em;"><span class="mo" id="MathJax-Span-3619" style=""><span><span style="font-size: 70.7%; font-family: MathJax_Main;">ˆ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.499em, 1000.63em, 4.27em, -1000.01em); top: -4.368em; left: 0.414em;"><span class="mrow" id="MathJax-Span-3620"><span class="mo" id="MathJax-Span-3621" style="font-size: 50%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-3622" style="font-size: 50%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-3623" style="font-size: 50%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.55em, 1000.27em, 4.27em, -1000.01em); top: -3.802em; left: 0.414em;"><span class="mi" id="MathJax-Span-3624" style="font-size: 50%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-3625" style="font-size: 70.7%; font-family: MathJax_Main;">≠</span><span class="msup" id="MathJax-Span-3626"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.55em, 1000.37em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3627" style="font-size: 70.7%; font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.213em; left: 0.414em;"><span class="mrow" id="MathJax-Span-3628"><span class="mo" id="MathJax-Span-3629" style="font-size: 50%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-3630" style="font-size: 50%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-3631" style="font-size: 50%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.447em, 1000.63em, 4.167em, -1000.01em); top: -5.139em; left: 1.134em;"><span class="mi" id="MathJax-Span-3632" style="font-size: 70.7%; font-family: MathJax_Math-italic;">m</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msup" id="MathJax-Span-3633" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.596em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.68em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3634" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 0.722em;"><span class="mrow" id="MathJax-Span-3635"><span class="mo" id="MathJax-Span-3636" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-3637" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-3638" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.99em;"></span></span><span style="position: absolute; clip: rect(2.316em, 1003.2em, 5.35em, -1000.01em); top: -2.465em; left: 50%; margin-left: -1.591em;"><span class="mstyle" id="MathJax-Span-3639"><span class="mrow" id="MathJax-Span-3640"><span class="mrow" id="MathJax-Span-3641"><span class="munderover" id="MathJax-Span-3642"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1001.4em, 4.63em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-3643" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.09em, 4.27em, -1000.01em); top: -2.928em; left: 0.157em;"><span class="mrow" id="MathJax-Span-3644"><span class="mi" id="MathJax-Span-3645" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-3646" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="mn" id="MathJax-Span-3647" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.447em, 1000.63em, 4.167em, -1000.01em); top: -5.139em; left: 0.414em;"><span class="mi" id="MathJax-Span-3648" style="font-size: 70.7%; font-family: MathJax_Math-italic;">m</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msup" id="MathJax-Span-3649" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.596em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.68em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3650" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 0.722em;"><span class="mrow" id="MathJax-Span-3651"><span class="mo" id="MathJax-Span-3652" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-3653" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-3654" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1004.74em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 4.733em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span><span class="mspace" id="MathJax-Span-3655" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-3656" style="font-family: MathJax_Main;">where</span><span class="mspace" id="MathJax-Span-3657" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="msubsup" id="MathJax-Span-3658"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1000.52em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mover" id="MathJax-Span-3659"><span style="display: inline-block; position: relative; width: 0.568em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.373em, -1000.01em); top: -4.008em; left: 0.003em;"><span class="mi" id="MathJax-Span-3660" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1000.37em, 3.653em, -1000.01em); top: -4.059em; left: 0.105em;"><span class="mo" id="MathJax-Span-3661" style=""><span style="font-family: MathJax_Main;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.345em, 1000.88em, 4.321em, -1000.01em); top: -4.522em; left: 0.568em;"><span class="mrow" id="MathJax-Span-3662"><span class="mo" id="MathJax-Span-3663" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-3664" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-3665" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1000.37em, 4.321em, -1000.01em); top: -3.699em; left: 0.568em;"><span class="mi" id="MathJax-Span-3666" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mspace" id="MathJax-Span-3667" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-3668" style="font-family: MathJax_Main;">is</span><span class="mspace" id="MathJax-Span-3669" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-3670" style="font-family: MathJax_Main;">the</span><span class="mspace" id="MathJax-Span-3671" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="msup" id="MathJax-Span-3672"><span style="display: inline-block; position: relative; width: 1.134em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.42em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3673" style="font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 0.414em;"><span class="mtext" id="MathJax-Span-3674" style="font-size: 70.7%; font-family: MathJax_Main;">th</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mspace" id="MathJax-Span-3675" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-3676" style="font-family: MathJax_Main;">predictor’s</span><span class="mspace" id="MathJax-Span-3677" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-3678" style="font-family: MathJax_Main;">prediction</span><span class="mspace" id="MathJax-Span-3679" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-3680" style="font-family: MathJax_Main;">for</span><span class="mspace" id="MathJax-Span-3681" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-3682" style="font-family: MathJax_Main;">the</span><span class="mspace" id="MathJax-Span-3683" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="msup" id="MathJax-Span-3684"><span style="display: inline-block; position: relative; width: 1.082em; height: 0px;"><span style="position: absolute; clip: rect(3.19em, 1000.32em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3685" style="font-family: MathJax_Math-italic;">i</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 0.362em;"><span class="mtext" id="MathJax-Span-3686" style="font-size: 70.7%; font-family: MathJax_Main;">th</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mspace" id="MathJax-Span-3687" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-3688" style="font-family: MathJax_Main;">instance.</span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -2.857em; border-left: 0px solid; width: 0px; height: 7.523em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>r</mi><mi>j</mi></msub><mo>=</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mstyle scriptlevel="0" displaystyle="true"><mrow><munderover><mo>∑</mo><mstyle scriptlevel="0" displaystyle="false"><mrow><mfrac linethickness="0pt"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>j</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup><mo>≠</mo><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></mstyle><mi>m</mi></munderover><msup><mi>w</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mstyle><mstyle scriptlevel="0" displaystyle="true"><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msup><mi>w</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mstyle></mfrac></mstyle><mspace width="1.em"></mspace><mtext>where</mtext><mspace width="4.pt"></mspace><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>j</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup><mspace width="4.pt"></mspace><mtext>is</mtext><mspace width="4.pt"></mspace><mtext>the</mtext><mspace width="4.pt"></mspace><msup><mi>j</mi><mtext>th</mtext></msup><mspace width="4.pt"></mspace><mtext>predictor’s</mtext><mspace width="4.pt"></mspace><mtext>prediction</mtext><mspace width="4.pt"></mspace><mtext>for</mtext><mspace width="4.pt"></mspace><mtext>the</mtext><mspace width="4.pt"></mspace><msup><mi>i</mi><mtext>th</mtext></msup><mspace width="4.pt"></mspace><mtext>instance.</mtext></mrow></math></span></span></div><script type="math/mml" id="MathJax-Element-89"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <msub><mi>r</mi> <mi>j</mi> </msub>
    <mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true">
      <mfrac><mstyle scriptlevel="0" displaystyle="true"><mrow><munderover><mo>∑</mo> <mstyle scriptlevel="0" displaystyle="false"><mrow><mfrac linethickness="0pt"><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mrow><msubsup><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msubsup><mo>≠</mo><msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup></mrow></mfrac></mrow></mstyle> <mi>m</mi> </munderover><msup><mi>w</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup></mrow></mstyle> <mstyle scriptlevel="0" displaystyle="true"><mrow><munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </munderover><msup><mi>w</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup></mrow></mstyle></mfrac>
    </mstyle>
    <mspace width="1.em"></mspace>
    <mtext>where</mtext>
    <mspace width="4.pt"></mspace>
    <msubsup><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msubsup>
    <mspace width="4.pt"></mspace>
    <mtext>is</mtext>
    <mspace width="4.pt"></mspace>
    <mtext>the</mtext>
    <mspace width="4.pt"></mspace>
    <msup><mi>j</mi> <mtext>th</mtext> </msup>
    <mspace width="4.pt"></mspace>
    <mtext>predictor’s</mtext>
    <mspace width="4.pt"></mspace>
    <mtext>prediction</mtext>
    <mspace width="4.pt"></mspace>
    <mtext>for</mtext>
    <mspace width="4.pt"></mspace>
    <mtext>the</mtext>
    <mspace width="4.pt"></mspace>
    <msup><mi>i</mi> <mtext>th</mtext> </msup>
    <mspace width="4.pt"></mspace>
    <mtext>instance.</mtext>
  </mrow>
</math></script>
</div>

<p>The predictor’s weight <em>α</em><sub><em>j</em></sub> is then computed using <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#predictor_weight">Equation 7-2</a>, where <em>η</em> is the learning rate hyperparameter (defaults to 1).<sup><a data-type="noteref" id="idm46263522436184-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522436184">15</a></sup> The more accurate the predictor is, the higher its weight will be. If it is just guessing randomly, then its weight will be close to zero. However, if it is most often wrong (i.e., less accurate than random guessing), then its weight will be negative.</p>
<div data-type="equation" id="predictor_weight">
<h5><span class="label">Equation 7-2. </span>Predictor weight</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-90-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mtable displaystyle=&quot;true&quot;&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;right&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B1;&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;&amp;#x3B7;&lt;/mi&gt;&lt;mo form=&quot;prefix&quot;&gt;log&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;-&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-3689" style="width: 8.126em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.869em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(0.517em, 1007.72em, 3.242em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-3690"><span class="mtable" id="MathJax-Span-3691" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 7.56em; height: 0px;"><span style="position: absolute; clip: rect(2.419em, 1007.57em, 5.144em, -1000.01em); top: -4.008em; left: 0em;"><span style="display: inline-block; position: relative; width: 7.56em; height: 0px;"><span style="position: absolute; clip: rect(2.419em, 1007.57em, 5.144em, -1000.01em); top: -4.059em; right: 0em;"><span class="mtd" id="MathJax-Span-3692"><span class="mrow" id="MathJax-Span-3693"><span class="mrow" id="MathJax-Span-3694"><span class="msub" id="MathJax-Span-3695"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3696" style="font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mi" id="MathJax-Span-3697" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-3698" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mi" id="MathJax-Span-3699" style="font-family: MathJax_Math-italic; padding-left: 0.26em;">η<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-3700" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">log</span><span class="mstyle" id="MathJax-Span-3701"><span class="mrow" id="MathJax-Span-3702"><span class="mfrac" id="MathJax-Span-3703"><span style="display: inline-block; position: relative; width: 2.625em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.19em, 1002.53em, 4.476em, -1000.01em); top: -4.779em; left: 50%; margin-left: -1.231em;"><span class="mrow" id="MathJax-Span-3704"><span class="mn" id="MathJax-Span-3705" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-3706" style="font-family: MathJax_Main; padding-left: 0.208em;">−</span><span class="msub" id="MathJax-Span-3707" style="padding-left: 0.208em;"><span style="display: inline-block; position: relative; width: 0.825em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.42em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3708" style="font-family: MathJax_Math-italic;">r</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.465em;"><span class="mi" id="MathJax-Span-3709" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1000.83em, 4.476em, -1000.01em); top: -3.339em; left: 50%; margin-left: -0.409em;"><span class="msub" id="MathJax-Span-3710"><span style="display: inline-block; position: relative; width: 0.825em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.42em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3711" style="font-family: MathJax_Math-italic;">r</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.465em;"><span class="mi" id="MathJax-Span-3712" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.877em, 1002.63em, 1.237em, -1000.01em); top: -1.283em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.625em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.082em;"></span></span></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -1.056em; border-left: 0px solid; width: 0px; height: 2.598em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>α</mi><mi>j</mi></msub><mo>=</mo><mi>η</mi><mo form="prefix">log</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mn>1</mn><mo>-</mo><msub><mi>r</mi><mi>j</mi></msub></mrow><msub><mi>r</mi><mi>j</mi></msub></mfrac></mstyle></mrow></mtd></mtr></mtable></math></span></span></div><script type="math/mml" id="MathJax-Element-90"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <msub><mi>α</mi> <mi>j</mi> </msub>
          <mo>=</mo>
          <mi>η</mi>
          <mo form="prefix">log</mo>
          <mstyle scriptlevel="0" displaystyle="true">
            <mfrac><mrow><mn>1</mn><mo>-</mo><msub><mi>r</mi> <mi>j</mi> </msub></mrow> <msub><mi>r</mi> <mi>j</mi> </msub></mfrac>
          </mstyle>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></script>
</div>

<p>Next, the AdaBoost algorithm updates the instance weights, using <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#instance_weight_update">Equation 7-3</a>, which boosts the weights of the misclassified instances.</p>
<div data-type="equation" id="instance_weight_update">
<h5><span class="label">Equation 7-3. </span>Weight update rule</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-91-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mtable displaystyle=&quot;true&quot;&gt;&lt;mtr&gt;&lt;mtd /&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;for&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;&amp;#x22EF;&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd /&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x2190;&lt;/mo&gt;&lt;mfenced separators=&quot;&quot; open=&quot;{&quot; close=&quot;&quot;&gt;&lt;mtable&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;msup&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;if&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;msup&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;mtr&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo form=&quot;prefix&quot;&gt;exp&lt;/mo&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B1;&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;mtd columnalign=&quot;left&quot;&gt;&lt;mrow&gt;&lt;mtext&gt;if&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;msup&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;&amp;#x2260;&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-3713" style="width: 17.741em; display: inline-block;"><span style="display: inline-block; position: relative; width: 17.226em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(-0.563em, 1016.93em, 4.27em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-3714"><span class="mtable" id="MathJax-Span-3715" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 16.918em; height: 0px;"><span style="position: absolute; clip: rect(2.162em, 1000.01em, 4.887em, -1000.01em); top: -4.008em; left: 0em;"><span style="display: inline-block; position: relative; width: 0em; height: 0px;"><span style="position: absolute; clip: rect(3.859em, 1000.01em, 4.167em, -1000.01em); top: -5.704em; left: 50%; margin-left: 0em;"><span class="mtd" id="MathJax-Span-3716"><span class="mrow" id="MathJax-Span-3717"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.859em, 1000.01em, 4.167em, -1000.01em); top: -3.288em; left: 50%; margin-left: 0em;"><span class="mtd" id="MathJax-Span-3733"><span class="mrow" id="MathJax-Span-3734"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.882em, 1015.95em, 7.612em, -1000.01em); top: -5.447em; left: 0.825em;"><span style="display: inline-block; position: relative; width: 16.095em; height: 0px;"><span style="position: absolute; clip: rect(3.139em, 1008.13em, 4.373em, -1000.01em); top: -5.704em; left: 0em;"><span class="mtd" id="MathJax-Span-3718"><span class="mrow" id="MathJax-Span-3719"><span class="mrow" id="MathJax-Span-3720"><span class="mspace" id="MathJax-Span-3721" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-3722" style="font-family: MathJax_Main;">for</span><span class="mspace" id="MathJax-Span-3723" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-3724" style="font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-3725" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="mn" id="MathJax-Span-3726" style="font-family: MathJax_Main; padding-left: 0.26em;">1</span><span class="mo" id="MathJax-Span-3727" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-3728" style="font-family: MathJax_Main; padding-left: 0.157em;">2</span><span class="mo" id="MathJax-Span-3729" style="font-family: MathJax_Main;">,</span><span class="mo" id="MathJax-Span-3730" style="font-family: MathJax_Main; padding-left: 0.157em;">⋯</span><span class="mo" id="MathJax-Span-3731" style="font-family: MathJax_Main; padding-left: 0.157em;">,</span><span class="mi" id="MathJax-Span-3732" style="font-family: MathJax_Math-italic; padding-left: 0.157em;">m</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.162em, 1015.95em, 5.555em, -1000.01em); top: -3.391em; left: 0em;"><span class="mtd" id="MathJax-Span-3735"><span class="mrow" id="MathJax-Span-3736"><span class="mrow" id="MathJax-Span-3737"><span class="msup" id="MathJax-Span-3738"><span style="display: inline-block; position: relative; width: 1.596em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.68em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3739" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.419em; left: 0.722em;"><span class="mrow" id="MathJax-Span-3740"><span class="mo" id="MathJax-Span-3741" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-3742" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-3743" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-3744" style="font-family: MathJax_Main; padding-left: 0.26em;">←</span><span class="mfenced" id="MathJax-Span-3745" style="padding-left: 0.26em;"><span class="mo" id="MathJax-Span-3746" style="vertical-align: 0em;"><span style="font-family: MathJax_Size4;">{</span></span><span class="mtable" id="MathJax-Span-3747" style="padding-right: 0.157em; padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 11.879em; height: 0px;"><span style="position: absolute; clip: rect(2.213em, 1005.41em, 5.452em, -1000.01em); top: -4.008em; left: 0em;"><span style="display: inline-block; position: relative; width: 5.504em; height: 0px;"><span style="position: absolute; clip: rect(2.985em, 1001.61em, 4.167em, -1000.01em); top: -4.779em; left: 0em;"><span class="mtd" id="MathJax-Span-3748"><span class="mrow" id="MathJax-Span-3749"><span class="msup" id="MathJax-Span-3750"><span style="display: inline-block; position: relative; width: 1.596em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.68em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3751" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.722em;"><span class="mrow" id="MathJax-Span-3752"><span class="mo" id="MathJax-Span-3753" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-3754" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-3755" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.985em, 1005.41em, 4.476em, -1000.01em); top: -3.031em; left: 0em;"><span class="mtd" id="MathJax-Span-3778"><span class="mrow" id="MathJax-Span-3779"><span class="mrow" id="MathJax-Span-3780"><span class="msup" id="MathJax-Span-3781"><span style="display: inline-block; position: relative; width: 1.596em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.68em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3782" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.722em;"><span class="mrow" id="MathJax-Span-3783"><span class="mo" id="MathJax-Span-3784" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-3785" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-3786" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-3787" style="font-family: MathJax_Main; padding-left: 0.311em; padding-right: 0.311em;">exp</span><span class="mrow" id="MathJax-Span-3788"><span class="mo" id="MathJax-Span-3789" style=""><span style="font-family: MathJax_Main;">(</span></span><span class="msub" id="MathJax-Span-3790"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3791" style="font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mi" id="MathJax-Span-3792" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-3793" style=""><span style="font-family: MathJax_Main;">)</span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.162em, 1005.56em, 5.555em, -1000.01em); top: -4.111em; left: 6.327em;"><span style="display: inline-block; position: relative; width: 5.555em; height: 0px;"><span style="position: absolute; clip: rect(2.83em, 1005.56em, 4.476em, -1000.01em); top: -4.779em; left: 0em;"><span class="mtd" id="MathJax-Span-3756"><span class="mrow" id="MathJax-Span-3757"><span class="mrow" id="MathJax-Span-3758"><span class="mtext" id="MathJax-Span-3759" style="font-family: MathJax_Main;">if</span><span class="mspace" id="MathJax-Span-3760" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="msup" id="MathJax-Span-3761"><span style="display: inline-block; position: relative; width: 1.853em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1000.99em, 4.476em, -1000.01em); top: -4.008em; left: 0em;"><span class="mover" id="MathJax-Span-3762"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.83em, 4.476em, -1000.01em); top: -4.008em; left: 0.054em;"><span class="msub" id="MathJax-Span-3763"><span style="display: inline-block; position: relative; width: 0.877em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3764" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.517em;"><span class="mi" id="MathJax-Span-3765" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1000.99em, 3.602em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-3766" style=""><span style="font-family: MathJax_Size2;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.522em; left: 0.979em;"><span class="mrow" id="MathJax-Span-3767"><span class="mo" id="MathJax-Span-3768" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-3769" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-3770" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-3771" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="msup" id="MathJax-Span-3772" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3773" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.568em;"><span class="mrow" id="MathJax-Span-3774"><span class="mo" id="MathJax-Span-3775" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-3776" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-3777" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.83em, 1005.56em, 4.476em, -1000.01em); top: -3.031em; left: 0em;"><span class="mtd" id="MathJax-Span-3794"><span class="mrow" id="MathJax-Span-3795"><span class="mrow" id="MathJax-Span-3796"><span class="mtext" id="MathJax-Span-3797" style="font-family: MathJax_Main;">if</span><span class="mspace" id="MathJax-Span-3798" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="msup" id="MathJax-Span-3799"><span style="display: inline-block; position: relative; width: 1.853em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1000.99em, 4.476em, -1000.01em); top: -4.008em; left: 0em;"><span class="mover" id="MathJax-Span-3800"><span style="display: inline-block; position: relative; width: 0.979em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.83em, 4.476em, -1000.01em); top: -4.008em; left: 0.054em;"><span class="msub" id="MathJax-Span-3801"><span style="display: inline-block; position: relative; width: 0.877em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3802" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.517em;"><span class="mi" id="MathJax-Span-3803" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.087em, 1000.99em, 3.602em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-3804" style=""><span style="font-family: MathJax_Size2;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.522em; left: 0.979em;"><span class="mrow" id="MathJax-Span-3805"><span class="mo" id="MathJax-Span-3806" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-3807" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-3808" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-3809" style="font-family: MathJax_Main; padding-left: 0.26em;">≠</span><span class="msup" id="MathJax-Span-3810" style="padding-left: 0.26em;"><span style="display: inline-block; position: relative; width: 1.442em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3811" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.568em;"><span class="mrow" id="MathJax-Span-3812"><span class="mo" id="MathJax-Span-3813" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-3814" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-3815" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.116em;"></span></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.116em;"></span></span></span><span style="display: inline-block; width: 0px; height: 5.452em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -2.116em; border-left: 0px solid; width: 0px; height: 4.769em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mtr><mtd></mtd><mtd columnalign="left"><mrow><mspace width="4.pt"></mspace><mtext>for</mtext><mspace width="4.pt"></mspace><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>m</mi></mrow></mtd></mtr><mtr><mtd></mtd><mtd columnalign="left"><mrow><msup><mi>w</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>←</mo><mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><msup><mi>w</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mtd><mtd columnalign="left"><mrow><mtext>if</mtext><mspace width="4.pt"></mspace><msup><mover accent="true"><msub><mi>y</mi><mi>j</mi></msub><mo>^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>=</mo><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><msup><mi>w</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>α</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow></mtd><mtd columnalign="left"><mrow><mtext>if</mtext><mspace width="4.pt"></mspace><msup><mover accent="true"><msub><mi>y</mi><mi>j</mi></msub><mo>^</mo></mover><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>≠</mo><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math></span></span></div><script type="math/mml" id="MathJax-Element-91"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd></mtd>
      <mtd columnalign="left">
        <mrow>
          <mspace width="4.pt"></mspace>
          <mtext>for</mtext>
          <mspace width="4.pt"></mspace>
          <mi>i</mi>
          <mo>=</mo>
          <mn>1</mn>
          <mo>,</mo>
          <mn>2</mn>
          <mo>,</mo>
          <mo>⋯</mo>
          <mo>,</mo>
          <mi>m</mi>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd></mtd>
      <mtd columnalign="left">
        <mrow>
          <msup><mi>w</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
          <mo>←</mo>
          <mfenced separators="" open="{" close="">
            <mtable>
              <mtr>
                <mtd columnalign="left">
                  <msup><mi>w</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
                </mtd>
                <mtd columnalign="left">
                  <mrow>
                    <mtext>if</mtext>
                    <mspace width="4.pt"></mspace>
                    <msup><mover accent="true"><msub><mi>y</mi> <mi>j</mi> </msub> <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
                    <mo>=</mo>
                    <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd columnalign="left">
                  <mrow>
                    <msup><mi>w</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
                    <mo form="prefix">exp</mo>
                    <mrow>
                      <mo>(</mo>
                      <msub><mi>α</mi> <mi>j</mi> </msub>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mtd>
                <mtd columnalign="left">
                  <mrow>
                    <mtext>if</mtext>
                    <mspace width="4.pt"></mspace>
                    <msup><mover accent="true"><msub><mi>y</mi> <mi>j</mi> </msub> <mo>^</mo></mover> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
                    <mo>≠</mo>
                    <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
          </mfenced>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math></script>
</div>

<p>Then all the instance weights are normalized (i.e., divided by <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-92-Frame" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; alttext=&quot;sigma-summation Underscript i equals 1 Overscript m Endscripts w Superscript left-parenthesis i right-parenthesis&quot;&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msubsup&gt;&lt;msup&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-3816" aria-label="sigma-summation Underscript i equals 1 Overscript m Endscripts w Superscript left-parenthesis i right-parenthesis" style="width: 4.167em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.013em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(1.082em, 1004.02em, 2.573em, -1000.01em); top: -2.105em; left: 0em;"><span class="mrow" id="MathJax-Span-3817"><span class="mrow" id="MathJax-Span-3818"><span class="msubsup" id="MathJax-Span-3819"><span style="display: inline-block; position: relative; width: 2.265em; height: 0px;"><span style="position: absolute; clip: rect(3.087em, 1000.99em, 4.424em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-3820" style="font-family: MathJax_Size1; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.55em, 1000.68em, 4.167em, -1000.01em); top: -4.47em; left: 1.082em;"><span class="mi" id="MathJax-Span-3821" style="font-size: 70.7%; font-family: MathJax_Math-italic;">m</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.396em, 1001.25em, 4.167em, -1000.01em); top: -3.699em; left: 1.082em;"><span class="mrow" id="MathJax-Span-3822"><span class="mi" id="MathJax-Span-3823" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-3824" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="mn" id="MathJax-Span-3825" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msup" id="MathJax-Span-3826" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.596em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.68em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3827" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.368em; left: 0.722em;"><span class="mrow" id="MathJax-Span-3828"><span class="mo" id="MathJax-Span-3829" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-3830" style="font-size: 70.7%; font-family: MathJax_Math-italic;">i</span><span class="mo" id="MathJax-Span-3831" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.111em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.368em; border-left: 0px solid; width: 0px; height: 1.327em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="sigma-summation Underscript i equals 1 Overscript m Endscripts w Superscript left-parenthesis i right-parenthesis"><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><msup><mi>w</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></math></span></span><script type="math/mml" id="MathJax-Element-92"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="sigma-summation Underscript i equals 1 Overscript m Endscripts w Superscript left-parenthesis i right-parenthesis">
  <mrow>
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </msubsup>
    <msup><mi>w</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msup>
  </mrow>
</math></script>).</p>

<p>Finally, a new predictor is trained using the updated weights, and the whole process is repeated (the new predictor’s weight is computed, the instance weights are updated, then another predictor is trained, and so on). The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found.</p>

<p>To make predictions, AdaBoost simply computes the predictions of all the predictors and weighs them using the predictor weights <em>α</em><sub><em>j</em></sub>. The predicted class is the one that receives the majority of weighted votes (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#adaboost_prediction">Equation 7-4</a>).</p>
<div id="adaboost_prediction" data-type="equation"><h5><span class="label">Equation 7-4. </span>AdaBoost predictions</h5>
<span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_Display" style="text-align: center;"><span class="MathJax" id="MathJax-Element-93-Frame" tabindex="0" style="text-align: center; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mo form=&quot;prefix&quot;&gt;argmax&lt;/mo&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mfrac linethickness=&quot;0pt&quot;&gt;&lt;mstyle scriptlevel=&quot;1&quot; displaystyle=&quot;false&quot;&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;mstyle scriptlevel=&quot;1&quot; displaystyle=&quot;false&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mfrac&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/munderover&gt;&lt;msub&gt;&lt;mi&gt;&amp;#x3B1;&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mspace width=&quot;1.em&quot; /&gt;&lt;mtext&gt;where&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;is&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;the&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;number&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;of&lt;/mtext&gt;&lt;mspace width=&quot;4.pt&quot; /&gt;&lt;mtext&gt;predictors.&lt;/mtext&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-3832" style="width: 29.617em; display: inline-block;"><span style="display: inline-block; position: relative; width: 28.743em; height: 0px; font-size: 103%;"><span style="position: absolute; clip: rect(0.26em, 1028.65em, 4.681em, -1000.01em); top: -2.157em; left: 0em;"><span class="mrow" id="MathJax-Span-3833"><span class="mrow" id="MathJax-Span-3834"><span class="mover" id="MathJax-Span-3835"><span style="display: inline-block; position: relative; width: 0.568em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.47em, 4.373em, -1000.01em); top: -4.008em; left: 0.003em;"><span class="mi" id="MathJax-Span-3836" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.19em, 1000.37em, 3.653em, -1000.01em); top: -4.059em; left: 0.105em;"><span class="mo" id="MathJax-Span-3837" style=""><span style="font-family: MathJax_Main;">ˆ</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-3838" style="padding-left: 0.157em;"><span class="mo" id="MathJax-Span-3839" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-3840" style="font-family: MathJax_Main-bold;">x</span><span class="mo" id="MathJax-Span-3841" style="font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-3842" style="font-family: MathJax_Main; padding-left: 0.26em;">=</span><span class="munder" id="MathJax-Span-3843" style="padding-left: 0.311em; padding-right: 0.311em;"><span style="display: inline-block; position: relative; width: 3.242em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1003.25em, 4.373em, -1000.01em); top: -4.008em; left: 0em;"><span class="mo" id="MathJax-Span-3844" style="font-family: MathJax_Main;">argmax</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.345em, 1000.37em, 4.27em, -1000.01em); top: -3.134em; left: 1.442em;"><span class="mi" id="MathJax-Span-3845" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-3846"><span class="munderover" id="MathJax-Span-3847"><span style="display: inline-block; position: relative; width: 2.83em; height: 0px;"><span style="position: absolute; clip: rect(2.933em, 1001.4em, 4.63em, -1000.01em); top: -4.008em; left: 0.722em;"><span class="mo" id="MathJax-Span-3848" style="font-family: MathJax_Size2; vertical-align: 0em;">∑</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(2.933em, 1002.84em, 4.938em, -1000.01em); top: -2.465em; left: 0em;"><span class="mfrac" id="MathJax-Span-3849"><span style="display: inline-block; position: relative; width: 2.573em; height: 0px; margin-right: 0.105em; margin-left: 0.105em;"><span style="position: absolute; clip: rect(3.396em, 1001.14em, 4.321em, -1000.01em); top: -4.47em; left: 50%; margin-left: -0.614em;"><span class="mstyle" id="MathJax-Span-3850"><span class="mrow" id="MathJax-Span-3851"><span class="mrow" id="MathJax-Span-3852"><span class="mi" id="MathJax-Span-3853" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span class="mo" id="MathJax-Span-3854" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="mn" id="MathJax-Span-3855" style="font-size: 70.7%; font-family: MathJax_Main;">1</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.293em, 1002.58em, 4.424em, -1000.01em); top: -3.596em; left: 50%; margin-left: -1.283em;"><span class="mstyle" id="MathJax-Span-3856"><span class="mrow" id="MathJax-Span-3857"><span class="mrow" id="MathJax-Span-3858"><span class="msub" id="MathJax-Span-3859"><span style="display: inline-block; position: relative; width: 0.722em; height: 0px;"><span style="position: absolute; clip: rect(3.293em, 1000.37em, 4.321em, -1000.01em); top: -4.008em; left: 0em;"><span class="mover" id="MathJax-Span-3860"><span style="display: inline-block; position: relative; width: 0.414em; height: 0px;"><span style="position: absolute; clip: rect(3.55em, 1000.37em, 4.321em, -1000.01em); top: -4.008em; left: 0.003em;"><span class="mi" id="MathJax-Span-3861" style="font-size: 70.7%; font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.345em, 1000.27em, 3.807em, -1000.01em); top: -4.059em; left: 0.105em;"><span class="mo" id="MathJax-Span-3862" style=""><span><span style="font-size: 70.7%; font-family: MathJax_Main;">ˆ</span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.414em;"><span class="mi" id="MathJax-Span-3863" style="font-size: 50%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mrow" id="MathJax-Span-3864"><span class="mo" id="MathJax-Span-3865" style="font-size: 70.7%; font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-3866" style="font-size: 70.7%; font-family: MathJax_Main-bold;">x</span><span class="mo" id="MathJax-Span-3867" style="font-size: 70.7%; font-family: MathJax_Main;">)</span></span><span class="mo" id="MathJax-Span-3868" style="font-size: 70.7%; font-family: MathJax_Main;">=</span><span class="mi" id="MathJax-Span-3869" style="font-size: 70.7%; font-family: MathJax_Math-italic;">k</span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.293em, 1000.63em, 4.167em, -1000.01em); top: -5.139em; left: 1.082em;"><span class="mi" id="MathJax-Span-3870" style="font-size: 70.7%; font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="msub" id="MathJax-Span-3871" style="padding-left: 0.157em;"><span style="display: inline-block; position: relative; width: 1.031em; height: 0px;"><span style="position: absolute; clip: rect(3.396em, 1000.63em, 4.167em, -1000.01em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-3872" style="font-family: MathJax_Math-italic;">α</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -3.854em; left: 0.62em;"><span class="mi" id="MathJax-Span-3873" style="font-size: 70.7%; font-family: MathJax_Math-italic;">j</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span class="mspace" id="MathJax-Span-3874" style="height: 0em; vertical-align: 0em; width: 0.979em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-3875" style="font-family: MathJax_Main;">where</span><span class="mspace" id="MathJax-Span-3876" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-3877" style="font-family: MathJax_Math-italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.105em;"></span></span><span class="mspace" id="MathJax-Span-3878" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-3879" style="font-family: MathJax_Main;">is</span><span class="mspace" id="MathJax-Span-3880" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-3881" style="font-family: MathJax_Main;">the</span><span class="mspace" id="MathJax-Span-3882" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-3883" style="font-family: MathJax_Main;">number</span><span class="mspace" id="MathJax-Span-3884" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-3885" style="font-family: MathJax_Main;">of</span><span class="mspace" id="MathJax-Span-3886" style="height: 0em; vertical-align: 0em; width: 0.414em; display: inline-block; overflow: hidden;"></span><span class="mtext" id="MathJax-Span-3887" style="font-family: MathJax_Main;">predictors.</span></span></span><span style="display: inline-block; width: 0px; height: 2.162em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -2.486em; border-left: 0px solid; width: 0px; height: 4.292em;"></span></span></nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow><mo>=</mo><munder><mo form="prefix">argmax</mo><mi>k</mi></munder><mrow><munderover><mo>∑</mo><mfrac linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mstyle><mstyle scriptlevel="1" displaystyle="false"><mrow><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>j</mi></msub><mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow><mo>=</mo><mi>k</mi></mrow></mstyle></mfrac><mi>N</mi></munderover><msub><mi>α</mi><mi>j</mi></msub></mrow><mspace width="1.em"></mspace><mtext>where</mtext><mspace width="4.pt"></mspace><mi>N</mi><mspace width="4.pt"></mspace><mtext>is</mtext><mspace width="4.pt"></mspace><mtext>the</mtext><mspace width="4.pt"></mspace><mtext>number</mtext><mspace width="4.pt"></mspace><mtext>of</mtext><mspace width="4.pt"></mspace><mtext>predictors.</mtext></mrow></math></span></span></div><script type="math/mml" id="MathJax-Element-93"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mrow>
      <mo>(</mo>
      <mi mathvariant="bold">x</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <munder><mo form="prefix">argmax</mo> <mi>k</mi></munder>
    <mrow>
      <munderover><mo>∑</mo> <mfrac linethickness="0pt"><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></mstyle> <mstyle scriptlevel="1" displaystyle="false"><mrow><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>j</mi> </msub><mrow><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo></mrow><mo>=</mo><mi>k</mi></mrow></mstyle></mfrac> <mi>N</mi> </munderover>
      <msub><mi>α</mi> <mi>j</mi> </msub>
    </mrow>
    <mspace width="1.em"></mspace>
    <mtext>where</mtext>
    <mspace width="4.pt"></mspace>
    <mi>N</mi>
    <mspace width="4.pt"></mspace>
    <mtext>is</mtext>
    <mspace width="4.pt"></mspace>
    <mtext>the</mtext>
    <mspace width="4.pt"></mspace>
    <mtext>number</mtext>
    <mspace width="4.pt"></mspace>
    <mtext>of</mtext>
    <mspace width="4.pt"></mspace>
    <mtext>predictors.</mtext>
  </mrow>
</math></script>
</div>

<p>Scikit-Learn uses a multiclass version of AdaBoost called <a href="https://homl.info/27"><em>SAMME</em></a><sup><a data-type="noteref" id="idm46263522315992-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522315992">16</a></sup> (which stands for <em>Stagewise Additive Modeling using a Multiclass Exponential loss function</em>). When there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the predictors can estimate class probabilities (i.e., if they have a <code>predict_proba()</code> method), Scikit-Learn can use a variant of SAMME called <em>SAMME.R</em> (the <em>R</em> stands for “Real”), which relies on class probabilities rather than predictions and generally performs better.</p>

<p>The following code trains an AdaBoost classifier based on 200 <em>Decision Stumps</em> using Scikit-Learn’s <code>AdaBoostClassifier</code> class (as you might expect, there is also an <code>AdaBoostRegressor</code> class). A Decision Stump is a Decision Tree with <code>max_depth=1</code>—in other words, a tree composed of a single decision node plus two leaf nodes. This is the default base estimator for the <code>AdaBoostClassifier</code> class:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">AdaBoostClassifier</code>

<code class="n">ada_clf</code> <code class="o">=</code> <code class="n">AdaBoostClassifier</code><code class="p">(</code>
    <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">1</code><code class="p">),</code> <code class="n">n_estimators</code><code class="o">=</code><code class="mi">200</code><code class="p">,</code>
    <code class="n">algorithm</code><code class="o">=</code><code class="s2">"SAMME.R"</code><code class="p">,</code> <code class="n">learning_rate</code><code class="o">=</code><code class="mf">0.5</code><code class="p">)</code>
<code class="n">ada_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of estimators or more strongly regularizing the base estimator.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Gradient Boosting"><div class="sect2" id="idm46263522493432">
<h2>Gradient Boosting</h2>

<p>Another very popular Boosting algorithm is <a href="https://homl.info/28"><em>Gradient Boosting</em></a>.<sup><a data-type="noteref" id="idm46263522273528-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522273528">17</a></sup> Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the <em>residual errors</em> made by the previous predictor.</p>

<p>Let’s go through a simple regression example, using Decision Trees as the base predictors (of course Gradient Boosting also works great with regression tasks). This is called <em>Gradient Tree Boosting</em>, or <em>Gradient Boosted Regression Trees</em> (<em>GBRT</em>). First, let’s fit a <code>DecisionTreeRegressor</code> to the training set (for example, a noisy quadratic training set):</p>

<pre data-type="programlisting" data-code-language="python" class="pagebreak-before"><code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeRegressor</code>

<code class="n">tree_reg1</code> <code class="o">=</code> <code class="n">DecisionTreeRegressor</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">tree_reg1</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

<p>Next, train a second <code>DecisionTreeRegressor</code> on the residual errors made by the first predictor:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">y2</code> <code class="o">=</code> <code class="n">y</code> <code class="o">-</code> <code class="n">tree_reg1</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="n">tree_reg2</code> <code class="o">=</code> <code class="n">DecisionTreeRegressor</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">tree_reg2</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y2</code><code class="p">)</code></pre>

<p>Then we train a third regressor on the residual errors made by the second predictor:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">y3</code> <code class="o">=</code> <code class="n">y2</code> <code class="o">-</code> <code class="n">tree_reg2</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="n">tree_reg3</code> <code class="o">=</code> <code class="n">DecisionTreeRegressor</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">tree_reg3</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y3</code><code class="p">)</code></pre>

<p>Now we have an ensemble containing three trees. It can make predictions on a new instance simply by adding up the predictions of all the trees:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">y_pred</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">(</code><code class="n">tree</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code> <code class="k">for</code> <code class="n">tree</code> <code class="ow">in</code> <code class="p">(</code><code class="n">tree_reg1</code><code class="p">,</code> <code class="n">tree_reg2</code><code class="p">,</code> <code class="n">tree_reg3</code><code class="p">))</code></pre>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#gradient_boosting_plot">Figure&nbsp;7-9</a> represents the predictions of these three trees in the left column, and the ensemble’s predictions in the right column. In the first row, the ensemble has just one tree, so its predictions are exactly the same as the first tree’s predictions. In the second row, a new tree is trained on the residual errors of the first tree. On the right you can see that the ensemble’s predictions are equal to the sum of the predictions of the first two trees. Similarly, in the third row another tree is trained on the residual errors of the second tree. You can see that the ensemble’s predictions gradually get better as trees are added to the ensemble.</p>

<p>A simpler way to train GBRT ensembles is to use Scikit-Learn’s <code>GradientBoostingRegressor</code> class.  Much like the <code>RandomForestRegressor</code> class, it has hyperparameters to control the growth of Decision Trees (e.g., <code>max_depth</code>, <code>min_samples_leaf</code>), as well as hyperparameters to control the ensemble training, such as the number of trees (<code>n_estimators</code>). The following code creates the same ensemble as the previous one:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">GradientBoostingRegressor</code>

<code class="n">gbrt</code> <code class="o">=</code> <code class="n">GradientBoostingRegressor</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">n_estimators</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">learning_rate</code><code class="o">=</code><code class="mf">1.0</code><code class="p">)</code>
<code class="n">gbrt</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

<figure><div id="gradient_boosting_plot" class="figure">
<img src="./Chapter7_files/mls2_0709.png" alt="mls2 0709" width="1440" height="1458" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0709.png">
<h6><span class="label">Figure 7-9. </span>In this depiction of Gradient Boosting, the first predictor (top left) is trained normally, then each consecutive predictor (middle left and lower left) is trained on the previous predictor’s residuals; the right column shows the resulting ensemble’s predictions</h6>
</div></figure>

<p>The <code>learning_rate</code> hyperparameter scales the contribution of each tree. If you set it to a low value, such as <code>0.1</code>, you will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This is a regularization technique called <em>shrinkage</em>. <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#gbrt_learning_rate_plot">Figure&nbsp;7-10</a> shows two GBRT ensembles trained with a low learning rate: the one on the left does not have enough trees to fit the training set, while the one on the right has too many trees and overfits the training set.</p>

<figure><div id="gbrt_learning_rate_plot" class="figure">
<img src="./Chapter7_files/mls2_0710.png" alt="mls2 0710" width="1441" height="493" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0710.png">
<h6><span class="label">Figure 7-10. </span>GBRT ensembles with not enough predictors (left) and too many (right)</h6>
</div></figure>

<p>In order to find the optimal number of trees, you can use early stopping (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>). A simple way to implement this is to use the <code>staged_predict()</code> method: it returns an iterator over the predictions made by the ensemble at each stage of training (with one tree, two trees, etc.). The following code trains a GBRT ensemble with 120 trees, then measures the validation error at each stage of training to find the optimal number of trees, and finally trains another GBRT ensemble using the optimal number of trees:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">mean_squared_error</code>

<code class="n">X_train</code><code class="p">,</code> <code class="n">X_val</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_val</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>

<code class="n">gbrt</code> <code class="o">=</code> <code class="n">GradientBoostingRegressor</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">n_estimators</code><code class="o">=</code><code class="mi">120</code><code class="p">)</code>
<code class="n">gbrt</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="n">errors</code> <code class="o">=</code> <code class="p">[</code><code class="n">mean_squared_error</code><code class="p">(</code><code class="n">y_val</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">)</code>
          <code class="k">for</code> <code class="n">y_pred</code> <code class="ow">in</code> <code class="n">gbrt</code><code class="o">.</code><code class="n">staged_predict</code><code class="p">(</code><code class="n">X_val</code><code class="p">)]</code>
<code class="n">bst_n_estimators</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">argmin</code><code class="p">(</code><code class="n">errors</code><code class="p">)</code> <code class="o">+</code> <code class="mi">1</code>

<code class="n">gbrt_best</code> <code class="o">=</code> <code class="n">GradientBoostingRegressor</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code><code class="n">n_estimators</code><code class="o">=</code><code class="n">bst_n_estimators</code><code class="p">)</code>
<code class="n">gbrt_best</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>

<p>The validation errors are represented on the left of <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#early_stopping_gbrt_plot">Figure&nbsp;7-11</a>, and the best model’s predictions are represented on the right.</p>

<figure><div id="early_stopping_gbrt_plot" class="figure">
<img src="./Chapter7_files/mls2_0711.png" alt="mls2 0711" width="1440" height="491" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0711.png">
<h6><span class="label">Figure 7-11. </span>Tuning the number of trees using early stopping</h6>
</div></figure>

<p>It is also possible to implement early stopping by actually stopping training early (instead of training a large number of trees first and then looking back to find the optimal number). You can do so by setting <code>warm_start=True</code>, which makes Scikit-Learn keep existing trees when the <code>fit()</code> method is called, allowing incremental training. The following code stops training when the validation error does not improve for five iterations in a row:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">gbrt</code> <code class="o">=</code> <code class="n">GradientBoostingRegressor</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">warm_start</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>

<code class="n">min_val_error</code> <code class="o">=</code> <code class="nb">float</code><code class="p">(</code><code class="s2">"inf"</code><code class="p">)</code>
<code class="n">error_going_up</code> <code class="o">=</code> <code class="mi">0</code>
<code class="k">for</code> <code class="n">n_estimators</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">120</code><code class="p">):</code>
    <code class="n">gbrt</code><code class="o">.</code><code class="n">n_estimators</code> <code class="o">=</code> <code class="n">n_estimators</code>
    <code class="n">gbrt</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
    <code class="n">y_pred</code> <code class="o">=</code> <code class="n">gbrt</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_val</code><code class="p">)</code>
    <code class="n">val_error</code> <code class="o">=</code> <code class="n">mean_squared_error</code><code class="p">(</code><code class="n">y_val</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">)</code>
    <code class="k">if</code> <code class="n">val_error</code> <code class="o">&lt;</code> <code class="n">min_val_error</code><code class="p">:</code>
        <code class="n">min_val_error</code> <code class="o">=</code> <code class="n">val_error</code>
        <code class="n">error_going_up</code> <code class="o">=</code> <code class="mi">0</code>
    <code class="k">else</code><code class="p">:</code>
        <code class="n">error_going_up</code> <code class="o">+=</code> <code class="mi">1</code>
        <code class="k">if</code> <code class="n">error_going_up</code> <code class="o">==</code> <code class="mi">5</code><code class="p">:</code>
            <code class="k">break</code>  <code class="c1"># early stopping</code></pre>

<p>The <code>GradientBoostingRegressor</code> class also supports a <code>subsample</code> hyperparameter, which specifies the fraction of training instances to be used for training each tree. For example, if <code>subsample=0.25</code>, then each tree is trained on 25% of the training instances, selected randomly. As you can probably guess by now, this technique trades a higher bias for a lower variance. It also speeds up training considerably. This technique is called <em>Stochastic Gradient Boosting</em>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>It is possible to use Gradient Boosting with other cost functions. This is controlled by the <code>loss</code> hyperparameter (see Scikit-Learn’s documentation for more details).</p>
</div>

<p>It is worth noting that an optimized implementation of Gradient Boosting is available in the popular python library <a href="https://github.com/dmlc/xgboost"><em>XGBoost</em></a>, which stands for Extreme Gradient Boosting. This package was initially developed by Tianqi Chen as part of the Distributed (Deep) Machine Learning Community (DMLC), and it aims to be extremely fast, scalable and portable. In fact, XGBoost is often an important component of the winning entries in ML competitions. XGBoost’s API is quite similar to Scikit-Learn’s:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">xgboost</code>

<code class="n">xgb_reg</code> <code class="o">=</code> <code class="n">xgboost</code><code class="o">.</code><code class="n">XGBRegressor</code><code class="p">()</code>
<code class="n">xgb_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">xgb_reg</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_val</code><code class="p">)</code></pre>

<p>XGBoost also offers several nice features, such as automatically taking care of early stopping:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">xgb_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code>
            <code class="n">eval_set</code><code class="o">=</code><code class="p">[(</code><code class="n">X_val</code><code class="p">,</code> <code class="n">y_val</code><code class="p">)],</code> <code class="n">early_stopping_rounds</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">xgb_reg</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_val</code><code class="p">)</code></pre>

<p>You should definitely check it out!</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Stacking"><div class="sect1" id="idm46263522274984">
<h1>Stacking</h1>

<p>The last Ensemble method we will discuss in this chapter is called <em>stacking</em> (short for <a href="https://homl.info/29"><em>stacked generalization</em></a>).<sup><a data-type="noteref" id="idm46263521620376-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263521620376">18</a></sup> It is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don’t we train a model to perform this aggregation? <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#blending_prediction_diagram">Figure&nbsp;7-12</a> shows such an ensemble performing a regression task on a new instance. Each of the bottom three predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor (called a <em>blender</em>, or a <em>meta learner</em>) takes these predictions as inputs and makes the final prediction (3.0).</p>

<figure class="smallersixty"><div id="blending_prediction_diagram" class="figure">
<img src="./Chapter7_files/mls2_0712.png" alt="mls2 0712" width="1441" height="1269" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0712.png">
<h6><span class="label">Figure 7-12. </span>Aggregating predictions using a blending predictor</h6>
</div></figure>

<p>To train the blender, a common approach is to use a hold-out set.<sup><a data-type="noteref" id="idm46263521614616-marker" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263521614616">19</a></sup> Let’s see how it works. First, the training set is split in two subsets. The first subset is used to train the predictors in the first layer (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#blending_layer_1_training_diagram">Figure&nbsp;7-13</a>).</p>

<figure class="smallersixty"><div id="blending_layer_1_training_diagram" class="figure">
<img src="./Chapter7_files/mls2_0713.png" alt="mls2 0713" width="1440" height="926" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0713.png">
<h6><span class="label">Figure 7-13. </span>Training the first layer</h6>
</div></figure>

<p>Next, the first layer predictors are used to make predictions on the second (held-out) set (see <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#blending_layer_2_training_diagram">Figure&nbsp;7-14</a>). This ensures that the predictions are “clean,” since the predictors never saw these instances during training. For each instance in the hold-out set, there are three predicted values. We can create a new training set using these predicted values as input features (which makes this new training set 3D), and keeping the target values. The blender is trained on this new training set, so it learns to predict the target value, given the first layer’s predictions.</p>

<figure class="smallersixty"><div id="blending_layer_2_training_diagram" class="figure">
<img src="./Chapter7_files/mls2_0714.png" alt="mls2 0714" width="1439" height="1436" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0714.png">
<h6><span class="label">Figure 7-14. </span>Training the blender</h6>
</div></figure>

<p>It is actually possible to train several different blenders this way (e.g., one using Linear Regression, another using Random Forest Regression): we get a whole layer of blenders. The trick is to split the training set into three subsets: the first one is used to train the first layer, the second one is used to create the training set used to train the second layer (using predictions made by the predictors of the first layer), and the third one is used to create the training set to train the third layer (using predictions made by the predictors of the second layer). Once this is done, we can make a prediction for a new instance by going through each layer sequentially, as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#multi_layer_blending_diagram">Figure&nbsp;7-15</a>.</p>

<figure class="smallersixty"><div id="multi_layer_blending_diagram" class="figure">
<img src="./Chapter7_files/mls2_0715.png" alt="mls2 0715" width="1346" height="1391" data-mfp-src="/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0715.png">
<h6><span class="label">Figure 7-15. </span>Predictions in a multilayer stacking ensemble</h6>
</div></figure>

<p>Unfortunately, Scikit-Learn does not support stacking directly, but it is not too hard to roll out your own implementation (see the following exercises). Alternatively, you can use an open source implementation such as <code>brew</code> (available at <a href="https://github.com/viisar/brew"><em class="hyperlink">https://github.com/viisar/brew</em></a>).</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Exercises"><div class="sect1" id="idm46263521688776">
<h1>Exercises</h1>
<ol>
<li>
<p>If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?</p>
</li>
<li>
<p>What is the difference between hard and soft voting classifiers?</p>
</li>
<li>
<p>Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?</p>
</li>
<li>
<p>What is the benefit of out-of-bag evaluation?</p>
</li>
<li>
<p>What makes Extra-Trees more random than regular Random Forests? How can this extra randomness help? Are Extra-Trees slower or faster than regular Random Forests?</p>
</li>
<li>
<p>If your AdaBoost ensemble underfits the training data, which hyperparameters should you tweak and how?</p>
</li>
<li>
<p>If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?</p>
</li>
<li>
<p>Load the MNIST data (introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch03.html#classification_chapter">Chapter&nbsp;3</a>), and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing). Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM. Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?</p>
</li>
<li>
<p>Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image’s class. Train a classifier on this new training set. Congratulations, you have just trained a blender, and together with the classifiers they form a stacking ensemble! Now let’s evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble’s predictions. How does it compare to the voting classifier you trained earlier?</p>
</li>

</ol>

<p>Solutions to these exercises are available in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/app01.html#solutions_appendix">Appendix&nbsp;A</a>.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46263523035048"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263523035048-marker" class="totri-footnote">1</a></sup> Leo Breiman, “Bagging Predictors,” <em>Machine Learning</em>, 24, no. 2 (August 1996): 123–140.</p><p data-type="footnote" id="idm46263523033688"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263523033688-marker" class="totri-footnote">2</a></sup> In statistics, resampling with replacement is called <em>bootstrapping</em>.</p><p data-type="footnote" id="idm46263523031320"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263523031320-marker" class="totri-footnote">3</a></sup> “Pasting small votes for classification in large databases and on-line,” L. Breiman (1999).</p><p data-type="footnote" id="idm46263523025816"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263523025816-marker" class="totri-footnote">4</a></sup> Bias and variance were introduced in <a data-type="xref" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch04.html#linear_models_chapter">Chapter&nbsp;4</a>.</p><p data-type="footnote" id="idm46263523019736"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263523019736-marker" class="totri-footnote">5</a></sup> <code>max_samples</code> can alternatively be set to a float between 0.0 and 1.0, in which case the max number of instances to sample is equal to the size of the training set times <code>max_samples</code>.</p><p data-type="footnote" id="idm46263522913128"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522913128-marker" class="totri-footnote">6</a></sup> As <em>m</em> grows, this ratio approaches 1 – exp(–1) ≈ 63.212%.</p><p data-type="footnote" id="idm46263522820664"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522820664-marker" class="totri-footnote">7</a></sup> Gilles Louppe and Pierre Geurts, “Ensembles on Random Patches,” <em>Lecture Notes in Computer Science</em> 7523 (2012): 346–361, <a href="https://doi.org/10.1007/978-3-642-33460-3_28"><em class="hyperlink">https://doi.org/10.1007/978-3-642-33460-3_28</em></a>.</p><p data-type="footnote" id="idm46263522745080"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522745080-marker" class="totri-footnote">8</a></sup> Tin Kam Ho, “The Random Subspace Method for Constructing Decision Forests,” <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 20, no. 8 (August 1998): 832–844, <a href="https://doi.org/10.1109/34.709601"><em class="hyperlink">https://doi.org/10.1109/34.709601</em></a>.</p><p data-type="footnote" id="idm46263522739912"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522739912-marker" class="totri-footnote">9</a></sup> Tin Kam Ho, “Random decision forests,” in <em>Proceedings of 3rd International Conference on Document Analysis and Recognition</em> (Montreal, Aug. 14–16, 1995).</p><p data-type="footnote" id="idm46263522737000"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522737000-marker">10</a></sup> The <code>BaggingClassifier</code> class remains useful if you want a bag of something other than Decision Trees.</p><p data-type="footnote" id="idm46263522681544"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522681544-marker">11</a></sup> There are a few notable exceptions: <code>splitter</code> is absent (forced to <code>"random"</code>), <code>presort</code> is absent (forced to <code>False</code>), <code>max_samples</code> is absent (forced to <code>1.0</code>),  and <code>base_estimator</code> is absent (forced to <code>DecisionTreeClassifier</code> with the provided hyperparameters).</p><p data-type="footnote" id="idm46263522611672"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522611672-marker">12</a></sup> Pierre Geurts et al., “Extremely randomized trees,” <em>Machine Learning</em> 63, no. 1 (April 2006): 3–42.</p><p data-type="footnote" id="idm46263522497528"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522497528-marker">13</a></sup> Yoav Freund and Robert E. Schapire, “A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting,” <em>Journal of Computer and System Sciences</em> 55, no. 1 (August 1997): 119–139, <a href="https://doi.org/10.1006/jcss.1997.1504"><em class="hyperlink">https://doi.org/10.1006/jcss.1997.1504</em></a>.</p><p data-type="footnote" id="idm46263522487064"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522487064-marker">14</a></sup> This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost, because they are slow and tend to be unstable with AdaBoost.</p><p data-type="footnote" id="idm46263522436184"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522436184-marker">15</a></sup> The original AdaBoost algorithm does not use a learning rate hyperparameter.</p><p data-type="footnote" id="idm46263522315992"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522315992-marker">16</a></sup> For more details, see Ji Zhu et al., “Multi-class AdaBoost,” <em>Statistics and Its Interface</em> 2, no. 3 (2009): 349–360, <a href="http://dx.doi.org/10.4310/SII.2009.v2.n3.a8"><em class="hyperlink">http://dx.doi.org/10.4310/SII.2009.v2.n3.a8</em></a>.</p><p data-type="footnote" id="idm46263522273528"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263522273528-marker">17</a></sup> Gradient Boosting was first introduced in “Arcing the Edge,” L. Breiman (1997) and was further developed in the <a href="https://homl.info/gradboost">paper</a> “Greedy Function Approximation: A Gradient Boosting Machine,” Jerome H. Friedman (1999).</p><p data-type="footnote" id="idm46263521620376"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263521620376-marker">18</a></sup> David H. Wolpert, “Stacked Generalization,” <em>Neural Networks</em> 5, no. 2 (1992): 241–259.</p><p data-type="footnote" id="idm46263521614616"><sup><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#idm46263521614616-marker">19</a></sup> Alternatively, it is possible to use out-of-fold predictions. In some contexts this is called <em>stacking</em>, while using a hold-out set is called <em>blending</em>. For many people these terms are synonymous.</p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-7" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders">
		
		<li class="copy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#">Add Highlight</a></li>
		<li class="add-note"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#">
			Add Note
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch06.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">6. Decision Trees</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch08.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">8. Dimensionality Reduction</div>
        </a>
    
  
  </div>

</section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    



        
      </div>
      



  <footer class="pagefoot">
    <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#" class="icon-up" style="display: none;"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      <li class="full-support"><a href="https://www.oreilly.com/online-learning/support/">Support</a></li>
      <li><a href="https://www.oreilly.com/online-learning/apps.html">Get the App</a></li>
      
        <li><a href="https://learning.oreilly.com/accounts/logout/">Sign Out</a></li>
      
    </ul>
    <span class="copyright">© 2019 <a href="https://learning.oreilly.com/" target="_blank">Safari</a>.</span>
    <a href="https://learning.oreilly.com/terms/">Terms of Service</a> /
    <a href="https://learning.oreilly.com/membership-agreement/">Membership Agreement</a> /
    <a href="https://www.oreilly.com/privacy.html">Privacy Policy</a>
  </footer>

<script type="text/javascript">window.NREUM||(NREUM={});NREUM.info={"applicationID":"172641827,79672898,93931619","errorBeacon":"bam.nr-data.net","agent":"","applicationTime":451,"licenseKey":"510f1a6865","transactionName":"YgdaZ0NSW0cEB0RdWltNfkZfUEFdCgofXFBHDVYdR1pQQxZeRl1QQj1aWkU=","beacon":"bam.nr-data.net","queueTime":4}</script>


    
    <script src="./Chapter7_files/saved_resource" charset="utf-8"></script><script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","1732687426968531");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=1732687426968531&amp;ev=PageView&amp;noscript=1"></noscript><script type="text/javascript" id="">(function(){window.medalliaUserIdentifier=document.documentElement.dataset.userUuid;window.medalliaUserName=document.documentElement.dataset.username})();</script>
<script type="text/javascript" id="" src="./Chapter7_files/embed.js.download"></script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script><script type="text/javascript" id="">adroll_adv_id="BOHFZPCX5ZAM5LXWJURNUB";adroll_pix_id="3QFV44ZHVZG53BOB75QP3D";
(function(){var a=function(){if(document.readyState&&!/loaded|complete/.test(document.readyState))setTimeout(a,10);else if(window.__adroll_loaded){var b=document.createElement("script"),c="https:"==document.location.protocol?"https://s.adroll.com":"http://a.adroll.com";b.setAttribute("async","true");b.type="text/javascript";b.src=c+"/j/roundtrip.js";((document.getElementsByTagName("head")||[null])[0]||document.getElementsByTagName("script")[0].parentNode).appendChild(b)}else __adroll_loaded=!0,setTimeout(a,
50)};window.addEventListener?window.addEventListener("load",a,!1):window.attachEvent("onload",a)})();</script>
<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("set","agent","tmgoogletagmanager","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>

<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>
<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.09556792590964802"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.5229133033930915" width="0" height="0" alt="" src="./Chapter7_files/0"></div>
    <script src="./Chapter7_files/saved_resource(1)" charset="utf-8"></script>
  

<script src="./Chapter7_files/saved_resource(2)" type="text/javascript"></script><script type="text/javascript" id="">window._pp=window._pp||[];if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/register/")_pp.targetUrl="/confirm/trial";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/subscribe/")_pp.targetUrl="/confirm/paid";else if("\/library\/view\/hands-on-machine-learning\/9781492032632\/part01.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/"=="https://learning.oreilly.com/signup/")_pp.targetUrl="/confirm/paid";_pp.siteId="2508";
_pp.siteUId="d59baa21-c0cd-4fcf-9c68-a2b8d4f52a79";_pp.orderValue="undefined";_pp.orderId="undefined";(function(){var ppjs=document.createElement("script");ppjs.type="text/javascript";ppjs.async=true;ppjs.src=("https:"==document.location.protocol?"https:":"http:")+"//cdn.pbbl.co/r/"+_pp.siteId+".js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(ppjs,s)})();</script><div class="annotator-notice"></div><div class="font-flyout" style="top: 201px; left: 1194px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch07.html#">Reset</a>
</div>
</div><script type="text/javascript" async="" src="./Chapter7_files/generic1566415868241.js.download" charset="UTF-8"></script><div style="display: none; visibility: hidden;"><script>(function(){if(null!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')&&void 0!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')){var a=!1;window.addEventListener("blur",function(){a&&dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"facebook",eventVal:0,nonInteraction:0})});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseover",function(){window.focus();
a=!0});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseout",function(){a=!1})}try{window.twttr=function(b,a,d){var c,e=b.getElementsByTagName(a)[0];if(!b.getElementById(d))return b=b.createElement(a),b.id=d,b.src="//platform.twitter.com/widgets.js",e.parentNode.insertBefore(b,e),window.twttr||(c={_e:[],ready:function(a){c._e.push(a)}})}(document,"script","twitter-wjs"),twttr.ready(function(a){a.events.bind("tweet",trackTwitter)})}catch(b){}})();
null!==document.querySelector(".IN-widget")&&void 0!==document.querySelector(".IN-widget")&&document.querySelector(".IN-widget").addEventListener("click",function(){dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"linkedin",eventVal:0,nonInteraction:0})});
function trackTwitter(a){a&&(a.target&&"IFRAME"==a.target.nodeName&&(opt_target=extractParamFromUri(a.target.src,"url")),dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"twitter",eventVal:0,nonInteraction:0}))}function extractParamFromUri(a,b){if(a){var c=new RegExp("[\\?\x26#]"+b+"\x3d([^\x26#]*)");c=c.exec(a);if(null!=c)return unescape(c[1])}};</script></div><span><div id="KampyleAnimationContainer" style="z-index: 2147483000; border: 0px; position: fixed; display: block; width: 0px; height: 0px;"></div></span><iframe scrolling="no" frameborder="0" allowtransparency="true" src="./Chapter7_files/widget_iframe.097c1f5038f9e8a0d62a39a892838d66.html" title="Twitter settings iframe" style="display: none;"></iframe><div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: MathJax_Size3, sans-serif;"></div></div></body></html>